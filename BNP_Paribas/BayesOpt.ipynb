{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Clearing...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "[joblib] Attempting to do parallel computing without protecting your import on a system that does not support forking. To use parallel-computing in a script, you must protect your main loop using \"if __name__ == '__main__'\". Please see the joblib documentation on Parallel for more information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-8ae7638d22aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mrsnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_search_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mrsnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    994\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m                                           random_state=self.random_state)\n\u001b[0;32m--> 996\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    551\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 for train, test in cv)\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_pool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_effective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_initialize_pool\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0malready_forked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJOBLIB_SPAWNED_PROCESS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0malready_forked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m                     raise ImportError('[joblib] Attempting to do parallel computing '\n\u001b[0m\u001b[1;32m    516\u001b[0m                             \u001b[0;34m'without protecting your import on a system that does '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                             \u001b[0;34m'not support forking. To use parallel-computing in a '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: [joblib] Attempting to do parallel computing without protecting your import on a system that does not support forking. To use parallel-computing in a script, you must protect your main loop using \"if __name__ == '__main__'\". Please see the joblib documentation on Parallel for more information"
     ]
    }
   ],
   "source": [
    "\n",
    "            \n",
    "#def xgbclassifiercv(max_depth, learning_rate, n_estimators, gamma, min_child_weight, max_delta_step, subsample,\n",
    "#              colsample_bytree, silent = True, nthread = -1, seed = 1234):\n",
    "    \n",
    "#    return cross_val_score(XGBClassifier(max_depth = int(max_depth), learning_rate = learning_rate,\n",
    "#                                         n_estimators = int(n_estimators), silent = silent,\n",
    "#                                         nthread = nthread, gamma = gamma, min_child_weight = min_child_weight,\n",
    "#                                         max_delta_step = max_delta_step, subsample = subsample,\n",
    "#                                         colsample_bytree = colsample_bytree, seed = seed,\n",
    "#                                         objective = 'binary:logistic'),\n",
    "#                           X_train,\n",
    "#                           y_train,\n",
    "#                           'log_loss',\n",
    "#                           n_jobs=-1,\n",
    "#                           cv=10).mean()\n",
    "\n",
    "\n",
    "#xgbclassifierBO = BayesianOptimization(xgbclassifiercv,\n",
    "#                                 {'max_depth': (5, 12),\n",
    "#                                  'learning_rate': (0.001, 0.5),\n",
    "#                                  'n_estimators': (50, 1000),\n",
    "#                                  'gamma': (1., 0.01),\n",
    "#                                  'min_child_weight': (1, 100),\n",
    "#                                  'max_delta_step': (0, 10),\n",
    "#                                  'subsample': (0.01, 0.9),\n",
    "#                                  'colsample_bytree' :(0.5, 0.99)\n",
    "#                                 })\n",
    "\n",
    "\n",
    "#print('Maximizing...')\n",
    "\n",
    "#xgbclassifierBO.maximize()\n",
    "#print('-'*53)\n",
    "#print('Final Results')\n",
    "#print('XGBClassifier: %f' % xgbclassifierBO.res['max']['max_val'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#def extratreesclassifiercv(n_estimators, max_depth, max_features):\n",
    "    \n",
    "#    return cross_val_score(ExtraTreesClassifier(max_depth = int(max_depth), \n",
    "#                                                n_estimators = int(n_estimators), \n",
    "#                                                max_features = int(max_features)),\n",
    "#                           X_train,\n",
    "#                           y_train,\n",
    "#                           'log_loss',\n",
    "#                           n_jobs=-1,\n",
    "#                           cv=10).mean()\n",
    "\n",
    "\n",
    "#extratreesclassifierBO = BayesianOptimization(extratreesclassifiercv,\n",
    "#                                 {'max_depth': (5, 12),\n",
    "#                                  'n_estimators': (50, 1000),\n",
    "#                                  'max_features' :(5, 130)\n",
    "#                                 })\n",
    "\n",
    "\n",
    "#print('Maximizing...')\n",
    "\n",
    "#extratreesclassifierBO.maximize()\n",
    "#print('-'*53)\n",
    "#print('Final Results')\n",
    "#print('ExtraTrees: %f' % extratreesclassifierBO.res['max']['max_val'])\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import scipy as sp\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "def get_data():\n",
    "    print('Load data...')\n",
    "\n",
    "    DATA_DIR = \"/Users/patrickkennedy/Desktop/Data_Science_MISC/Kaggle\"\n",
    "    train = pd.read_csv(DATA_DIR + \"/BNP_Paribas/train.csv\")\n",
    "    test = pd.read_csv(DATA_DIR + \"/BNP_Paribas/test.csv\")\n",
    "\n",
    "    target = train['target'].values\n",
    "\n",
    "    train = train.drop(['ID','target'],axis=1)\n",
    "    id_test = test['ID'].values\n",
    "    test = test.drop(['ID'],axis=1)\n",
    "\n",
    "    print('Clearing...')\n",
    "    for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(),test.iteritems()):\n",
    "        if train_series.dtype == 'O':\n",
    "            #for objects: factorize\n",
    "            train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "            test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "            #but now we have -1 values (NaN)\n",
    "        else:\n",
    "            #for int or float: fill NaN\n",
    "            tmp_len = len(train[train_series.isnull()])\n",
    "            if tmp_len>0:\n",
    "                #print \"mean\", train_series.mean()\n",
    "                train.loc[train_series.isnull(), train_name] = -9999 #train_series.mean()\n",
    "            #and Test\n",
    "            tmp_len = len(test[test_series.isnull()])\n",
    "            if tmp_len>0:\n",
    "                test.loc[test_series.isnull(), test_name] = -9999 #train_series.mean()  #TODO\n",
    "\n",
    "            \n",
    "    #Spliting data into train and test sets.\n",
    "    X, X_test, y, y_test = train_test_split(train, target, test_size=0.2)\n",
    "    \n",
    "    #Spliting train data into training and validation sets.\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25)\n",
    "    \n",
    "    return X, X_test, y, y_test, X_train, X_valid, y_train, y_valid\n",
    "    \n",
    "    \n",
    "def random_search_nn():\n",
    "    nn = Classifier(layers = [Layer(\"Rectifier\",units=10),\n",
    "                              Layer(\"Softmax\")])\n",
    "\n",
    "    params={'learning_rate': sp.stats.uniform(0.001, 0.05),\n",
    "            'hidden0__units': sp.stats.randint(4, 20),\n",
    "            'hidden0__type': [\"Rectifier\", \"Sigmoid\", \"Tanh\"]}\n",
    "\n",
    "    return RandomizedSearchCV(nn,param_distributions=params,n_iter=5,cv=10,scoring='log_loss',n_jobs=-1)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    X, X_test, y, y_test, X_train, X_valid, y_train, y_valid = get_data()\n",
    "    rsnn = random_search_nn()\n",
    "    rsnn.fit(X_train, y_train)\n",
    "    report(random_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CV=10\n",
    "\n",
    "#####################################\n",
    "#XGBClassifier.....\n",
    "#Optimization finished with maximum: -0.467777, at position: {'colsample_bytree': 0.77638333498678636, 'learning_rate': 0.030567867858705199, 'max_delta_step': 4.6626180513766657, 'min_child_weight': 57.354121041109124, 'n_estimators': 477.72777553323016, 'subsample': 0.8069399976204783, 'max_depth': 6.6059347713535548, 'gamma': 0.2966938071810209}.\n",
    "#Time taken: 499 minutes and 40.785203 seconds.\n",
    "#-----------------------------------------------------\n",
    "#Final Results\n",
    "#XGBClassifier: -0.467777\n",
    "#####################################\n",
    "#ExtraTreesClassifier....\n",
    "#Optimization finished with maximum: -0.472478, at position: {'n_estimators': 108.03076614800023, 'max_features': 130.0, 'max_depth': 12.0}.\n",
    "#Time taken: 350 minutes and 16.242177 seconds.\n",
    "#-----------------------------------------------------\n",
    "#Final Results\n",
    "#ExtraTrees: -0.472478\n",
    "#####################################\n",
    "#NN....\n",
    "#####################################\n",
    "#RandomForest...\n",
    "#####################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add NN, RF to the bayes_opt to get at their params\n",
    "#run the script with calibrations\n",
    "#when i am ready add the whole model (using the same weighting as with the training set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
