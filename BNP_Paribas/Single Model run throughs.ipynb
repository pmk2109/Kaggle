{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "#METHODS:\n",
    "#['SLSQP', 'L-BFGS-B', 'CG', 'Newton-CG', 'TNC', 'BFGS', 'dogleg', 'trust-ncg']\n",
    "\n",
    "#FUNCTIONS:\n",
    "#objf_ens_optA_SLSQP\n",
    "#objf_ens_optB_L-BFGS-B\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def objf_ens_optA(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (12 in Airbnb competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optA, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA, x0, args=(Xs, y, self.n_class), \n",
    "                       method='SLSQP', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objf_ens_optB(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 12\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optB, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB, x0, args=(Xs, y, self.n_class), \n",
    "                       method='L-BFGS-B', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Clearing...\n"
     ]
    }
   ],
   "source": [
    "print('Load data...')\n",
    "DATA_DIR = \"/Users/patrickkennedy/Desktop/Data_Science_MISC/Kaggle\"\n",
    "train = pd.read_csv(DATA_DIR + \"/BNP_Paribas/train.csv\")\n",
    "test = pd.read_csv(DATA_DIR + \"/BNP_Paribas/test.csv\")\n",
    "\n",
    "target = train['target'].values\n",
    "\n",
    "train = train.drop(['ID','target'],axis=1)\n",
    "id_test = test['ID'].values\n",
    "test = test.drop(['ID'],axis=1)\n",
    "\n",
    "print('Clearing...')\n",
    "for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(),test.iteritems()):\n",
    "    if train_series.dtype == 'O':\n",
    "        #for objects: factorize\n",
    "        train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "        test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "        #but now we have -1 values (NaN)\n",
    "    else:\n",
    "        #for int or float: fill NaN\n",
    "        tmp_len = len(train[train_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            #print \"mean\", train_series.mean()\n",
    "            train.loc[train_series.isnull(), train_name] = -9999 #train_series.mean()\n",
    "        #and Test\n",
    "        tmp_len = len(test[test_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            test.loc[test_series.isnull(), test_name] = -9999 #train_series.mean()  #TODO\n",
    "            \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#i can also think about initializing the split with a different random state too\n",
    "\n",
    "n_classes = 2 \n",
    "for_real = True\n",
    "\n",
    "#this is what i'll change when i run the whole data set...\n",
    "#essentially my train and test sets are already split\n",
    "\n",
    "#Spliting data into train and test sets.\n",
    "#X, X_test, y, y_test = train_test_split(train, target, test_size=0.2)\n",
    "    \n",
    "#Spliting train data into training and validation sets.\n",
    "#X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "#print('Data shape:')\n",
    "#print('X_train: %s, X_valid: %s, X_test: %s \\n' %(X_train.shape, X_valid.shape, \n",
    "#                                                  X_test.shape))\n",
    "\n",
    "#if for_real:\n",
    "    #take the train, target and test data, and come up with a validation set from train\n",
    "#    X_real = train\n",
    "#    X_test_real = test\n",
    "#    y_real = target\n",
    "    \n",
    "#    X_train_real, X_valid_real, y_train_real, y_valid_real = train_test_split(X_real, y_real, test_size=0.25)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the classifiers\n",
    "#think about jittering the random state and then averaging the predictions together ...\n",
    "#only good for extra trees, RF?, native XGB, NN\n",
    "clfs = {#'LR'  : LogisticRegression(), \n",
    "        #'SVM' : SVC(probability=True, random_state=random_state), \n",
    "        #'RF'  : RandomForestClassifier(n_estimators=100, n_jobs=-1), \n",
    "        #'GBM' : GradientBoostingClassifier(n_estimators=50), \n",
    "        'ETC' : ExtraTreesClassifier(n_estimators=108, max_features=130, max_depth=12, n_jobs=-1),\n",
    "        #'KNN' : KNeighborsClassifier(n_neighbors=5),\n",
    "        'XGBc': XGBClassifier(objective='binary:logistic',\n",
    "                              colsample_bytree=0.77638333498678636,\n",
    "                              learning_rate=0.030567867858705199,\n",
    "                              max_delta_step=4.6626180513766657,\n",
    "                              min_child_weight=57.354121041109124,\n",
    "                              n_estimators=478,\n",
    "                              subsample=0.8069399976204783,\n",
    "                              max_depth=6,\n",
    "                              gamma=0.2966938071810209) #,\n",
    "        #'NN'  : Pipeline([('min/max scaler', MinMaxScaler(feature_range=(-1.0, 1.0))),\n",
    "        #                  ('neural network', Classifier(layers=[Layer(\"Rectifier\", units=10),\n",
    "        #                                                        Layer(\"Tanh\", units=10),\n",
    "        #                                                        Layer(\"Softmax\")], \n",
    "        #                                                n_iter=5))])       \n",
    "       }\n",
    "\n",
    "\n",
    "\n",
    "#let's try best random without tuning and best random with tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of individual classifiers (1st layer) on X_test\n",
      "------------------------------------------------------------\n",
      "ETC:       logloss  => 0.4603708\n",
      "XGBc:      logloss  => 0.4586818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#predictions on the validation and test sets\n",
    "p_valid = []\n",
    "p_test = []\n",
    "\n",
    "p_valid_real = []\n",
    "p_test_real = []\n",
    "   \n",
    "print('Performance of individual classifiers (1st layer) on X_test')   \n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "#Spliting data into train and test sets.\n",
    "X, X_test, y, y_test = train_test_split(train, target, test_size=0.2, random_state=117)\n",
    "    \n",
    "#Spliting train data into training and validation sets.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=117)\n",
    "\n",
    "X_real = train\n",
    "X_test_real = test\n",
    "y_real = target\n",
    "    \n",
    "X_train_real, X_valid_real, y_train_real, y_valid_real = train_test_split(X_real, y_real, test_size=0.25, \n",
    "                                                                          random_state=117)\n",
    "\n",
    "random.seed(117)\n",
    "\n",
    "for nm, clf in clfs.items():\n",
    "    if nm == 'NN':\n",
    "        #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "        clf.fit(X_train.as_matrix(), y_train)\n",
    "        yv = clf.predict_proba(X_valid.as_matrix())\n",
    "        p_valid.append(yv)\n",
    "        \n",
    "        #Second run. Training on (X, y) and predicting on X_test.\n",
    "        clf.fit(X.as_matrix(), y)\n",
    "        yt = clf.predict_proba(X_test.as_matrix())\n",
    "        p_test.append(yt)\n",
    "        \n",
    "        if for_real:\n",
    "            #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "            clf.fit(X_train_real.as_matrix(), y_train_real)\n",
    "            yv_real = clf.predict_proba(X_valid_real.as_matrix())\n",
    "            p_valid_real.append(yv_real)\n",
    "        \n",
    "            #Second run. Training on (X, y) and predicting on X_test.\n",
    "            clf.fit(X_real.as_matrix(), y_real)\n",
    "            yt_real = clf.predict_proba(X_test_real.as_matrix())\n",
    "            p_test_real.append(yt_real)\n",
    "    \n",
    "    else:\n",
    "        #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "        clf.fit(X_train, y_train)\n",
    "        yv = clf.predict_proba(X_valid)\n",
    "        p_valid.append(yv)\n",
    "        \n",
    "        #Second run. Training on (X, y) and predicting on X_test.\n",
    "        clf.fit(X, y)\n",
    "        yt = clf.predict_proba(X_test)\n",
    "        p_test.append(yt)\n",
    "        \n",
    "        if for_real:\n",
    "            #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "            clf.fit(X_train_real, y_train_real)\n",
    "            yv_real = clf.predict_proba(X_valid_real)\n",
    "            p_valid_real.append(yv_real)\n",
    "        \n",
    "            #Second run. Training on (X, y) and predicting on X_test.\n",
    "            clf.fit(X_real, y_real)\n",
    "            yt_real = clf.predict_proba(X_test_real)\n",
    "            p_test_real.append(yt_real)\n",
    "       \n",
    "    #Printing out the performance of the classifier\n",
    "    print('{:10s} {:2s} {:1.7f}'.format('%s: ' %(nm), 'logloss  =>', log_loss(y_test, yt)))\n",
    "print('')\n",
    "\n",
    "\n",
    "#when running the full data\n",
    "#take out the logloss function... or alternatively, run both the split data and full data model so that\n",
    "#i can compare my training logloss vs kaggle logloss\n",
    "\n",
    "\n",
    "#maybe build a system that tries base models, shakes random states in order to find uncorrelated models?\n",
    "#don't know why but my bayes_opt script is giving a worse score than untuned model (default params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN MODELS: Performance of individual classifiers (1st layer) on X_test\n",
      "------------------------------------------------------------------------\n",
      "ETC - mean:  logloss  => 0.5442996\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-0f5d139b85bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"print('MEAN MODELS: Performance of individual classifiers (1st layer) on X_test')   \\nprint('------------------------------------------------------------------------')\\np_valid_mean = []\\np_test_mean = []\\n\\np_valid_mean_real = []\\np_test_mean_real = []\\n\\nbest_model_score = 10.0\\n\\nfor nm, clf in clfs.items():\\n    p_valid_clf = []\\n    p_test_clf = []\\n    p_valid_clf_real = []\\n    p_test_clf_real = []\\n    \\n    holder = []\\n    \\n    for i in xrange(25):\\n    \\n        dummy = random.randint(1,10000)\\n        x = True\\n        while x == True:\\n            if dummy in holder:\\n                dummy = random.randint(1,10000)\\n            else:\\n                x = False\\n        holder.append(dummy)\\n    \\n        random.seed(dummy)\\n        \\n        #Spliting data into train and test sets.\\n        X, X_test, y, y_test = train_test_split(train, target, test_size=0.2, random_state=dummy)\\n    \\n        #Spliting train data into training and validation sets.\\n        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=dummy)\\n\\n        X_real = train\\n        X_test_real = test\\n        y_real = target\\n    \\n        X_train_real, X_valid_real, y_train_real, y_valid_real = train_test_split(X_real, y_real, test_size=0.25, \\n                                                                                  random_state=dummy)\\n        \\n        \\n        if nm == 'NN':\\n            #First run. Training on (X_train, y_train) and predicting on X_valid.\\n            clf.fit(X_train.as_matrix(), y_train)\\n            yv = clf.predict_proba(X_valid.as_matrix())\\n            p_valid_clf.append(yv)\\n        \\n            #Second run. Training on (X, y) and predicting on X_test.\\n            clf.fit(X.as_matrix(), y)\\n            yt = clf.predict_proba(X_test.as_matrix())\\n            p_test_clf.append(yt)\\n        \\n            if for_real:\\n                #First run. Training on (X_train, y_train) and predicting on X_valid.\\n                clf.fit(X_train_real.as_matrix(), y_train_real)\\n                yv_real = clf.predict_proba(X_valid_real.as_matrix())\\n                p_valid_clf_real.append(yv_real)\\n        \\n                #Second run. Training on (X, y) and predicting on X_test.\\n                clf.fit(X_real.as_matrix(), y_real)\\n                yt_real = clf.predict_proba(X_test_real.as_matrix())\\n                p_test_clf_real.append(yt_real)\\n    \\n        else:\\n            #First run. Training on (X_train, y_train) and predicting on X_valid.\\n            clf.fit(X_train, y_train)\\n            yv = clf.predict_proba(X_valid)\\n            p_valid_clf.append(yv)\\n        \\n            #Second run. Training on (X, y) and predicting on X_test.\\n            clf.fit(X, y)\\n            yt = clf.predict_proba(X_test)\\n            p_test_clf.append(yt)\\n        \\n            #if log_loss(y_test, yt) < best_model_score:\\n            #    best_model_score = log_loss(y_test, yt)\\n            #    best_dummy = dummy\\n            #    print nm, i, best_model_score, best_dummy\\n            #else:\\n            #    print nm, i\\n        \\n            if for_real:\\n                #First run. Training on (X_train, y_train) and predicting on X_valid.\\n                clf.fit(X_train_real, y_train_real)\\n                yv_real = clf.predict_proba(X_valid_real)\\n                p_valid_clf_real.append(yv_real)\\n        \\n                #Second run. Training on (X, y) and predicting on X_test.\\n                clf.fit(X_real, y_real)\\n                yt_real = clf.predict_proba(X_test_real)\\n                p_test_clf_real.append(yt_real)\\n            \\n       \\n    #Printing out the performance of the classifier\\n    mean_pred_cv = np.mean(p_valid_clf, axis=0)\\n    mean_pred_test = np.mean(p_test_clf, axis=0)\\n    p_valid_mean.append(mean_pred_cv)\\n    p_test_mean.append(mean_pred_test)\\n    \\n    mean_real_pred_cv = np.mean(p_valid_clf_real, axis=0)\\n    mean_real_pred_test = np.mean(p_test_clf_real, axis=0)\\n    p_valid_mean_real.append(mean_real_pred_cv)\\n    p_test_mean_real.append(mean_real_pred_test)\\n    \\n    \\n    #i take the predictions from each of the models of each iteration through the random states\\n    #and then i will run logloss on each set of preds and take the best models? best uncorrelated models?\\n    #if nm == 'ETC':\\n    #    predsETC_valid = p_valid_clf\\n    #    predsETC_test = p_test_clf\\n    #    predsETC_valid_real = p_valid_clf_real\\n    #    predsETC_test_real = p_test_clf_real\\n    if nm == 'XGBc':\\n        predsXGBc_valid = p_valid_clf\\n        predsXGBc_test = p_test_clf\\n        predsXGBc_valid_real = p_valid_clf_real\\n        predsXGBc_test_real = p_test_clf_real\\n    elif nm == 'NN':\\n        predsNN_valid = p_valid_clf\\n        predsNN_test = p_test_clf\\n        predsNN_valid_real = p_valid_clf_real\\n        predsNN_test_real = p_test_clf_real\\n        \\n        \\n    print('{:10s} {:2s} {:1.7f}'.format('%s - mean: ' %(nm), 'logloss  =>', log_loss(y_test, mean_pred_test)))\\n    \\n\\n        \\n        \\n    \\nprint('')\\n\\n#also try setting different parameters for the XGB and add a NN to the mix\\n#either use bayesopt for each classifier and putting those into this model, -or- \\n#randomize both parameters and by random_state... i could do several loops here\\n#lots of comp time but each random_state run a series of different parameters and\\n#take the average result for that particular random_state, then run the same\\n#parameters on the next random_state (don't want random X random as that is hard to replicate)...\\n#too many combos... let's do bayes_opt\\n\\n#Without Bayes_Opt\\n#ETC 9 0.466434680757 9958\\n#ETC 63 0.464552567051 117\\n#ETC 242 0.464125730276 5299\\n\\n#XGBc 17 0.465919276422 6384\\n#XGBc 51 0.46307720874 9958\\n\\n\\n#With Bayes_Opt\\n#ETC 3 0.461308763189 8830\\n#ETC 14 0.460268248358 117\\n#cancelled early\\n\\n\\n\\n#think about testing particular random states across models... like 117 but for xgbclassifier\\n#just using 117 overfits bad\\n\\n#how about 117, 9958, 8830\\n\\n\\n#next look at the results and kludge together models with relevant random states / params (if needed)\\n#filter through real data so i can finish the algo\\n\\n\\n\\n#also think about running a long stretch of code where correlations are made between models and the two models\\n#with the lowest correlations are kept... but what about multiple that have low correlations with one another?\\n#maybe this is a future project...\\n\\n#at the least i can correlate the two models? why would this matter? how do those optimizers work?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2120\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/xgboost/sklearn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose)\u001b[0m\n\u001b[1;32m    341\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m                               verbose_eval=verbose)\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mnboost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('MEAN MODELS: Performance of individual classifiers (1st layer) on X_test')   \n",
    "print('------------------------------------------------------------------------')\n",
    "p_valid_mean = []\n",
    "p_test_mean = []\n",
    "\n",
    "p_valid_mean_real = []\n",
    "p_test_mean_real = []\n",
    "\n",
    "best_model_score = 10.0\n",
    "\n",
    "for nm, clf in clfs.items():\n",
    "    p_valid_clf = []\n",
    "    p_test_clf = []\n",
    "    p_valid_clf_real = []\n",
    "    p_test_clf_real = []\n",
    "    \n",
    "    holder = []\n",
    "    \n",
    "    for i in xrange(25):\n",
    "    \n",
    "        dummy = random.randint(1,10000)\n",
    "        x = True\n",
    "        while x == True:\n",
    "            if dummy in holder:\n",
    "                dummy = random.randint(1,10000)\n",
    "            else:\n",
    "                x = False\n",
    "        holder.append(dummy)\n",
    "    \n",
    "        random.seed(dummy)\n",
    "        \n",
    "        #Spliting data into train and test sets.\n",
    "        X, X_test, y, y_test = train_test_split(train, target, test_size=0.2, random_state=dummy)\n",
    "    \n",
    "        #Spliting train data into training and validation sets.\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=dummy)\n",
    "\n",
    "        X_real = train\n",
    "        X_test_real = test\n",
    "        y_real = target\n",
    "    \n",
    "        X_train_real, X_valid_real, y_train_real, y_valid_real = train_test_split(X_real, y_real, test_size=0.25, \n",
    "                                                                                  random_state=dummy)\n",
    "        \n",
    "        \n",
    "        if nm == 'NN':\n",
    "            #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "            clf.fit(X_train.as_matrix(), y_train)\n",
    "            yv = clf.predict_proba(X_valid.as_matrix())\n",
    "            p_valid_clf.append(yv)\n",
    "        \n",
    "            #Second run. Training on (X, y) and predicting on X_test.\n",
    "            clf.fit(X.as_matrix(), y)\n",
    "            yt = clf.predict_proba(X_test.as_matrix())\n",
    "            p_test_clf.append(yt)\n",
    "        \n",
    "            if for_real:\n",
    "                #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "                clf.fit(X_train_real.as_matrix(), y_train_real)\n",
    "                yv_real = clf.predict_proba(X_valid_real.as_matrix())\n",
    "                p_valid_clf_real.append(yv_real)\n",
    "        \n",
    "                #Second run. Training on (X, y) and predicting on X_test.\n",
    "                clf.fit(X_real.as_matrix(), y_real)\n",
    "                yt_real = clf.predict_proba(X_test_real.as_matrix())\n",
    "                p_test_clf_real.append(yt_real)\n",
    "    \n",
    "        else:\n",
    "            #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "            clf.fit(X_train, y_train)\n",
    "            yv = clf.predict_proba(X_valid)\n",
    "            p_valid_clf.append(yv)\n",
    "        \n",
    "            #Second run. Training on (X, y) and predicting on X_test.\n",
    "            clf.fit(X, y)\n",
    "            yt = clf.predict_proba(X_test)\n",
    "            p_test_clf.append(yt)\n",
    "        \n",
    "            #if log_loss(y_test, yt) < best_model_score:\n",
    "            #    best_model_score = log_loss(y_test, yt)\n",
    "            #    best_dummy = dummy\n",
    "            #    print nm, i, best_model_score, best_dummy\n",
    "            #else:\n",
    "            #    print nm, i\n",
    "        \n",
    "            if for_real:\n",
    "                #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "                clf.fit(X_train_real, y_train_real)\n",
    "                yv_real = clf.predict_proba(X_valid_real)\n",
    "                p_valid_clf_real.append(yv_real)\n",
    "        \n",
    "                #Second run. Training on (X, y) and predicting on X_test.\n",
    "                clf.fit(X_real, y_real)\n",
    "                yt_real = clf.predict_proba(X_test_real)\n",
    "                p_test_clf_real.append(yt_real)\n",
    "            \n",
    "       \n",
    "    #Printing out the performance of the classifier\n",
    "    mean_pred_cv = np.mean(p_valid_clf, axis=0)\n",
    "    mean_pred_test = np.mean(p_test_clf, axis=0)\n",
    "    p_valid_mean.append(mean_pred_cv)\n",
    "    p_test_mean.append(mean_pred_test)\n",
    "    \n",
    "    mean_real_pred_cv = np.mean(p_valid_clf_real, axis=0)\n",
    "    mean_real_pred_test = np.mean(p_test_clf_real, axis=0)\n",
    "    p_valid_mean_real.append(mean_real_pred_cv)\n",
    "    p_test_mean_real.append(mean_real_pred_test)\n",
    "    \n",
    "    \n",
    "    #i take the predictions from each of the models of each iteration through the random states\n",
    "    #and then i will run logloss on each set of preds and take the best models? best uncorrelated models?\n",
    "    #if nm == 'ETC':\n",
    "    #    predsETC_valid = p_valid_clf\n",
    "    #    predsETC_test = p_test_clf\n",
    "    #    predsETC_valid_real = p_valid_clf_real\n",
    "    #    predsETC_test_real = p_test_clf_real\n",
    "    if nm == 'XGBc':\n",
    "        predsXGBc_valid = p_valid_clf\n",
    "        predsXGBc_test = p_test_clf\n",
    "        predsXGBc_valid_real = p_valid_clf_real\n",
    "        predsXGBc_test_real = p_test_clf_real\n",
    "    elif nm == 'NN':\n",
    "        predsNN_valid = p_valid_clf\n",
    "        predsNN_test = p_test_clf\n",
    "        predsNN_valid_real = p_valid_clf_real\n",
    "        predsNN_test_real = p_test_clf_real\n",
    "        \n",
    "        \n",
    "    print('{:10s} {:2s} {:1.7f}'.format('%s - mean: ' %(nm), 'logloss  =>', log_loss(y_test, mean_pred_test)))\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "print('')\n",
    "\n",
    "#also try setting different parameters for the XGB and add a NN to the mix\n",
    "#either use bayesopt for each classifier and putting those into this model, -or- \n",
    "#randomize both parameters and by random_state... i could do several loops here\n",
    "#lots of comp time but each random_state run a series of different parameters and\n",
    "#take the average result for that particular random_state, then run the same\n",
    "#parameters on the next random_state (don't want random X random as that is hard to replicate)...\n",
    "#too many combos... let's do bayes_opt\n",
    "\n",
    "#Without Bayes_Opt\n",
    "#ETC 9 0.466434680757 9958\n",
    "#ETC 63 0.464552567051 117\n",
    "#ETC 242 0.464125730276 5299\n",
    "\n",
    "#XGBc 17 0.465919276422 6384\n",
    "#XGBc 51 0.46307720874 9958\n",
    "\n",
    "\n",
    "#With Bayes_Opt\n",
    "#ETC 3 0.461308763189 8830\n",
    "#ETC 14 0.460268248358 117\n",
    "#cancelled early\n",
    "\n",
    "\n",
    "\n",
    "#think about testing particular random states across models... like 117 but for xgbclassifier\n",
    "#just using 117 overfits bad\n",
    "\n",
    "#how about 117, 9958, 8830\n",
    "\n",
    "\n",
    "#next look at the results and kludge together models with relevant random states / params (if needed)\n",
    "#filter through real data so i can finish the algo\n",
    "\n",
    "\n",
    "\n",
    "#also think about running a long stretch of code where correlations are made between models and the two models\n",
    "#with the lowest correlations are kept... but what about multiple that have low correlations with one another?\n",
    "#maybe this is a future project...\n",
    "\n",
    "#at the least i can correlate the two models? why would this matter? how do those optimizers work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predsETC_valid -- run correlations on each and see which ones are least correlated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame({\"p_valid_0_0\": p_valid[0][:,0],\n",
    "#              \"p_valid_0_1\": p_valid[0][:,1],\n",
    "#              \"p_valid_1_0\": p_valid[1][:,0],\n",
    "#              \"p_valid_1_1\": p_valid[1][:,1],\n",
    "#             }).to_csv('p_valid_preds_for_EXT_and_XGBc_single_models.csv',index=False)\n",
    "\n",
    "#pd.DataFrame({\"p_test_0_0\" : p_test[0][:,0],\n",
    "#              \"p_test_0_1\" : p_test[0][:,1],\n",
    "#              \"p_test_1_0\" : p_test[1][:,0],\n",
    "#              \"p_test_1_1\" : p_test[1][:,1]\n",
    "#             }).to_csv('p_test_preds_for_EXT_and_XGBc_single_models.csv',index=False)\n",
    "\n",
    "#pd.DataFrame({\"p_valid_real_0_0\": p_valid_real[0][:,0],\n",
    "#              \"p_valid_real_0_1\": p_valid_real[0][:,1],\n",
    "#              \"p_valid_real_1_0\": p_valid_real[1][:,0],\n",
    "#              \"p_valid_real_1_1\": p_valid_real[1][:,1]\n",
    "#             }).to_csv('p_valid_real_preds_for_EXT_and_XGBc_single_models.csv',index=False)\n",
    "\n",
    "#pd.DataFrame({\"p_test_real_0_0\" : p_test_real[0][:,0],\n",
    "#              \"p_test_real_0_1\" : p_test_real[0][:,1],\n",
    "#              \"p_test_real_1_0\" : p_test_real[1][:,0],\n",
    "#              \"p_test_real_1_1\" : p_test_real[1][:,1]\n",
    "#             }).to_csv('p_test_real_preds_for_EXT_and_XGBc_single_models.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#p_valid = pd.read_csv(\"p_valid_preds_for_EXT_and_XGBc_single_models.csv\")\n",
    "#p_test = pd.read_csv(\"p_test_preds_for_EXT_and_XGBc_single_models.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for_real = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of optimization based ensemblers (2nd layer) on X_test\n",
      "------------------------------------------------------------\n",
      "EN_optA_SLSQP        logloss  => 0.4575958\n",
      "Calibrated_EN_optA   logloss  => 0.4566262\n",
      "EN_optA_L-BFGS-B     logloss  => 0.4575340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/scipy/optimize/_minimize.py:397: RuntimeWarning: Method L-BFGS-B cannot handle constraints.\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated_EN_optA   logloss  => 0.4566628\n",
      "EN_optB_SLSQP        logloss  => 0.4575958\n",
      "Calibrated_EN_optB   logloss  => 0.4566262\n",
      "EN_optB_L-BFGS-B     logloss  => 0.4576971\n",
      "Calibrated_EN_optB   logloss  => 0.4569401\n",
      "REAL: Performance of optimization based ensemblers (2nd layer) on X_test\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Performance of optimization based ensemblers (2nd layer) on X_test')   \n",
    "print('------------------------------------------------------------')\n",
    "    \n",
    "#Creating the data for the 2nd layer.\n",
    "XV = np.hstack(p_valid)\n",
    "XT = np.hstack(p_test)  \n",
    "\n",
    "\n",
    "#XV = p_valid.as_matrix()\n",
    "#XT = p_test.as_matrix()\n",
    "\n",
    "n_classes = 2\n",
    "\n",
    " \n",
    "method = ['SLSQP',\n",
    "          'L-BFGS-B',\n",
    "          #'CG',\n",
    "          #'Newton-CG',\n",
    "          #'TNC',\n",
    "          #'BFGS',\n",
    "          #'dogleg',\n",
    "          #'trust-ncg',\n",
    "          #'Nelder-Mead',\n",
    "          #'Powell'\n",
    "          #'COBYLA'\n",
    "         ]         \n",
    "          \n",
    "y_enA = []\n",
    "y_enB = []\n",
    "y_ccA = []\n",
    "y_ccB = []\n",
    "\n",
    "\n",
    "classesA = [EN_optA_SLSQP(n_classes), \n",
    "           EN_optA_L_BFGS_B(n_classes)#,\n",
    "           #EN_optA_CG(n_classes),\n",
    "           #EN_optA_Newton_CG(n_classes),\n",
    "           #EN_optA_TNC(n_classes),\n",
    "           #EN_optA_BFGS(n_classes),\n",
    "           #EN_optA_dogleg(n_classes),\n",
    "           #EN_optA_trust_ncg(n_classes),\n",
    "           #EN_optA_Nelder_Mead(n_classes),\n",
    "           #EN_optA_Powell(n_classes)\n",
    "           #EN_optA_COBYLA(n_classes)\n",
    "            ]\n",
    "\n",
    "classesB = [EN_optB_SLSQP(n_classes), \n",
    "           EN_optB_L_BFGS_B(n_classes)#,\n",
    "           #EN_optB_CG(n_classes),\n",
    "           #EN_optB_Newton_CG(n_classes),\n",
    "           #EN_optB_TNC(n_classes),\n",
    "           #EN_optB_BFGS(n_classes),\n",
    "           #EN_optB_dogleg(n_classes),\n",
    "           #EN_optB_trust_ncg(n_classes)\n",
    "           #EN_optB_Nelder_Mead(n_classes),\n",
    "           #EN_optB_Powell(n_classes)\n",
    "           #EN_optB_COBYLA(n_classes)\n",
    "           ]\n",
    "\n",
    "for c, m in zip(classesA, method):\n",
    "    enA = c\n",
    "    enA.fit(XV, y_valid)\n",
    "    w_enA = enA.w\n",
    "    y_enA.append(enA.predict_proba(XT))\n",
    "    print('{:20s} {:2s} {:1.7f}'.format('EN_optA_'+m, 'logloss  =>', log_loss(y_test, y_enA[-1])))\n",
    "    \n",
    "    #Calibrated version of EN_optA_SLSQP \n",
    "    cc_optA = CalibratedClassifierCV(enA, method='isotonic', cv=10)\n",
    "    cc_optA.fit(XV, y_valid)\n",
    "    y_ccA.append(cc_optA.predict_proba(XT))\n",
    "    print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA', 'logloss  =>', log_loss(y_test, y_ccA[-1])))\n",
    "    \n",
    "\n",
    "for c, m in zip(classesB, method):\n",
    "    enB = c\n",
    "    enB.fit(XV, y_valid)\n",
    "    w_enB = enB.w\n",
    "    y_enB.append(enB.predict_proba(XT))\n",
    "    print('{:20s} {:2s} {:1.7f}'.format('EN_optB_'+m, 'logloss  =>', log_loss(y_test, y_enB[-1])))\n",
    "    \n",
    "    #Calibrated version of EN_optA_SLSQP \n",
    "    cc_optB = CalibratedClassifierCV(enB, method='isotonic', cv=10)\n",
    "    cc_optB.fit(XV, y_valid)\n",
    "    y_ccB.append(cc_optB.predict_proba(XT))\n",
    "    print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB', 'logloss  =>', log_loss(y_test, y_ccB[-1])))\n",
    "\n",
    "#enA = EN_optA_SLSQP(n_classes)\n",
    "#enA.fit(XV, y_valid)\n",
    "#w_enA = enA.w\n",
    "#y_enA = enA.predict_proba(XT)\n",
    "#print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA)))\n",
    "    \n",
    "\n",
    "#EN_optA_SLSQP\n",
    "#enA = EN_optA(n_classes)\n",
    "#enA.fit(XV, y_valid)\n",
    "#w_enA = enA.w\n",
    "#y_enA = enA.predict_proba(XT)\n",
    "#print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA)))\n",
    "    \n",
    "#Calibrated version of EN_optA_SLSQP \n",
    "#cc_optA = CalibratedClassifierCV(enA, method='isotonic', cv=10)\n",
    "#cc_optA.fit(XV, y_valid)\n",
    "#y_ccA = cc_optA.predict_proba(XT)\n",
    "#print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA)))\n",
    "\n",
    "\n",
    "#EN_optB\n",
    "#enB = EN_optB(n_classes) \n",
    "#enB.fit(XV, y_valid)\n",
    "#w_enB = enB.w\n",
    "#y_enB = enB.predict_proba(XT)\n",
    "#print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB)))\n",
    "\n",
    "#Calibrated version of EN_optB\n",
    "#cc_optB = CalibratedClassifierCV(enB, method='isotonic', cv=10)\n",
    "#cc_optB.fit(XV, y_valid)\n",
    "#y_ccB = cc_optB.predict_proba(XT)\n",
    "#print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB)))\n",
    "#print('')\n",
    "\n",
    "\n",
    "\n",
    "if for_real:\n",
    "    print('REAL: Performance of optimization based ensemblers (2nd layer) on X_test')   \n",
    "    print('------------------------------------------------------------')\n",
    "    \n",
    "    #Creating the data for the 2nd layer.\n",
    "    XV_real = np.hstack(p_valid_real)\n",
    "    XT_real = np.hstack(p_test_real)  \n",
    "    \n",
    "    #XV_real = p_valid_real.as_matrix()\n",
    "    #XT_real = p_test_real.as_matrix()\n",
    "    \n",
    "    n_classes = 2\n",
    "    \n",
    "    y_enA_real = []\n",
    "    y_ccA_real = []\n",
    "    y_enB_real = []\n",
    "    y_ccB_real = []\n",
    "    \n",
    "    for c, m in zip(classesA, method):\n",
    "        enA_real = c\n",
    "        enA_real.fit(XV_real, y_valid_real)\n",
    "        w_enA_real = enA_real.w\n",
    "        y_enA_real.append(enA_real.predict_proba(XT_real))\n",
    "    \n",
    "        #Calibrated version of EN_optA_SLSQP \n",
    "        cc_optA_real = CalibratedClassifierCV(enA_real, method='isotonic', cv=10)\n",
    "        cc_optA_real.fit(XV_real, y_valid_real)\n",
    "        y_ccA_real.append(cc_optA_real.predict_proba(XT_real))\n",
    "    \n",
    "\n",
    "    for c, m in zip(classesB, method):\n",
    "        enB_real = c\n",
    "        enB_real.fit(XV_real, y_valid_real)\n",
    "        w_enB_real = enB_real.w\n",
    "        y_enB_real.append(enB_real.predict_proba(XT_real))\n",
    "    \n",
    "        #Calibrated version of EN_optA_SLSQP \n",
    "        cc_optB_real = CalibratedClassifierCV(enB_real, method='isotonic', cv=10)\n",
    "        cc_optB_real.fit(XV_real, y_valid_real)\n",
    "        y_ccB_real.append(cc_optB_real.predict_proba(XT_real))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #EN_optA\n",
    "    #enA_real = EN_optA(n_classes)\n",
    "    #enA_real.fit(XV_real, y_valid_real)\n",
    "    #w_enA_real = enA_real.w\n",
    "    #y_enA_real = enA_real.predict_proba(XT_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA)))\n",
    "    \n",
    "    #Calibrated version of EN_optA \n",
    "    #cc_optA_real = CalibratedClassifierCV(enA_real, method='isotonic')\n",
    "    #cc_optA_real.fit(XV_real, y_valid_real)\n",
    "    #y_ccA_real = cc_optA_real.predict_proba(XT_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA)))\n",
    "        \n",
    "    #EN_optB\n",
    "    #enB_real = EN_optB(n_classes) \n",
    "    #enB_real.fit(XV_real, y_valid_real)\n",
    "    #w_enB_real = enB_real.w\n",
    "    #y_enB_real = enB_real.predict_proba(XT_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB)))\n",
    "\n",
    "    #Calibrated version of EN_optB\n",
    "    #cc_optB_real = CalibratedClassifierCV(enB_real, method='isotonic')\n",
    "    #cc_optB_real.fit(XV_real, y_valid_real)\n",
    "    #y_ccB_real = cc_optB_real.predict_proba(XT_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB)))\n",
    "    #print('')\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN:  Performance of optimization based ensemblers (2nd layer) on X_test\n",
      "------------------------------------------------------------\n",
      "EN_optA:             logloss  => 0.5162222\n",
      "Calibrated_EN_optA:  logloss  => 0.5162544\n",
      "EN_optB:             logloss  => 0.5161750\n",
      "Calibrated_EN_optB:  logloss  => 0.5162259\n",
      "\n",
      "MEAN:  Performance of optimization based ensemblers (2nd layer) on X_test\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('MEAN:  Performance of optimization based ensemblers (2nd layer) on X_test')   \n",
    "print('------------------------------------------------------------')\n",
    "    \n",
    "#Creating the data for the 2nd layer.\n",
    "XV_mean = np.hstack(p_valid_mean)\n",
    "XT_mean = np.hstack(p_test_mean)  \n",
    "        \n",
    "#EN_optA\n",
    "enA_mean = EN_optA(n_classes)\n",
    "enA_mean.fit(XV_mean, y_valid)\n",
    "w_enA_mean = enA_mean.w\n",
    "y_enA_mean = enA_mean.predict_proba(XT_mean)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA_mean)))\n",
    "    \n",
    "#Calibrated version of EN_optA \n",
    "cc_optA_mean = CalibratedClassifierCV(enA_mean, method='isotonic')\n",
    "cc_optA_mean.fit(XV_mean, y_valid)\n",
    "y_ccA_mean = cc_optA_mean.predict_proba(XT_mean)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA_mean)))\n",
    "        \n",
    "#EN_optB\n",
    "enB_mean = EN_optB(n_classes) \n",
    "enB_mean.fit(XV_mean, y_valid)\n",
    "w_enB_mean = enB_mean.w\n",
    "y_enB_mean = enB_mean.predict_proba(XT_mean)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB_mean)))\n",
    "\n",
    "#Calibrated version of EN_optB\n",
    "cc_optB_mean = CalibratedClassifierCV(enB_mean, method='isotonic')\n",
    "cc_optB_mean.fit(XV_mean, y_valid)\n",
    "y_ccB_mean = cc_optB_mean.predict_proba(XT_mean)  \n",
    "print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB_mean)))\n",
    "print('')\n",
    "\n",
    "if for_real:\n",
    "    print('MEAN:  Performance of optimization based ensemblers (2nd layer) on X_test')   \n",
    "    print('------------------------------------------------------------')\n",
    "    \n",
    "    #Creating the data for the 2nd layer.\n",
    "    XV_mean_real = np.hstack(p_valid_mean_real)\n",
    "    XT_mean_real = np.hstack(p_test_mean_real)  \n",
    "        \n",
    "    #EN_optA\n",
    "    enA_mean_real = EN_optA(n_classes)\n",
    "    enA_mean_real.fit(XV_mean_real, y_valid_real)\n",
    "    w_enA_mean_real = enA_mean_real.w\n",
    "    y_enA_mean_real = enA_mean_real.predict_proba(XT_mean_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA_mean)))\n",
    "    \n",
    "    #Calibrated version of EN_optA \n",
    "    cc_optA_mean_real = CalibratedClassifierCV(enA_mean_real, method='isotonic')\n",
    "    cc_optA_mean_real.fit(XV_mean_real, y_valid_real)\n",
    "    y_ccA_mean_real = cc_optA_mean_real.predict_proba(XT_mean_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA_mean)))\n",
    "        \n",
    "    #EN_optB\n",
    "    enB_mean_real = EN_optB(n_classes) \n",
    "    enB_mean_real.fit(XV_mean_real, y_valid_real)\n",
    "    w_enB_mean_real = enB_mean_real.w\n",
    "    y_enB_mean_real = enB_mean_real.predict_proba(XT_mean_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB_mean)))\n",
    "\n",
    "    #Calibrated version of EN_optB\n",
    "    cc_optB_mean_real = CalibratedClassifierCV(enB_mean_real, method='isotonic')\n",
    "    cc_optB_mean_real.fit(XV_mean_real, y_valid_real)\n",
    "    y_ccB_mean_real = cc_optB_mean_real.predict_proba(XT_mean_real)  \n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB_mean)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr   \n",
    "\n",
    "slsqpA = y_enA[0][:,1]\n",
    "lbfgsbA = y_enA[1][:,1]\n",
    "tncA = y_enA[2][:,1]\n",
    "bfgsA = y_enA[3][:,1]\n",
    "neldermeadA = y_enA[4][:,1]\n",
    "powellA = y_enA[5][:,1]\n",
    "\n",
    "slsqpB = y_enB[0][:,1]\n",
    "lbfgsbB = y_enB[1][:,1]\n",
    "tncB = y_enB[2][:,1]\n",
    "bfgsB = y_enB[3][:,1]\n",
    "neldermeadB = y_enB[4][:,1]\n",
    "powellB = y_enB[5][:,1]\n",
    "\n",
    "\n",
    "cc_slsqpA = y_ccA[0][:,1]\n",
    "cc_lbfgsbA = y_ccA[1][:,1]\n",
    "cc_tncA = y_ccA[2][:,1]\n",
    "cc_bfgsA = y_ccA[3][:,1]\n",
    "cc_neldermeadA = y_ccA[4][:,1]\n",
    "cc_powellA = y_ccA[5][:,1]\n",
    "\n",
    "cc_slsqpB = y_ccB[0][:,1]\n",
    "cc_lbfgsbB = y_ccB[1][:,1]\n",
    "cc_tncB = y_ccB[2][:,1]\n",
    "cc_bfgsB = y_ccB[3][:,1]\n",
    "cc_neldermeadB = y_ccB[4][:,1]\n",
    "cc_powellB = y_ccB[5][:,1]\n",
    "\n",
    "opt_results = pd.DataFrame({\"SLSQP_A\":slsqpA, \n",
    "                            \"L_BFGS_B_A\":lbfgsbA, \n",
    "                            \"TNC_A\":tncA,\n",
    "                            \"BFGS_A\":bfgsA, \n",
    "                            \"Nelder_Mead_A\":neldermeadA,\n",
    "                            \"Powell_A\":powellA,\n",
    "                            \"SLSQP_B\":slsqpB, \n",
    "                            \"L_BFGS_B_B\":lbfgsbB, \n",
    "                            \"TNC_B\":tncB,\n",
    "                            \"BFGS_B\":bfgsB, \n",
    "                            \"Nelder_Mead_B\":neldermeadB,\n",
    "                            \"Powell_B\":powellB,\n",
    "                            \"cc_SLSQP_A\":cc_slsqpA, \n",
    "                            \"cc_L_BFGS_B_A\":cc_lbfgsbA, \n",
    "                            \"cc_TNC_A\":cc_tncA,\n",
    "                            \"cc_BFGS_A\":cc_bfgsA, \n",
    "                            \"cc_Nelder_Mead_A\":cc_neldermeadA,\n",
    "                            \"cc_Powell_A\":cc_powellA,\n",
    "                            \"cc_SLSQP_B\":cc_slsqpB, \n",
    "                            \"cc_L_BFGS_B_B\":cc_lbfgsbB, \n",
    "                            \"cc_TNC_B\":cc_tncB,\n",
    "                            \"cc_BFGS_B\":cc_bfgsB, \n",
    "                            \"cc_Nelder_Mead_B\":cc_neldermeadB,\n",
    "                            \"cc_Powell_B\":cc_powellB})\n",
    "            \n",
    "\n",
    "opt_results.corr()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#any of these uncorrelated?? maybe not... makes the next part a bit more difficult\n",
    " \n",
    "#method = ['SLSQP',\n",
    "#          'L-BFGS-B',\n",
    "          #'CG',\n",
    "          #'Newton-CG',\n",
    "#          'TNC',\n",
    "#          'BFGS',\n",
    "          #'dogleg',\n",
    "          #'trust-ncg',\n",
    "#          'Nelder-Mead',\n",
    "#          'Powell'\n",
    "          #'COBYLA'\n",
    "#         ]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_enA_total = np.mean(y_enA, axis=0)\n",
    "y_ccA_total = np.mean(y_ccA, axis=0)\n",
    "y_enB_total = np.mean(y_enB, axis=0)\n",
    "y_ccB_total = np.mean(y_ccB, axis=0)\n",
    "\n",
    "#y_enA_real_total = np.mean(y_enA_real, axis=0)\n",
    "#y_ccA_real_total = np.mean(y_ccA_real, axis=0)\n",
    "#y_enB_real_total = np.mean(y_enB_real, axis=0)\n",
    "#y_ccB_real_total = np.mean(y_ccB_real, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3rd_layer:           logloss  => 0.5160226\n",
      "3rd_layer:           logloss  => 0.5160198\n",
      "3rd_layer:           logloss  => 0.5160193\n",
      "3rd_layer:           logloss  => 0.5160189\n",
      "3rd_layer:           logloss  => 0.5160188\n"
     ]
    }
   ],
   "source": [
    "#come up with better weights here... reflect that in calibration performance\n",
    "#y_3l = (y_enA_total * 2./9.) + (y_ccA_total * 4./9.) + (y_enB_total * 2./9.) + (y_ccB_total * 1./9.)\n",
    "#print('{:20s} {:2s} {:1.7f}'.format('3rd_layer:', 'logloss  =>', log_loss(y_test, y_3l)))\n",
    "\n",
    "\n",
    "#optimize weighting of the 3rd level - keep the same weighting for real data\n",
    "best_score = 10.0\n",
    "\n",
    "for i in range(10000):\n",
    "    first = random.randint(0,20)\n",
    "    second = random.randint(0,20)\n",
    "    third = random.randint(0,20)\n",
    "    fourth = random.randint(0,20)\n",
    "    total = first + second + third + fourth\n",
    "    first = first / (total * 1.0)\n",
    "    second = second / (total * 1.0)\n",
    "    third = third / (total * 1.0)\n",
    "    fourth = fourth / (total * 1.0)\n",
    "    \n",
    "    y_3l = (y_enA_mean * first) + (y_ccA_mean * second) + (y_enB_mean * third) + (y_ccB_mean * fourth)\n",
    "    current_score = log_loss(y_test, y_3l)\n",
    "    \n",
    "    if current_score < best_score:\n",
    "        print('{:20s} {:2s} {:1.7f}'.format('3rd_layer:', 'logloss  =>', log_loss(y_test, y_3l)))\n",
    "        #print first, second, third, fourth\n",
    "        best_score = current_score\n",
    "        best_first = first\n",
    "        best_second = second\n",
    "        best_third = third\n",
    "        best_fourth = fourth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22865"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if for_real:\n",
    "    preds = (y_enA_mean_real * best_first) + \\\n",
    "            (y_ccA_mean_real * best_second) + \\\n",
    "            (y_enB_mean_real * best_third) + \\\n",
    "            (y_ccB_mean_real * best_fourth)\n",
    "            \n",
    "else:\n",
    "    preds = (y_enA_total * best_first) + \\\n",
    "            (y_ccA_total * best_second) + \\\n",
    "            (y_enB_total * best_third) + \\\n",
    "            (y_ccB_total * best_fourth)\n",
    "            \n",
    "pd.DataFrame({\"ID\": id_test, \"PredictedProb\": preds[:,1]}).to_csv('3-level_calibrated_model_XGBc_ET__picked3randomstate.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#.46631 -- mean of min methods\n",
    "#.46591 -- slsqp for A, l-bfgs-b for B\n",
    "\n",
    "#non-bayes-opt\n",
    "#.45256 -- mean of min methods\n",
    "#.45233 -- slsqp for A, l-bfgs-b for B\n",
    "\n",
    "#kNN, NN, XGBc, ET, RF\n",
    "#.45554 / train: .46515\n",
    "\n",
    "\n",
    "\n",
    "pd.DataFrame({\"ID\": id_test, \"PredictedProb\": preds[:,1]}).to_csv('3-level_calibrated_model_bayesopt.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#METHODS:\n",
    "\n",
    "#Nelder-Mead\n",
    "#Powell\n",
    "#CG\n",
    "#BFGS\n",
    "#Newton-CG\n",
    "#Anneal (deprecated as of scipy version 0.14.0)\n",
    "#L-BFGS-B\n",
    "#TNC\n",
    "#COBYLA\n",
    "#SLSQP\n",
    "#dogleg\n",
    "#trust-ncg\n",
    "\n",
    "\n",
    "#FUNCTIONS:\n",
    "#objf_ens_optA_SLSQP\n",
    "#objf_ens_optB_L-BFGS-B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#COBYLA\n",
    "def objf_ens_optA_COBYLA(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (12 in Airbnb competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA_COBYLA(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optA_COBYLA, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA_COBYLA, x0, args=(Xs, y, self.n_class), \n",
    "                       method='COBYLA', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "def objf_ens_optB_COBYLA(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 12\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB_COBYLA(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optB_COBYLA, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB_COBYLA, x0, args=(Xs, y, self.n_class), \n",
    "                       method='COBYLA', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Powell\n",
    "def objf_ens_optA_Powell(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (12 in Airbnb competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA_Powell(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optA_Powell, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA_Powell, x0, args=(Xs, y, self.n_class), \n",
    "                       method='Powell', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "def objf_ens_optB_Powell(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 12\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB_Powell(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optB_Powell, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB_Powell, x0, args=(Xs, y, self.n_class), \n",
    "                       method='Powell', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Nelder-Mead\n",
    "def objf_ens_optA_Nelder_Mead(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (12 in Airbnb competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA_Nelder_Mead(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optA_Nelder_Mead, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA_Nelder_Mead, x0, args=(Xs, y, self.n_class), \n",
    "                       method='Nelder-Mead', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "def objf_ens_optB_Nelder_Mead(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 12\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB_Nelder_Mead(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optB_Nelder_Mead, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB_Nelder_Mead, x0, args=(Xs, y, self.n_class), \n",
    "                       method='Nelder-Mead', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SLSQP\n",
    "def objf_ens_optA_SLSQP(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (12 in Airbnb competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA_SLSQP(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optA_SLSQP, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA_SLSQP, x0, args=(Xs, y, self.n_class), \n",
    "                       method='SLSQP', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "def objf_ens_optB_SLSQP(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 12\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB_SLSQP(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optB_SLSQP, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB_SLSQP, x0, args=(Xs, y, self.n_class), \n",
    "                       method='SLSQP', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#L-BFGS-B\n",
    "def objf_ens_optA_L_BFGS_B(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (12 in Airbnb competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA_L_BFGS_B(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optA_L_BFGS_B, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA_L_BFGS_B, x0, args=(Xs, y, self.n_class), \n",
    "                       method='L-BFGS-B', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "def objf_ens_optB_L_BFGS_B(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 12\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB_L_BFGS_B(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optB_L_BFGS_B, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB_L_BFGS_B, x0, args=(Xs, y, self.n_class), \n",
    "                       method='L-BFGS-B', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CG\n",
    "def objf_ens_optA_CG(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (12 in Airbnb competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA_CG(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optA_CG, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA_CG, x0, args=(Xs, y, self.n_class), \n",
    "                       method='CG', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "def objf_ens_optB_CG(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 12\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB_CG(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optB_CG, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB_CG, x0, args=(Xs, y, self.n_class), \n",
    "                       method='CG', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Newton-CG\n",
    "def objf_ens_optA_Newton_CG(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (12 in Airbnb competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA_Newton_CG(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optA_Newton_CG, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA_Newton_CG, x0, args=(Xs, y, self.n_class), \n",
    "                       method='Newton-CG', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "def objf_ens_optB_Newton_CG(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 12\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB_Newton_CG(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optB_Newton_CG, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB_Newton_CG, x0, args=(Xs, y, self.n_class), \n",
    "                       method='Newton-CG', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TNC \n",
    "def objf_ens_optA_TNC(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (12 in Airbnb competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA_TNC(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optA_TNC, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA_TNC, x0, args=(Xs, y, self.n_class), \n",
    "                       method='TNC', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "def objf_ens_optB_TNC(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 12\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB_TNC(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optB_TNC, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB_TNC, x0, args=(Xs, y, self.n_class), \n",
    "                       method='TNC', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BFGS\n",
    "def objf_ens_optA_BFGS(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (12 in Airbnb competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA_BFGS(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optA_BFGS, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA_BFGS, x0, args=(Xs, y, self.n_class), \n",
    "                       method='BFGS', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "def objf_ens_optB_BFGS(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 12\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB_BFGS(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optB_BFGS, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB_BFGS, x0, args=(Xs, y, self.n_class), \n",
    "                       method='BFGS', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dogleg\n",
    "def objf_ens_optA_dogleg(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (12 in Airbnb competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA_dogleg(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optA_dogleg, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA_dogleg, x0, args=(Xs, y, self.n_class), \n",
    "                       method='dogleg', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "def objf_ens_optB_dogleg(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 12\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB_dogleg(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optB_dogleg, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB_dogleg, x0, args=(Xs, y, self.n_class), \n",
    "                       method='dogleg', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trust-ncg\n",
    "def objf_ens_optA_trust_ncg(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (12 in Airbnb competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA_trust_ncg(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optA_trust_ncg, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA_trust_ncg, x0, args=(Xs, y, self.n_class), \n",
    "                       method='trust_ncg', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "def objf_ens_optB_trust_ncg(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 12\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB_trust_ncg(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optB_trust_ncg, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB_trust_ncg, x0, args=(Xs, y, self.n_class), \n",
    "                       method='trust_ncg', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
