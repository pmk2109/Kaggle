{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First ensemble technique (EN_optA)\n",
    "Given a set of predictions  `X1,X2,...,XnX1,X2,...,Xn ,` it computes the optimal set of weights  w1,w2,...,wnw1,w2,...,wn ; such that minimizes  log_loss(yT,yE)log_loss(yT,yE) , where  yE=X1∗w1+X2∗w2+...+Xn∗wnyE=X1∗w1+X2∗w2+...+Xn∗wn  and  yTyT  is the true solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objf_ens_optA(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (12 in Airbnb competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optA, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA, x0, args=(Xs, y, self.n_class), \n",
    "                       method='SLSQP', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second ensemble technique (EN_optB)\n",
    "Given a set of predictions  X1,X2,...,XnX1,X2,...,Xn , where each  Xi  has  m=12  clases, i.e.  Xi=Xi1,Xi2,...,XimXi=Xi1,Xi2,...,Xim . The algorithm finds the optimal set of weights  w11,w12,...,wnmw11,w12,...,wnm ; such that minimizes  log_loss(yT,yE)log_loss(yT,yE) , where  yE=X11∗w11+...+X21∗w21+...+Xnm∗wnmyE=X11∗w11+...+X21∗w21+...+Xnm∗wnm  and and  yTyT  is the true solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objf_ens_optB(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 12\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optB, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB, x0, args=(Xs, y, self.n_class), \n",
    "                       method='L-BFGS-B', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Clearing...\n"
     ]
    }
   ],
   "source": [
    "print('Load data...')\n",
    "DATA_DIR = \"/Users/patrickkennedy/Desktop/Data_Science_MISC/Kaggle\"\n",
    "train = pd.read_csv(DATA_DIR + \"/BNP_Paribas/train.csv\")\n",
    "test = pd.read_csv(DATA_DIR + \"/BNP_Paribas/test.csv\")\n",
    "\n",
    "target = train['target'].values\n",
    "\n",
    "train = train.drop(['ID','target'],axis=1)\n",
    "id_test = test['ID'].values\n",
    "test = test.drop(['ID'],axis=1)\n",
    "\n",
    "print('Clearing...')\n",
    "for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(),test.iteritems()):\n",
    "    if train_series.dtype == 'O':\n",
    "        #for objects: factorize\n",
    "        train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "        test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "        #but now we have -1 values (NaN)\n",
    "    else:\n",
    "        #for int or float: fill NaN\n",
    "        tmp_len = len(train[train_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            #print \"mean\", train_series.mean()\n",
    "            train.loc[train_series.isnull(), train_name] = -9999 #train_series.mean()\n",
    "        #and Test\n",
    "        tmp_len = len(test[test_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            test.loc[test_series.isnull(), test_name] = -9999 #train_series.mean()  #TODO\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:\n",
      "X_train: (68592, 131), X_valid: (22864, 131), X_test: (22865, 131) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_classes = 2 \n",
    "for_real = True\n",
    "random_state = 8312\n",
    "#this is what i'll change when i run the whole data set...\n",
    "#essentially my train and test sets are already split\n",
    "\n",
    "#Spliting data into train and test sets.\n",
    "X, X_test, y, y_test = train_test_split(train, target, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "#Spliting train data into training and validation sets.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=random_state)\n",
    "\n",
    "print('Data shape:')\n",
    "print('X_train: %s, X_valid: %s, X_test: %s \\n' %(X_train.shape, X_valid.shape, \n",
    "                                                  X_test.shape))\n",
    "\n",
    "if for_real:\n",
    "    #take the train, target and test data, and come up with a validation set from train\n",
    "    X_real = train\n",
    "    X_test_real = test\n",
    "    y_real = target\n",
    "    \n",
    "    X_train_real, X_valid_real, y_train_real, y_valid_real = train_test_split(X_real, y_real, \n",
    "                                                                              test_size=0.25, \n",
    "                                                                              random_state=random_state)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First layer (individual classifiers)\n",
    "All classifiers are applied twice:\n",
    "Training on (X_train, y_train) and predicting on (X_valid)\n",
    "Training on (X, y) and predicting on (X_test)\n",
    "You can add / remove classifiers or change parameter values to see the effect on final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of individual classifiers (1st layer) on X_test\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Defining the classifiers\n",
    "#think about jittering the random state and then averaging the predictions together ...\n",
    "#only good for extra trees, RF?, native XGB, NN\n",
    "clfs = {#'LR'  : LogisticRegression(), \n",
    "        'linSVM' : CalibratedClassifierCV(LinearSVC(), method='isotonic', cv=3)#, \n",
    "        #'RF'  : RandomForestClassifier(n_estimators=100, n_jobs=-1), \n",
    "        #'GBM' : GradientBoostingClassifier(n_estimators=50), \n",
    "        #'ETC1' : ExtraTreesClassifier(n_estimators=100),#, max_features=130, max_depth=12, n_jobs=-1),\n",
    "        #'ETC2' : ExtraTreesClassifier(n_estimators=250),#, max_features=130, max_depth=12, n_jobs=-1),\n",
    "        #'ETC3' : ExtraTreesClassifier(n_estimators=1000),#, max_features=130, max_depth=12, n_jobs=-1),\n",
    "\n",
    "        #'KNN' : KNeighborsClassifier(n_neighbors=30)}\n",
    "        #'XGBc1': XGBClassifier(objective='binary:logistic', learning_rate=0.03),\n",
    "        #'XGBc2': XGBClassifier(objective='binary:logistic', learning_rate=0.01),\n",
    "        #'XGBc3': XGBClassifier(objective='binary:logistic',\n",
    "        #                      colsample_bytree=0.77638333498678636,\n",
    "        #                      learning_rate=0.030567867858705199,\n",
    "        #                      max_delta_step=4.6626180513766657,\n",
    "        #                      min_child_weight=57.354121041109124,\n",
    "        #                      n_estimators=478,\n",
    "        #                      subsample=0.8069399976204783,\n",
    "        #                      max_depth=6,\n",
    "        #                      gamma=0.2966938071810209),\n",
    "        #'NN'  : Pipeline([('min/max scaler', MinMaxScaler(feature_range=(-1.0, 1.0))),\n",
    "        #                  ('neural network', Classifier(layers=[Layer(\"Rectifier\", units=10),\n",
    "        #                                                        Layer(\"Tanh\", units=10),\n",
    "        #                                                        Layer(\"Softmax\")], \n",
    "        #                                                n_iter=15))])       \n",
    "       }\n",
    "\n",
    "\n",
    "\n",
    "#predictions on the validation and test sets\n",
    "p_valid = []\n",
    "p_test = []\n",
    "\n",
    "p_valid_real = []\n",
    "p_test_real = []\n",
    "   \n",
    "print('Performance of individual classifiers (1st layer) on X_test')   \n",
    "print('------------------------------------------------------------')\n",
    "   \n",
    "for nm, clf in clfs.items():\n",
    "    if nm == 'NN':\n",
    "        #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "        clf.fit(X_train.as_matrix(), y_train)\n",
    "        yv = clf.predict_proba(X_valid.as_matrix())\n",
    "        p_valid.append(yv)\n",
    "        \n",
    "        #Second run. Training on (X, y) and predicting on X_test.\n",
    "        clf.fit(X.as_matrix(), y)\n",
    "        yt = clf.predict_proba(X_test.as_matrix())\n",
    "        p_test.append(yt)\n",
    "        \n",
    "        if for_real:\n",
    "            #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "            clf.fit(X_train_real.as_matrix(), y_train_real)\n",
    "            yv_real = clf.predict_proba(X_valid_real.as_matrix())\n",
    "            p_valid_real.append(yv_real)\n",
    "        \n",
    "            #Second run. Training on (X, y) and predicting on X_test.\n",
    "            clf.fit(X_real.as_matrix(), y_real)\n",
    "            yt_real = clf.predict_proba(X_test_real.as_matrix())\n",
    "            p_test_real.append(yt_real)\n",
    "    \n",
    "    else:\n",
    "        #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "        clf.fit(X_train, y_train)\n",
    "        yv = clf.predict_proba(X_valid)\n",
    "        p_valid.append(yv)\n",
    "        \n",
    "        #Second run. Training on (X, y) and predicting on X_test.\n",
    "        clf.fit(X, y)\n",
    "        yt = clf.predict_proba(X_test)\n",
    "        p_test.append(yt)\n",
    "        \n",
    "        if for_real:\n",
    "            #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "            clf.fit(X_train_real, y_train_real)\n",
    "            yv_real = clf.predict_proba(X_valid_real)\n",
    "            p_valid_real.append(yv_real)\n",
    "        \n",
    "            #Second run. Training on (X, y) and predicting on X_test.\n",
    "            clf.fit(X_real, y_real)\n",
    "            yt_real = clf.predict_proba(X_test_real)\n",
    "            p_test_real.append(yt_real)\n",
    "       \n",
    "    #Printing out the performance of the classifier\n",
    "    print('{:10s} {:2s} {:1.7f}'.format('%s: ' %(nm), 'logloss  =>', log_loss(y_test, yt)))\n",
    "    print('CV score: ' + str(np.mean(cross_val_score(clf, X_test, y_test, scoring='log_loss', cv=10))))\n",
    "print('')\n",
    "\n",
    "#when running the full data\n",
    "#take out the logloss function... or alternatively, run both the split data and full data model so that\n",
    "#i can compare my training logloss vs kaggle logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN MODELS: Performance of individual classifiers (1st layer) on X_test\n",
      "------------------------------------------------------------------------\n",
      "XGBc - mean:  logloss  => 0.4644625\n",
      "rs: 8312;   CV score: -0.468037340424\n",
      "\n",
      "CPU times: user 4min 39s, sys: 2.02 s, total: 4min 41s\n",
      "Wall time: 4min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('MEAN MODELS: Performance of individual classifiers (1st layer) on X_test')   \n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "rs_holder = []\n",
    "\n",
    "for r in xrange(1):    \n",
    "    rs = random.randint(1,10000)\n",
    "    x1 = True\n",
    "    while x1 == True:\n",
    "        if rs in rs_holder:\n",
    "            rs = random.randint(1,10000)\n",
    "        else:\n",
    "            x1 = False\n",
    "    rs_holder.append(rs)\n",
    "    \n",
    "    rs = 8312\n",
    "    \n",
    "    n_classes = 2 \n",
    "    for_real = True\n",
    "\n",
    "    #this is what i'll change when i run the whole data set...\n",
    "    #essentially my train and test sets are already split\n",
    "\n",
    "    #Spliting data into train and test sets.\n",
    "    X, X_test, y, y_test = train_test_split(train, target, test_size=0.2, random_state=rs)\n",
    "    \n",
    "    #Spliting train data into training and validation sets.\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=rs)\n",
    "\n",
    "    #print('Data shape:')\n",
    "    #print('X_train: %s, X_valid: %s, X_test: %s \\n' %(X_train.shape, X_valid.shape, \n",
    "    #                                              X_test.shape))\n",
    "\n",
    "    if for_real:\n",
    "        #take the train, target and test data, and come up with a validation set from train\n",
    "        X_real = train\n",
    "        X_test_real = test\n",
    "        y_real = target\n",
    "    \n",
    "        X_train_real, X_valid_real, y_train_real, y_valid_real = train_test_split(X_real, y_real, \n",
    "                                                                              test_size=0.25, random_state=rs)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    p_valid_mean = []\n",
    "    p_test_mean = []\n",
    "\n",
    "    p_valid_mean_real = []\n",
    "    p_test_mean_real = []\n",
    "\n",
    "    for nm, clf in clfs.items():\n",
    "        p_valid_clf = []\n",
    "        p_test_clf = []\n",
    "        p_valid_clf_real = []\n",
    "        p_test_clf_real = []\n",
    "    \n",
    "        holder = []\n",
    "    \n",
    "        for i in range(1):\n",
    "    \n",
    "            dummy = random.randint(1,10000)\n",
    "            x = True\n",
    "            while x == True:\n",
    "                if dummy in holder:\n",
    "                    dummy = random.randint(1,10000)\n",
    "                else:\n",
    "                    x = False\n",
    "            holder.append(dummy)\n",
    "    \n",
    "            #random.seed(rs)\n",
    "    \n",
    "            if nm == 'NN':\n",
    "                #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "                clf.fit(X_train.as_matrix(), y_train)\n",
    "                yv = clf.predict_proba(X_valid.as_matrix())\n",
    "                p_valid_clf.append(yv)\n",
    "        \n",
    "                #Second run. Training on (X, y) and predicting on X_test.\n",
    "                clf.fit(X.as_matrix(), y)\n",
    "                yt = clf.predict_proba(X_test.as_matrix())\n",
    "                p_test_clf.append(yt)\n",
    "        \n",
    "                if for_real:\n",
    "                    #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "                    clf.fit(X_train_real.as_matrix(), y_train_real)\n",
    "                    yv_real = clf.predict_proba(X_valid_real.as_matrix())\n",
    "                    p_valid_clf_real.append(yv_real)\n",
    "        \n",
    "                    #Second run. Training on (X, y) and predicting on X_test.\n",
    "                    clf.fit(X_real.as_matrix(), y_real)\n",
    "                    yt_real = clf.predict_proba(X_test_real.as_matrix())\n",
    "                    p_test_clf_real.append(yt_real)\n",
    "    \n",
    "            else:\n",
    "                #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "                clf.fit(X_train, y_train)\n",
    "                yv = clf.predict_proba(X_valid)\n",
    "                p_valid_clf.append(yv)\n",
    "        \n",
    "                #Second run. Training on (X, y) and predicting on X_test.\n",
    "                clf.fit(X, y)\n",
    "                yt = clf.predict_proba(X_test)\n",
    "                p_test_clf.append(yt)\n",
    "        \n",
    "                if for_real:\n",
    "                    #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "                    clf.fit(X_train_real, y_train_real)\n",
    "                    yv_real = clf.predict_proba(X_valid_real)\n",
    "                    p_valid_clf_real.append(yv_real)\n",
    "        \n",
    "                    #Second run. Training on (X, y) and predicting on X_test.\n",
    "                    clf.fit(X_real, y_real)\n",
    "                    yt_real = clf.predict_proba(X_test_real)\n",
    "                    p_test_clf_real.append(yt_real)\n",
    "            \n",
    "       \n",
    "        #Printing out the performance of the classifier\n",
    "        mean_pred_cv = np.mean(p_valid_clf, axis=0)\n",
    "        mean_pred_test = np.mean(p_test_clf, axis=0)\n",
    "        p_valid_mean.append(mean_pred_cv)\n",
    "        p_test_mean.append(mean_pred_test)\n",
    "    \n",
    "        mean_real_pred_cv = np.mean(p_valid_clf_real, axis=0)\n",
    "        mean_real_pred_test = np.mean(p_test_clf_real, axis=0)\n",
    "        p_valid_mean_real.append(mean_real_pred_cv)\n",
    "        p_test_mean_real.append(mean_real_pred_test)\n",
    "    \n",
    "        print('{:10s} {:2s} {:1.7f}'.format('%s - mean: ' %(nm), 'logloss  =>', log_loss(y_test, mean_pred_test)))\n",
    "        cv_score = np.mean(cross_val_score(clf, X_test, y_test, scoring='log_loss', cv=10))\n",
    "        print('rs: ' + str(rs) + ';   CV score: ' + str(cv_score))\n",
    "        \n",
    "    print('')\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "#also try setting different parameters for the XGB and add a NN to the mix\n",
    "#either use bayesopt for each classifier and putting those into this model, -or- \n",
    "#randomize both parameters and by random_state... i could do several loops here\n",
    "#lots of comp time but each random_state run a series of different parameters and\n",
    "#take the average result for that particular random_state, then run the same\n",
    "#parameters on the next random_state (don't want random X random as that is hard to replicate)...\n",
    "#too many combos... let's do bayes_opt\n",
    "\n",
    "#Random state 8312 = mean xgb: .464, cv: .468 -- confirmed rs, ___ seed\n",
    "#Random state 3900 = mean xgb: .465, cv: .468\n",
    "#Random state 3233 = mean xgb: .466, cv: .469\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBc - mean:  logloss  => 0.4644625\n",
      "8560\n",
      "\n",
      "XGBc - mean:  logloss  => 0.4644625\n",
      "5663\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    seed = random.randint(1,10000)\n",
    "    clf = XGBClassifier(objective=\"binary:logistic\", seed=seed)\n",
    "    clf.fit(X, y)\n",
    "    print('{:10s} {:2s} {:1.7f}'.format('%s - mean: ' %(nm), 'logloss  =>', log_loss(y_test, clf.predict_proba(X_test))))\n",
    "    print str(seed)\n",
    "    print('')\n",
    "    #next step is to change the seed to a randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of optimization based ensemblers (2nd layer) on X_test\n",
      "------------------------------------------------------------\n",
      "EN_optA:             logloss  => 0.4533706\n",
      "Calibrated_EN_optA:  logloss  => 0.4503315\n",
      "EN_optB:             logloss  => 0.4523598\n",
      "Calibrated_EN_optB:  logloss  => 0.4590920\n",
      "\n",
      "REAL: Performance of optimization based ensemblers (2nd layer) on X_test\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Performance of optimization based ensemblers (2nd layer) on X_test')   \n",
    "print('------------------------------------------------------------')\n",
    "    \n",
    "#Creating the data for the 2nd layer.\n",
    "XV = np.hstack(p_valid)\n",
    "XT = np.hstack(p_test)  \n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "#EN_optA\n",
    "enA = EN_optA(n_classes)\n",
    "enA.fit(XV, y_valid)\n",
    "w_enA = enA.w\n",
    "y_enA = enA.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA)))\n",
    "    \n",
    "#Calibrated version of EN_optA \n",
    "cc_optA = CalibratedClassifierCV(enA, method='isotonic')\n",
    "cc_optA.fit(XV, y_valid)\n",
    "y_ccA = cc_optA.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA)))\n",
    "        \n",
    "#EN_optB\n",
    "enB = EN_optB(n_classes) \n",
    "enB.fit(XV, y_valid)\n",
    "w_enB = enB.w\n",
    "y_enB = enB.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB)))\n",
    "\n",
    "#Calibrated version of EN_optB\n",
    "cc_optB = CalibratedClassifierCV(enB, method='isotonic')\n",
    "cc_optB.fit(XV, y_valid)\n",
    "y_ccB = cc_optB.predict_proba(XT)  \n",
    "print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB)))\n",
    "print('')\n",
    "\n",
    "\n",
    "if for_real:\n",
    "    print('REAL: Performance of optimization based ensemblers (2nd layer) on X_test')   \n",
    "    print('------------------------------------------------------------')\n",
    "    \n",
    "    #Creating the data for the 2nd layer.\n",
    "    XV_real = np.hstack(p_valid_real)\n",
    "    XT_real = np.hstack(p_test_real)  \n",
    "\n",
    "    n_classes = 2\n",
    "\n",
    "    #EN_optA\n",
    "    enA_real = EN_optA(n_classes)\n",
    "    enA_real.fit(XV_real, y_valid_real)\n",
    "    w_enA_real = enA_real.w\n",
    "    y_enA_real = enA_real.predict_proba(XT_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA)))\n",
    "    \n",
    "    #Calibrated version of EN_optA \n",
    "    cc_optA_real = CalibratedClassifierCV(enA_real, method='isotonic')\n",
    "    cc_optA_real.fit(XV_real, y_valid_real)\n",
    "    y_ccA_real = cc_optA_real.predict_proba(XT_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA)))\n",
    "        \n",
    "    #EN_optB\n",
    "    enB_real = EN_optB(n_classes) \n",
    "    enB_real.fit(XV_real, y_valid_real)\n",
    "    w_enB_real = enB_real.w\n",
    "    y_enB_real = enB_real.predict_proba(XT_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB)))\n",
    "\n",
    "    #Calibrated version of EN_optB\n",
    "    cc_optB_real = CalibratedClassifierCV(enB_real, method='isotonic')\n",
    "    cc_optB_real.fit(XV_real, y_valid_real)\n",
    "    y_ccB_real = cc_optB_real.predict_proba(XT_real)  \n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB)))\n",
    "    #print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN:  Performance of optimization based ensemblers (2nd layer) on X_test\n",
      "------------------------------------------------------------\n",
      "EN_optA:             logloss  => 0.4596911\n",
      "Calibrated_EN_optA:  logloss  => 0.4564359\n",
      "EN_optB:             logloss  => 0.4582061\n",
      "Calibrated_EN_optB:  logloss  => 0.4705139\n",
      "\n",
      "MEAN:  Performance of optimization based ensemblers (2nd layer) on X_test\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('MEAN:  Performance of optimization based ensemblers (2nd layer) on X_test')   \n",
    "print('------------------------------------------------------------')\n",
    "    \n",
    "#Creating the data for the 2nd layer.\n",
    "XV_mean = np.hstack(p_valid_mean)\n",
    "XT_mean = np.hstack(p_test_mean)  \n",
    "        \n",
    "#EN_optA\n",
    "enA_mean = EN_optA(n_classes)\n",
    "enA_mean.fit(XV_mean, y_valid)\n",
    "w_enA_mean = enA_mean.w\n",
    "y_enA_mean = enA_mean.predict_proba(XT_mean)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA_mean)))\n",
    "    \n",
    "#Calibrated version of EN_optA \n",
    "cc_optA_mean = CalibratedClassifierCV(enA_mean, method='isotonic')\n",
    "cc_optA_mean.fit(XV_mean, y_valid)\n",
    "y_ccA_mean = cc_optA_mean.predict_proba(XT_mean)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA_mean)))\n",
    "        \n",
    "#EN_optB\n",
    "enB_mean = EN_optB(n_classes) \n",
    "enB_mean.fit(XV_mean, y_valid)\n",
    "w_enB_mean = enB_mean.w\n",
    "y_enB_mean = enB_mean.predict_proba(XT_mean)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB_mean)))\n",
    "\n",
    "#Calibrated version of EN_optB\n",
    "cc_optB_mean = CalibratedClassifierCV(enB_mean, method='isotonic')\n",
    "cc_optB_mean.fit(XV_mean, y_valid)\n",
    "y_ccB_mean = cc_optB_mean.predict_proba(XT_mean)  \n",
    "print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB_mean)))\n",
    "print('')\n",
    "\n",
    "if for_real:\n",
    "    print('MEAN:  Performance of optimization based ensemblers (2nd layer) on X_test')   \n",
    "    print('------------------------------------------------------------')\n",
    "    \n",
    "    #Creating the data for the 2nd layer.\n",
    "    XV_mean_real = np.hstack(p_valid_mean_real)\n",
    "    XT_mean_real = np.hstack(p_test_mean_real)  \n",
    "        \n",
    "    #EN_optA\n",
    "    enA_mean_real = EN_optA(n_classes)\n",
    "    enA_mean_real.fit(XV_mean_real, y_valid_real)\n",
    "    w_enA_mean_real = enA_mean_real.w\n",
    "    y_enA_mean_real = enA_mean_real.predict_proba(XT_mean_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA_mean)))\n",
    "    \n",
    "    #Calibrated version of EN_optA \n",
    "    cc_optA_mean_real = CalibratedClassifierCV(enA_mean_real, method='isotonic')\n",
    "    cc_optA_mean_real.fit(XV_mean_real, y_valid_real)\n",
    "    y_ccA_mean_real = cc_optA_mean_real.predict_proba(XT_mean_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA_mean)))\n",
    "        \n",
    "    #EN_optB\n",
    "    enB_mean_real = EN_optB(n_classes) \n",
    "    enB_mean_real.fit(XV_mean_real, y_valid_real)\n",
    "    w_enB_mean_real = enB_mean_real.w\n",
    "    y_enB_mean_real = enB_mean_real.predict_proba(XT_mean_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB_mean)))\n",
    "\n",
    "    #Calibrated version of EN_optB\n",
    "    cc_optB_mean_real = CalibratedClassifierCV(enB_mean_real, method='isotonic')\n",
    "    cc_optB_mean_real.fit(XV_mean_real, y_valid_real)\n",
    "    y_ccB_mean_real = cc_optB_mean_real.predict_proba(XT_mean_real)  \n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB_mean)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3rd_layer:           logloss  => 0.4623964\n"
     ]
    }
   ],
   "source": [
    "#come up with better weights here... reflect that in calibration performance\n",
    "y_3l = (y_enA * 2./9.) + (y_ccA * 4./9.) + (y_enB * 2./9.) + (y_ccB * 1./9.)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('3rd_layer:', 'logloss  =>', log_loss(y_test, y_3l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3rd_layer:           logloss  => 0.4545998\n"
     ]
    }
   ],
   "source": [
    "#come up with better weights here... reflect that in calibration performance\n",
    "y_3l_mean = (y_enA_mean * 2./9.) + (y_ccA_mean * 4./9.) + (y_enB_mean * 2./9.) + (y_ccB_mean * 1./9.)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('3rd_layer:', 'logloss  =>', log_loss(y_test, y_3l_mean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07981462409886715"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 10% baby!  currently: 155/1942."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3rd_layer:           logloss  => 0.4513214\n",
      "3rd_layer:           logloss  => 0.4510378\n",
      "3rd_layer:           logloss  => 0.4509523\n",
      "3rd_layer:           logloss  => 0.4509051\n",
      "3rd_layer:           logloss  => 0.4501084\n",
      "3rd_layer:           logloss  => 0.4498186\n",
      "3rd_layer:           logloss  => 0.4480240\n",
      "3rd_layer:           logloss  => 0.4478673\n",
      "3rd_layer:           logloss  => 0.4477606\n"
     ]
    }
   ],
   "source": [
    "#optimize weighting of the 3rd level - keep the same weighting for real data\n",
    "best_score = 10.0\n",
    "\n",
    "for i in range(10000):\n",
    "    first = random.randint(0,20)\n",
    "    second = random.randint(0,20)\n",
    "    third = random.randint(0,20)\n",
    "    fourth = random.randint(0,20)\n",
    "    total = first + second + third + fourth\n",
    "    first = first / (total * 1.0)\n",
    "    second = second / (total * 1.0)\n",
    "    third = third / (total * 1.0)\n",
    "    fourth = fourth / (total * 1.0)\n",
    "    \n",
    "    y_3l = (y_enA * first) + (y_ccA * second) + (y_enB * third) + (y_ccB * fourth)\n",
    "    current_score = log_loss(y_test, y_3l)\n",
    "    \n",
    "    if current_score < best_score:\n",
    "        print('{:20s} {:2s} {:1.7f}'.format('3rd_layer:', 'logloss  =>', log_loss(y_test, y_3l)))\n",
    "        #print first, second, third, fourth\n",
    "        best_score = current_score\n",
    "        best_first = first\n",
    "        best_second = second\n",
    "        best_third = third\n",
    "        best_fourth = fourth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3rd_layer:           logloss  => 0.4576157\n",
      "0.295081967213 0.262295081967 0.196721311475 0.245901639344\n",
      "3rd_layer:           logloss  => 0.4576157\n",
      "0.409090909091 0.340909090909 0.159090909091 0.0909090909091\n",
      "3rd_layer:           logloss  => 0.4576157\n",
      "0.461538461538 0.487179487179 0.025641025641 0.025641025641\n",
      "3rd_layer:           logloss  => 0.4576157\n",
      "0.333333333333 0.555555555556 0.0 0.111111111111\n",
      "3rd_layer:           logloss  => 0.4576157\n",
      "0.0857142857143 0.542857142857 0.285714285714 0.0857142857143\n",
      "3rd_layer:           logloss  => 0.4576157\n",
      "0.24 0.56 0.16 0.04\n",
      "3rd_layer:           logloss  => 0.4576157\n",
      "0.0 0.933333333333 0.0 0.0666666666667\n",
      "3rd_layer:           logloss  => 0.4576157\n",
      "0.130434782609 0.739130434783 0.130434782609 0.0\n",
      "3rd_layer:           logloss  => 0.4576157\n",
      "0.0434782608696 0.869565217391 0.0434782608696 0.0434782608696\n",
      "3rd_layer:           logloss  => 0.4576157\n",
      "0.0625 0.8125 0.125 0.0\n",
      "3rd_layer:           logloss  => 0.4576157\n",
      "0.0714285714286 0.857142857143 0.0714285714286 0.0\n",
      "3rd_layer:           logloss  => 0.4576157\n",
      "0.0 0.85 0.15 0.0\n"
     ]
    }
   ],
   "source": [
    "#optimize weighting of the 3rd level - keep the same weighting for real data\n",
    "best_mean_score = 10.0\n",
    "\n",
    "for i in range(10000):\n",
    "    first = random.randint(0,20)\n",
    "    second = random.randint(0,20)\n",
    "    third = random.randint(0,20)\n",
    "    fourth = random.randint(0,20)\n",
    "    total = first + second + third + fourth\n",
    "    first = first / (total * 1.0)\n",
    "    second = second / (total * 1.0)\n",
    "    third = third / (total * 1.0)\n",
    "    fourth = fourth / (total * 1.0)\n",
    "    \n",
    "    y_3l_mean = (y_enA_mean * first) + (y_ccA_mean * second) + (y_enB_mean * third) + (y_ccB_mean * fourth)\n",
    "    current_score = log_loss(y_test, y_3l_mean)\n",
    "    \n",
    "    if current_score < best_mean_score:\n",
    "        print('{:20s} {:2s} {:1.7f}'.format('3rd_layer:', 'logloss  =>', log_loss(y_test, y_3l)))\n",
    "        print first, second, third, fourth\n",
    "        best_mean_score = current_score\n",
    "        best_mean_first = first\n",
    "        best_mean_second = second\n",
    "        best_mean_third = third\n",
    "        best_mean_fourth = fourth\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#well awesome .. CV score is .4577 and my kaggle score is .45373\n",
    "best_fourth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if for_real:\n",
    "    preds = (y_enA_real * best_first) + \\\n",
    "            (y_ccA_real * best_second) + \\\n",
    "            (y_enB_real * best_third) + \\\n",
    "            (y_ccB_real * best_fourth)\n",
    "            \n",
    "    #preds_mean = (y_enA_mean_real * best_mean_first) + \\\n",
    "    #            (y_ccA_mean_real * best_mean_second) + \\\n",
    "    #            (y_enB_mean_real * best_mean_third) + \\\n",
    "    #            (y_ccB_mean_real * best_mean_fourth)\n",
    "\n",
    "    pd.DataFrame({\"ID\": id_test, \"PredictedProb\": preds[:,1]}).to_csv('3-level_calibrated_model_8312randomstate_7classifiers.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#best optimized score i can get with training data is .44956 using untrained XGBClassifier and ExtraTrees, \n",
    "#jittered 100 random_states with mean predictions with weights [0.0, .9047619047, 0.095238095, 0.0]\n",
    "\n",
    "#let's try with additional models (logistic regression, random forest, NN), try with maybe randomized params,\n",
    "#try maybe with bayes_optimized params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the weights of each ensemble\n",
    "In the case of EN_optA, there is a weight for each prediction and in the case of EN_optB there is a weight for each class for each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Weights of EN_optA:\n",
      "|---------------------------------------|\n",
      "|   ETC |   XGBc |\n",
      "|-------+--------|\n",
      "|  0.41 |   0.59 |\n",
      "\n",
      "     Weights of EN_optB:\n",
      "|---------------------------|\n",
      "|      |   y0 |   y1 |\n",
      "|------+------+------|\n",
      "| ETC  | 0.65 |    0 |\n",
      "| XGBc | 0.35 |    1 |\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "print('         Weights of EN_optA:')\n",
    "print('|---------------------------------------|')\n",
    "wA = np.round(w_enA, decimals=2).reshape(1,-1)\n",
    "print(tabulate(wA, headers=clfs.keys(), tablefmt=\"orgtbl\"))\n",
    "print('')\n",
    "print('     Weights of EN_optB:')\n",
    "print('|---------------------------|')\n",
    "wB = np.round(w_enB.reshape((-1,n_classes)), decimals=2)\n",
    "wB = np.hstack((np.array(list(clfs.keys()), dtype=str).reshape(-1,1), wB))\n",
    "print(tabulate(wB, headers=['y%s'%(i) for i in range(n_classes)], tablefmt=\"orgtbl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing our ensemble results with sklearn LogisticRegression based stacking of classifiers.¶\n",
    "Both techniques EN_optA and EN_optB optimizes an objective function. In this experiment I am using the multi-class logloss as objective function. Therefore, the two proposed methods basically become implementations of LogisticRegression. The following code allows to compare the results of sklearn implementation of LogisticRegression with the proposed ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#By default the best C parameter is obtained with a cross-validation approach, doing grid search with\n",
    "#10 values defined in a logarithmic scale between 1e-4 and 1e4.\n",
    "#Change parameters to see how they affect the final results.\n",
    "lr = LogisticRegressionCV(Cs=10, dual=False, fit_intercept=True, \n",
    "                          intercept_scaling=1.0, max_iter=100,\n",
    "                          multi_class='ovr', n_jobs=1, penalty='l2', \n",
    "                          random_state=random_state,\n",
    "                          solver='lbfgs', tol=0.0001)\n",
    "\n",
    "lr.fit(XV, y_valid)\n",
    "y_lr = lr.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('Log_Reg:', 'logloss  =>', log_loss(y_test, y_lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 22864\n",
      "91456\n"
     ]
    }
   ],
   "source": [
    "print len(p_valid), len(p_valid[0])\n",
    "print len(np.hstack(p_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    \n",
    "    dummy = random.randint(1,10000)\n",
    "    x = True\n",
    "    while x == True:\n",
    "        print dummy, str(len(holder)+1), holder\n",
    "        #print holder\n",
    "        if dummy in holder:\n",
    "            dummy = random.randint(1,10000)\n",
    "        else:\n",
    "            x = False\n",
    "    holder.append(dummy)\n",
    "    \n",
    "    random.seed(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame({\"ID\": id_test, \"PredictedProb\": np.mean(y_pred, axis=0)}).to_csv('extra_trees_and_log_and_gradientboost_with adas_jitteredrandomstate_500iterations.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
