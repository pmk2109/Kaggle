{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First ensemble technique (EN_optA)\n",
    "Given a set of predictions  `X1,X2,...,XnX1,X2,...,Xn ,` it computes the optimal set of weights  w1,w2,...,wnw1,w2,...,wn ; such that minimizes  log_loss(yT,yE)log_loss(yT,yE) , where  yE=X1∗w1+X2∗w2+...+Xn∗wnyE=X1∗w1+X2∗w2+...+Xn∗wn  and  yTyT  is the true solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objf_ens_optA(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (12 in Airbnb competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optA, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA, x0, args=(Xs, y, self.n_class), \n",
    "                       method='SLSQP', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second ensemble technique (EN_optB)\n",
    "Given a set of predictions  X1,X2,...,XnX1,X2,...,Xn , where each  Xi  has  m=12  clases, i.e.  Xi=Xi1,Xi2,...,XimXi=Xi1,Xi2,...,Xim . The algorithm finds the optimal set of weights  w11,w12,...,wnmw11,w12,...,wnm ; such that minimizes  log_loss(yT,yE)log_loss(yT,yE) , where  yE=X11∗w11+...+X21∗w21+...+Xnm∗wnmyE=X11∗w11+...+X21∗w21+...+Xnm∗wnm  and and  yTyT  is the true solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objf_ens_optB(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 12\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optB, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB, x0, args=(Xs, y, self.n_class), \n",
    "                       method='L-BFGS-B', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Clearing...\n"
     ]
    }
   ],
   "source": [
    "print('Load data...')\n",
    "DATA_DIR = \"/Users/patrickkennedy/Desktop/Data_Science_MISC/Kaggle\"\n",
    "train = pd.read_csv(DATA_DIR + \"/BNP_Paribas/train.csv\")\n",
    "test = pd.read_csv(DATA_DIR + \"/BNP_Paribas/test.csv\")\n",
    "\n",
    "target = train['target'].values\n",
    "\n",
    "train = train.drop(['ID','target'],axis=1)\n",
    "id_test = test['ID'].values\n",
    "test = test.drop(['ID'],axis=1)\n",
    "\n",
    "print('Clearing...')\n",
    "for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(),test.iteritems()):\n",
    "    if train_series.dtype == 'O':\n",
    "        #for objects: factorize\n",
    "        train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "        test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "        #but now we have -1 values (NaN)\n",
    "    else:\n",
    "        #for int or float: fill NaN\n",
    "        tmp_len = len(train[train_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            #print \"mean\", train_series.mean()\n",
    "            train.loc[train_series.isnull(), train_name] = -9999 #train_series.mean()\n",
    "        #and Test\n",
    "        tmp_len = len(test[test_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            test.loc[test_series.isnull(), test_name] = -9999 #train_series.mean()  #TODO\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:\n",
      "X_train: (68592, 131), X_valid: (22864, 131), X_test: (22865, 131) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_classes = 2 \n",
    "\n",
    "#this is what i'll change when i run the whole data set...\n",
    "#essentially my train and test sets are already split\n",
    "\n",
    "#Spliting data into train and test sets.\n",
    "X, X_test, y, y_test = train_test_split(train, target, test_size=0.2)\n",
    "    \n",
    "#Spliting train data into training and validation sets.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "print('Data shape:')\n",
    "print('X_train: %s, X_valid: %s, X_test: %s \\n' %(X_train.shape, X_valid.shape, \n",
    "                                                  X_test.shape))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First layer (individual classifiers)\n",
    "All classifiers are applied twice:\n",
    "Training on (X_train, y_train) and predicting on (X_valid)\n",
    "Training on (X, y) and predicting on (X_test)\n",
    "You can add / remove classifiers or change parameter values to see the effect on final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of individual classifiers (1st layer) on X_test\n",
      "------------------------------------------------------------\n",
      "ETC:       logloss  => 0.4695700\n",
      "XGBc:      logloss  => 0.4670779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Defining the classifiers\n",
    "#think about jittering the random state and then averaging the predictions together ...\n",
    "#only good for extra trees, RF?, native XGB, NN\n",
    "clfs = {#'LR'  : LogisticRegression(), \n",
    "        #'SVM' : SVC(probability=True, random_state=random_state), \n",
    "        #'RF'  : RandomForestClassifier(n_estimators=100, n_jobs=-1), \n",
    "        #'GBM' : GradientBoostingClassifier(n_estimators=50), \n",
    "        'ETC' : ExtraTreesClassifier(n_estimators=100, n_jobs=-1),\n",
    "        #'KNN' : KNeighborsClassifier(n_neighbors=30)}\n",
    "        'XGBc': XGBClassifier(objective='binary:logistic')#,\n",
    "        #'NN'  : Pipeline([('min/max scaler', MinMaxScaler(feature_range=(-1.0, 1.0))),\n",
    "        #                  ('neural network', Classifier(layers=[Layer(\"Rectifier\", units=random.randint(10,100)),\n",
    "        #                                                        Layer(\"Tanh\", units=random.randint(10,100)),\n",
    "        #                                                        Layer(\"Softmax\")], \n",
    "        #                                                n_iter=5))])       \n",
    "       }\n",
    "\n",
    "#predictions on the validation and test sets\n",
    "p_valid = []\n",
    "p_test = []\n",
    "   \n",
    "print('Performance of individual classifiers (1st layer) on X_test')   \n",
    "print('------------------------------------------------------------')\n",
    "   \n",
    "for nm, clf in clfs.items():\n",
    "    if nm == 'NN':\n",
    "        #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "        clf.fit(X_train.as_matrix(), y_train)\n",
    "        yv = clf.predict_proba(X_valid.as_matrix())\n",
    "        p_valid.append(yv)\n",
    "        \n",
    "        #Second run. Training on (X, y) and predicting on X_test.\n",
    "        clf.fit(X.as_matrix(), y)\n",
    "        yt = clf.predict_proba(X_test.as_matrix())\n",
    "        p_test.append(yt)\n",
    "    \n",
    "    else:\n",
    "        #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "        clf.fit(X_train, y_train)\n",
    "        yv = clf.predict_proba(X_valid)\n",
    "        p_valid.append(yv)\n",
    "        \n",
    "        #Second run. Training on (X, y) and predicting on X_test.\n",
    "        clf.fit(X, y)\n",
    "        yt = clf.predict_proba(X_test)\n",
    "        p_test.append(yt)\n",
    "       \n",
    "    #Printing out the performance of the classifier\n",
    "    print('{:10s} {:2s} {:1.7f}'.format('%s: ' %(nm), 'logloss  =>', log_loss(y_test, yt)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN MODELS: Performance of individual classifiers (1st layer) on X_test\n",
      "------------------------------------------------------------------------\n",
      "ETC - mean:  logloss  => 0.4639964"
     ]
    }
   ],
   "source": [
    "print('MEAN MODELS: Performance of individual classifiers (1st layer) on X_test')   \n",
    "print('------------------------------------------------------------------------')\n",
    "p_valid_mean = []\n",
    "p_test_mean = []\n",
    "\n",
    "for nm, clf in clfs.items():\n",
    "    p_valid_clf = []\n",
    "    p_test_clf = []\n",
    "    holder = []\n",
    "    \n",
    "    for i in range(100):\n",
    "    \n",
    "        dummy = random.randint(1,10000)\n",
    "        x = True\n",
    "        while x == True:\n",
    "            if dummy in holder:\n",
    "                dummy = random.randint(1,10000)\n",
    "            else:\n",
    "                x = False\n",
    "        holder.append(dummy)\n",
    "    \n",
    "        random.seed(dummy)\n",
    "    \n",
    "        if nm == 'NN':\n",
    "            #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "            clf.fit(X_train.as_matrix(), y_train)\n",
    "            yv = clf.predict_proba(X_valid.as_matrix())\n",
    "            p_valid_clf.append(yv)\n",
    "        \n",
    "            #Second run. Training on (X, y) and predicting on X_test.\n",
    "            clf.fit(X.as_matrix(), y)\n",
    "            yt = clf.predict_proba(X_test.as_matrix())\n",
    "            p_test_clf.append(yt)\n",
    "            \n",
    "        else:\n",
    "            #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "            clf.fit(X_train, y_train)\n",
    "            yv = clf.predict_proba(X_valid)\n",
    "            p_valid_clf.append(yv)\n",
    "        \n",
    "            #Second run. Training on (X, y) and predicting on X_test.\n",
    "            clf.fit(X, y)\n",
    "            yt = clf.predict_proba(X_test)\n",
    "            p_test_clf.append(yt)\n",
    "            \n",
    "       \n",
    "    #Printing out the performance of the classifier\n",
    "    mean_pred_cv = np.mean(p_valid_clf, axis=0)\n",
    "    mean_pred_test = np.mean(p_test_clf, axis=0)\n",
    "    p_valid_mean.append(mean_pred_cv)\n",
    "    p_test_mean.append(mean_pred_test)\n",
    "    \n",
    "    print('{:10s} {:2s} {:1.7f}'.format('%s - mean: ' %(nm), 'logloss  =>', log_loss(y_test, mean_pred_test)))\n",
    "    \n",
    "print('')\n",
    "\n",
    "#also try setting different parameters for the XGB and add a NN to the mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of optimization based ensemblers (2nd layer) on X_test\n",
      "------------------------------------------------------------\n",
      "EN_optA:             logloss  => 0.4605359\n",
      "Calibrated_EN_optA:  logloss  => 0.4635575\n",
      "EN_optB:             logloss  => 0.4597320\n",
      "Calibrated_EN_optB:  logloss  => 0.4690561\n",
      "\n",
      "MEAN:  Performance of optimization based ensemblers (2nd layer) on X_test\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-337fa4b9d508>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#Creating the data for the 2nd layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mXV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_valid_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mXT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_test_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/numpy/core/shape_base.pyc\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;31m# As a special case, dimension 0 of 1-dimensional arrays is \"horizontal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0marrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print('Performance of optimization based ensemblers (2nd layer) on X_test')   \n",
    "print('------------------------------------------------------------')\n",
    "    \n",
    "#Creating the data for the 2nd layer.\n",
    "XV = np.hstack(p_valid)\n",
    "XT = np.hstack(p_test)  \n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "#EN_optA\n",
    "enA = EN_optA(n_classes)\n",
    "enA.fit(XV, y_valid)\n",
    "w_enA = enA.w\n",
    "y_enA = enA.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA)))\n",
    "    \n",
    "#Calibrated version of EN_optA \n",
    "cc_optA = CalibratedClassifierCV(enA, method='isotonic')\n",
    "cc_optA.fit(XV, y_valid)\n",
    "y_ccA = cc_optA.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA)))\n",
    "        \n",
    "#EN_optB\n",
    "enB = EN_optB(n_classes) \n",
    "enB.fit(XV, y_valid)\n",
    "w_enB = enB.w\n",
    "y_enB = enB.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB)))\n",
    "\n",
    "#Calibrated version of EN_optB\n",
    "cc_optB = CalibratedClassifierCV(enB, method='isotonic')\n",
    "cc_optB.fit(XV, y_valid)\n",
    "y_ccB = cc_optB.predict_proba(XT)  \n",
    "print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('MEAN:  Performance of optimization based ensemblers (2nd layer) on X_test')   \n",
    "print('------------------------------------------------------------')\n",
    "    \n",
    "#Creating the data for the 2nd layer.\n",
    "XV_mean = np.hstack(p_valid_mean)\n",
    "XT_mean = np.hstack(p_test_mean)  \n",
    "        \n",
    "#EN_optA\n",
    "enA_mean = EN_optA(n_classes)\n",
    "enA_mean.fit(XV_mean, y_valid)\n",
    "w_enA_mean = enA_mean.w\n",
    "y_enA_mean = enA_mean.predict_proba(XT_mean)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA_mean)))\n",
    "    \n",
    "#Calibrated version of EN_optA \n",
    "cc_optA_mean = CalibratedClassifierCV(enA_mean, method='isotonic')\n",
    "cc_optA_mean.fit(XV_mean, y_valid)\n",
    "y_ccA_mean = cc_optA_mean.predict_proba(XT_mean)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA_mean)))\n",
    "        \n",
    "#EN_optB\n",
    "enB_mean = EN_optB(n_classes) \n",
    "enB_mean.fit(XV_mean, y_valid)\n",
    "w_enB_mean = enB_mean.w\n",
    "y_enB_mean = enB_mean.predict_proba(XT_mean)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB_mean)))\n",
    "\n",
    "#Calibrated version of EN_optB\n",
    "cc_optB_mean = CalibratedClassifierCV(enB_mean, method='isotonic')\n",
    "cc_optB_mean.fit(XV_mean, y_valid)\n",
    "y_ccB_mean = cc_optB_mean.predict_proba(XT_mean)  \n",
    "print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB_mean)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3rd_layer:           logloss  => 0.4567277\n"
     ]
    }
   ],
   "source": [
    "#come up with better weights here... reflect that in calibration performance\n",
    "y_3l = (y_enA * 4./9.) + (y_ccA * 2./9.) + (y_enB * 2./9.) + (y_ccB * 1./9.)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('3rd_layer:', 'logloss  =>', log_loss(y_test, y_3l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#come up with better weights here... reflect that in calibration performance\n",
    "y_3l_mean = (y_enA_mean * 4./9.) + (y_ccA_mean * 2./9.) + (y_enB_mean * 2./9.) + (y_ccB_mean * 1./9.)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('3rd_layer:', 'logloss  =>', log_loss(y_test, y_3l_mean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add in the code to create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the weights of each ensemble\n",
    "In the case of EN_optA, there is a weight for each prediction and in the case of EN_optB there is a weight for each class for each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Weights of EN_optA:\n",
      "|---------------------------------------------|\n",
      "|   ETC |   RF |   GBM |   LR |   XGBc |\n",
      "|-------+------+-------+------+--------|\n",
      "|  0.39 |    0 |     0 |    0 |   0.61 |\n",
      "\n",
      "                                    Weights of EN_optB:\n",
      "|-------------------------------------------------------------------------------------------|\n",
      "|      |   y0 |   y1 |\n",
      "|------+------+------|\n",
      "| ETC  | 0.58 |    0 |\n",
      "| RF   | 0    |    0 |\n",
      "| GBM  | 0    |    0 |\n",
      "| LR   | 0    |    0 |\n",
      "| XGBc | 0.42 |    1 |\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "print('         Weights of EN_optA:')\n",
    "print('|---------------------------------------|')\n",
    "wA = np.round(w_enA, decimals=2).reshape(1,-1)\n",
    "print(tabulate(wA, headers=clfs.keys(), tablefmt=\"orgtbl\"))\n",
    "print('')\n",
    "print('     Weights of EN_optB:')\n",
    "print('|---------------------------|')\n",
    "wB = np.round(w_enB.reshape((-1,n_classes)), decimals=2)\n",
    "wB = np.hstack((np.array(list(clfs.keys()), dtype=str).reshape(-1,1), wB))\n",
    "print(tabulate(wB, headers=['y%s'%(i) for i in range(n_classes)], tablefmt=\"orgtbl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing our ensemble results with sklearn LogisticRegression based stacking of classifiers.¶\n",
    "Both techniques EN_optA and EN_optB optimizes an objective function. In this experiment I am using the multi-class logloss as objective function. Therefore, the two proposed methods basically become implementations of LogisticRegression. The following code allows to compare the results of sklearn implementation of LogisticRegression with the proposed ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#By default the best C parameter is obtained with a cross-validation approach, doing grid search with\n",
    "#10 values defined in a logarithmic scale between 1e-4 and 1e4.\n",
    "#Change parameters to see how they affect the final results.\n",
    "lr = LogisticRegressionCV(Cs=10, dual=False, fit_intercept=True, \n",
    "                          intercept_scaling=1.0, max_iter=100,\n",
    "                          multi_class='ovr', n_jobs=1, penalty='l2', \n",
    "                          random_state=random_state,\n",
    "                          solver='lbfgs', tol=0.0001)\n",
    "\n",
    "lr.fit(XV, y_valid)\n",
    "y_lr = lr.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('Log_Reg:', 'logloss  =>', log_loss(y_test, y_lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 22864\n",
      "91456\n"
     ]
    }
   ],
   "source": [
    "print len(p_valid), len(p_valid[0])\n",
    "print len(np.hstack(p_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    \n",
    "    dummy = random.randint(1,10000)\n",
    "    x = True\n",
    "    while x == True:\n",
    "        print dummy, str(len(holder)+1), holder\n",
    "        #print holder\n",
    "        if dummy in holder:\n",
    "            dummy = random.randint(1,10000)\n",
    "        else:\n",
    "            x = False\n",
    "    holder.append(dummy)\n",
    "    \n",
    "    random.seed(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame({\"ID\": id_test, \"PredictedProb\": np.mean(y_pred, axis=0)}).to_csv('extra_trees_and_log_and_gradientboost_with adas_jitteredrandomstate_500iterations.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
