{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "DATA_DIR = \"/Users/patrickkennedy/Desktop/Data_Science/Numerai/\"\n",
    "train = pd.read_csv(DATA_DIR + \"numerai_training_data.csv\")\n",
    "test = pd.read_csv(DATA_DIR + \"numerai_tournament_data.csv\")\n",
    "\n",
    "#upload file with two columns ala kaggle: t_id, probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = train['target'].values\n",
    "\n",
    "t_id = test['t_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holder = []\n",
    "preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of individual classifiers (1st layer) on X_test\n",
      "------------------------------------------------------------\n",
      "ETC:       logloss  => 0.7040552\n",
      "XGBc:      logloss  => 0.6920312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Defining the classifiers\n",
    "#think about jittering the random state and then averaging the predictions together ...\n",
    "#only good for extra trees, RF?, native XGB, NN\n",
    "clfs = {#'LR'  : LogisticRegression(), \n",
    "        #'SVM' : SVC(probability=True, random_state=random_state), \n",
    "        #'RF'  : RandomForestClassifier(n_estimators=100, n_jobs=-1), \n",
    "        #'GBM' : GradientBoostingClassifier(n_estimators=50), \n",
    "        'ETC' : ExtraTreesClassifier(n_estimators=100, n_jobs=-1),\n",
    "        #'KNN' : KNeighborsClassifier(n_neighbors=5),\n",
    "        'XGBc': XGBClassifier(objective='binary:logistic') #,\n",
    "        #'NN'  : Pipeline([('min/max scaler', MinMaxScaler(feature_range=(-1.0, 1.0))),\n",
    "        #                  ('neural network', Classifier(layers=[Layer(\"Rectifier\", units=10),\n",
    "        #                                                        Layer(\"Tanh\", units=10),\n",
    "        #                                                        Layer(\"Softmax\")], \n",
    "        #                                                n_iter=5))])       \n",
    "       }\n",
    "\n",
    "\n",
    "for_real = True\n",
    "\n",
    "#predictions on the validation and test sets\n",
    "p_valid = []\n",
    "p_test = []\n",
    "\n",
    "p_valid_real = []\n",
    "p_test_real = []\n",
    "   \n",
    "print('Performance of individual classifiers (1st layer) on X_test')   \n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "#Spliting data into train and test sets.\n",
    "X, X_test, y, y_test = train_test_split(train.drop(['target'], axis=1), target, test_size=0.2)\n",
    "    \n",
    "#Spliting train data into training and validation sets.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "X_real = train.drop(['target'], axis=1)\n",
    "X_test_real = test.drop(['t_id'], axis=1)\n",
    "y_real = target\n",
    "    \n",
    "X_train_real, X_valid_real, y_train_real, y_valid_real = train_test_split(X_real, y_real, test_size=0.25)\n",
    "\n",
    "random.seed(117)\n",
    "\n",
    "for nm, clf in clfs.items():\n",
    "    if nm == 'NN':\n",
    "        #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "        clf.fit(X_train.as_matrix(), y_train)\n",
    "        yv = clf.predict_proba(X_valid.as_matrix())\n",
    "        p_valid.append(yv)\n",
    "        \n",
    "        #Second run. Training on (X, y) and predicting on X_test.\n",
    "        clf.fit(X.as_matrix(), y)\n",
    "        yt = clf.predict_proba(X_test.as_matrix())\n",
    "        p_test.append(yt)\n",
    "        \n",
    "        if for_real:\n",
    "            #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "            clf.fit(X_train_real.as_matrix(), y_train_real)\n",
    "            yv_real = clf.predict_proba(X_valid_real.as_matrix())\n",
    "            p_valid_real.append(yv_real)\n",
    "        \n",
    "            #Second run. Training on (X, y) and predicting on X_test.\n",
    "            clf.fit(X_real.as_matrix(), y_real)\n",
    "            yt_real = clf.predict_proba(X_test_real.as_matrix())\n",
    "            p_test_real.append(yt_real)\n",
    "    \n",
    "    else:\n",
    "        #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "        clf.fit(X_train, y_train)\n",
    "        yv = clf.predict_proba(X_valid)\n",
    "        p_valid.append(yv)\n",
    "        \n",
    "        #Second run. Training on (X, y) and predicting on X_test.\n",
    "        clf.fit(X, y)\n",
    "        yt = clf.predict_proba(X_test)\n",
    "        p_test.append(yt)\n",
    "        \n",
    "        if for_real:\n",
    "            #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "            clf.fit(X_train_real, y_train_real)\n",
    "            yv_real = clf.predict_proba(X_valid_real)\n",
    "            p_valid_real.append(yv_real)\n",
    "        \n",
    "            #Second run. Training on (X, y) and predicting on X_test.\n",
    "            clf.fit(X_real, y_real)\n",
    "            yt_real = clf.predict_proba(X_test_real)\n",
    "            p_test_real.append(yt_real)\n",
    "       \n",
    "    #Printing out the performance of the classifier\n",
    "    print('{:10s} {:2s} {:1.7f}'.format('%s: ' %(nm), 'logloss  =>', log_loss(y_test, yt)))\n",
    "print('')\n",
    "\n",
    "\n",
    "#when running the full data\n",
    "#take out the logloss function... or alternatively, run both the split data and full data model so that\n",
    "#i can compare my training logloss vs kaggle logloss\n",
    "\n",
    "\n",
    "#maybe build a system that tries base models, shakes random states in order to find uncorrelated models?\n",
    "#don't know why but my bayes_opt script is giving a worse score than untuned model (default params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of optimization based ensemblers (2nd layer) on X_test\n",
      "------------------------------------------------------------\n",
      "EN_optA:             logloss  => 0.6920534\n",
      "Calibrated_EN_optA:  logloss  => 0.6921747\n",
      "EN_optB:             logloss  => 0.6920597\n",
      "Calibrated_EN_optB:  logloss  => 0.6924957\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Performance of optimization based ensemblers (2nd layer) on X_test')   \n",
    "print('------------------------------------------------------------')\n",
    "    \n",
    "#Creating the data for the 2nd layer.\n",
    "XV = np.hstack(p_valid)\n",
    "XT = np.hstack(p_test)  \n",
    "\n",
    "\n",
    "#XV = p_valid.as_matrix()\n",
    "#XT = p_test.as_matrix()\n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "    \n",
    "#EN_optA\n",
    "enA = EN_optA(n_classes)\n",
    "enA.fit(XV, y_valid)\n",
    "w_enA = enA.w\n",
    "y_enA = enA.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA)))\n",
    "    \n",
    "#Calibrated version of EN_optA \n",
    "cc_optA = CalibratedClassifierCV(enA, method='isotonic')\n",
    "cc_optA.fit(XV, y_valid)\n",
    "y_ccA = cc_optA.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA)))\n",
    "        \n",
    "#EN_optB\n",
    "enB = EN_optB(n_classes) \n",
    "enB.fit(XV, y_valid)\n",
    "w_enB = enB.w\n",
    "y_enB = enB.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB)))\n",
    "\n",
    "#Calibrated version of EN_optB\n",
    "cc_optB = CalibratedClassifierCV(enB, method='isotonic')\n",
    "cc_optB.fit(XV, y_valid)\n",
    "y_ccB = cc_optB.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB)))\n",
    "print('')\n",
    "\n",
    "\n",
    "if for_real:\n",
    "    XV_real = np.hstack(p_valid_real)\n",
    "    XT_real = np.hstack(p_test_real) \n",
    "    \n",
    "    #EN_optA\n",
    "    enA_real = EN_optA(n_classes)\n",
    "    enA_real.fit(XV_real, y_valid_real)\n",
    "    w_enA_real = enA_real.w\n",
    "    y_enA_real = enA_real.predict_proba(XT_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA)))\n",
    "    \n",
    "    #Calibrated version of EN_optA \n",
    "    cc_optA_real = CalibratedClassifierCV(enA_real, method='isotonic')\n",
    "    cc_optA_real.fit(XV_real, y_valid_real)\n",
    "    y_ccA_real = cc_optA_real.predict_proba(XT_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA)))\n",
    "        \n",
    "    #EN_optB\n",
    "    enB_real = EN_optB(n_classes) \n",
    "    enB_real.fit(XV_real, y_valid_real)\n",
    "    w_enB_real = enB_real.w\n",
    "    y_enB_real = enB_real.predict_proba(XT_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB)))\n",
    "\n",
    "    #Calibrated version of EN_optB\n",
    "    cc_optB_real = CalibratedClassifierCV(enB_real, method='isotonic')\n",
    "    cc_optB_real.fit(XV_real, y_valid_real)\n",
    "    y_ccB_real = cc_optB_real.predict_proba(XT_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3rd_layer:           logloss  => 0.6921187\n",
      "3rd_layer:           logloss  => 0.6921004\n",
      "3rd_layer:           logloss  => 0.6920914\n",
      "3rd_layer:           logloss  => 0.6920411\n",
      "3rd_layer:           logloss  => 0.6920351\n",
      "3rd_layer:           logloss  => 0.6920240\n",
      "3rd_layer:           logloss  => 0.6920239\n",
      "3rd_layer:           logloss  => 0.6920233\n",
      "3rd_layer:           logloss  => 0.6920232\n",
      "3rd_layer:           logloss  => 0.6920230\n",
      "3rd_layer:           logloss  => 0.6920229\n"
     ]
    }
   ],
   "source": [
    "best_score = 10.0\n",
    "\n",
    "for i in range(10000):\n",
    "    first = random.randint(0,20)\n",
    "    second = random.randint(0,20)\n",
    "    third = random.randint(0,20)\n",
    "    fourth = random.randint(0,20)\n",
    "    total = first + second + third + fourth\n",
    "    first = first / (total * 1.0)\n",
    "    second = second / (total * 1.0)\n",
    "    third = third / (total * 1.0)\n",
    "    fourth = fourth / (total * 1.0)\n",
    "    \n",
    "    y_3l = (y_enA * first) + (y_ccA * second) + (y_enB * third) + (y_ccB * fourth)\n",
    "    current_score = log_loss(y_test, y_3l)\n",
    "    \n",
    "    if current_score < best_score:\n",
    "        print('{:20s} {:2s} {:1.7f}'.format('3rd_layer:', 'logloss  =>', log_loss(y_test, y_3l)))\n",
    "        #print first, second, third, fourth\n",
    "        best_score = current_score\n",
    "        best_first = first\n",
    "        best_second = second\n",
    "        best_third = third\n",
    "        best_fourth = fourth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = (y_enA_real * best_first) + \\\n",
    "        (y_ccA_real * best_second) + \\\n",
    "        (y_enB_real * best_third) + \\\n",
    "        (y_ccB_real * best_fourth)\n",
    "            \n",
    "pd.DataFrame({\"t_id\": t_id, \"probability\": preds[:,1]}).to_csv('3-level-calibrated-xgbc-extc.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def objf_ens_optA(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (12 in Airbnb competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optA, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA, x0, args=(Xs, y, self.n_class), \n",
    "                       method='SLSQP', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def objf_ens_optB(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 12\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optB, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB, x0, args=(Xs, y, self.n_class), \n",
    "                       method='L-BFGS-B', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
