{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Clearing...\n",
      "Data shape:\n",
      "X_train: (68592, 131), X_valid: (22864, 131), X_test: (22865, 131) \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "MEAN MODELS: Performance of individual classifiers (1st layer) on X_test\n",
      "------------------------------------------------------------------------\n",
      "Iteration: 0\n",
      "Iteration: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ca0a4b3bfda3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfor_real\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;31m#First run. Training on (X_train, y_train) and predicting on X_valid.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0myv_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mp_valid_clf_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myv_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    288\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 290\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    808\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0;31m# Stop dispatching any new job in the async callback thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s.wait(): got it\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print('Load data...')\n",
    "\n",
    "\n",
    "###################################################\n",
    "########     TO CHANGE    #########################\n",
    "###################################################\n",
    "\n",
    "DATA_DIR = \"/Desktop/Kaggle/BNP_Paribas/\"\n",
    "train = pd.read_csv(DATA_DIR + \"train.csv\")\n",
    "test = pd.read_csv(DATA_DIR + \"test.csv\")\n",
    "\n",
    "###################################################\n",
    "########     END OF CHANGE     ####################\n",
    "###################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "target = train['target'].values\n",
    "\n",
    "train = train.drop(['ID','target'],axis=1)\n",
    "id_test = test['ID'].values\n",
    "test = test.drop(['ID'],axis=1)\n",
    "\n",
    "print('Clearing...')\n",
    "for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(),test.iteritems()):\n",
    "    if train_series.dtype == 'O':\n",
    "        #for objects: factorize\n",
    "        train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "        test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "        #but now we have -1 values (NaN)\n",
    "    else:\n",
    "        #for int or float: fill NaN\n",
    "        tmp_len = len(train[train_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            #print \"mean\", train_series.mean()\n",
    "            train.loc[train_series.isnull(), train_name] = -9999 #train_series.mean()\n",
    "        #and Test\n",
    "        tmp_len = len(test[test_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            test.loc[test_series.isnull(), test_name] = -9999 #train_series.mean()  #TODO\n",
    "            \n",
    "\n",
    "            \n",
    "n_classes = 2 \n",
    "for_real = True\n",
    "\n",
    "#this is what i'll change when i run the whole data set...\n",
    "#essentially my train and test sets are already split\n",
    "\n",
    "#Spliting data into train and test sets.\n",
    "X, X_test, y, y_test = train_test_split(train, target, test_size=0.2)\n",
    "    \n",
    "#Spliting train data into training and validation sets.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "print('Data shape:')\n",
    "print('X_train: %s, X_valid: %s, X_test: %s \\n' %(X_train.shape, X_valid.shape, \n",
    "                                                  X_test.shape))\n",
    "\n",
    "if for_real:\n",
    "    #take the train, target and test data, and come up with a validation set from train\n",
    "    X_real = train\n",
    "    X_test_real = test\n",
    "    y_real = target\n",
    "    \n",
    "    X_train_real, X_valid_real, y_train_real, y_valid_real = train_test_split(X_real, y_real, test_size=0.25)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#Defining the classifiers\n",
    "#think about jittering the random state and then averaging the predictions together ...\n",
    "#only good for extra trees, RF?, native XGB, NN\n",
    "clfs = {#'LR'  : LogisticRegression(), \n",
    "        #'SVM' : SVC(probability=True, random_state=random_state), \n",
    "        #'RF'  : RandomForestClassifier(n_estimators=100, n_jobs=-1), \n",
    "        #'GBM' : GradientBoostingClassifier(n_estimators=50), \n",
    "        'ETC' : ExtraTreesClassifier(n_estimators=108, max_features=130, max_depth=12, n_jobs=-1),\n",
    "        #'KNN' : KNeighborsClassifier(n_neighbors=30)}\n",
    "        'XGBc': XGBClassifier(objective='binary:logistic',\n",
    "                              colsample_bytree=0.77638333498678636,\n",
    "                              learning_rate=0.030567867858705199,\n",
    "                              max_delta_step=4.6626180513766657,\n",
    "                              min_child_weight=57.354121041109124,\n",
    "                              n_estimators=478,\n",
    "                              subsample=0.8069399976204783,\n",
    "                              max_depth=6,\n",
    "                              gamma=0.2966938071810209)#,\n",
    "        #'NN'  : Pipeline([('min/max scaler', MinMaxScaler(feature_range=(-1.0, 1.0))),\n",
    "        #                  ('neural network', Classifier(layers=[Layer(\"Rectifier\", units=10),\n",
    "        #                                                        Layer(\"Tanh\", units=10),\n",
    "        #                                                        Layer(\"Softmax\")], \n",
    "        #                                                n_iter=5))])       \n",
    "       }\n",
    "\n",
    "\n",
    "print('------------------------------------------------------------------------')    \n",
    "print('MEAN MODELS: Performance of individual classifiers (1st layer) on X_test')   \n",
    "print('------------------------------------------------------------------------')\n",
    "p_valid_mean = []\n",
    "p_test_mean = []\n",
    "\n",
    "p_valid_mean_real = []\n",
    "p_test_mean_real = []\n",
    "\n",
    "for nm, clf in clfs.items():\n",
    "    p_valid_clf = []\n",
    "    p_test_clf = []\n",
    "    p_valid_clf_real = []\n",
    "    p_test_clf_real = []\n",
    "    \n",
    "    holder = []\n",
    "    \n",
    "    for i in range(250):\n",
    "        print \"Iteration: \" + str(i+1)\n",
    "        dummy = random.randint(1,10000)\n",
    "        x = True\n",
    "        while x == True:\n",
    "            if dummy in holder:\n",
    "                dummy = random.randint(1,10000)\n",
    "            else:\n",
    "                x = False\n",
    "        holder.append(dummy)\n",
    "    \n",
    "        random.seed(dummy)\n",
    "    \n",
    "        if nm == 'NN':\n",
    "            #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "            clf.fit(X_train.as_matrix(), y_train)\n",
    "            yv = clf.predict_proba(X_valid.as_matrix())\n",
    "            p_valid_clf.append(yv)\n",
    "        \n",
    "            #Second run. Training on (X, y) and predicting on X_test.\n",
    "            clf.fit(X.as_matrix(), y)\n",
    "            yt = clf.predict_proba(X_test.as_matrix())\n",
    "            p_test_clf.append(yt)\n",
    "        \n",
    "            if for_real:\n",
    "                #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "                clf.fit(X_train_real.as_matrix(), y_train_real)\n",
    "                yv_real = clf.predict_proba(X_valid_real.as_matrix())\n",
    "                p_valid_clf_real.append(yv_real)\n",
    "        \n",
    "                #Second run. Training on (X, y) and predicting on X_test.\n",
    "                clf.fit(X_real.as_matrix(), y_real)\n",
    "                yt_real = clf.predict_proba(X_test_real.as_matrix())\n",
    "                p_test_clf_real.append(yt_real)\n",
    "    \n",
    "        else:\n",
    "            #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "            clf.fit(X_train, y_train)\n",
    "            yv = clf.predict_proba(X_valid)\n",
    "            p_valid_clf.append(yv)\n",
    "        \n",
    "            #Second run. Training on (X, y) and predicting on X_test.\n",
    "            clf.fit(X, y)\n",
    "            yt = clf.predict_proba(X_test)\n",
    "            p_test_clf.append(yt)\n",
    "        \n",
    "            if for_real:\n",
    "                #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "                clf.fit(X_train_real, y_train_real)\n",
    "                yv_real = clf.predict_proba(X_valid_real)\n",
    "                p_valid_clf_real.append(yv_real)\n",
    "        \n",
    "                #Second run. Training on (X, y) and predicting on X_test.\n",
    "                clf.fit(X_real, y_real)\n",
    "                yt_real = clf.predict_proba(X_test_real)\n",
    "                p_test_clf_real.append(yt_real)\n",
    "            \n",
    "       \n",
    "    #Printing out the performance of the classifier\n",
    "    mean_pred_cv = np.mean(p_valid_clf, axis=0)\n",
    "    mean_pred_test = np.mean(p_test_clf, axis=0)\n",
    "    p_valid_mean.append(mean_pred_cv)\n",
    "    p_test_mean.append(mean_pred_test)\n",
    "    \n",
    "    mean_real_pred_cv = np.mean(p_valid_clf_real, axis=0)\n",
    "    mean_real_pred_test = np.mean(p_test_clf_real, axis=0)\n",
    "    p_valid_mean_real.append(mean_real_pred_cv)\n",
    "    p_test_mean_real.append(mean_real_pred_test)\n",
    "    \n",
    "    print('{:10s} {:2s} {:1.7f}'.format('%s - mean: ' %(nm), 'logloss  =>', log_loss(y_test, mean_pred_test)))\n",
    "    \n",
    "print('')\n",
    "\n",
    "#also try setting different parameters for the XGB and add a NN to the mix\n",
    "#either use bayesopt for each classifier and putting those into this model, -or- \n",
    "#randomize both parameters and by random_state... i could do several loops here\n",
    "#lots of comp time but each random_state run a series of different parameters and\n",
    "#take the average result for that particular random_state, then run the same\n",
    "#parameters on the next random_state (don't want random X random as that is hard to replicate)...\n",
    "#too many combos... let's do bayes_opt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.DataFrame({\"p_valid_mean_0_0\": p_valid_mean[0][:,0],\n",
    "              \"p_valid_mean_0_1\": p_valid_mean[0][:,1],\n",
    "              \"p_valid_mean_1_0\": p_valid_mean[1][:,0],\n",
    "              \"p_valid_mean_1_1\": p_valid_mean[1][:,1]\n",
    "             }).to_csv('p_valid_mean_preds_for_EXT_and_XGBc_mean_models.csv',index=False)\n",
    "\n",
    "pd.DataFrame({\"p_test_mean_0_0\" : p_test_mean[0][:,0],\n",
    "              \"p_test_mean_0_1\" : p_test_mean[0][:,1],\n",
    "              \"p_test_mean_1_0\" : p_test_mean[1][:,0],\n",
    "              \"p_test_mean_1_1\" : p_test_mean[1][:,1]\n",
    "             }).to_csv('p_test_mean_preds_for_EXT_and_XGBc_mean_models.csv',index=False)\n",
    "\n",
    "pd.DataFrame({\"p_valid_mean_real_0_0\": p_valid_mean_real[0][:,0],\n",
    "              \"p_valid_mean_real_0_1\": p_valid_mean_real[0][:,1],\n",
    "              \"p_valid_mean_real_1_0\": p_valid_mean_real[1][:,0],\n",
    "              \"p_valid_mean_real_1_1\": p_valid_mean_real[1][:,1]\n",
    "             }).to_csv('p_valid_mean_real_preds_for_EXT_and_XGBc_mean_models.csv',index=False)\n",
    "\n",
    "pd.DataFrame({\"p_test_mean_real_0_0\" : p_test_mean_real[0][:,0],\n",
    "              \"p_test_mean_real_0_1\" : p_test_mean_real[0][:,1],\n",
    "              \"p_test_mean_real_1_0\" : p_test_mean_real[1][:,0],\n",
    "              \"p_test_mean_real_1_1\" : p_test_mean_real[1][:,1]\n",
    "             }).to_csv('p_test_mean_real_preds_for_EXT_and_XGBc_mean_models.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
