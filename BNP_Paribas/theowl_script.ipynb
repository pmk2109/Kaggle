{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ensemble Model: ', 'etc', ' Best CV score: ', -0.47004251022039067, ' Time: ', 6.71)\n",
      "('Ensemble Model: ', 'etc', ' Time: ', 9.23)\n",
      "('Ensemble Model: ', 'etc', ' Time: ', 17.42)\n",
      "('Ensemble Model: ', 'etc', ' Time: ', 22.02)\n",
      "('Ensemble Model: ', 'etc', ' Best CV score: ', -0.46794681173707342, ' Time: ', 26.71)\n",
      "('Ensemble Model: ', 'etc', ' Time: ', 29.25)\n"
     ]
    }
   ],
   "source": [
    "#scikit learn ensembe workflow for binary probability\n",
    "import time; start_time = time.time()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import random; random.seed(2016)\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "DATA_DIR = \"/Users/patrickkennedy/Desktop/Data_Science_MISC/Kaggle\"\n",
    "TRAIN_PATH = \"/BNP_Paribas/train.csv\"\n",
    "TEST_PATH = \"/BNP_Paribas/test.csv\"\n",
    "\n",
    "train, test, target = load_and_process(DATA_DIR, TRAIN_PATH, TEST_PATH)\n",
    "\n",
    "#g={'ne':150,'md':6,'mf':80,'rs':2016} #change to g={'ne':500,'md':40,'mf':60,'rs':2016}\n",
    "g={'ne':500,'md':40,'mf':60,'rs':2016}\n",
    "etc = ensemble.ExtraTreesClassifier(n_estimators=g['ne'], max_depth=g['md'], max_features=g['mf'], random_state=g['rs'], criterion='entropy', min_samples_split= 4, min_samples_leaf= 2, verbose = 0, n_jobs =-1)      \n",
    "etr = ensemble.ExtraTreesRegressor(n_estimators=g['ne'], max_depth=g['md'], max_features=g['mf'], random_state=g['rs'], min_samples_split= 4, min_samples_leaf= 2, verbose = 0, n_jobs =-1)      \n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=g['ne'], max_depth=g['md'], max_features=g['mf'], random_state=g['rs'], criterion='entropy', min_samples_split= 4, min_samples_leaf= 2, verbose = 0, n_jobs =-1)\n",
    "rfr = ensemble.RandomForestRegressor(n_estimators=g['ne'], max_depth=g['md'], max_features=g['mf'], random_state=g['rs'], min_samples_split= 4, min_samples_leaf= 2, verbose = 0, n_jobs =-1)\n",
    "xgr = xgb.XGBRegressor(n_estimators=g['ne'], max_depth=g['md'], seed=g['rs'], missing=np.nan, learning_rate=0.02, subsample=0.9, colsample_bytree=0.85, objective='reg:linear')\n",
    "xgc = xgb.XGBClassifier(n_estimators=g['ne'], max_depth=g['md'], seed=g['rs'], missing=np.nan, learning_rate=0.02, subsample=0.9, colsample_bytree=0.85, objective='binary:logistic') #try 'binary:logitraw'\n",
    "#clf = {'etc':etc, 'etr':etr, 'rfc':rfc, 'rfr':rfr, 'xgr':xgr, 'xgc':xgc} # use this line instead\n",
    "clf = {'etc':etc, 'xgc':xgc} # removed due to kaggle performance, would prefer less time and more cores than more time and less cores :)\n",
    "\n",
    "\n",
    "p_valid, p_test, p_valid_real, p_test_real = run_classifiers(train, target, test, clf, 2)\n",
    "      \n",
    "\n",
    "#what about the flog_loss params? uncomment?\n",
    "#can i condense more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of optimization based ensemblers (2nd layer) on X_test\n",
      "------------------------------------------------------------\n",
      "EN_optA:             logloss  => 0.4641693\n",
      "Calibrated_EN_optA:  logloss  => 0.4648965\n",
      "EN_optB:             logloss  => 0.4633164\n",
      "Calibrated_EN_optB:  logloss  => 0.4751522\n",
      "\n",
      "REAL: Performance of optimization based ensemblers (2nd layer) on X_test\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "3rd_layer:           logloss  => 0.4642404\n",
      "3rd_layer:           logloss  => 0.4629070\n",
      "3rd_layer:           logloss  => 0.4628577\n",
      "3rd_layer:           logloss  => 0.4628372\n",
      "3rd_layer:           logloss  => 0.4628219\n",
      "3rd_layer:           logloss  => 0.4628186\n",
      "3rd_layer:           logloss  => 0.4628052\n",
      "3rd_layer:           logloss  => 0.4627997\n",
      "3rd_layer:           logloss  => 0.4627934\n"
     ]
    }
   ],
   "source": [
    "preds = optimizer(p_valid, p_test, y_valid, p_valid_real, p_test_real, y_valid_real, y_test, for_real)\n",
    "pd.DataFrame({\"ID\": id_test, \"PredictedProb\": preds[:,1]}).to_csv('owls_script_better_params.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3rd layer gives .462, kaggle score is .464"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def objf_ens_optA(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (12 in Airbnb competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optA, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA, x0, args=(Xs, y, self.n_class), \n",
    "                       method='SLSQP', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def objf_ens_optB(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 12\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optB, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB, x0, args=(Xs, y, self.n_class), \n",
    "                       method='L-BFGS-B', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimizer(p_valid, p_test, y_valid, p_valid_real, p_test_real, y_valid_real, y_test, for_real):\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "    print('Performance of optimization based ensemblers (2nd layer) on X_test')   \n",
    "    print('------------------------------------------------------------')\n",
    "    \n",
    "    #Creating the data for the 2nd layer.\n",
    "    XV = np.hstack(p_valid)\n",
    "    XT = np.hstack(p_test)  \n",
    "\n",
    "    #XV = np.array(p_valid)\n",
    "    #XT = np.array(p_test)\n",
    "\n",
    "    n_classes = 2\n",
    "\n",
    "    #EN_optA\n",
    "    enA = EN_optA(n_classes)\n",
    "    enA.fit(XV, y_valid)\n",
    "    w_enA = enA.w\n",
    "    y_enA = enA.predict_proba(XT)\n",
    "    print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA)))\n",
    "    \n",
    "    #Calibrated version of EN_optA \n",
    "    cc_optA = CalibratedClassifierCV(enA, method='isotonic')\n",
    "    cc_optA.fit(XV, y_valid)\n",
    "    y_ccA = cc_optA.predict_proba(XT)\n",
    "    print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA)))\n",
    "        \n",
    "    #EN_optB\n",
    "    enB = EN_optB(n_classes) \n",
    "    enB.fit(XV, y_valid)\n",
    "    w_enB = enB.w\n",
    "    y_enB = enB.predict_proba(XT)\n",
    "    print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB)))\n",
    "\n",
    "    #Calibrated version of EN_optB\n",
    "    cc_optB = CalibratedClassifierCV(enB, method='isotonic')\n",
    "    cc_optB.fit(XV, y_valid)\n",
    "    y_ccB = cc_optB.predict_proba(XT)  \n",
    "    print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB)))\n",
    "    print('')\n",
    "\n",
    "\n",
    "    if for_real:\n",
    "        print('REAL: Performance of optimization based ensemblers (2nd layer) on X_test')   \n",
    "        print('------------------------------------------------------------')\n",
    "    \n",
    "        #Creating the data for the 2nd layer.\n",
    "        XV_real = np.hstack(p_valid_real)\n",
    "        XT_real = np.hstack(p_test_real)  \n",
    "\n",
    "        n_classes = 2\n",
    "\n",
    "        #EN_optA\n",
    "        enA_real = EN_optA(n_classes)\n",
    "        enA_real.fit(XV_real, y_valid_real)\n",
    "        w_enA_real = enA_real.w\n",
    "        y_enA_real = enA_real.predict_proba(XT_real)\n",
    "        #print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA)))\n",
    "    \n",
    "        #Calibrated version of EN_optA \n",
    "        cc_optA_real = CalibratedClassifierCV(enA_real, method='isotonic')\n",
    "        cc_optA_real.fit(XV_real, y_valid_real)\n",
    "        y_ccA_real = cc_optA_real.predict_proba(XT_real)\n",
    "        #print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA)))\n",
    "        \n",
    "        #EN_optB\n",
    "        enB_real = EN_optB(n_classes) \n",
    "        enB_real.fit(XV_real, y_valid_real)\n",
    "        w_enB_real = enB_real.w\n",
    "        y_enB_real = enB_real.predict_proba(XT_real)\n",
    "        #print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB)))\n",
    "\n",
    "        #Calibrated version of EN_optB\n",
    "        cc_optB_real = CalibratedClassifierCV(enB_real, method='isotonic')\n",
    "        cc_optB_real.fit(XV_real, y_valid_real)\n",
    "        y_ccB_real = cc_optB_real.predict_proba(XT_real)  \n",
    "        #print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB)))\n",
    "    print('')\n",
    "    print('')\n",
    "        \n",
    "        \n",
    "    #optimize weighting of the 3rd level - keep the same weighting for real data\n",
    "    best_score = 10.0\n",
    "\n",
    "    for i in range(10000):\n",
    "        first = random.randint(0,20)\n",
    "        second = random.randint(0,20)\n",
    "        third = random.randint(0,20)\n",
    "        fourth = random.randint(0,20)\n",
    "        total = first + second + third + fourth\n",
    "        first = first / (total * 1.0)\n",
    "        second = second / (total * 1.0)\n",
    "        third = third / (total * 1.0)\n",
    "        fourth = fourth / (total * 1.0)\n",
    "    \n",
    "        y_3l = (y_enA * first) + (y_ccA * second) + (y_enB * third) + (y_ccB * fourth)\n",
    "        current_score = log_loss(y_test, y_3l)\n",
    "    \n",
    "        if current_score < best_score:\n",
    "            print('{:20s} {:2s} {:1.7f}'.format('3rd_layer:', 'logloss  =>', log_loss(y_test, y_3l)))\n",
    "            #print first, second, third, fourth\n",
    "            best_score = current_score\n",
    "            best_first = first\n",
    "            best_second = second\n",
    "            best_third = third\n",
    "            best_fourth = fourth\n",
    "            \n",
    "    preds = (y_enA_real * first) + (y_ccA_real * second) + (y_enB_real * third) + (y_ccB_real * fourth)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_classifiers(train, target, test, clf, jitters):\n",
    "    y_pred=[]\n",
    "    p_valid = []\n",
    "    p_test = []\n",
    "\n",
    "    p_valid_real = []\n",
    "    p_test_real = []\n",
    "    \n",
    "    for c in clf:\n",
    "        holder = []\n",
    "        for i in xrange(jitters):\n",
    "            dummy = random.randint(1,10000)\n",
    "            x = True\n",
    "            while x == True:\n",
    "                if dummy in holder:\n",
    "                    dummy = random.randint(1,10000)\n",
    "                else:\n",
    "                    x = False\n",
    "            holder.append(dummy)\n",
    "            random.seed(dummy)\n",
    "            \n",
    "            \n",
    "            X, X_test, y, y_test = train_test_split(train, target, test_size=0.33, random_state=dummy) #Spliting data into train and test sets.\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=dummy) #Spliting train data into training and validation sets.\n",
    "\n",
    "            X_real = train\n",
    "            X_test_real = test\n",
    "            y_real = target\n",
    "            X_train_real, X_valid_real, y_train_real, y_valid_real = train_test_split(X_real, y_real, test_size=0.33, random_state=dummy)\n",
    "            LL  = make_scorer(flog_loss, greater_is_better=False)\n",
    "            \n",
    "            if c[:1] != \"x\": #not xgb\n",
    "                model = GridSearchCV(estimator=clf[c], param_grid={}, n_jobs =-1, cv=2, verbose=0, scoring=LL, random_state=dummy)\n",
    "                model.fit(X, y)\n",
    "                best_score = (log_loss(y_test, model.predict_proba(X_test)))*-1\n",
    "                p_test.append(model.predict_proba(X_test))\n",
    "                print(\"Ensemble Model: \", c, \" Best CV score: \", best_score, \" Time: \", round(((time.time() - start_time)/60),2))\n",
    "    \n",
    "                model.fit(X_train, y_train)\n",
    "                p_valid.append(model.predict_proba(X_valid))\n",
    "                print(\"Ensemble Model: \", c, \" Time: \", round(((time.time() - start_time)/60),2))\n",
    "        \n",
    "                model.fit(X_real, y_real)\n",
    "                p_test_real.append(model.predict_proba(X_test_real))\n",
    "                print(\"Ensemble Model: \", c, \" Time: \", round(((time.time() - start_time)/60),2))\n",
    "    \n",
    "                model.fit(X_train_real, y_train_real)\n",
    "                p_valid_real.append(model.predict_proba(X_valid_real))\n",
    "                print(\"Ensemble Model: \", c, \" Time: \", round(((time.time() - start_time)/60),2))\n",
    "        \n",
    "            else: #xgb\n",
    "                X_fit, X_eval, y_fit, y_eval= train_test_split(X, y, test_size=0.35, train_size=0.65, random_state=dummy)\n",
    "                model = clf[c]\n",
    "                model.fit(X_fit, y_fit.values, early_stopping_rounds=20, eval_metric=\"logloss\", eval_set=[(X_eval, y_eval)], verbose=0)\n",
    "                best_score = (log_loss(y_test, model.predict_proba(X_test)))*-1\n",
    "                p_test.append(model.predict_proba(X_test))\n",
    "                print(\"Ensemble Model: \", c, \" Best CV score: \", best_score, \" Time: \", round(((time.time() - start_time)/60),2))\n",
    "\n",
    "                X_fit, X_eval, y_fit, y_eval= train_test_split(X_train, y_train, test_size=0.35, train_size=0.65, random_state=g['rs'])\n",
    "                model = clf[c]\n",
    "                model.fit(X_fit, y_fit.values, early_stopping_rounds=20, eval_metric=\"logloss\", eval_set=[(X_eval, y_eval)], verbose=0)\n",
    "                p_valid.append(model.predict_proba(X_valid))\n",
    "                print(\"Ensemble Model: \", c, \" Time: \", round(((time.time() - start_time)/60),2))\n",
    "        \n",
    "                X_fit, X_eval, y_fit, y_eval= train_test_split(X_real, y_real, test_size=0.35, train_size=0.65, random_state=g['rs'])\n",
    "                model = clf[c]\n",
    "                model.fit(X_fit, y_fit.values, early_stopping_rounds=20, eval_metric=\"logloss\", eval_set=[(X_eval, y_eval)], verbose=0)\n",
    "                p_test_real.append(model.predict_proba(X_test_real))\n",
    "                print(\"Ensemble Model: \", c, \" Time: \", round(((time.time() - start_time)/60),2))        \n",
    "    \n",
    "                X_fit, X_eval, y_fit, y_eval= train_test_split(X_train_real, y_train_real, test_size=0.35, train_size=0.65, random_state=g['rs'])\n",
    "                model = clf[c]\n",
    "                model.fit(X_fit, y_fit.values, early_stopping_rounds=20, eval_metric=\"logloss\", eval_set=[(X_eval, y_eval)], verbose=0)\n",
    "                p_valid_real.append(model.predict_proba(X_valid_real))\n",
    "                print(\"Ensemble Model: \", c, \" Time: \", round(((time.time() - start_time)/60),2))    \n",
    "    \n",
    "    return p_valid, p_test, p_valid_real, p_test_real\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_and_process(DATA_DIR, TRAIN_PATH, TEST_PATH):\n",
    "\n",
    "    train = pd.read_csv(DATA_DIR + TRAIN_PATH)\n",
    "    test = pd.read_csv(DATA_DIR + TEST_PATH)\n",
    "\n",
    "    target = train['target']\n",
    "    train = train.drop(['target'],axis=1)\n",
    "    id_test = test['ID']\n",
    "    num_train = train.shape[0]\n",
    "\n",
    "    def fill_nan_null(val):\n",
    "        ret_fill_nan_null = 0.0\n",
    "        if val == True:\n",
    "            ret_fill_nan_null = 1.0\n",
    "        return ret_fill_nan_null\n",
    "\n",
    "    df_all = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "    df_all['null_count'] = df_all.isnull().sum(axis=1).tolist()\n",
    "    df_all_temp = df_all['ID']\n",
    "    df_all = df_all.drop(['ID'],axis=1)\n",
    "    df_data_types = df_all.dtypes[:] #{'object':0,'int64':0,'float64':0,'datetime64':0}\n",
    "    d_col_drops = []\n",
    "\n",
    "    for i in range(len(df_data_types)):\n",
    "        df_all[str(df_data_types.index[i])+'_nan_'] = df_all[str(df_data_types.index[i])].map(lambda x:fill_nan_null(pd.isnull(x)))\n",
    "    df_all = df_all.fillna(-9999)\n",
    "    #df_all = df_all.replace(0, -9999)\n",
    "\n",
    "    for i in range(len(df_data_types)):\n",
    "        if str(df_data_types[i])=='object':\n",
    "            df_u = pd.unique(df_all[str(df_data_types.index[i])].ravel())\n",
    "            #print(\"Column: \", str(df_data_types.index[i]), \" Length: \", len(df_u))\n",
    "            d={}\n",
    "            j = 1000\n",
    "            for s in df_u:\n",
    "                d[str(s)]=j\n",
    "                j+=5\n",
    "            df_all[str(df_data_types.index[i])+'_vect_'] = df_all[str(df_data_types.index[i])].map(lambda x:d[str(x)])\n",
    "            d_col_drops.append(str(df_data_types.index[i]))\n",
    "            if len(df_u)<150:\n",
    "                dummies = pd.get_dummies(df_all[str(df_data_types.index[i])]).rename(columns=lambda x: str(df_data_types.index[i]) + '_' + str(x))\n",
    "                df_all_temp = pd.concat([df_all_temp, dummies], axis=1)\n",
    "\n",
    "    df_all_temp = df_all_temp.drop(['ID'],axis=1)\n",
    "    df_all = pd.concat([df_all, df_all_temp], axis=1)\n",
    "    #print(len(df_all), len(df_all.columns))\n",
    "    #df_all.to_csv(\"df_all.csv\")\n",
    "    train = df_all.iloc[:num_train]\n",
    "    test = df_all.iloc[num_train:]\n",
    "    train = train.drop(d_col_drops,axis=1)\n",
    "    test = test.drop(d_col_drops,axis=1)\n",
    "    \n",
    "    return train, test, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flog_loss(ground_truth, predictions):\n",
    "    flog_loss_ = log_loss(ground_truth, predictions) #, eps=1e-15, normalize=True, sample_weight=None)\n",
    "    return flog_loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
