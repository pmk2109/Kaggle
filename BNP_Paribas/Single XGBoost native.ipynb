{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Clearing...\n"
     ]
    }
   ],
   "source": [
    "print('Load data...')\n",
    "DATA_DIR = \"/Users/patrickkennedy/Desktop/Data_Science_MISC/Kaggle\"\n",
    "train = pd.read_csv(DATA_DIR + \"/BNP_Paribas/train.csv\")\n",
    "test = pd.read_csv(DATA_DIR + \"/BNP_Paribas/test.csv\")\n",
    "\n",
    "target = train['target'].values\n",
    "\n",
    "train = train.drop(['ID','target'],axis=1)\n",
    "id_test = test['ID'].values\n",
    "test = test.drop(['ID'],axis=1)\n",
    "\n",
    "print('Clearing...')\n",
    "for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(),test.iteritems()):\n",
    "    if train_series.dtype == 'O':\n",
    "        #for objects: factorize\n",
    "        train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "        test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "        #but now we have -1 values (NaN)\n",
    "    else:\n",
    "        #for int or float: fill NaN\n",
    "        tmp_len = len(train[train_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            #print \"mean\", train_series.mean()\n",
    "            train.loc[train_series.isnull(), train_name] = -9999 #train_series.mean()\n",
    "        #and Test\n",
    "        tmp_len = len(test[test_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            test.loc[test_series.isnull(), test_name] = -9999 #train_series.mean()  #TODO\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for_real=True   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of individual classifiers (1st layer) on X_test\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Defining the classifiers\n",
    "clfs = {#'LR'  : LogisticRegression(), \n",
    "        #'SVM' : SVC(probability=True, random_state=random_state), \n",
    "        #'RF'  : RandomForestClassifier(n_estimators=100, n_jobs=-1), \n",
    "        #'GBM' : GradientBoostingClassifier(n_estimators=50), \n",
    "        'ETC' : ExtraTreesClassifier(n_estimators=500, \n",
    "                                     max_features=60, \n",
    "                                     max_depth=40, \n",
    "                                     criterion='entropy', \n",
    "                                     min_samples_split= 4, \n",
    "                                     min_samples_leaf= 2, \n",
    "                                     verbose = 0, \n",
    "                                     n_jobs =-1),\n",
    "        #'KNN' : KNeighborsClassifier(n_neighbors=30)}\n",
    "        'XGBc': XGBClassifier(objective='binary:logistic')#,\n",
    "                              #colsample_bytree=0.77638333498678636,\n",
    "                              #learning_rate=0.030567867858705199,\n",
    "                              #max_delta_step=4.6626180513766657,\n",
    "                              #min_child_weight=57.354121041109124,\n",
    "                              #n_estimators=478,\n",
    "                              #subsample=0.8069399976204783,\n",
    "                              #max_depth=6,\n",
    "                              #gamma=0.2966938071810209)#,\n",
    "        #'NN'  : Pipeline([('min/max scaler', MinMaxScaler(feature_range=(-1.0, 1.0))),\n",
    "        #                  ('neural network', Classifier(layers=[Layer(\"Rectifier\", units=10),\n",
    "        #                                                        Layer(\"Tanh\", units=10),\n",
    "        #                                                        Layer(\"Softmax\")], \n",
    "        #                                                n_iter=5))])       \n",
    "       }\n",
    "\n",
    "\n",
    "p_valid, p_test, y_valid, p_valid_real, p_test_real, y_valid_real = run_classifiers(train, target, test, clfs, 1)\n",
    "\n",
    "\n",
    "\n",
    "#.4881511 - untuned xgboost\n",
    "#.4674201 - tuned\n",
    "#.4662923 - tuned 500 rounds\n",
    "#.4775601 - tuned 5000 rounds\n",
    "#---------------\n",
    "#.4662923 - tuned 500 rounds - add eval_metric:logloss to params\n",
    "# - tuned 500 rounds - 100 jitters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e95815718a7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize_and_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_valid_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_test_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"ID\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mid_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PredictedProb\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xgb_native_with_owl_etc_params.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_valid' is not defined"
     ]
    }
   ],
   "source": [
    "preds = optimize_and_score(p_valid, p_test, y_valid, p_valid_real, p_test_real, y_valid_real)\n",
    "pd.DataFrame({\"ID\": id_test, \"PredictedProb\": preds[:,1]}).to_csv('xgb_native_with_owl_etc_params.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3rd layer gives .549, kaggle gives .537"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_and_score(p_valid, p_test, y_valid, p_valid_real, p_test_real, y_valid_real):\n",
    "    \n",
    "    \n",
    "    p_valid_df = pd.DataFrame()\n",
    "    p_valid_df[\"etc-0\"] = p_valid[0][:,0]\n",
    "    p_valid_df[\"etc-1\"] = p_valid[0][:,1]\n",
    "    p_valid_df[\"xgb-1\"] = p_valid[1]\n",
    "    p_valid_df[\"xgb-0\"] = 1-p_valid_df[\"xgb-1\"]\n",
    "\n",
    "    p_test_df = pd.DataFrame()\n",
    "    p_test_df[\"etc-0\"] = p_test[0][:,0]\n",
    "    p_test_df[\"etc-1\"] = p_test[0][:,1]\n",
    "    p_test_df[\"xgb-1\"] = p_test[1]\n",
    "    p_test_df[\"xgb-0\"] = 1-p_test_df[\"xgb-1\"]\n",
    "\n",
    "    p_valid_real_df = pd.DataFrame()\n",
    "    p_valid_real_df[\"etc-0\"] = p_valid_real[0][:,0]\n",
    "    p_valid_real_df[\"etc-1\"] = p_valid_real[0][:,1]\n",
    "    p_valid_real_df[\"xgb-1\"] = p_valid_real[1]\n",
    "    p_valid_real_df[\"xgb-0\"] = 1-p_valid_real_df[\"xgb-1\"]\n",
    "\n",
    "    p_test_real_df = pd.DataFrame()\n",
    "    p_test_real_df[\"etc-0\"] = p_test_real[0][:,0]\n",
    "    p_test_real_df[\"etc-1\"] = p_test_real[0][:,1]\n",
    "    p_test_real_df[\"xgb-1\"] = p_test_real[1]\n",
    "    p_test_real_df[\"xgb-0\"] = 1-p_test_real_df[\"xgb-1\"]\n",
    "\n",
    "    cols = p_valid_df.columns\n",
    "    cols = cols[:2] | cols[3:] | cols[2:3]\n",
    "    p_valid_df = p_valid_df[cols]\n",
    "    p_test_df = p_test_df[cols]\n",
    "    p_valid_real_df = p_valid_real_df[cols]\n",
    "    p_test_real_df = p_test_real_df[cols]\n",
    "\n",
    "    \n",
    "    \n",
    "    print('Performance of optimization based ensemblers (2nd layer) on X_test')   \n",
    "    print('------------------------------------------------------------')\n",
    "    \n",
    "    #Creating the data for the 2nd layer.\n",
    "    #XV = np.hstack(p_valid)\n",
    "    #XT = np.hstack(p_test)  \n",
    "\n",
    "    XV = p_valid_df.as_matrix()\n",
    "    XT = p_test_df.as_matrix()\n",
    "\n",
    "    n_classes = 2\n",
    "\n",
    "    #EN_optA\n",
    "    enA = EN_optA(n_classes)\n",
    "    enA.fit(XV, y_valid)\n",
    "    w_enA = enA.w\n",
    "    y_enA = enA.predict_proba(XT)\n",
    "    print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA)))\n",
    "    \n",
    "    #Calibrated version of EN_optA \n",
    "    cc_optA = CalibratedClassifierCV(enA, method='isotonic')\n",
    "    cc_optA.fit(XV, y_valid)\n",
    "    y_ccA = cc_optA.predict_proba(XT)\n",
    "    print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA)))\n",
    "        \n",
    "    #EN_optB\n",
    "    enB = EN_optB(n_classes) \n",
    "    enB.fit(XV, y_valid)\n",
    "    w_enB = enB.w\n",
    "    y_enB = enB.predict_proba(XT)\n",
    "    print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB)))\n",
    "\n",
    "    #Calibrated version of EN_optB\n",
    "    cc_optB = CalibratedClassifierCV(enB, method='isotonic')\n",
    "    cc_optB.fit(XV, y_valid)\n",
    "    y_ccB = cc_optB.predict_proba(XT)  \n",
    "    print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB)))\n",
    "    print('')\n",
    "\n",
    "\n",
    "    print('REAL: Performance of optimization based ensemblers (2nd layer) on X_test')   \n",
    "    print('------------------------------------------------------------')\n",
    "    \n",
    "    #Creating the data for the 2nd layer.\n",
    "    #XV_real = np.hstack(p_valid_real)\n",
    "    #XT_real = np.hstack(p_test_real)  \n",
    "    \n",
    "    XV_real = p_valid_real_df.as_matrix()\n",
    "    XT_real = p_test_real_df.as_matrix()\n",
    "    \n",
    "    n_classes = 2\n",
    "\n",
    "    #EN_optA\n",
    "    enA_real = EN_optA(n_classes)\n",
    "    enA_real.fit(XV_real, y_valid_real)\n",
    "    w_enA_real = enA_real.w\n",
    "    y_enA_real = enA_real.predict_proba(XT_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('EN_optA:', 'logloss  =>', log_loss(y_test, y_enA)))\n",
    "    \n",
    "    #Calibrated version of EN_optA \n",
    "    cc_optA_real = CalibratedClassifierCV(enA_real, method='isotonic')\n",
    "    cc_optA_real.fit(XV_real, y_valid_real)\n",
    "    y_ccA_real = cc_optA_real.predict_proba(XT_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optA:', 'logloss  =>', log_loss(y_test, y_ccA)))\n",
    "        \n",
    "    #EN_optB\n",
    "    enB_real = EN_optB(n_classes) \n",
    "    enB_real.fit(XV_real, y_valid_real)\n",
    "    w_enB_real = enB_real.w\n",
    "    y_enB_real = enB_real.predict_proba(XT_real)\n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('EN_optB:', 'logloss  =>', log_loss(y_test, y_enB)))\n",
    "\n",
    "    #Calibrated version of EN_optB\n",
    "    cc_optB_real = CalibratedClassifierCV(enB_real, method='isotonic')\n",
    "    cc_optB_real.fit(XV_real, y_valid_real)\n",
    "    y_ccB_real = cc_optB_real.predict_proba(XT_real)  \n",
    "    #print('{:20s} {:2s} {:1.7f}'.format('Calibrated_EN_optB:', 'logloss  =>', log_loss(y_test, y_ccB)))\n",
    "    #print('')\n",
    "    \n",
    "    #ummm why am i optimizing on y_valid_real??? why not y_real?????\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #optimize weighting of the 3rd level - keep the same weighting for real data\n",
    "    best_score = 10.0\n",
    "\n",
    "    for i in range(10000):\n",
    "        first = random.randint(0,20)\n",
    "        second = random.randint(0,20)\n",
    "        third = random.randint(0,20)\n",
    "        fourth = random.randint(0,20)\n",
    "        total = first + second + third + fourth\n",
    "        first = first / (total * 1.0)\n",
    "        second = second / (total * 1.0)\n",
    "        third = third / (total * 1.0)\n",
    "        fourth = fourth / (total * 1.0)\n",
    "    \n",
    "        y_3l = (y_enA * first) + (y_ccA * second) + (y_enB * third) + (y_ccB * fourth)\n",
    "        current_score = log_loss(y_test, y_3l)\n",
    "    \n",
    "        if current_score < best_score:\n",
    "            print('{:20s} {:2s} {:1.7f}'.format('3rd_layer:', 'logloss  =>', log_loss(y_test, y_3l)))\n",
    "            #print first, second, third, fourth\n",
    "            best_score = current_score\n",
    "            best_first = first\n",
    "            best_second = second\n",
    "            best_third = third\n",
    "            best_fourth = fourth\n",
    "            \n",
    "            \n",
    "    preds = (best_first * y_enA_real) + (best_second * y_ccA_real) + \\\n",
    "            (best_third * y_enB_real) + (best_fourth * y_ccB_real) \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_classifiers(train, target, test, clfs, jitters):\n",
    "    #predictions on the validation and test sets\n",
    "    p_valid = []\n",
    "    p_test = []\n",
    "\n",
    "    p_valid_real = []\n",
    "    p_test_real = []\n",
    "  \n",
    "    holder = []\n",
    "\n",
    "    print('Performance of individual classifiers (1st layer) on X_test')   \n",
    "    print('------------------------------------------------------------')\n",
    "   \n",
    "    for nm, clf in clfs.items():\n",
    "        for i in xrange(jitters):\n",
    "            dummy = random.randint(1,10000)\n",
    "            x = True\n",
    "            while x == True:\n",
    "                if dummy in holder:\n",
    "                    dummy = random.randint(1,10000)\n",
    "                else:\n",
    "                    x = False\n",
    "            holder.append(dummy)\n",
    "    \n",
    "            random.seed(dummy)\n",
    "        \n",
    "            #Spliting data into train and test sets.\n",
    "            X, X_test, y, y_test = train_test_split(train, target, test_size=0.33, random_state=dummy)\n",
    "    \n",
    "            #Spliting train data into training and validation sets.\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=dummy)\n",
    "\n",
    "            #print('Data shape:')\n",
    "            #print('X_train: %s, X_valid: %s, X_test: %s \\n' %(X_train.shape, X_valid.shape, \n",
    "            #                                          X_test.shape))\n",
    "\n",
    "            if for_real:\n",
    "                #take the train, target and test data, and come up with a validation set from train\n",
    "                X_real = train\n",
    "                test['labels'] = -1\n",
    "                y_test_real = test['labels']\n",
    "                test = test.drop('labels', axis=1)\n",
    "                X_test_real = test\n",
    "\n",
    "                y_real = target\n",
    "    \n",
    "                X_train_real, X_valid_real, y_train_real, y_valid_real = train_test_split(X_real, y_real, test_size=0.33,\n",
    "                                                                                     random_state=dummy)\n",
    "    \n",
    "\n",
    "            if nm == 'NN':\n",
    "                #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "                clf.fit(X_train.as_matrix(), y_train)\n",
    "                yv = clf.predict_proba(X_valid.as_matrix())\n",
    "                p_valid.append(yv)\n",
    "        \n",
    "                #Second run. Training on (X, y) and predicting on X_test.\n",
    "                clf.fit(X.as_matrix(), y)\n",
    "                yt = clf.predict_proba(X_test.as_matrix())\n",
    "                p_test.append(yt)\n",
    "        \n",
    "                if for_real:\n",
    "                    #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "                    clf.fit(X_train_real.as_matrix(), y_train_real)\n",
    "                    yv_real = clf.predict_proba(X_valid_real.as_matrix())\n",
    "                    p_valid_real.append(yv_real)\n",
    "            \n",
    "                    #Second run. Training on (X, y) and predicting on X_test.\n",
    "                    clf.fit(X_real.as_matrix(), y_real)\n",
    "                    yt_real = clf.predict_proba(X_test_real.as_matrix())\n",
    "                    p_test_real.append(yt_real)\n",
    "    \n",
    "            elif nm == 'XGBc':\n",
    "                #think about making a function that handles this part\n",
    "                xgtrain = xgb.DMatrix(X_train, y_train)\n",
    "                xgtest = xgb.DMatrix(X_valid, label=y_valid)\n",
    "                params = {\"objective\":\"binary:logistic\",\n",
    "                          \"colsample_bytree\":0.77638333498678636,\n",
    "                          \"learning_rate\":0.030567867858705199,\n",
    "                          \"max_delta_step\":4.6626180513766657,\n",
    "                          \"min_child_weight\":57.354121041109124,\n",
    "                          \"n_estimators\":478,\n",
    "                          \"subsample\":0.8069399976204783,\n",
    "                          \"max_depth\":6,\n",
    "                          \"gamma\":0.2966938071810209,\n",
    "                          \"eval_metric\":\"logloss\"}\n",
    "                #can't use predict proba, so i'll need to generate the alternative scores for p_valid, p_test\n",
    "                model = xgb.train(params, xgtrain, 500) \n",
    "                yv = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "                p_valid.append(yv)\n",
    "        \n",
    "                xgtrain = xgb.DMatrix(X, y)\n",
    "                xgtest = xgb.DMatrix(X_test, label=y_test)\n",
    "                model = xgb.train(params, xgtrain, 500)\n",
    "                yt = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "                p_test.append(yt)\n",
    "        \n",
    "                if for_real:\n",
    "                    xgtrain = xgb.DMatrix(X_train_real, y_train_real)\n",
    "                    xgtest = xgb.DMatrix(X_valid_real, label=y_valid_real)\n",
    "                    model = xgb.train(params, xgtrain, 500)\n",
    "                    yv_real = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "                    p_valid_real.append(yv_real)\n",
    "                \n",
    "                    xgtrain = xgb.DMatrix(X_real, y_real)\n",
    "                    xgtest = xgb.DMatrix(X_test_real, label=y_test_real)\n",
    "                    model = xgb.train(params, xgtrain, 500)\n",
    "                    yt_real = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "                    p_test_real.append(yt_real)\n",
    "        \n",
    "            else:\n",
    "                #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "                clf.fit(X_train, y_train)\n",
    "                yv = clf.predict_proba(X_valid)\n",
    "                p_valid.append(yv)\n",
    "        \n",
    "                #Second run. Training on (X, y) and predicting on X_test.\n",
    "                clf.fit(X, y)\n",
    "                yt = clf.predict_proba(X_test)\n",
    "                p_test.append(yt)\n",
    "        \n",
    "                if for_real:\n",
    "                    #First run. Training on (X_train, y_train) and predicting on X_valid.\n",
    "                    clf.fit(X_train_real, y_train_real)\n",
    "                    yv_real = clf.predict_proba(X_valid_real)\n",
    "                    p_valid_real.append(yv_real)\n",
    "        \n",
    "                    #Second run. Training on (X, y) and predicting on X_test.\n",
    "                    clf.fit(X_real, y_real)\n",
    "                    yt_real = clf.predict_proba(X_test_real)\n",
    "                    p_test_real.append(yt_real)\n",
    "       \n",
    "            #Printing out the performance of the classifier\n",
    "            print('{:10s} {:2s} {:1.7f}'.format('%s: ' %(nm), 'logloss  =>', log_loss(y_test, yt)))\n",
    "    print('')\n",
    "    \n",
    "    return p_valid, p_test, y_valid, p_valid_real, p_test_real, y_valid_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objf_ens_optA(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optA ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem (12 in Airbnb competition)\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    w = np.abs(w)\n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol += Xs[i] * w[i]\n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "        \n",
    "\n",
    "class EN_optA(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$,  it computes the optimal set of weights\n",
    "    $w_1, w_2, ..., w_n$; such that minimizes $log\\_loss(y_T, y_E)$, \n",
    "    where $y_E = X_1*w_1 + X_2*w_2 +...+ X_n*w_n$ and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optA, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #All weights must sum to 1\n",
    "        cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "        #Calling the solver\n",
    "        res = minimize(objf_ens_optA, x0, args=(Xs, y, self.n_class), \n",
    "                       method='SLSQP', \n",
    "                       bounds=bounds,\n",
    "                       constraints=cons\n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be blended.\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The blended prediction.\n",
    "        \"\"\"\n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred += Xs[i] * self.w[i] \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objf_ens_optB(w, Xs, y, n_class):\n",
    "    \"\"\"\n",
    "    Function to be minimized in the EN_optB ensembler.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    w: array-like, shape=(n_preds)\n",
    "       Candidate solution to the optimization problem (vector of weights).\n",
    "    Xs: list of predictions to combine\n",
    "       Each prediction is the solution of an individual classifier and has a\n",
    "       shape=(n_samples, n_classes).\n",
    "    y: array-like sahpe=(n_samples,)\n",
    "       Class labels\n",
    "    n_class: int\n",
    "       Number of classes in the problem, i.e. = 12\n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "    score: Score of the candidate solution.\n",
    "    \"\"\"\n",
    "    #Constraining the weights for each class to sum up to 1.\n",
    "    #This constraint can be defined in the scipy.minimize function, but doing \n",
    "    #it here gives more flexibility to the scipy.minimize function \n",
    "    #(e.g. more solvers are allowed).\n",
    "    w_range = np.arange(len(w))%n_class \n",
    "    for i in range(n_class): \n",
    "        w[w_range==i] = w[w_range==i] / np.sum(w[w_range==i])\n",
    "        \n",
    "    sol = np.zeros(Xs[0].shape)\n",
    "    for i in range(len(w)):\n",
    "        sol[:, i % n_class] += Xs[int(i / n_class)][:, i % n_class] * w[i] \n",
    "        \n",
    "    #Using log-loss as objective function (different objective functions can be used here). \n",
    "    score = log_loss(y, sol)   \n",
    "    return score\n",
    "    \n",
    "\n",
    "class EN_optB(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Given a set of predictions $X_1, X_2, ..., X_n$, where each $X_i$ has\n",
    "    $m=12$ clases, i.e. $X_i = X_{i1}, X_{i2},...,X_{im}$. The algorithm finds the optimal \n",
    "    set of weights $w_{11}, w_{12}, ..., w_{nm}$; such that minimizes \n",
    "    $log\\_loss(y_T, y_E)$, where $y_E = X_{11}*w_{11} +... + X_{21}*w_{21} + ... \n",
    "    + X_{nm}*w_{nm}$ and and $y_T$ is the true solution.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(EN_optB, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn the optimal weights by solving an optimization problem.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "           Each prediction is the solution of an individual classifier and has \n",
    "           shape=(n_samples, n_classes).\n",
    "        y: array-like\n",
    "           Class labels\n",
    "        \"\"\"\n",
    "        #print X.shape[1], self.n_class\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        #Initial solution has equal weight for all individual predictions.\n",
    "        x0 = np.ones(self.n_class * len(Xs)) / float(len(Xs)) \n",
    "        #Weights must be bounded in [0, 1]\n",
    "        bounds = [(0,1)]*len(x0)   \n",
    "        #Calling the solver (constraints are directly defined in the objective\n",
    "        #function)\n",
    "        res = minimize(objf_ens_optB, x0, args=(Xs, y, self.n_class), \n",
    "                       method='L-BFGS-B', \n",
    "                       bounds=bounds, \n",
    "                       )\n",
    "        self.w = res.x\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the weights learned in training to predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        Xs: list of predictions to be ensembled\n",
    "            Each prediction is the solution of an individual classifier and has \n",
    "            shape=(n_samples, n_classes).\n",
    "            \n",
    "        Return:\n",
    "        ------\n",
    "        y_pred: array_like, shape=(n_samples, n_class)\n",
    "                The ensembled prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        Xs = np.hsplit(X, X.shape[1]/self.n_class)\n",
    "        y_pred = np.zeros(Xs[0].shape)\n",
    "        for i in range(len(self.w)):\n",
    "            y_pred[:, i % self.n_class] += \\\n",
    "                   Xs[int(i / self.n_class)][:, i % self.n_class] * self.w[i]  \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
