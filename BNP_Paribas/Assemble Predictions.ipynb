{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combining predictions so far\n",
    "\n",
    "#what about combining predictions from training set so i can run logloss on those and then combine the real models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = \"/Users/patrickkennedy/Desktop/Data_Science/Kaggle/BNP_Paribas/Predictions/\"\n",
    "\n",
    "pred1 = pd.read_csv(DATA_DIR + \"ada_boosted_extra_trees_jitteredrandomstate_250iterations.csv\")\n",
    "pred2 = pd.read_csv(DATA_DIR + \"extra_trees_and_log_and_gradientboost_jitteredrandomstate_295iterations.csv\")\n",
    "pred3 = pd.read_csv(DATA_DIR + \"extra_trees_classifier_jitteredrandomstate_1000iterations_engineeredfeatures.csv\")\n",
    "pred4 = pd.read_csv(DATA_DIR + \"extra_trees_jitteredrandomstate_250iterations.csv\")\n",
    "pred5 = pd.read_csv(DATA_DIR + \"extra_trees_jitteredrandomstate_5000iterations.csv\")\n",
    "pred6 = pd.read_csv(DATA_DIR + \"XGBClassifier_jitteredrandomstate_250iterations.csv\")\n",
    "pred7 = pd.read_csv(DATA_DIR + \"NN-sigmoid64-softmax_jitteredrandomstate_25iterations_engineeredfeatures.csv\")\n",
    "pred8 = pd.read_csv(DATA_DIR + \"XGBoost-native_jitteredrandomstate_25iterations_engineeredfeatures.csv\")\n",
    "pred9 = pd.read_csv(DATA_DIR + \"gradientboost_jitteredrandomstate_100iterations.csv\")\n",
    "pred10 = pd.read_csv(DATA_DIR + \"autolearn.csv\")\n",
    "pred11 = pd.read_csv(DATA_DIR + \"NN-rectifier100-tanh100-softmax_jitteredrandomstate_10iterations_engineeredfeatures.csv\")\n",
    "pred12 = pd.read_csv(DATA_DIR + \"3-level_calibrated_model_250iterationsjitteredrandomstate.csv\")\n",
    "pred13 = pd.read_csv(DATA_DIR + \"3-level_calibrated_model.csv\")\n",
    "pred14 = pd.read_csv(DATA_DIR + \"extra_trees_classifier_jitteredrandomstate_250iterations_engineeredfeatures.csv\")\n",
    "pred15 = pd.read_csv(DATA_DIR + \"Log_model_engineeredfeatures.csv\")\n",
    "pred16 = pd.read_csv(DATA_DIR + \"XGBClassifier_jitteredrandomstate_204iterations_engineeredfeatures.csv\")\n",
    "pred17 = pd.read_csv(DATA_DIR + \"XGBoost-native_engineeredfeatures.csv\")\n",
    "\n",
    "\n",
    "pred_list = [pred1, pred2, pred3, pred4, pred5, pred6, pred7, pred8, \n",
    "             pred9, pred10, pred11, pred12, pred13, pred14, pred15, pred16, pred17]\n",
    "ids = pred1.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for pred in pred_list:\n",
    "    predictions.append(pred.PredictedProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\"ID\": ids, \"PredictedProb\": np.mean(predictions, axis=0)}).to_csv('Results_of_6_models.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44364004,  0.83400556,  0.712109  , ...,  0.81756754,\n",
       "        0.89852303,  0.55387757])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989861</td>\n",
       "      <td>0.969952</td>\n",
       "      <td>0.997329</td>\n",
       "      <td>0.998607</td>\n",
       "      <td>0.701037</td>\n",
       "      <td>0.734540</td>\n",
       "      <td>0.719690</td>\n",
       "      <td>0.701581</td>\n",
       "      <td>0.836384</td>\n",
       "      <td>0.732699</td>\n",
       "      <td>0.912267</td>\n",
       "      <td>0.874994</td>\n",
       "      <td>0.969965</td>\n",
       "      <td>-0.028425</td>\n",
       "      <td>0.701036</td>\n",
       "      <td>0.724994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.989861</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971453</td>\n",
       "      <td>0.989862</td>\n",
       "      <td>0.991117</td>\n",
       "      <td>0.783422</td>\n",
       "      <td>0.795437</td>\n",
       "      <td>0.772540</td>\n",
       "      <td>0.785134</td>\n",
       "      <td>0.861716</td>\n",
       "      <td>0.786557</td>\n",
       "      <td>0.952718</td>\n",
       "      <td>0.924614</td>\n",
       "      <td>0.971410</td>\n",
       "      <td>-0.031142</td>\n",
       "      <td>0.783422</td>\n",
       "      <td>0.798996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.969952</td>\n",
       "      <td>0.971453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969888</td>\n",
       "      <td>0.971027</td>\n",
       "      <td>0.734361</td>\n",
       "      <td>0.771757</td>\n",
       "      <td>0.747984</td>\n",
       "      <td>0.732846</td>\n",
       "      <td>0.850418</td>\n",
       "      <td>0.768329</td>\n",
       "      <td>0.911112</td>\n",
       "      <td>0.880205</td>\n",
       "      <td>0.999224</td>\n",
       "      <td>-0.029508</td>\n",
       "      <td>0.734361</td>\n",
       "      <td>0.758207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.997329</td>\n",
       "      <td>0.989862</td>\n",
       "      <td>0.969888</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998607</td>\n",
       "      <td>0.701022</td>\n",
       "      <td>0.734528</td>\n",
       "      <td>0.719749</td>\n",
       "      <td>0.701605</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.732872</td>\n",
       "      <td>0.912298</td>\n",
       "      <td>0.875038</td>\n",
       "      <td>0.969905</td>\n",
       "      <td>-0.028560</td>\n",
       "      <td>0.701022</td>\n",
       "      <td>0.725008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.998607</td>\n",
       "      <td>0.991117</td>\n",
       "      <td>0.971027</td>\n",
       "      <td>0.998607</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.701939</td>\n",
       "      <td>0.735400</td>\n",
       "      <td>0.720528</td>\n",
       "      <td>0.702520</td>\n",
       "      <td>0.837155</td>\n",
       "      <td>0.733751</td>\n",
       "      <td>0.913459</td>\n",
       "      <td>0.876155</td>\n",
       "      <td>0.971032</td>\n",
       "      <td>-0.028350</td>\n",
       "      <td>0.701938</td>\n",
       "      <td>0.725903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.701037</td>\n",
       "      <td>0.783422</td>\n",
       "      <td>0.734361</td>\n",
       "      <td>0.701022</td>\n",
       "      <td>0.701939</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.898640</td>\n",
       "      <td>0.841322</td>\n",
       "      <td>0.993594</td>\n",
       "      <td>0.768957</td>\n",
       "      <td>0.856095</td>\n",
       "      <td>0.909591</td>\n",
       "      <td>0.934067</td>\n",
       "      <td>0.734097</td>\n",
       "      <td>-0.012680</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.734540</td>\n",
       "      <td>0.795437</td>\n",
       "      <td>0.771757</td>\n",
       "      <td>0.734528</td>\n",
       "      <td>0.735400</td>\n",
       "      <td>0.898640</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.773794</td>\n",
       "      <td>0.894126</td>\n",
       "      <td>0.750411</td>\n",
       "      <td>0.958305</td>\n",
       "      <td>0.854457</td>\n",
       "      <td>0.866873</td>\n",
       "      <td>0.771295</td>\n",
       "      <td>-0.041131</td>\n",
       "      <td>0.898641</td>\n",
       "      <td>0.883936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.719690</td>\n",
       "      <td>0.772540</td>\n",
       "      <td>0.747984</td>\n",
       "      <td>0.719749</td>\n",
       "      <td>0.720528</td>\n",
       "      <td>0.841322</td>\n",
       "      <td>0.773794</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.841105</td>\n",
       "      <td>0.742571</td>\n",
       "      <td>0.721993</td>\n",
       "      <td>0.844293</td>\n",
       "      <td>0.849772</td>\n",
       "      <td>0.748024</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.841322</td>\n",
       "      <td>0.884878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.701581</td>\n",
       "      <td>0.785134</td>\n",
       "      <td>0.732846</td>\n",
       "      <td>0.701605</td>\n",
       "      <td>0.702520</td>\n",
       "      <td>0.993594</td>\n",
       "      <td>0.894126</td>\n",
       "      <td>0.841105</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.770610</td>\n",
       "      <td>0.853341</td>\n",
       "      <td>0.911365</td>\n",
       "      <td>0.935785</td>\n",
       "      <td>0.732587</td>\n",
       "      <td>-0.012987</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.964260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.836384</td>\n",
       "      <td>0.861716</td>\n",
       "      <td>0.850418</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.837155</td>\n",
       "      <td>0.768957</td>\n",
       "      <td>0.750411</td>\n",
       "      <td>0.742571</td>\n",
       "      <td>0.770610</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.734198</td>\n",
       "      <td>0.857629</td>\n",
       "      <td>0.844852</td>\n",
       "      <td>0.850219</td>\n",
       "      <td>-0.019175</td>\n",
       "      <td>0.768956</td>\n",
       "      <td>0.782782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.732699</td>\n",
       "      <td>0.786557</td>\n",
       "      <td>0.768329</td>\n",
       "      <td>0.732872</td>\n",
       "      <td>0.733751</td>\n",
       "      <td>0.856095</td>\n",
       "      <td>0.958305</td>\n",
       "      <td>0.721993</td>\n",
       "      <td>0.853341</td>\n",
       "      <td>0.734198</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.821724</td>\n",
       "      <td>0.831821</td>\n",
       "      <td>0.767803</td>\n",
       "      <td>-0.012820</td>\n",
       "      <td>0.856095</td>\n",
       "      <td>0.830788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.912267</td>\n",
       "      <td>0.952718</td>\n",
       "      <td>0.911112</td>\n",
       "      <td>0.912298</td>\n",
       "      <td>0.913459</td>\n",
       "      <td>0.909591</td>\n",
       "      <td>0.854457</td>\n",
       "      <td>0.844293</td>\n",
       "      <td>0.911365</td>\n",
       "      <td>0.857629</td>\n",
       "      <td>0.821724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985995</td>\n",
       "      <td>0.911028</td>\n",
       "      <td>-0.019121</td>\n",
       "      <td>0.909590</td>\n",
       "      <td>0.909128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.874994</td>\n",
       "      <td>0.924614</td>\n",
       "      <td>0.880205</td>\n",
       "      <td>0.875038</td>\n",
       "      <td>0.876155</td>\n",
       "      <td>0.934067</td>\n",
       "      <td>0.866873</td>\n",
       "      <td>0.849772</td>\n",
       "      <td>0.935785</td>\n",
       "      <td>0.844852</td>\n",
       "      <td>0.831821</td>\n",
       "      <td>0.985995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.880064</td>\n",
       "      <td>-0.017656</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.927543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.969965</td>\n",
       "      <td>0.971410</td>\n",
       "      <td>0.999224</td>\n",
       "      <td>0.969905</td>\n",
       "      <td>0.971032</td>\n",
       "      <td>0.734097</td>\n",
       "      <td>0.771295</td>\n",
       "      <td>0.748024</td>\n",
       "      <td>0.732587</td>\n",
       "      <td>0.850219</td>\n",
       "      <td>0.767803</td>\n",
       "      <td>0.911028</td>\n",
       "      <td>0.880064</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.029587</td>\n",
       "      <td>0.734097</td>\n",
       "      <td>0.758010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.028425</td>\n",
       "      <td>-0.031142</td>\n",
       "      <td>-0.029508</td>\n",
       "      <td>-0.028560</td>\n",
       "      <td>-0.028350</td>\n",
       "      <td>-0.012680</td>\n",
       "      <td>-0.041131</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>-0.012987</td>\n",
       "      <td>-0.019175</td>\n",
       "      <td>-0.012820</td>\n",
       "      <td>-0.019121</td>\n",
       "      <td>-0.017656</td>\n",
       "      <td>-0.029587</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.012686</td>\n",
       "      <td>-0.008429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.701036</td>\n",
       "      <td>0.783422</td>\n",
       "      <td>0.734361</td>\n",
       "      <td>0.701022</td>\n",
       "      <td>0.701938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.898641</td>\n",
       "      <td>0.841322</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.768956</td>\n",
       "      <td>0.856095</td>\n",
       "      <td>0.909590</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.734097</td>\n",
       "      <td>-0.012686</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.724994</td>\n",
       "      <td>0.798996</td>\n",
       "      <td>0.758207</td>\n",
       "      <td>0.725008</td>\n",
       "      <td>0.725903</td>\n",
       "      <td>0.965957</td>\n",
       "      <td>0.883936</td>\n",
       "      <td>0.884878</td>\n",
       "      <td>0.964260</td>\n",
       "      <td>0.782782</td>\n",
       "      <td>0.830788</td>\n",
       "      <td>0.909128</td>\n",
       "      <td>0.927543</td>\n",
       "      <td>0.758010</td>\n",
       "      <td>-0.008429</td>\n",
       "      <td>0.965956</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           1         2         3         4         5         6         7  \\\n",
       "1   1.000000  0.989861  0.969952  0.997329  0.998607  0.701037  0.734540   \n",
       "2   0.989861  1.000000  0.971453  0.989862  0.991117  0.783422  0.795437   \n",
       "3   0.969952  0.971453  1.000000  0.969888  0.971027  0.734361  0.771757   \n",
       "4   0.997329  0.989862  0.969888  1.000000  0.998607  0.701022  0.734528   \n",
       "5   0.998607  0.991117  0.971027  0.998607  1.000000  0.701939  0.735400   \n",
       "6   0.701037  0.783422  0.734361  0.701022  0.701939  1.000000  0.898640   \n",
       "7   0.734540  0.795437  0.771757  0.734528  0.735400  0.898640  1.000000   \n",
       "8   0.719690  0.772540  0.747984  0.719749  0.720528  0.841322  0.773794   \n",
       "9   0.701581  0.785134  0.732846  0.701605  0.702520  0.993594  0.894126   \n",
       "10  0.836384  0.861716  0.850418  0.836000  0.837155  0.768957  0.750411   \n",
       "11  0.732699  0.786557  0.768329  0.732872  0.733751  0.856095  0.958305   \n",
       "12  0.912267  0.952718  0.911112  0.912298  0.913459  0.909591  0.854457   \n",
       "13  0.874994  0.924614  0.880205  0.875038  0.876155  0.934067  0.866873   \n",
       "14  0.969965  0.971410  0.999224  0.969905  0.971032  0.734097  0.771295   \n",
       "15 -0.028425 -0.031142 -0.029508 -0.028560 -0.028350 -0.012680 -0.041131   \n",
       "16  0.701036  0.783422  0.734361  0.701022  0.701938  1.000000  0.898641   \n",
       "17  0.724994  0.798996  0.758207  0.725008  0.725903  0.965957  0.883936   \n",
       "\n",
       "           8         9        10        11        12        13        14  \\\n",
       "1   0.719690  0.701581  0.836384  0.732699  0.912267  0.874994  0.969965   \n",
       "2   0.772540  0.785134  0.861716  0.786557  0.952718  0.924614  0.971410   \n",
       "3   0.747984  0.732846  0.850418  0.768329  0.911112  0.880205  0.999224   \n",
       "4   0.719749  0.701605  0.836000  0.732872  0.912298  0.875038  0.969905   \n",
       "5   0.720528  0.702520  0.837155  0.733751  0.913459  0.876155  0.971032   \n",
       "6   0.841322  0.993594  0.768957  0.856095  0.909591  0.934067  0.734097   \n",
       "7   0.773794  0.894126  0.750411  0.958305  0.854457  0.866873  0.771295   \n",
       "8   1.000000  0.841105  0.742571  0.721993  0.844293  0.849772  0.748024   \n",
       "9   0.841105  1.000000  0.770610  0.853341  0.911365  0.935785  0.732587   \n",
       "10  0.742571  0.770610  1.000000  0.734198  0.857629  0.844852  0.850219   \n",
       "11  0.721993  0.853341  0.734198  1.000000  0.821724  0.831821  0.767803   \n",
       "12  0.844293  0.911365  0.857629  0.821724  1.000000  0.985995  0.911028   \n",
       "13  0.849772  0.935785  0.844852  0.831821  0.985995  1.000000  0.880064   \n",
       "14  0.748024  0.732587  0.850219  0.767803  0.911028  0.880064  1.000000   \n",
       "15  0.004237 -0.012987 -0.019175 -0.012820 -0.019121 -0.017656 -0.029587   \n",
       "16  0.841322  0.993593  0.768956  0.856095  0.909590  0.934066  0.734097   \n",
       "17  0.884878  0.964260  0.782782  0.830788  0.909128  0.927543  0.758010   \n",
       "\n",
       "          15        16        17  \n",
       "1  -0.028425  0.701036  0.724994  \n",
       "2  -0.031142  0.783422  0.798996  \n",
       "3  -0.029508  0.734361  0.758207  \n",
       "4  -0.028560  0.701022  0.725008  \n",
       "5  -0.028350  0.701938  0.725903  \n",
       "6  -0.012680  1.000000  0.965957  \n",
       "7  -0.041131  0.898641  0.883936  \n",
       "8   0.004237  0.841322  0.884878  \n",
       "9  -0.012987  0.993593  0.964260  \n",
       "10 -0.019175  0.768956  0.782782  \n",
       "11 -0.012820  0.856095  0.830788  \n",
       "12 -0.019121  0.909590  0.909128  \n",
       "13 -0.017656  0.934066  0.927543  \n",
       "14 -0.029587  0.734097  0.758010  \n",
       "15  1.000000 -0.012686 -0.008429  \n",
       "16 -0.012686  1.000000  0.965956  \n",
       "17 -0.008429  0.965956  1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "\n",
    "data['1'] = pred1['PredictedProb']\n",
    "data['2'] = pred2['PredictedProb']\n",
    "data['3'] = pred3['PredictedProb']\n",
    "data['4'] = pred4['PredictedProb']\n",
    "data['5'] = pred5['PredictedProb']\n",
    "data['6'] = pred6['PredictedProb']\n",
    "data['7'] = pred7['PredictedProb']\n",
    "data['8'] = pred8['PredictedProb']\n",
    "data['9'] = pred9['PredictedProb']\n",
    "data['10'] = pred10['PredictedProb']\n",
    "data['11'] = pred11['PredictedProb']\n",
    "data['12'] = pred12['PredictedProb']\n",
    "data['13'] = pred13['PredictedProb']\n",
    "data['14'] = pred14['PredictedProb']\n",
    "data['15'] = pred15['PredictedProb']\n",
    "data['16'] = pred16['PredictedProb']\n",
    "data['17'] = pred17['PredictedProb']\n",
    "\n",
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#submitted pred4 and pred6... but what about averaging pred6 against every single one?\n",
    "predictions = []\n",
    "predictions.append(pred12.PredictedProb*0.8)\n",
    "predictions.append(pred11.PredictedProb*0.2)\n",
    "pd.DataFrame({\"ID\": ids, \"PredictedProb\": np.sum(predictions, axis=0)}).to_csv('Results_of_2_uncorrelated_models[NN_rectifier_engineered_x_calibrated_w_random].csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#so that seems to work! get a bunch of uncorrelated models together... best way to jump up leaderboard!\n",
    "predictions = []\n",
    "predictions.append(pred4.PredictedProb*0.25)\n",
    "predictions.append(pred8.PredictedProb*0.25)\n",
    "predictions.append(pred10.PredictedProb*0.25)\n",
    "predictions.append(pred11.PredictedProb*0.25)\n",
    "\n",
    "pd.DataFrame({\"ID\": ids, \"PredictedProb\": np.sum(predictions, axis=0)}).to_csv('Results_of_4_uncorrelated_models.csv',index=False)\n",
    "#gives .462.. a bit behind here...\n",
    "\n",
    "#also think of putting all the models together again? find all the uncorrelated models:\n",
    "#right now preds1-5 are alike, 6-7 are alike\n",
    "\n",
    "#i can change the structure of the NN to see if that makes a difference too\n",
    "#i want a series of uncorrelated models!!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1690977849810913"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#next steps are to rerun xgbclassifier with random params, gradient boost with random params, \n",
    "#maybe a NN with random params\n",
    "#blending models together that are uncorrelated and with appropriate weights (how to optimize?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
