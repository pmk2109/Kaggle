{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prudential Life Insurance Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Names and Descriptions\n",
    "__Variable__ Description  \n",
    "__Id__\tA unique identifier associated with an application.  \n",
    "__Product_Info_1-7__\tA set of normalized variables relating to the product applied for  \n",
    "__Ins_Age__\tNormalized age of applicant  \n",
    "__Ht__\tNormalized height of applicant  \n",
    "__Wt__\tNormalized weight of applicant  \n",
    "__BMI__\tNormalized BMI of applicant  \n",
    "__Employment_Info_1-6__\tA set of normalized variables relating to the employment history of the applicant.  \n",
    "__InsuredInfo_1-6__\tA set of normalized variables providing information about the applicant.  \n",
    "__Insurance_History_1-9__\tA set of normalized variables relating to the insurance history of the applicant.  \n",
    "__Family_Hist_1-5__\tA set of normalized variables relating to the family history of the applicant.  \n",
    "__Medical_History_1-41__\tA set of normalized variables relating to the medical history of the applicant.  \n",
    "__Medical_Keyword_1-48__\tA set of dummy variables relating to the presence of/absence of a medical keyword being associated with the application.  \n",
    "__Response__\tThis is the target variable, an ordinal variable relating to the final decision associated with an application  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables\n",
    "Product_Info_1, Product_Info_2, Product_Info_3, Product_Info_5, Product_Info_6, Product_Info_7, Employment_Info_2, Employment_Info_3, Employment_Info_5, InsuredInfo_1, InsuredInfo_2, InsuredInfo_3, InsuredInfo_4, InsuredInfo_5, InsuredInfo_6, InsuredInfo_7, Insurance_History_1, Insurance_History_2, Insurance_History_3, Insurance_History_4, Insurance_History_7, Insurance_History_8, Insurance_History_9, Family_Hist_1, Medical_History_2, Medical_History_3, Medical_History_4, Medical_History_5, Medical_History_6, Medical_History_7, Medical_History_8, Medical_History_9, Medical_History_10, Medical_History_11, Medical_History_12, Medical_History_13, Medical_History_14, Medical_History_16, Medical_History_17, Medical_History_18, Medical_History_19, Medical_History_20, Medical_History_21, Medical_History_22, Medical_History_23, Medical_History_25, Medical_History_26, Medical_History_27, Medical_History_28, Medical_History_29, Medical_History_30, Medical_History_31, Medical_History_33, Medical_History_34, Medical_History_35, Medical_History_36, Medical_History_37, Medical_History_38, Medical_History_39, Medical_History_40, Medical_History_41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Variables\n",
    "Product_Info_4, Ins_Age, Ht, Wt, BMI, Employment_Info_1, Employment_Info_4, Employment_Info_6, Insurance_History_5, Family_Hist_2, Family_Hist_3, Family_Hist_4, Family_Hist_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Variables\n",
    "Medical_History_1, Medical_History_15, Medical_History_24, Medical_History_32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in the data, import the necessary stuff, look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "DATA_DIR = '/Users/patrickkennedy/Desktop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D3    14321\n",
       "D4    10812\n",
       "A8     6835\n",
       "D1     6554\n",
       "D2     6286\n",
       "E1     2647\n",
       "A1     2363\n",
       "A6     2098\n",
       "A2     1974\n",
       "A7     1383\n",
       "B2     1122\n",
       "A3      977\n",
       "A5      775\n",
       "C3      306\n",
       "C1      285\n",
       "C4      219\n",
       "A4      210\n",
       "C2      160\n",
       "B1       54\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(DATA_DIR + '/Project DATA/train.csv')\n",
    "df_test = pd.read_csv(DATA_DIR + '/Project DATA/test.csv')\n",
    "\n",
    "df_train['Product_Info_2'].unique()\n",
    "counts = df_train['Product_Info_2'].value_counts()\n",
    "\n",
    "#do a comparison from counts here and the distribution of outcome variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['Product_Info_2'] = pd.factorize(df_train['Product_Info_2'])[0]\n",
    "df_train.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_corr = df_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_corr_sorted = df_corr.sort(columns='Response', ascending=False)\n",
    "#print df_corr_sorted['Response']\n",
    "\n",
    "response_corr = df_corr_sorted[abs(df_corr_sorted['Response']) > .02].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response_corr = list(response_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response_correlations = response_corr[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Medical_History_23', 'Medical_History_4', 'Medical_History_39', 'Product_Info_4', 'Family_Hist_2', 'Medical_History_6', 'Family_Hist_4', 'Medical_History_13', 'Medical_History_40', 'InsuredInfo_6', 'Medical_History_33', 'Medical_History_27', 'InsuredInfo_1', 'Medical_History_1', 'Employment_Info_2', 'Medical_History_29', 'Medical_History_20', 'Medical_History_17', 'Family_Hist_1', 'Medical_History_9', 'Employment_Info_5', 'Medical_History_22', 'Medical_History_31', 'Employment_Info_1', 'Medical_History_34', 'Medical_History_11', 'Medical_History_14', 'Medical_History_37', 'Medical_History_7', 'InsuredInfo_4', 'Medical_History_3', 'Medical_Keyword_7', 'Medical_Keyword_14', 'Medical_Keyword_21', 'Product_Info_2', 'Medical_Keyword_30', 'Medical_Keyword_11', 'Medical_Keyword_28', 'Medical_History_24', 'Medical_History_10', 'Product_Info_6', 'Medical_Keyword_34', 'Medical_Keyword_13', 'Medical_History_19', 'Medical_Keyword_4', 'Medical_Keyword_33', 'Medical_Keyword_36', 'Medical_Keyword_19', 'Medical_Keyword_12', 'Medical_Keyword_35', 'Medical_Keyword_18', 'Medical_History_41', 'Medical_Keyword_9', 'Medical_History_35', 'Medical_History_8', 'Medical_Keyword_22', 'Medical_Keyword_27', 'Medical_History_21', 'Medical_Keyword_31', 'Medical_Keyword_16', 'Medical_Keyword_10', 'Medical_Keyword_43', 'Medical_Keyword_46', 'Medical_History_38', 'Medical_Keyword_47', 'Product_Info_3', 'Medical_History_12', 'Medical_History_15', 'Medical_History_5', 'Medical_Keyword_24', 'Medical_Keyword_40', 'Medical_Keyword_37', 'InsuredInfo_2', 'Medical_Keyword_1', 'Medical_History_18', 'Medical_History_28', 'Medical_Keyword_38', 'Ht', 'Medical_Keyword_42', 'InsuredInfo_7', 'InsuredInfo_5', 'Medical_Keyword_25', 'Medical_Keyword_23', 'Medical_History_30', 'Employment_Info_3', 'Insurance_History_2', 'Medical_History_16', 'Family_Hist_3', 'Medical_Keyword_48', 'Family_Hist_5', 'Ins_Age', 'Medical_Keyword_3', 'Medical_Keyword_15', 'Wt', 'BMI']\n"
     ]
    }
   ],
   "source": [
    "print response_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Product_Info_1</th>\n",
       "      <th>Product_Info_3</th>\n",
       "      <th>Product_Info_4</th>\n",
       "      <th>Product_Info_5</th>\n",
       "      <th>Product_Info_6</th>\n",
       "      <th>Product_Info_7</th>\n",
       "      <th>Ins_Age</th>\n",
       "      <th>Ht</th>\n",
       "      <th>Wt</th>\n",
       "      <th>...</th>\n",
       "      <th>Medical_Keyword_40</th>\n",
       "      <th>Medical_Keyword_41</th>\n",
       "      <th>Medical_Keyword_42</th>\n",
       "      <th>Medical_Keyword_43</th>\n",
       "      <th>Medical_Keyword_44</th>\n",
       "      <th>Medical_Keyword_45</th>\n",
       "      <th>Medical_Keyword_46</th>\n",
       "      <th>Medical_Keyword_47</th>\n",
       "      <th>Medical_Keyword_48</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39507.211515</td>\n",
       "      <td>1.026355</td>\n",
       "      <td>24.415655</td>\n",
       "      <td>0.328952</td>\n",
       "      <td>2.006955</td>\n",
       "      <td>2.673599</td>\n",
       "      <td>1.043583</td>\n",
       "      <td>0.405567</td>\n",
       "      <td>0.707283</td>\n",
       "      <td>0.292587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056954</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.010710</td>\n",
       "      <td>0.007528</td>\n",
       "      <td>0.013691</td>\n",
       "      <td>0.008488</td>\n",
       "      <td>0.019905</td>\n",
       "      <td>0.054496</td>\n",
       "      <td>5.636837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>22815.883089</td>\n",
       "      <td>0.160191</td>\n",
       "      <td>5.072885</td>\n",
       "      <td>0.282562</td>\n",
       "      <td>0.083107</td>\n",
       "      <td>0.739103</td>\n",
       "      <td>0.291949</td>\n",
       "      <td>0.197190</td>\n",
       "      <td>0.074239</td>\n",
       "      <td>0.089037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231757</td>\n",
       "      <td>0.099764</td>\n",
       "      <td>0.208479</td>\n",
       "      <td>0.102937</td>\n",
       "      <td>0.086436</td>\n",
       "      <td>0.116207</td>\n",
       "      <td>0.091737</td>\n",
       "      <td>0.139676</td>\n",
       "      <td>0.226995</td>\n",
       "      <td>2.456833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19780.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.238806</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.225941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39487.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.402985</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.288703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>59211.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.345188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>79146.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Id  Product_Info_1  Product_Info_3  Product_Info_4  \\\n",
       "count  59381.000000    59381.000000    59381.000000    59381.000000   \n",
       "mean   39507.211515        1.026355       24.415655        0.328952   \n",
       "std    22815.883089        0.160191        5.072885        0.282562   \n",
       "min        2.000000        1.000000        1.000000        0.000000   \n",
       "25%    19780.000000        1.000000       26.000000        0.076923   \n",
       "50%    39487.000000        1.000000       26.000000        0.230769   \n",
       "75%    59211.000000        1.000000       26.000000        0.487179   \n",
       "max    79146.000000        2.000000       38.000000        1.000000   \n",
       "\n",
       "       Product_Info_5  Product_Info_6  Product_Info_7       Ins_Age  \\\n",
       "count    59381.000000    59381.000000    59381.000000  59381.000000   \n",
       "mean         2.006955        2.673599        1.043583      0.405567   \n",
       "std          0.083107        0.739103        0.291949      0.197190   \n",
       "min          2.000000        1.000000        1.000000      0.000000   \n",
       "25%          2.000000        3.000000        1.000000      0.238806   \n",
       "50%          2.000000        3.000000        1.000000      0.402985   \n",
       "75%          2.000000        3.000000        1.000000      0.567164   \n",
       "max          3.000000        3.000000        3.000000      1.000000   \n",
       "\n",
       "                 Ht            Wt      ...       Medical_Keyword_40  \\\n",
       "count  59381.000000  59381.000000      ...             59381.000000   \n",
       "mean       0.707283      0.292587      ...                 0.056954   \n",
       "std        0.074239      0.089037      ...                 0.231757   \n",
       "min        0.000000      0.000000      ...                 0.000000   \n",
       "25%        0.654545      0.225941      ...                 0.000000   \n",
       "50%        0.709091      0.288703      ...                 0.000000   \n",
       "75%        0.763636      0.345188      ...                 0.000000   \n",
       "max        1.000000      1.000000      ...                 1.000000   \n",
       "\n",
       "       Medical_Keyword_41  Medical_Keyword_42  Medical_Keyword_43  \\\n",
       "count        59381.000000        59381.000000        59381.000000   \n",
       "mean             0.010054            0.045536            0.010710   \n",
       "std              0.099764            0.208479            0.102937   \n",
       "min              0.000000            0.000000            0.000000   \n",
       "25%              0.000000            0.000000            0.000000   \n",
       "50%              0.000000            0.000000            0.000000   \n",
       "75%              0.000000            0.000000            0.000000   \n",
       "max              1.000000            1.000000            1.000000   \n",
       "\n",
       "       Medical_Keyword_44  Medical_Keyword_45  Medical_Keyword_46  \\\n",
       "count        59381.000000        59381.000000        59381.000000   \n",
       "mean             0.007528            0.013691            0.008488   \n",
       "std              0.086436            0.116207            0.091737   \n",
       "min              0.000000            0.000000            0.000000   \n",
       "25%              0.000000            0.000000            0.000000   \n",
       "50%              0.000000            0.000000            0.000000   \n",
       "75%              0.000000            0.000000            0.000000   \n",
       "max              1.000000            1.000000            1.000000   \n",
       "\n",
       "       Medical_Keyword_47  Medical_Keyword_48      Response  \n",
       "count        59381.000000        59381.000000  59381.000000  \n",
       "mean             0.019905            0.054496      5.636837  \n",
       "std              0.139676            0.226995      2.456833  \n",
       "min              0.000000            0.000000      1.000000  \n",
       "25%              0.000000            0.000000      4.000000  \n",
       "50%              0.000000            0.000000      6.000000  \n",
       "75%              0.000000            0.000000      8.000000  \n",
       "max              1.000000            1.000000      8.000000  \n",
       "\n",
       "[8 rows x 127 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'Id', u'Product_Info_1', u'Product_Info_2', u'Product_Info_3',\n",
      "       u'Product_Info_4', u'Product_Info_5', u'Product_Info_6',\n",
      "       u'Product_Info_7', u'Ins_Age', u'Ht', \n",
      "       ...\n",
      "       u'Medical_Keyword_40', u'Medical_Keyword_41', u'Medical_Keyword_42',\n",
      "       u'Medical_Keyword_43', u'Medical_Keyword_44', u'Medical_Keyword_45',\n",
      "       u'Medical_Keyword_46', u'Medical_Keyword_47', u'Medical_Keyword_48',\n",
      "       u'Response'],\n",
      "      dtype='object', length=128)\n"
     ]
    }
   ],
   "source": [
    "categorical = ['Product_Info_1', 'Product_Info_2', 'Product_Info_3', 'Product_Info_5', 'Product_Info_6',\n",
    "               'Product_Info_7', 'Employment_Info_2', 'Employment_Info_3', 'Employment_Info_5', 'InsuredInfo_1',\n",
    "               'InsuredInfo_2', 'InsuredInfo_3', 'InsuredInfo_4', 'InsuredInfo_5', 'InsuredInfo_6', 'InsuredInfo_7',\n",
    "               'Insurance_History_1', 'Insurance_History_2', 'Insurance_History_3', 'Insurance_History_4',\n",
    "               'Insurance_History_7', 'Insurance_History_8', 'Insurance_History_9', 'Family_Hist_1', 'Medical_History_2',\n",
    "               'Medical_History_3', 'Medical_History_4', 'Medical_History_5', 'Medical_History_6', 'Medical_History_7',\n",
    "               'Medical_History_8', 'Medical_History_9', 'Medical_History_10', 'Medical_History_11', 'Medical_History_12',\n",
    "               'Medical_History_13', 'Medical_History_14', 'Medical_History_16', 'Medical_History_17', 'Medical_History_18',\n",
    "               'Medical_History_19', 'Medical_History_20', 'Medical_History_21', 'Medical_History_22', 'Medical_History_23',\n",
    "               'Medical_History_25', 'Medical_History_26', 'Medical_History_27', 'Medical_History_28', 'Medical_History_29',\n",
    "               'Medical_History_30', 'Medical_History_31', 'Medical_History_33', 'Medical_History_34', 'Medical_History_35',\n",
    "               'Medical_History_36', 'Medical_History_37', 'Medical_History_38', 'Medical_History_39', 'Medical_History_40',\n",
    "               'Medical_History_41']\n",
    "\n",
    "\n",
    "continuous = ['Product_Info_4', 'Ins_Age', 'Ht', 'Wt', 'BMI', 'Employment_Info_1', 'Employment_Info_4', 'Employment_Info_6',\n",
    "              'Insurance_History_5', 'Family_Hist_2', 'Family_Hist_3', 'Family_Hist_4', 'Family_Hist_5']\n",
    "\n",
    "discrete = ['Medical_History_1', 'Medical_History_15', 'Medical_History_24', 'Medical_History_32']\n",
    "\n",
    "features = df_train.columns\n",
    "features.drop(categorical)\n",
    "features.drop(continuous)\n",
    "features.drop(discrete)\n",
    "print features\n",
    "\n",
    "#df_train[categorical].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3282026237348647"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "19489/59381.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11117be90>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4QAAAFICAYAAAAI+zDdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYFD/H3QlAkjNIGkXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110ba1850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# response\n",
    "\n",
    "fig, (axis1) = plt.subplots(1,1,figsize=(15,5))\n",
    "\n",
    "sns.countplot(x=df_train[\"Response\"], order=[1,2,3,4,5,6,7,8], ax=axis1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There are some columns with non-numerical values(i.e. dtype='object'),\n",
    "# So, We will create a corresponding unique numerical value for each non-numerical value in a column of training and testing set.\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "for f in df_train.columns:\n",
    "    if df_train[f].dtype == 'object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(np.unique(list(df_train[f].values) + list(df_test[f].values)))\n",
    "        df_train[f] = lbl.transform(list(df_train[f].values))\n",
    "        df_test[f] = lbl.transform(list(df_test[f].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill NaN values\n",
    "\n",
    "for f in df_train.columns:\n",
    "    if f == \"Response\": continue\n",
    "    if df_train[f].dtype == 'float64':\n",
    "        df_train[f].fillna(df_train[f].mean(), inplace=True)\n",
    "        df_test[f].fillna(df_test[f].mean(), inplace=True)\n",
    "    else:\n",
    "        df_train[f].fillna(df_train[f].median(), inplace=True)\n",
    "        df_test[f].fillna(df_test[f].median(), inplace=True)\n",
    "\n",
    "# df_train.fillna(0, inplace=True)\n",
    "# df_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define training and testing sets\n",
    "\n",
    "X_train = df_train.drop([\"Response\", \"Id\"],axis=1)\n",
    "y_train = df_train[\"Response\"]\n",
    "\n",
    "y_train_1 = y_train.map({1:1, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0})\n",
    "y_train_2 = y_train.map({1:0, 2:1, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0})\n",
    "y_train_3 = y_train.map({1:0, 2:0, 3:1, 4:0, 5:0, 6:0, 7:0, 8:0})\n",
    "y_train_4 = y_train.map({1:0, 2:0, 3:0, 4:1, 5:0, 6:0, 7:0, 8:0})\n",
    "y_train_5 = y_train.map({1:0, 2:0, 3:0, 4:0, 5:1, 6:0, 7:0, 8:0})\n",
    "y_train_6 = y_train.map({1:0, 2:0, 3:0, 4:0, 5:0, 6:1, 7:0, 8:0})\n",
    "y_train_7 = y_train.map({1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:1, 8:0})\n",
    "y_train_8 = y_train.map({1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:1})\n",
    "\n",
    "y_train_ind_list = [y_train_1, y_train_2, y_train_3, y_train_4, y_train_5, y_train_6, y_train_7, y_train_8]\n",
    "\n",
    "X_test  = df_test.drop(\"Id\",axis=1).copy()\n",
    "\n",
    "\n",
    "#think about training 8 different models and then putting them together... read "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Product_Info_1  Product_Info_2  Product_Info_3  Product_Info_4  \\\n",
      "0                   1              16              10        0.076923   \n",
      "1                   1               0              26        0.076923   \n",
      "2                   1              18              26        0.076923   \n",
      "3                   1              17              10        0.487179   \n",
      "4                   1              15              26        0.230769   \n",
      "5                   1              15              26        0.230769   \n",
      "6                   1               7              10        0.166194   \n",
      "7                   1              15              26        0.076923   \n",
      "8                   1              16              26        0.230769   \n",
      "9                   1              18              21        0.076923   \n",
      "10                  1              16              26        0.128205   \n",
      "11                  1              17              26        0.230769   \n",
      "12                  1               1              26        0.102564   \n",
      "13                  2              14              26        0.487179   \n",
      "14                  1              17              26        0.487179   \n",
      "15                  1               6              26        0.000000   \n",
      "16                  2              17              26        0.487179   \n",
      "17                  1              16              26        0.384615   \n",
      "18                  1              16              26        0.076923   \n",
      "19                  1              17              26        0.487179   \n",
      "20                  1              15              26        0.435897   \n",
      "21                  1               0              26        1.000000   \n",
      "22                  1              17              26        0.230769   \n",
      "23                  1               1              26        0.179487   \n",
      "24                  1              14              26        0.487179   \n",
      "25                  1               5              26        0.230769   \n",
      "26                  1               0              26        1.000000   \n",
      "27                  1              16              26        0.230769   \n",
      "28                  1              17              26        0.487179   \n",
      "29                  1              16              26        1.000000   \n",
      "...               ...             ...             ...             ...   \n",
      "59351               1               6              26        0.000000   \n",
      "59352               1              15              10        0.230769   \n",
      "59353               1              17              26        0.589744   \n",
      "59354               1              15              26        0.487179   \n",
      "59355               1              16              26        0.230769   \n",
      "59356               1              16              10        0.076923   \n",
      "59357               1              16              26        1.000000   \n",
      "59358               1              17              26        0.282051   \n",
      "59359               1              16              26        0.230769   \n",
      "59360               1              17              26        1.000000   \n",
      "59361               1               0              26        0.230769   \n",
      "59362               1              17              26        0.230769   \n",
      "59363               1              15               4        0.076923   \n",
      "59364               1              15              26        0.076923   \n",
      "59365               1              14              29        0.076923   \n",
      "59366               1              14              26        0.282051   \n",
      "59367               1              18              26        0.179487   \n",
      "59368               1              17              26        0.230769   \n",
      "59369               1              14              26        0.179487   \n",
      "59370               1              16              26        0.230769   \n",
      "59371               1              16              26        0.487179   \n",
      "59372               1              16              26        0.487179   \n",
      "59373               2              17              29        0.487179   \n",
      "59374               1              17              26        0.307692   \n",
      "59375               1              10              26        0.076923   \n",
      "59376               1              14              10        0.230769   \n",
      "59377               1              16              26        0.230769   \n",
      "59378               1              18              26        0.076923   \n",
      "59379               1              15              10        0.230769   \n",
      "59380               1               7              26        0.076923   \n",
      "\n",
      "       Product_Info_5  Product_Info_6  Product_Info_7   Ins_Age        Ht  \\\n",
      "0                   2               1               1  0.641791  0.581818   \n",
      "1                   2               3               1  0.059701  0.600000   \n",
      "2                   2               3               1  0.029851  0.745455   \n",
      "3                   2               3               1  0.164179  0.672727   \n",
      "4                   2               3               1  0.417910  0.654545   \n",
      "5                   3               1               1  0.507463  0.836364   \n",
      "6                   2               3               1  0.373134  0.581818   \n",
      "7                   2               3               1  0.611940  0.781818   \n",
      "8                   2               3               1  0.522388  0.618182   \n",
      "9                   2               3               1  0.552239  0.600000   \n",
      "10                  2               3               1  0.537313  0.690909   \n",
      "11                  2               3               1  0.298507  0.690909   \n",
      "12                  2               3               1  0.567164  0.618182   \n",
      "13                  2               3               1  0.223881  0.781818   \n",
      "14                  2               3               1  0.328358  0.636364   \n",
      "15                  2               3               1  0.626866  0.672727   \n",
      "16                  2               3               1  0.208955  0.745455   \n",
      "17                  2               3               1  0.268657  0.636364   \n",
      "18                  2               3               1  0.388060  0.781818   \n",
      "19                  2               3               1  0.223881  0.600000   \n",
      "20                  2               3               1  0.388060  0.745455   \n",
      "21                  2               1               1  0.537313  0.709091   \n",
      "22                  2               3               1  0.179104  0.800000   \n",
      "23                  2               3               1  0.164179  0.745455   \n",
      "24                  2               1               1  0.164179  0.818182   \n",
      "25                  2               3               1  0.268657  0.781818   \n",
      "26                  2               3               1  0.507463  0.654545   \n",
      "27                  2               3               1  0.134328  0.763636   \n",
      "28                  2               3               1  0.492537  0.618182   \n",
      "29                  2               3               1  0.582090  0.654545   \n",
      "...               ...             ...             ...       ...       ...   \n",
      "59351               2               3               1  0.134328  0.781818   \n",
      "59352               2               3               1  0.358209  0.618182   \n",
      "59353               2               1               1  0.179104  0.781818   \n",
      "59354               2               1               1  0.402985  0.763636   \n",
      "59355               2               3               1  0.223881  0.745455   \n",
      "59356               2               3               1  0.522388  0.600000   \n",
      "59357               2               1               3  0.582090  0.781818   \n",
      "59358               2               3               1  0.238806  0.727273   \n",
      "59359               2               3               1  0.447761  0.781818   \n",
      "59360               2               3               1  0.194030  0.654545   \n",
      "59361               2               1               1  0.268657  0.727273   \n",
      "59362               2               3               1  0.253731  0.781818   \n",
      "59363               2               3               1  0.746269  0.563636   \n",
      "59364               2               3               1  0.552239  0.727273   \n",
      "59365               2               3               1  0.641791  0.709091   \n",
      "59366               2               3               1  0.582090  0.781818   \n",
      "59367               2               3               3  0.373134  0.600000   \n",
      "59368               2               1               1  0.417910  0.727273   \n",
      "59369               2               3               1  0.611940  0.745455   \n",
      "59370               2               3               1  0.238806  0.763636   \n",
      "59371               2               1               1  0.537313  0.709091   \n",
      "59372               2               3               1  0.477612  0.763636   \n",
      "59373               2               3               1  0.208955  0.800000   \n",
      "59374               2               3               1  0.164179  0.690909   \n",
      "59375               2               3               1  0.477612  0.654545   \n",
      "59376               2               3               1  0.074627  0.709091   \n",
      "59377               2               3               1  0.432836  0.800000   \n",
      "59378               2               3               1  0.104478  0.745455   \n",
      "59379               2               3               1  0.507463  0.690909   \n",
      "59380               2               3               1  0.447761  0.781818   \n",
      "\n",
      "             Wt         ...          Medical_Keyword_39  Medical_Keyword_40  \\\n",
      "0      0.148536         ...                           0                   0   \n",
      "1      0.131799         ...                           0                   0   \n",
      "2      0.288703         ...                           0                   0   \n",
      "3      0.205021         ...                           0                   0   \n",
      "4      0.234310         ...                           0                   0   \n",
      "5      0.299163         ...                           0                   0   \n",
      "6      0.173640         ...                           0                   0   \n",
      "7      0.403766         ...                           0                   0   \n",
      "8      0.184100         ...                           0                   0   \n",
      "9      0.284519         ...                           0                   0   \n",
      "10     0.309623         ...                           0                   0   \n",
      "11     0.271967         ...                           0                   0   \n",
      "12     0.163180         ...                           0                   0   \n",
      "13     0.361925         ...                           0                   0   \n",
      "14     0.142259         ...                           0                   0   \n",
      "15     0.330544         ...                           0                   0   \n",
      "16     0.246862         ...                           0                   1   \n",
      "17     0.228033         ...                           0                   0   \n",
      "18     0.309623         ...                           0                   0   \n",
      "19     0.138075         ...                           0                   0   \n",
      "20     0.246862         ...                           0                   0   \n",
      "21     0.370293         ...                           0                   0   \n",
      "22     0.539749         ...                           0                   0   \n",
      "23     0.288703         ...                           0                   0   \n",
      "24     0.435146         ...                           0                   0   \n",
      "25     0.368201         ...                           0                   0   \n",
      "26     0.299163         ...                           0                   0   \n",
      "27     0.215481         ...                           0                   0   \n",
      "28     0.276151         ...                           0                   0   \n",
      "29     0.278243         ...                           0                   0   \n",
      "...         ...         ...                         ...                 ...   \n",
      "59351  0.351464         ...                           0                   0   \n",
      "59352  0.246862         ...                           0                   0   \n",
      "59353  0.382845         ...                           0                   0   \n",
      "59354  0.341004         ...                           0                   0   \n",
      "59355  0.361925         ...                           0                   0   \n",
      "59356  0.299163         ...                           0                   0   \n",
      "59357  0.351464         ...                           0                   0   \n",
      "59358  0.372385         ...                           0                   0   \n",
      "59359  0.424686         ...                           0                   0   \n",
      "59360  0.146444         ...                           0                   0   \n",
      "59361  0.267782         ...                           0                   0   \n",
      "59362  0.351464         ...                           0                   0   \n",
      "59363  0.205021         ...                           0                   0   \n",
      "59364  0.177824         ...                           0                   0   \n",
      "59365  0.284519         ...                           0                   0   \n",
      "59366  0.320084         ...                           0                   0   \n",
      "59367  0.320084         ...                           0                   0   \n",
      "59368  0.299163         ...                           0                   0   \n",
      "59369  0.451883         ...                           0                   0   \n",
      "59370  0.330544         ...                           0                   0   \n",
      "59371  0.343096         ...                           0                   0   \n",
      "59372  0.305439         ...                           0                   0   \n",
      "59373  0.257322         ...                           0                   0   \n",
      "59374  0.288703         ...                           0                   0   \n",
      "59375  0.271967         ...                           0                   0   \n",
      "59376  0.320084         ...                           0                   0   \n",
      "59377  0.403766         ...                           0                   0   \n",
      "59378  0.246862         ...                           0                   0   \n",
      "59379  0.276151         ...                           0                   1   \n",
      "59380  0.382845         ...                           0                   0   \n",
      "\n",
      "       Medical_Keyword_41  Medical_Keyword_42  Medical_Keyword_43  \\\n",
      "0                       0                   0                   0   \n",
      "1                       0                   0                   0   \n",
      "2                       0                   0                   0   \n",
      "3                       0                   0                   0   \n",
      "4                       0                   0                   0   \n",
      "5                       0                   0                   0   \n",
      "6                       0                   0                   0   \n",
      "7                       0                   0                   0   \n",
      "8                       0                   0                   0   \n",
      "9                       0                   0                   0   \n",
      "10                      0                   0                   0   \n",
      "11                      0                   0                   0   \n",
      "12                      0                   0                   0   \n",
      "13                      0                   0                   0   \n",
      "14                      0                   0                   0   \n",
      "15                      0                   0                   0   \n",
      "16                      0                   0                   0   \n",
      "17                      0                   0                   0   \n",
      "18                      0                   0                   0   \n",
      "19                      0                   0                   0   \n",
      "20                      0                   0                   0   \n",
      "21                      0                   0                   0   \n",
      "22                      0                   0                   0   \n",
      "23                      0                   0                   0   \n",
      "24                      0                   0                   0   \n",
      "25                      0                   0                   0   \n",
      "26                      0                   0                   0   \n",
      "27                      0                   0                   0   \n",
      "28                      0                   0                   0   \n",
      "29                      0                   0                   0   \n",
      "...                   ...                 ...                 ...   \n",
      "59351                   0                   0                   0   \n",
      "59352                   0                   0                   0   \n",
      "59353                   0                   0                   0   \n",
      "59354                   0                   0                   0   \n",
      "59355                   0                   1                   0   \n",
      "59356                   0                   0                   0   \n",
      "59357                   0                   0                   0   \n",
      "59358                   0                   0                   0   \n",
      "59359                   0                   0                   0   \n",
      "59360                   0                   0                   0   \n",
      "59361                   0                   0                   0   \n",
      "59362                   0                   0                   0   \n",
      "59363                   0                   0                   0   \n",
      "59364                   0                   0                   0   \n",
      "59365                   0                   0                   0   \n",
      "59366                   0                   0                   0   \n",
      "59367                   0                   0                   0   \n",
      "59368                   0                   0                   0   \n",
      "59369                   0                   0                   0   \n",
      "59370                   0                   0                   0   \n",
      "59371                   0                   0                   0   \n",
      "59372                   0                   1                   0   \n",
      "59373                   0                   0                   0   \n",
      "59374                   0                   0                   0   \n",
      "59375                   0                   0                   0   \n",
      "59376                   0                   0                   0   \n",
      "59377                   0                   0                   0   \n",
      "59378                   0                   0                   0   \n",
      "59379                   0                   0                   0   \n",
      "59380                   0                   0                   0   \n",
      "\n",
      "       Medical_Keyword_44  Medical_Keyword_45  Medical_Keyword_46  \\\n",
      "0                       0                   0                   0   \n",
      "1                       0                   0                   0   \n",
      "2                       0                   0                   0   \n",
      "3                       0                   0                   0   \n",
      "4                       0                   0                   0   \n",
      "5                       0                   0                   0   \n",
      "6                       0                   0                   0   \n",
      "7                       0                   0                   0   \n",
      "8                       0                   0                   0   \n",
      "9                       0                   0                   0   \n",
      "10                      1                   0                   0   \n",
      "11                      0                   0                   0   \n",
      "12                      0                   0                   0   \n",
      "13                      0                   0                   0   \n",
      "14                      0                   0                   0   \n",
      "15                      0                   0                   0   \n",
      "16                      0                   0                   0   \n",
      "17                      0                   0                   0   \n",
      "18                      0                   0                   0   \n",
      "19                      0                   0                   0   \n",
      "20                      0                   0                   0   \n",
      "21                      0                   0                   0   \n",
      "22                      0                   0                   0   \n",
      "23                      0                   0                   0   \n",
      "24                      0                   0                   0   \n",
      "25                      0                   0                   0   \n",
      "26                      0                   0                   0   \n",
      "27                      0                   0                   0   \n",
      "28                      0                   0                   0   \n",
      "29                      0                   0                   0   \n",
      "...                   ...                 ...                 ...   \n",
      "59351                   0                   0                   0   \n",
      "59352                   0                   0                   0   \n",
      "59353                   0                   0                   0   \n",
      "59354                   0                   0                   0   \n",
      "59355                   0                   0                   0   \n",
      "59356                   0                   0                   0   \n",
      "59357                   0                   0                   0   \n",
      "59358                   0                   0                   0   \n",
      "59359                   1                   0                   0   \n",
      "59360                   0                   0                   0   \n",
      "59361                   0                   0                   0   \n",
      "59362                   0                   0                   0   \n",
      "59363                   0                   0                   0   \n",
      "59364                   0                   0                   0   \n",
      "59365                   0                   0                   0   \n",
      "59366                   0                   0                   0   \n",
      "59367                   0                   0                   0   \n",
      "59368                   0                   0                   0   \n",
      "59369                   0                   0                   0   \n",
      "59370                   0                   0                   0   \n",
      "59371                   0                   0                   0   \n",
      "59372                   0                   0                   0   \n",
      "59373                   0                   0                   0   \n",
      "59374                   0                   0                   0   \n",
      "59375                   0                   0                   0   \n",
      "59376                   0                   0                   0   \n",
      "59377                   0                   0                   0   \n",
      "59378                   0                   0                   0   \n",
      "59379                   0                   0                   0   \n",
      "59380                   0                   0                   0   \n",
      "\n",
      "       Medical_Keyword_47  Medical_Keyword_48  \n",
      "0                       0                   0  \n",
      "1                       0                   0  \n",
      "2                       0                   0  \n",
      "3                       0                   0  \n",
      "4                       0                   0  \n",
      "5                       0                   0  \n",
      "6                       0                   0  \n",
      "7                       0                   0  \n",
      "8                       0                   0  \n",
      "9                       0                   0  \n",
      "10                      1                   1  \n",
      "11                      0                   0  \n",
      "12                      0                   0  \n",
      "13                      0                   0  \n",
      "14                      0                   0  \n",
      "15                      0                   0  \n",
      "16                      0                   0  \n",
      "17                      0                   0  \n",
      "18                      0                   0  \n",
      "19                      0                   0  \n",
      "20                      0                   0  \n",
      "21                      0                   0  \n",
      "22                      0                   0  \n",
      "23                      0                   0  \n",
      "24                      0                   0  \n",
      "25                      0                   0  \n",
      "26                      0                   0  \n",
      "27                      0                   0  \n",
      "28                      0                   0  \n",
      "29                      0                   0  \n",
      "...                   ...                 ...  \n",
      "59351                   0                   0  \n",
      "59352                   0                   0  \n",
      "59353                   0                   0  \n",
      "59354                   0                   0  \n",
      "59355                   0                   0  \n",
      "59356                   0                   0  \n",
      "59357                   0                   0  \n",
      "59358                   0                   0  \n",
      "59359                   0                   0  \n",
      "59360                   0                   0  \n",
      "59361                   0                   0  \n",
      "59362                   0                   0  \n",
      "59363                   0                   0  \n",
      "59364                   0                   0  \n",
      "59365                   0                   0  \n",
      "59366                   0                   0  \n",
      "59367                   0                   0  \n",
      "59368                   0                   0  \n",
      "59369                   0                   1  \n",
      "59370                   0                   0  \n",
      "59371                   0                   0  \n",
      "59372                   0                   1  \n",
      "59373                   0                   0  \n",
      "59374                   0                   0  \n",
      "59375                   0                   0  \n",
      "59376                   0                   0  \n",
      "59377                   0                   0  \n",
      "59378                   0                   0  \n",
      "59379                   0                   0  \n",
      "59380                   0                   0  \n",
      "\n",
      "[59381 rows x 126 columns]\n"
     ]
    }
   ],
   "source": [
    "# modify response values so that range of values is from 0-7 instead of 1-8\n",
    "y_train = y_train - 1\n",
    "\n",
    "print X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Family_Hist_3': 34241, 'Family_Hist_2': 28656, 'Medical_History_15': 44596, 'Medical_History_10': 58824, 'Family_Hist_5': 41811, 'Family_Hist_4': 19184, 'Insurance_History_5': 25396, 'Medical_History_24': 55580, 'DATASET': 59381, 'Employment_Info_4': 6779, 'Employment_Info_6': 10854, 'Employment_Info_1': 19, 'Medical_History_32': 58274, 'Medical_History_1': 8889}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAFUCAYAAADMLzySAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM2sgJ38zswZy8jczayAnfzOzBvr/5ugk1KP4KycAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ec265810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "isnull_dict = {}\n",
    "for col in df_train.columns:\n",
    "    var = df_train[col].isnull().sum()\n",
    "    if var > 0:\n",
    "        isnull_dict[col] = var\n",
    "\n",
    "isnull_dict['DATASET'] = df_train.shape[0]\n",
    "print isnull_dict\n",
    "\n",
    "\n",
    "plt.bar(range(len(isnull_dict)), isnull_dict.values(), align='center')\n",
    "plt.xticks(range(len(isnull_dict)), isnull_dict.keys(), rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.30747223  0.2953551   0.30185185  0.29925901  0.28848097  0.30447962\n",
      "  0.29925901  0.30626685  0.29755687  0.30771823]\n",
      "[ 0.29585998  0.28946483  0.29848485  0.30161671  0.28595487  0.29151229\n",
      "  0.29083867  0.29346361  0.2946925   0.30367374]\n",
      "[ 0.33288455  0.31588691  0.32592593  0.33007747  0.32266756  0.33496127\n",
      "  0.32452004  0.32766173  0.33715249  0.33400741]\n",
      "[ 0.35207001  0.33372602  0.34057239  0.34220276  0.3383294   0.35281239\n",
      "  0.33967666  0.34989892  0.34995788  0.34496124]\n",
      "[ 0.35947492  0.34634803  0.3543771   0.36173796  0.34708656  0.36443247\n",
      "  0.3555069   0.35630054  0.35922494  0.35776879]\n",
      "[ 0.36603837  0.35610905  0.36094276  0.36561132  0.35954867  0.37521051\n",
      "  0.36695857  0.36556604  0.36647009  0.36535221]\n",
      "[ 0.37361158  0.36385056  0.37053872  0.36881105  0.35870664  0.37672617\n",
      "  0.37369485  0.3712938   0.37337826  0.37158746]\n",
      "[ 0.37495793  0.36704813  0.37760943  0.3701583   0.36577972  0.38009431\n",
      "  0.37689458  0.37011456  0.37809604  0.37175598]\n",
      "[ 0.37512622  0.3705823   0.38265993  0.37066352  0.37201078  0.38043112\n",
      "  0.37942068  0.37634771  0.38213985  0.37664307]\n",
      "[ 0.37983844  0.37344329  0.38518519  0.37605254  0.37268441  0.38632536\n",
      "  0.38076794  0.37769542  0.37978096  0.38169869]\n",
      "[ 0.38101649  0.37394817  0.38653199  0.37470529  0.37723139  0.38329404\n",
      "  0.38093634  0.38308625  0.38365628  0.38355241]\n",
      "[ 0.38168967  0.37899697  0.39158249  0.37773661  0.37807343  0.38598855\n",
      "  0.38346245  0.3824124   0.38028644  0.38523761]\n",
      "[ 0.38488724  0.37731404  0.38973064  0.38144156  0.37773661  0.38649377\n",
      "  0.38295722  0.3824124   0.38011794  0.38624874]\n",
      "[ 0.38556042  0.38017503  0.38872054  0.38043112  0.37689458  0.38784102\n",
      "  0.38177838  0.38645553  0.38247683  0.3859117 ]\n",
      "[ 0.38572871  0.37714574  0.39057239  0.38093634  0.37790502  0.38262041\n",
      "  0.38076794  0.38662399  0.38382477  0.3859117 ]\n",
      "[ 0.38438236  0.3783238   0.39377104  0.382452    0.37403166  0.38632536\n",
      "  0.38211519  0.38662399  0.3851727   0.3884395 ]\n",
      "[ 0.38387748  0.38051161  0.39242424  0.38497811  0.37453688  0.38346245\n",
      "  0.38278882  0.38931941  0.38753159  0.38894506]\n",
      "[ 0.38741165  0.38219455  0.38989899  0.38447289  0.37571573  0.38784102\n",
      "  0.38817784  0.38881402  0.38685762  0.38945062]\n",
      "[ 0.38657018  0.38017503  0.39124579  0.38851465  0.37335803  0.38548333\n",
      "  0.38851465  0.38881402  0.38871104  0.38658578]\n",
      "[ 0.38657018  0.38000673  0.38720539  0.38548333  0.37436847  0.38413607\n",
      "  0.38699899  0.38645553  0.38887953  0.38776542]\n",
      "[ 0.38673847  0.38034332  0.38804714  0.38379926  0.37318963  0.38868306\n",
      "  0.38565173  0.38999326  0.38786858  0.38405797]\n",
      "[ 0.38539212  0.37899697  0.39124579  0.38497811  0.37521051  0.38952509\n",
      "  0.38935669  0.39083558  0.38786858  0.38641726]\n",
      "[ 0.38690677  0.3781555   0.38703704  0.38649377  0.37672617  0.38817784\n",
      "  0.39003031  0.39100404  0.38668913  0.38658578]\n",
      "[ 0.38808482  0.37967014  0.38737374  0.38817784  0.37908387  0.38935669\n",
      "  0.39087235  0.39100404  0.3853412   0.38709134]\n",
      "[ 0.38774823  0.38101649  0.38754209  0.38514651  0.37773661  0.38851465\n",
      "  0.38952509  0.39100404  0.38416175  0.3884395 ]\n",
      "0.386605601713 47\n"
     ]
    }
   ],
   "source": [
    "# kNN\n",
    "\n",
    "from sklearn import cross_validation\n",
    "n_neighbors = range(1,51,2)\n",
    "for i, n in enumerate(range(1,51,2)):\n",
    "    model = kNN(n)\n",
    "    model.fit(X_train, y_train)\n",
    "    kscores = cross_validation.cross_val_score(model, X_train, y_train, cv=10)  \n",
    "    print kscores\n",
    "    \n",
    "    if i == 0:\n",
    "        score = kscores.mean()\n",
    "    else:\n",
    "        if kscores.mean() > score:\n",
    "            score = kscores.mean()\n",
    "            k = n\n",
    "            \n",
    "print score, k\n",
    "#print(X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-26205539aa57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0;34m'classification'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m ])\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"feature selection w random forest:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \"\"\"\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36m_pre_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/from_model.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"estimator_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/sklearn/svm/classes.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             self.loss)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"crammer_singer\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0;31m# LibLinear wants targets as doubles, even for classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0my_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0msolver_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_liblinear_solver_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     raw_coef_, n_iter_ = liblinear.train_wrap(\n\u001b[1;32m    914\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36m_get_liblinear_solver_type\u001b[0;34m(multi_class, penalty, loss, dual)\u001b[0m\n\u001b[1;32m    767\u001b[0m     raise ValueError('Unsupported set of arguments: %s, '\n\u001b[1;32m    768\u001b[0m                      \u001b[0;34m'Parameters: penalty=%r, loss=%r, dual=%r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m                      % (error_string, penalty, loss, dual))\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
    "from itertools import product\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "\n",
    "# Loading some example data\n",
    "#iris = datasets.load_iris()\n",
    "#X = iris.data[:, [0,2]]\n",
    "#y = iris.target\n",
    "\n",
    "#nystroem_feature = Nystroem(kernel='rbf', gamma=None, coef0=1, degree=3, kernel_params=None, n_components=126, random_state=42)\n",
    "#X_features = nystroem_feature.fit_transform(X_train)\n",
    "\n",
    "# Training classifiers\n",
    "#clf1 = RandomForestClassifier(n_estimators=120, max_features=int(len(df_train.columns)**0.5), max_depth=None)\n",
    "#clf2 = kNN(47)\n",
    "#clf3 = SVC(kernel='rbf', probability=True)\n",
    "#clf3 = SGDClassifier(loss='modified_huber', penalty='l2')\n",
    "#eclf = VotingClassifier(estimators=[('rf', clf1), ('kNN', clf2), ('sgd', clf3)], voting='soft', weights=[4,2,1])\n",
    "\n",
    "#clf1 = clf1.fit(X_train,y_train)\n",
    "#clf2 = clf2.fit(X_train,y_train)\n",
    "#clf3 = clf3.fit(X_train,y_train)\n",
    "#eclf = eclf.fit(X_train,y_train)\n",
    "\n",
    "#print(clf1.score(X_train, y_train))\n",
    "#clf1.score(X_train, y_train)\n",
    "\n",
    "#print(clf2.score(X_train, y_train))\n",
    "\n",
    "#print(clf3.score(X_train, y_train))\n",
    "#clf3.score(X_train, y_train)\n",
    "\n",
    "#print(eclf.score(X_train, y_train))\n",
    "#eclf.score(X_train, y_train)\n",
    "\n",
    "\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "#forest = RandomForestClassifier(n_estimators=3000,\n",
    "#     max_features=int(len(X_train.columns)**0.5), random_state=0, max_depth=None)\n",
    "#scores = cross_validation.cross_val_score(forest, X_train, y_train)\n",
    "#print \"RandomForestClassifier:\", scores.mean()\n",
    "#print time.clock()-t0\n",
    "\n",
    "#250 estimators: .57 -- time: \n",
    "#2500 estimators: .578 -- time: \n",
    "\n",
    "\n",
    "categorical = ['Product_Info_1', 'Product_Info_2', 'Product_Info_3', 'Product_Info_5', 'Product_Info_6', 'Product_Info_7', 'Employment_Info_2', 'Employment_Info_3', 'Employment_Info_5', 'InsuredInfo_1', 'InsuredInfo_2', 'InsuredInfo_3', 'InsuredInfo_4', 'InsuredInfo_5', 'InsuredInfo_6', 'InsuredInfo_7', 'Insurance_History_1', 'Insurance_History_2', 'Insurance_History_3', 'Insurance_History_4', 'Insurance_History_7', 'Insurance_History_8', 'Insurance_History_9', 'Family_Hist_1', 'Medical_History_2', 'Medical_History_3', 'Medical_History_4', 'Medical_History_5', 'Medical_History_6', 'Medical_History_7', 'Medical_History_8', 'Medical_History_9', 'Medical_History_10', 'Medical_History_11', 'Medical_History_12', 'Medical_History_13', 'Medical_History_14', 'Medical_History_16', 'Medical_History_17', 'Medical_History_18', 'Medical_History_19', 'Medical_History_20', 'Medical_History_21', 'Medical_History_22', 'Medical_History_23', 'Medical_History_25', 'Medical_History_26', 'Medical_History_27', 'Medical_History_28', 'Medical_History_29', 'Medical_History_30', 'Medical_History_31', 'Medical_History_33', 'Medical_History_34', 'Medical_History_35', 'Medical_History_36', 'Medical_History_37', 'Medical_History_38', 'Medical_History_39', 'Medical_History_40', 'Medical_History_41','Medical_History_1', 'Medical_History_15', 'Medical_History_24', 'Medical_History_32']\n",
    "\n",
    "cat_train_df = X_train[categorical]\n",
    "cat_train_dict = cat_train_df.T.to_dict().values()\n",
    "\n",
    "x_train_no_cat = X_train.drop(categorical, axis=1)\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "\n",
    "vectorizer = DV(sparse=False)\n",
    "vec_x_cat_train = pd.DataFrame(vectorizer.fit_transform(cat_train_dict))\n",
    "\n",
    "x_train = pd.concat([x_train_no_cat, vec_x_cat_train], axis=1)\n",
    "\n",
    "features=x_train.columns.tolist()\n",
    "#print len(features)\n",
    "#features.remove(\"Id\")\n",
    "#features.remove(\"Response\")\n",
    "\n",
    "train_features=x_train[features].fillna(-9999)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "t0 = time.clock()\n",
    "\n",
    "clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\"))),\n",
    "  ('classification', RandomForestClassifier())\n",
    "])\n",
    "clf.fit(train_features, y_train)\n",
    "scores = cross_validation.cross_val_score(clf, train_features, y_train)\n",
    "print \"feature selection w random forest:\", scores.mean()\n",
    "print time.clock()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import RandomTreesEmbedding, ExtraTreesClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#bagging = BaggingClassifier(ExtraTreesClassifier(max_depth=50, n_estimators=100, random_state=0), max_samples=0.5, max_features=0.5)\n",
    "#bagging_clf = bagging.fit(X_train, y_train)\n",
    "#print bagging_clf.score(X_train, y_train)\n",
    "#scores = cross_val_score(bagging_clf, X_train, y_train)\n",
    "#print scores.mean()\n",
    "\n",
    "\n",
    "#trees = ExtraTreesClassifier(max_depth=500, n_estimators=100, random_state=42)\n",
    "#trees_clf = trees.fit(X_train, y_train)\n",
    "#print trees_clf.score(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#XGBoost try #1\n",
    "\n",
    "#gbm = xgb.XGBClassifier(max_depth=30, n_estimators=100, learning_rate=0.01)\n",
    "#gbm_clf = gbm.fit(X_train, y_train)\n",
    "#print gbm_clf.score(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#XGBoost try #2\n",
    "params = {\"objective\": \"multi:softmax\", \"num_class\": 8}\n",
    "T_train_xgb = xgb.DMatrix(X_train, y_train)\n",
    "X_test_xgb  = xgb.DMatrix(X_test)\n",
    "gbm2 = xgb.train(params, T_train_xgb, 20)\n",
    "y_pred3 = gbm2.predict(X_test_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print metrics.confusion_matrix(y_train, gbm_clf.predict(X_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change values back to range of values is from 1-8 instead of 0-7\n",
    "#y_pred = clf1.predict(X_test)\n",
    "#y_pred = y_pred + 1\n",
    "#y_pred = y_pred.astype(int)\n",
    "\n",
    "y_pred2 = gbm_clf.predict(X_test)\n",
    "y_pred2 = y_pred2 + 1\n",
    "y_pred2 = y_pred2.astype(int)\n",
    "\n",
    "\n",
    "y_pred3 = y_pred3 + 1\n",
    "y_pred3 = y_pred3.astype(int)\n",
    "\n",
    "#sumsquares = sum((y_pred - y_pred2)**2)\n",
    "\n",
    "#print metrics.confusion_matrix(y_train, eclf.predict(X_train))\n",
    "\n",
    "\n",
    "#feature importance\n",
    "#print pd.DataFrame(clf1.feature_importances_,index=X_train.columns).sort([0], ascending=False) [:10]\n",
    "#print pd.DataFrame(clf2.feature_importances_,index=X_train.columns).sort([0], ascending=False) [:10]\n",
    "#print pd.DataFrame(clf3.feature_importances_,index=X_train.columns).sort([0], ascending=False) [:10]\n",
    "\n",
    "\n",
    "\n",
    "#print X_train.shape\n",
    "\n",
    "\n",
    "\n",
    "#use this block to keep testing the difference between my best model and the newest model\n",
    "#right now the best model is just using random forest -- the kernal approximation doesn't add much to it (the way I have it set up)\n",
    "\n",
    "#kernal approx --> sgd + random forest --> soft voting\n",
    "#think of another model that may work better\n",
    "\n",
    "#alternatively, think of how to mess with the different types of data so that what is fed into the model is a bit better captured\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create submission\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"Id\": df_test[\"Id\"],\n",
    "        \"Response\": y_pred2\n",
    "    })\n",
    "submission.to_csv('prudential.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## My next steps are to:\n",
    "#1) organize this code to make it slightly more readable\n",
    "#2) investigate making multiple models for each response output (ala Jonathan Smakla or whatever his name is)\n",
    "#3) investigate XGBoost and what it is doing specifically\n",
    "#4) investigate any feature selection, imputation that I am doing and modify accordingly\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'Id', u'Product_Info_1', u'Product_Info_2', u'Product_Info_3',\n",
      "       u'Product_Info_4', u'Product_Info_5', u'Product_Info_6',\n",
      "       u'Product_Info_7', u'Ins_Age', u'Ht', \n",
      "       ...\n",
      "       u'Medical_Keyword_40', u'Medical_Keyword_41', u'Medical_Keyword_42',\n",
      "       u'Medical_Keyword_43', u'Medical_Keyword_44', u'Medical_Keyword_45',\n",
      "       u'Medical_Keyword_46', u'Medical_Keyword_47', u'Medical_Keyword_48',\n",
      "       u'Response'],\n",
      "      dtype='object', length=128)\n"
     ]
    }
   ],
   "source": [
    "#starting here again... pk 1/6 @ 6:30pm\n",
    "#todo: using the train data as a train/test set to evaluate the model before submitting\n",
    "#use cross validation to really see how will perform in the wild\n",
    "#can i use xgboost in this way?\n",
    "#what about a bunch of log regressions with one vs all?\n",
    "\n",
    "#think about using a function or a pipeline (see bayesian-methods in-class exercise):\n",
    "#\n",
    "#\n",
    "#pipeline = Pipeline((\n",
    "#    ('vec', TfidfVectorizer(max_df = 0.8, ngram_range = (1, 2), use_idf=True)),\n",
    "#    ('clf', MultinomialNB(alpha = 0.01)),\n",
    "#))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "DATA_DIR = '/Users/patrickkennedy/Desktop'\n",
    "\n",
    "train = pd.read_csv(DATA_DIR + '/Project DATA/train.csv')\n",
    "test = pd.read_csv(DATA_DIR + '/Project DATA/test.csv')\n",
    "\n",
    "\n",
    "features = train.columns\n",
    "features.drop(\"Id\")\n",
    "features.drop(\"Response\")\n",
    "\n",
    "#let's split the train data into train/test so we can do some model validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[features], train.Response, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "#We transform categorical values to dummies 0/1\n",
    "categorical = ['Product_Info_1', 'Product_Info_2', 'Product_Info_3', 'Product_Info_5', 'Product_Info_6', 'Product_Info_7', 'Employment_Info_2', 'Employment_Info_3', 'Employment_Info_5', 'InsuredInfo_1', 'InsuredInfo_2', 'InsuredInfo_3', 'InsuredInfo_4', 'InsuredInfo_5', 'InsuredInfo_6', 'InsuredInfo_7', 'Insurance_History_1', 'Insurance_History_2', 'Insurance_History_3', 'Insurance_History_4', 'Insurance_History_7', 'Insurance_History_8', 'Insurance_History_9', 'Family_Hist_1', 'Medical_History_2', 'Medical_History_3', 'Medical_History_4', 'Medical_History_5', 'Medical_History_6', 'Medical_History_7', 'Medical_History_8', 'Medical_History_9', 'Medical_History_10', 'Medical_History_11', 'Medical_History_12', 'Medical_History_13', 'Medical_History_14', 'Medical_History_16', 'Medical_History_17', 'Medical_History_18', 'Medical_History_19', 'Medical_History_20', 'Medical_History_21', 'Medical_History_22', 'Medical_History_23', 'Medical_History_25', 'Medical_History_26', 'Medical_History_27', 'Medical_History_28', 'Medical_History_29', 'Medical_History_30', 'Medical_History_31', 'Medical_History_33', 'Medical_History_34', 'Medical_History_35', 'Medical_History_36', 'Medical_History_37', 'Medical_History_38', 'Medical_History_39', 'Medical_History_40', 'Medical_History_41','Medical_History_1', 'Medical_History_15', 'Medical_History_24', 'Medical_History_32']\n",
    "\n",
    "#cat_train_df = train[categorical]\n",
    "#cat_train_dict = cat_train_df.T.to_dict().values()\n",
    "\n",
    "#cat_test_df = test[categorical]\n",
    "#cat_test_dict = cat_test_df.T.to_dict().values()\n",
    "\n",
    "\n",
    "#x_train_no_cat = train.drop(categorical, axis=1)\n",
    "#x_test_no_cat = test.drop(categorical, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using dict vectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "\n",
    "vectorizer = DV(sparse=False)\n",
    "vec_x_cat_train = pd.DataFrame(vectorizer.fit_transform(cat_train_dict))\n",
    "vec_x_cat_test = pd.DataFrame(vectorizer.transform(cat_test_dict)) \n",
    "\n",
    "x_train = pd.concat([x_train_no_cat, vec_x_cat_train], axis=1)\n",
    "x_test = pd.concat([x_test_no_cat, vec_x_cat_test], axis=1)\n",
    "\n",
    "#how to ensemble xgboost given that the data has been vectorized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Product_Info_4</th>\n",
       "      <th>Ins_Age</th>\n",
       "      <th>Ht</th>\n",
       "      <th>Wt</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Employment_Info_1</th>\n",
       "      <th>Employment_Info_4</th>\n",
       "      <th>Employment_Info_6</th>\n",
       "      <th>Insurance_History_5</th>\n",
       "      <th>...</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.148536</td>\n",
       "      <td>0.323008</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.131799</td>\n",
       "      <td>0.272288</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.288703</td>\n",
       "      <td>0.428780</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.164179</td>\n",
       "      <td>0.672727</td>\n",
       "      <td>0.205021</td>\n",
       "      <td>0.352438</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.417910</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.234310</td>\n",
       "      <td>0.424046</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Product_Info_4   Ins_Age        Ht        Wt       BMI  \\\n",
       "0   2        0.076923  0.641791  0.581818  0.148536  0.323008   \n",
       "1   5        0.076923  0.059701  0.600000  0.131799  0.272288   \n",
       "2   6        0.076923  0.029851  0.745455  0.288703  0.428780   \n",
       "3   7        0.487179  0.164179  0.672727  0.205021  0.352438   \n",
       "4   8        0.230769  0.417910  0.654545  0.234310  0.424046   \n",
       "\n",
       "   Employment_Info_1  Employment_Info_4  Employment_Info_6  \\\n",
       "0              0.028                  0                NaN   \n",
       "1              0.000                  0             0.0018   \n",
       "2              0.030                  0             0.0300   \n",
       "3              0.042                  0             0.2000   \n",
       "4              0.027                  0             0.0500   \n",
       "\n",
       "   Insurance_History_5 ...  73  74  75  76  77  78  79  80  81  82  \n",
       "0             0.000667 ...   0   0   0   1   0   0  10   2   1   1  \n",
       "1             0.000133 ...   0   0   0   0   0   0  26   2   3   1  \n",
       "2                  NaN ...   0   0   0   0   0   1  26   2   3   1  \n",
       "3                  NaN ...   0   0   0   0   1   0  10   2   3   1  \n",
       "4                  NaN ...   0   0   1   0   0   0  26   2   3   1  \n",
       "\n",
       "[5 rows x 146 columns]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:4.809948\n",
      "[1]\ttrain-rmse:4.807632\n",
      "[2]\ttrain-rmse:4.805288\n",
      "[3]\ttrain-rmse:4.803633\n",
      "[4]\ttrain-rmse:4.801734\n",
      "[5]\ttrain-rmse:4.799393\n",
      "[6]\ttrain-rmse:4.797567\n",
      "[7]\ttrain-rmse:4.795777\n",
      "[8]\ttrain-rmse:4.793694\n",
      "[9]\ttrain-rmse:4.791641\n",
      "[10]\ttrain-rmse:4.789949\n",
      "[11]\ttrain-rmse:4.787881\n",
      "[12]\ttrain-rmse:4.785823\n",
      "[13]\ttrain-rmse:4.783958\n",
      "[14]\ttrain-rmse:4.782052\n",
      "[15]\ttrain-rmse:4.779992\n",
      "[16]\ttrain-rmse:4.777930\n",
      "[17]\ttrain-rmse:4.776231\n",
      "[18]\ttrain-rmse:4.774220\n",
      "[19]\ttrain-rmse:4.772139\n",
      "[20]\ttrain-rmse:4.770239\n",
      "[21]\ttrain-rmse:4.768413\n",
      "[22]\ttrain-rmse:4.766463\n",
      "[23]\ttrain-rmse:4.764475\n",
      "[24]\ttrain-rmse:4.762676\n",
      "[25]\ttrain-rmse:4.760787\n",
      "[26]\ttrain-rmse:4.758659\n",
      "[27]\ttrain-rmse:4.756709\n",
      "[28]\ttrain-rmse:4.754959\n",
      "[29]\ttrain-rmse:4.752934\n",
      "[30]\ttrain-rmse:4.750872\n",
      "[31]\ttrain-rmse:4.749055\n",
      "[32]\ttrain-rmse:4.747135\n",
      "[33]\ttrain-rmse:4.745081\n",
      "[34]\ttrain-rmse:4.743157\n",
      "[35]\ttrain-rmse:4.741351\n",
      "[36]\ttrain-rmse:4.739406\n",
      "[37]\ttrain-rmse:4.737375\n",
      "[38]\ttrain-rmse:4.735545\n",
      "[39]\ttrain-rmse:4.733650\n",
      "[40]\ttrain-rmse:4.731625\n",
      "[41]\ttrain-rmse:4.729661\n",
      "[42]\ttrain-rmse:4.727854\n",
      "[43]\ttrain-rmse:4.725873\n",
      "[44]\ttrain-rmse:4.723825\n",
      "[45]\ttrain-rmse:4.721966\n",
      "[46]\ttrain-rmse:4.720098\n",
      "[47]\ttrain-rmse:4.718137\n",
      "[48]\ttrain-rmse:4.716193\n",
      "[49]\ttrain-rmse:4.714332\n",
      "[50]\ttrain-rmse:4.712420\n",
      "[51]\ttrain-rmse:4.710478\n",
      "[52]\ttrain-rmse:4.708604\n",
      "[53]\ttrain-rmse:4.706790\n",
      "[54]\ttrain-rmse:4.704792\n",
      "[55]\ttrain-rmse:4.702843\n",
      "[56]\ttrain-rmse:4.701004\n",
      "[57]\ttrain-rmse:4.699139\n",
      "[58]\ttrain-rmse:4.697118\n",
      "[59]\ttrain-rmse:4.695188\n",
      "[60]\ttrain-rmse:4.693328\n",
      "[61]\ttrain-rmse:4.691412\n",
      "[62]\ttrain-rmse:4.689472\n",
      "[63]\ttrain-rmse:4.687613\n",
      "[64]\ttrain-rmse:4.685761\n",
      "[65]\ttrain-rmse:4.683859\n",
      "[66]\ttrain-rmse:4.681873\n",
      "[67]\ttrain-rmse:4.680013\n",
      "[68]\ttrain-rmse:4.678114\n",
      "[69]\ttrain-rmse:4.676149\n",
      "[70]\ttrain-rmse:4.674260\n",
      "[71]\ttrain-rmse:4.672416\n",
      "[72]\ttrain-rmse:4.670518\n",
      "[73]\ttrain-rmse:4.668577\n",
      "[74]\ttrain-rmse:4.666712\n",
      "[75]\ttrain-rmse:4.664837\n",
      "[76]\ttrain-rmse:4.662897\n",
      "[77]\ttrain-rmse:4.661004\n",
      "[78]\ttrain-rmse:4.659115\n",
      "[79]\ttrain-rmse:4.657195\n",
      "[80]\ttrain-rmse:4.655286\n",
      "[81]\ttrain-rmse:4.653433\n",
      "[82]\ttrain-rmse:4.651581\n",
      "[83]\ttrain-rmse:4.649662\n",
      "[84]\ttrain-rmse:4.647720\n",
      "[85]\ttrain-rmse:4.645920\n",
      "[86]\ttrain-rmse:4.644073\n",
      "[87]\ttrain-rmse:4.642115\n",
      "[88]\ttrain-rmse:4.640265\n",
      "[89]\ttrain-rmse:4.638414\n",
      "[90]\ttrain-rmse:4.636550\n",
      "[91]\ttrain-rmse:4.634617\n",
      "[92]\ttrain-rmse:4.632733\n",
      "[93]\ttrain-rmse:4.630898\n",
      "[94]\ttrain-rmse:4.628997\n",
      "[95]\ttrain-rmse:4.627066\n",
      "[96]\ttrain-rmse:4.625257\n",
      "[97]\ttrain-rmse:4.623414\n",
      "[98]\ttrain-rmse:4.621496\n",
      "[99]\ttrain-rmse:4.619600\n",
      "[100]\ttrain-rmse:4.617778\n",
      "[101]\ttrain-rmse:4.615911\n",
      "[102]\ttrain-rmse:4.614017\n",
      "[103]\ttrain-rmse:4.612123\n",
      "[104]\ttrain-rmse:4.610285\n",
      "[105]\ttrain-rmse:4.608426\n",
      "[106]\ttrain-rmse:4.606528\n",
      "[107]\ttrain-rmse:4.604692\n",
      "[108]\ttrain-rmse:4.602845\n",
      "[109]\ttrain-rmse:4.600961\n",
      "[110]\ttrain-rmse:4.599048\n",
      "[111]\ttrain-rmse:4.597208\n",
      "[112]\ttrain-rmse:4.595335\n",
      "[113]\ttrain-rmse:4.593460\n",
      "[114]\ttrain-rmse:4.591615\n",
      "[115]\ttrain-rmse:4.589770\n",
      "[116]\ttrain-rmse:4.587945\n",
      "[117]\ttrain-rmse:4.586030\n",
      "[118]\ttrain-rmse:4.584175\n",
      "[119]\ttrain-rmse:4.582356\n",
      "[120]\ttrain-rmse:4.580535\n",
      "[121]\ttrain-rmse:4.578636\n",
      "[122]\ttrain-rmse:4.576834\n",
      "[123]\ttrain-rmse:4.575007\n",
      "[124]\ttrain-rmse:4.573167\n",
      "[125]\ttrain-rmse:4.571269\n",
      "[126]\ttrain-rmse:4.569438\n",
      "[127]\ttrain-rmse:4.567575\n",
      "[128]\ttrain-rmse:4.565707\n",
      "[129]\ttrain-rmse:4.563883\n",
      "[130]\ttrain-rmse:4.562020\n",
      "[131]\ttrain-rmse:4.560223\n",
      "[132]\ttrain-rmse:4.558334\n",
      "[133]\ttrain-rmse:4.556491\n",
      "[134]\ttrain-rmse:4.554692\n",
      "[135]\ttrain-rmse:4.552907\n",
      "[136]\ttrain-rmse:4.551035\n",
      "[137]\ttrain-rmse:4.549210\n",
      "[138]\ttrain-rmse:4.547372\n",
      "[139]\ttrain-rmse:4.545562\n",
      "[140]\ttrain-rmse:4.543703\n",
      "[141]\ttrain-rmse:4.541879\n",
      "[142]\ttrain-rmse:4.540064\n",
      "[143]\ttrain-rmse:4.538255\n",
      "[144]\ttrain-rmse:4.536374\n",
      "[145]\ttrain-rmse:4.534607\n",
      "[146]\ttrain-rmse:4.532798\n",
      "[147]\ttrain-rmse:4.530970\n",
      "[148]\ttrain-rmse:4.529104\n",
      "[149]\ttrain-rmse:4.527328\n",
      "[150]\ttrain-rmse:4.525525\n",
      "[151]\ttrain-rmse:4.523678\n",
      "[152]\ttrain-rmse:4.521855\n",
      "[153]\ttrain-rmse:4.520023\n",
      "[154]\ttrain-rmse:4.518254\n",
      "[155]\ttrain-rmse:4.516395\n",
      "[156]\ttrain-rmse:4.514627\n",
      "[157]\ttrain-rmse:4.512775\n",
      "[158]\ttrain-rmse:4.510962\n",
      "[159]\ttrain-rmse:4.509121\n",
      "[160]\ttrain-rmse:4.507349\n",
      "[161]\ttrain-rmse:4.505497\n",
      "[162]\ttrain-rmse:4.503715\n",
      "[163]\ttrain-rmse:4.501903\n",
      "[164]\ttrain-rmse:4.500116\n",
      "[165]\ttrain-rmse:4.498266\n",
      "[166]\ttrain-rmse:4.496495\n",
      "[167]\ttrain-rmse:4.494660\n",
      "[168]\ttrain-rmse:4.492884\n",
      "[169]\ttrain-rmse:4.491039\n",
      "[170]\ttrain-rmse:4.489263\n",
      "[171]\ttrain-rmse:4.487445\n",
      "[172]\ttrain-rmse:4.485671\n",
      "[173]\ttrain-rmse:4.483840\n",
      "[174]\ttrain-rmse:4.482060\n",
      "[175]\ttrain-rmse:4.480229\n",
      "[176]\ttrain-rmse:4.478486\n",
      "[177]\ttrain-rmse:4.476651\n",
      "[178]\ttrain-rmse:4.474870\n",
      "[179]\ttrain-rmse:4.473033\n",
      "[180]\ttrain-rmse:4.471293\n",
      "[181]\ttrain-rmse:4.469465\n",
      "[182]\ttrain-rmse:4.467695\n",
      "[183]\ttrain-rmse:4.465858\n",
      "[184]\ttrain-rmse:4.464091\n",
      "[185]\ttrain-rmse:4.462268\n",
      "[186]\ttrain-rmse:4.460524\n",
      "[187]\ttrain-rmse:4.458702\n",
      "[188]\ttrain-rmse:4.456962\n",
      "[189]\ttrain-rmse:4.455133\n",
      "[190]\ttrain-rmse:4.453401\n",
      "[191]\ttrain-rmse:4.451580\n",
      "[192]\ttrain-rmse:4.449849\n",
      "[193]\ttrain-rmse:4.448004\n",
      "[194]\ttrain-rmse:4.446279\n",
      "[195]\ttrain-rmse:4.444442\n",
      "[196]\ttrain-rmse:4.442730\n",
      "[197]\ttrain-rmse:4.440926\n",
      "[198]\ttrain-rmse:4.439183\n",
      "[199]\ttrain-rmse:4.437375\n",
      "[200]\ttrain-rmse:4.435646\n",
      "[201]\ttrain-rmse:4.433852\n",
      "[202]\ttrain-rmse:4.432082\n",
      "[203]\ttrain-rmse:4.430308\n",
      "[204]\ttrain-rmse:4.428596\n",
      "[205]\ttrain-rmse:4.426790\n",
      "[206]\ttrain-rmse:4.425030\n",
      "[207]\ttrain-rmse:4.423249\n",
      "[208]\ttrain-rmse:4.421489\n",
      "[209]\ttrain-rmse:4.419706\n",
      "[210]\ttrain-rmse:4.417947\n",
      "[211]\ttrain-rmse:4.416179\n",
      "[212]\ttrain-rmse:4.414429\n",
      "[213]\ttrain-rmse:4.412676\n",
      "[214]\ttrain-rmse:4.410901\n",
      "[215]\ttrain-rmse:4.409134\n",
      "[216]\ttrain-rmse:4.407399\n",
      "[217]\ttrain-rmse:4.405608\n",
      "[218]\ttrain-rmse:4.403870\n",
      "[219]\ttrain-rmse:4.402123\n",
      "[220]\ttrain-rmse:4.400348\n",
      "[221]\ttrain-rmse:4.398623\n",
      "[222]\ttrain-rmse:4.396886\n",
      "[223]\ttrain-rmse:4.395145\n",
      "[224]\ttrain-rmse:4.393345\n",
      "[225]\ttrain-rmse:4.391622\n",
      "[226]\ttrain-rmse:4.389843\n",
      "[227]\ttrain-rmse:4.388132\n",
      "[228]\ttrain-rmse:4.386334\n",
      "[229]\ttrain-rmse:4.384620\n",
      "[230]\ttrain-rmse:4.382832\n",
      "[231]\ttrain-rmse:4.381104\n",
      "[232]\ttrain-rmse:4.379332\n",
      "[233]\ttrain-rmse:4.377618\n",
      "[234]\ttrain-rmse:4.375873\n",
      "[235]\ttrain-rmse:4.374136\n",
      "[236]\ttrain-rmse:4.372377\n",
      "[237]\ttrain-rmse:4.370670\n",
      "[238]\ttrain-rmse:4.368889\n",
      "[239]\ttrain-rmse:4.367177\n",
      "[240]\ttrain-rmse:4.365417\n",
      "[241]\ttrain-rmse:4.363705\n",
      "[242]\ttrain-rmse:4.361976\n",
      "[243]\ttrain-rmse:4.360247\n",
      "[244]\ttrain-rmse:4.358515\n",
      "[245]\ttrain-rmse:4.356782\n",
      "[246]\ttrain-rmse:4.355095\n",
      "[247]\ttrain-rmse:4.353354\n",
      "[248]\ttrain-rmse:4.351638\n",
      "[249]\ttrain-rmse:4.349874\n",
      "[250]\ttrain-rmse:4.348180\n",
      "[251]\ttrain-rmse:4.346463\n",
      "[252]\ttrain-rmse:4.344726\n",
      "[253]\ttrain-rmse:4.342993\n",
      "[254]\ttrain-rmse:4.341299\n",
      "[255]\ttrain-rmse:4.339543\n",
      "[256]\ttrain-rmse:4.337863\n",
      "[257]\ttrain-rmse:4.336100\n",
      "[258]\ttrain-rmse:4.334421\n",
      "[259]\ttrain-rmse:4.332684\n",
      "[260]\ttrain-rmse:4.330989\n",
      "[261]\ttrain-rmse:4.329269\n",
      "[262]\ttrain-rmse:4.327561\n",
      "[263]\ttrain-rmse:4.325868\n",
      "[264]\ttrain-rmse:4.324140\n",
      "[265]\ttrain-rmse:4.322413\n",
      "[266]\ttrain-rmse:4.320704\n",
      "[267]\ttrain-rmse:4.319006\n",
      "[268]\ttrain-rmse:4.317285\n",
      "[269]\ttrain-rmse:4.315640\n",
      "[270]\ttrain-rmse:4.313890\n",
      "[271]\ttrain-rmse:4.312253\n",
      "[272]\ttrain-rmse:4.310488\n",
      "[273]\ttrain-rmse:4.308795\n",
      "[274]\ttrain-rmse:4.307076\n",
      "[275]\ttrain-rmse:4.305386\n",
      "[276]\ttrain-rmse:4.303684\n",
      "[277]\ttrain-rmse:4.301967\n",
      "[278]\ttrain-rmse:4.300298\n",
      "[279]\ttrain-rmse:4.298573\n",
      "[280]\ttrain-rmse:4.296911\n",
      "[281]\ttrain-rmse:4.295192\n",
      "[282]\ttrain-rmse:4.293525\n",
      "[283]\ttrain-rmse:4.291787\n",
      "[284]\ttrain-rmse:4.290131\n",
      "[285]\ttrain-rmse:4.288416\n",
      "[286]\ttrain-rmse:4.286750\n",
      "[287]\ttrain-rmse:4.285047\n",
      "[288]\ttrain-rmse:4.283364\n",
      "[289]\ttrain-rmse:4.281671\n",
      "[290]\ttrain-rmse:4.279979\n",
      "[291]\ttrain-rmse:4.278315\n",
      "[292]\ttrain-rmse:4.276618\n",
      "[293]\ttrain-rmse:4.274981\n",
      "[294]\ttrain-rmse:4.273257\n",
      "[295]\ttrain-rmse:4.271606\n",
      "[296]\ttrain-rmse:4.269896\n",
      "[297]\ttrain-rmse:4.268216\n",
      "[298]\ttrain-rmse:4.266541\n",
      "[299]\ttrain-rmse:4.264895\n",
      "[300]\ttrain-rmse:4.263225\n",
      "[301]\ttrain-rmse:4.261536\n",
      "[302]\ttrain-rmse:4.259910\n",
      "[303]\ttrain-rmse:4.258183\n",
      "[304]\ttrain-rmse:4.256531\n",
      "[305]\ttrain-rmse:4.254831\n",
      "[306]\ttrain-rmse:4.253185\n",
      "[307]\ttrain-rmse:4.251504\n",
      "[308]\ttrain-rmse:4.249853\n",
      "[309]\ttrain-rmse:4.248188\n",
      "[310]\ttrain-rmse:4.246506\n",
      "[311]\ttrain-rmse:4.244862\n",
      "[312]\ttrain-rmse:4.243151\n",
      "[313]\ttrain-rmse:4.241508\n",
      "[314]\ttrain-rmse:4.239825\n",
      "[315]\ttrain-rmse:4.238194\n",
      "[316]\ttrain-rmse:4.236516\n",
      "[317]\ttrain-rmse:4.234869\n",
      "[318]\ttrain-rmse:4.233188\n",
      "[319]\ttrain-rmse:4.231521\n",
      "[320]\ttrain-rmse:4.229908\n",
      "[321]\ttrain-rmse:4.228207\n",
      "[322]\ttrain-rmse:4.226604\n",
      "[323]\ttrain-rmse:4.224885\n",
      "[324]\ttrain-rmse:4.223267\n",
      "[325]\ttrain-rmse:4.221579\n",
      "[326]\ttrain-rmse:4.219941\n",
      "[327]\ttrain-rmse:4.218314\n",
      "[328]\ttrain-rmse:4.216636\n",
      "[329]\ttrain-rmse:4.215049\n",
      "[330]\ttrain-rmse:4.213362\n",
      "[331]\ttrain-rmse:4.211767\n",
      "[332]\ttrain-rmse:4.210053\n",
      "[333]\ttrain-rmse:4.208438\n",
      "[334]\ttrain-rmse:4.206785\n",
      "[335]\ttrain-rmse:4.205140\n",
      "[336]\ttrain-rmse:4.203558\n",
      "[337]\ttrain-rmse:4.201859\n",
      "[338]\ttrain-rmse:4.200271\n",
      "[339]\ttrain-rmse:4.198595\n",
      "[340]\ttrain-rmse:4.196982\n",
      "[341]\ttrain-rmse:4.195346\n",
      "[342]\ttrain-rmse:4.193734\n",
      "[343]\ttrain-rmse:4.192100\n",
      "[344]\ttrain-rmse:4.190423\n",
      "[345]\ttrain-rmse:4.188854\n",
      "[346]\ttrain-rmse:4.187140\n",
      "[347]\ttrain-rmse:4.185571\n",
      "[348]\ttrain-rmse:4.183898\n",
      "[349]\ttrain-rmse:4.182296\n",
      "[350]\ttrain-rmse:4.180691\n",
      "[351]\ttrain-rmse:4.179024\n",
      "[352]\ttrain-rmse:4.177439\n",
      "[353]\ttrain-rmse:4.175751\n",
      "[354]\ttrain-rmse:4.174206\n",
      "[355]\ttrain-rmse:4.172503\n",
      "[356]\ttrain-rmse:4.170921\n",
      "[357]\ttrain-rmse:4.169298\n",
      "[358]\ttrain-rmse:4.167638\n",
      "[359]\ttrain-rmse:4.166074\n",
      "[360]\ttrain-rmse:4.164382\n",
      "[361]\ttrain-rmse:4.162829\n",
      "[362]\ttrain-rmse:4.161165\n",
      "[363]\ttrain-rmse:4.159565\n",
      "[364]\ttrain-rmse:4.157915\n",
      "[365]\ttrain-rmse:4.156298\n",
      "[366]\ttrain-rmse:4.154729\n",
      "[367]\ttrain-rmse:4.153073\n",
      "[368]\ttrain-rmse:4.151524\n",
      "[369]\ttrain-rmse:4.149866\n",
      "[370]\ttrain-rmse:4.148299\n",
      "[371]\ttrain-rmse:4.146698\n",
      "[372]\ttrain-rmse:4.145047\n",
      "[373]\ttrain-rmse:4.143498\n",
      "[374]\ttrain-rmse:4.141827\n",
      "[375]\ttrain-rmse:4.140268\n",
      "[376]\ttrain-rmse:4.138628\n",
      "[377]\ttrain-rmse:4.137042\n",
      "[378]\ttrain-rmse:4.135495\n",
      "[379]\ttrain-rmse:4.133818\n",
      "[380]\ttrain-rmse:4.132254\n",
      "[381]\ttrain-rmse:4.130582\n",
      "[382]\ttrain-rmse:4.129043\n",
      "[383]\ttrain-rmse:4.127390\n",
      "[384]\ttrain-rmse:4.125824\n",
      "[385]\ttrain-rmse:4.124274\n",
      "[386]\ttrain-rmse:4.122607\n",
      "[387]\ttrain-rmse:4.121068\n",
      "[388]\ttrain-rmse:4.119414\n",
      "[389]\ttrain-rmse:4.117868\n",
      "[390]\ttrain-rmse:4.116284\n",
      "[391]\ttrain-rmse:4.114662\n",
      "[392]\ttrain-rmse:4.113133\n",
      "[393]\ttrain-rmse:4.111482\n",
      "[394]\ttrain-rmse:4.109942\n",
      "[395]\ttrain-rmse:4.108291\n",
      "[396]\ttrain-rmse:4.106745\n",
      "[397]\ttrain-rmse:4.105199\n",
      "[398]\ttrain-rmse:4.103550\n",
      "[399]\ttrain-rmse:4.102008\n",
      "[400]\ttrain-rmse:4.100375\n",
      "[401]\ttrain-rmse:4.098816\n",
      "[402]\ttrain-rmse:4.097229\n",
      "[403]\ttrain-rmse:4.095616\n",
      "[404]\ttrain-rmse:4.094096\n",
      "[405]\ttrain-rmse:4.092454\n",
      "[406]\ttrain-rmse:4.090938\n",
      "[407]\ttrain-rmse:4.089304\n",
      "[408]\ttrain-rmse:4.087736\n",
      "[409]\ttrain-rmse:4.086222\n",
      "[410]\ttrain-rmse:4.084581\n",
      "[411]\ttrain-rmse:4.083052\n",
      "[412]\ttrain-rmse:4.081432\n",
      "[413]\ttrain-rmse:4.079882\n",
      "[414]\ttrain-rmse:4.078302\n",
      "[415]\ttrain-rmse:4.076705\n",
      "[416]\ttrain-rmse:4.075188\n",
      "[417]\ttrain-rmse:4.073559\n",
      "[418]\ttrain-rmse:4.072054\n",
      "[419]\ttrain-rmse:4.070453\n",
      "[420]\ttrain-rmse:4.068860\n",
      "[421]\ttrain-rmse:4.067346\n",
      "[422]\ttrain-rmse:4.065723\n",
      "[423]\ttrain-rmse:4.064199\n",
      "[424]\ttrain-rmse:4.062608\n",
      "[425]\ttrain-rmse:4.061046\n",
      "[426]\ttrain-rmse:4.059524\n",
      "[427]\ttrain-rmse:4.057935\n",
      "[428]\ttrain-rmse:4.056410\n",
      "[429]\ttrain-rmse:4.054833\n",
      "[430]\ttrain-rmse:4.053270\n",
      "[431]\ttrain-rmse:4.051739\n",
      "[432]\ttrain-rmse:4.050126\n",
      "[433]\ttrain-rmse:4.048628\n",
      "[434]\ttrain-rmse:4.047058\n",
      "[435]\ttrain-rmse:4.045516\n",
      "[436]\ttrain-rmse:4.043952\n",
      "[437]\ttrain-rmse:4.042371\n",
      "[438]\ttrain-rmse:4.040882\n",
      "[439]\ttrain-rmse:4.039301\n",
      "[440]\ttrain-rmse:4.037776\n",
      "[441]\ttrain-rmse:4.036230\n",
      "[442]\ttrain-rmse:4.034692\n",
      "[443]\ttrain-rmse:4.033186\n",
      "[444]\ttrain-rmse:4.031595\n",
      "[445]\ttrain-rmse:4.030082\n",
      "[446]\ttrain-rmse:4.028521\n",
      "[447]\ttrain-rmse:4.026964\n",
      "[448]\ttrain-rmse:4.025455\n",
      "[449]\ttrain-rmse:4.023885\n",
      "[450]\ttrain-rmse:4.022359\n",
      "[451]\ttrain-rmse:4.020840\n",
      "[452]\ttrain-rmse:4.019258\n",
      "[453]\ttrain-rmse:4.017766\n",
      "[454]\ttrain-rmse:4.016202\n",
      "[455]\ttrain-rmse:4.014683\n",
      "[456]\ttrain-rmse:4.013172\n",
      "[457]\ttrain-rmse:4.011619\n",
      "[458]\ttrain-rmse:4.010122\n",
      "[459]\ttrain-rmse:4.008549\n",
      "[460]\ttrain-rmse:4.007016\n",
      "[461]\ttrain-rmse:4.005486\n",
      "[462]\ttrain-rmse:4.003961\n",
      "[463]\ttrain-rmse:4.002455\n",
      "[464]\ttrain-rmse:4.000876\n",
      "[465]\ttrain-rmse:3.999392\n",
      "[466]\ttrain-rmse:3.997857\n",
      "[467]\ttrain-rmse:3.996307\n",
      "[468]\ttrain-rmse:3.994833\n",
      "[469]\ttrain-rmse:3.993273\n",
      "[470]\ttrain-rmse:3.991773\n",
      "[471]\ttrain-rmse:3.990281\n",
      "[472]\ttrain-rmse:3.988710\n",
      "[473]\ttrain-rmse:3.987234\n",
      "[474]\ttrain-rmse:3.985673\n",
      "[475]\ttrain-rmse:3.984168\n",
      "[476]\ttrain-rmse:3.982663\n",
      "[477]\ttrain-rmse:3.981102\n",
      "[478]\ttrain-rmse:3.979642\n",
      "[479]\ttrain-rmse:3.978072\n",
      "[480]\ttrain-rmse:3.976564\n",
      "[481]\ttrain-rmse:3.975091\n",
      "[482]\ttrain-rmse:3.973510\n",
      "[483]\ttrain-rmse:3.972060\n",
      "[484]\ttrain-rmse:3.970544\n",
      "[485]\ttrain-rmse:3.969002\n",
      "[486]\ttrain-rmse:3.967552\n",
      "[487]\ttrain-rmse:3.965999\n",
      "[488]\ttrain-rmse:3.964509\n",
      "[489]\ttrain-rmse:3.963032\n",
      "[490]\ttrain-rmse:3.961475\n",
      "[491]\ttrain-rmse:3.960003\n",
      "[492]\ttrain-rmse:3.958498\n",
      "[493]\ttrain-rmse:3.956950\n",
      "[494]\ttrain-rmse:3.955498\n",
      "[495]\ttrain-rmse:3.953945\n",
      "[496]\ttrain-rmse:3.952475\n",
      "[497]\ttrain-rmse:3.950979\n",
      "[498]\ttrain-rmse:3.949450\n",
      "[499]\ttrain-rmse:3.947996\n",
      "[500]\ttrain-rmse:3.946486\n",
      "[501]\ttrain-rmse:3.944976\n",
      "[502]\ttrain-rmse:3.943514\n",
      "[503]\ttrain-rmse:3.941967\n",
      "[504]\ttrain-rmse:3.940519\n",
      "[505]\ttrain-rmse:3.939014\n",
      "[506]\ttrain-rmse:3.937491\n",
      "[507]\ttrain-rmse:3.936039\n",
      "[508]\ttrain-rmse:3.934521\n",
      "[509]\ttrain-rmse:3.933017\n",
      "[510]\ttrain-rmse:3.931580\n",
      "[511]\ttrain-rmse:3.930047\n",
      "[512]\ttrain-rmse:3.928614\n",
      "[513]\ttrain-rmse:3.927109\n",
      "[514]\ttrain-rmse:3.925592\n",
      "[515]\ttrain-rmse:3.924157\n",
      "[516]\ttrain-rmse:3.922626\n",
      "[517]\ttrain-rmse:3.921175\n",
      "[518]\ttrain-rmse:3.919703\n",
      "[519]\ttrain-rmse:3.918185\n",
      "[520]\ttrain-rmse:3.916744\n",
      "[521]\ttrain-rmse:3.915267\n",
      "[522]\ttrain-rmse:3.913787\n",
      "[523]\ttrain-rmse:3.912330\n",
      "[524]\ttrain-rmse:3.910818\n",
      "[525]\ttrain-rmse:3.909364\n",
      "[526]\ttrain-rmse:3.907912\n",
      "[527]\ttrain-rmse:3.906379\n",
      "[528]\ttrain-rmse:3.904938\n",
      "[529]\ttrain-rmse:3.903436\n",
      "[530]\ttrain-rmse:3.901966\n",
      "[531]\ttrain-rmse:3.900516\n",
      "[532]\ttrain-rmse:3.899024\n",
      "[533]\ttrain-rmse:3.897557\n",
      "[534]\ttrain-rmse:3.896114\n",
      "[535]\ttrain-rmse:3.894597\n",
      "[536]\ttrain-rmse:3.893168\n",
      "[537]\ttrain-rmse:3.891700\n",
      "[538]\ttrain-rmse:3.890219\n",
      "[539]\ttrain-rmse:3.888795\n",
      "[540]\ttrain-rmse:3.887304\n",
      "[541]\ttrain-rmse:3.885836\n",
      "[542]\ttrain-rmse:3.884434\n",
      "[543]\ttrain-rmse:3.882912\n",
      "[544]\ttrain-rmse:3.881473\n",
      "[545]\ttrain-rmse:3.880041\n",
      "[546]\ttrain-rmse:3.878540\n",
      "[547]\ttrain-rmse:3.877105\n",
      "[548]\ttrain-rmse:3.875624\n",
      "[549]\ttrain-rmse:3.874149\n",
      "[550]\ttrain-rmse:3.872738\n",
      "[551]\ttrain-rmse:3.871253\n",
      "[552]\ttrain-rmse:3.869794\n",
      "[553]\ttrain-rmse:3.868375\n",
      "[554]\ttrain-rmse:3.866873\n",
      "[555]\ttrain-rmse:3.865443\n",
      "[556]\ttrain-rmse:3.864014\n",
      "[557]\ttrain-rmse:3.862548\n",
      "[558]\ttrain-rmse:3.861130\n",
      "[559]\ttrain-rmse:3.859681\n",
      "[560]\ttrain-rmse:3.858218\n",
      "[561]\ttrain-rmse:3.856794\n",
      "[562]\ttrain-rmse:3.855341\n",
      "[563]\ttrain-rmse:3.853889\n",
      "[564]\ttrain-rmse:3.852474\n",
      "[565]\ttrain-rmse:3.851023\n",
      "[566]\ttrain-rmse:3.849584\n",
      "[567]\ttrain-rmse:3.848154\n",
      "[568]\ttrain-rmse:3.846693\n",
      "[569]\ttrain-rmse:3.845278\n",
      "[570]\ttrain-rmse:3.843871\n",
      "[571]\ttrain-rmse:3.842421\n",
      "[572]\ttrain-rmse:3.840992\n",
      "[573]\ttrain-rmse:3.839539\n",
      "[574]\ttrain-rmse:3.838097\n",
      "[575]\ttrain-rmse:3.836684\n",
      "[576]\ttrain-rmse:3.835231\n",
      "[577]\ttrain-rmse:3.833821\n",
      "[578]\ttrain-rmse:3.832403\n",
      "[579]\ttrain-rmse:3.830954\n",
      "[580]\ttrain-rmse:3.829517\n",
      "[581]\ttrain-rmse:3.828115\n",
      "[582]\ttrain-rmse:3.826662\n",
      "[583]\ttrain-rmse:3.825246\n",
      "[584]\ttrain-rmse:3.823804\n",
      "[585]\ttrain-rmse:3.822377\n",
      "[586]\ttrain-rmse:3.820971\n",
      "[587]\ttrain-rmse:3.819534\n",
      "[588]\ttrain-rmse:3.818112\n",
      "[589]\ttrain-rmse:3.816727\n",
      "[590]\ttrain-rmse:3.815292\n",
      "[591]\ttrain-rmse:3.813871\n",
      "[592]\ttrain-rmse:3.812465\n",
      "[593]\ttrain-rmse:3.811033\n",
      "[594]\ttrain-rmse:3.809618\n",
      "[595]\ttrain-rmse:3.808213\n",
      "[596]\ttrain-rmse:3.806791\n",
      "[597]\ttrain-rmse:3.805376\n",
      "[598]\ttrain-rmse:3.803952\n",
      "[599]\ttrain-rmse:3.802544\n",
      "[600]\ttrain-rmse:3.801165\n",
      "[601]\ttrain-rmse:3.799752\n",
      "[602]\ttrain-rmse:3.798331\n",
      "[603]\ttrain-rmse:3.796931\n",
      "[604]\ttrain-rmse:3.795508\n",
      "[605]\ttrain-rmse:3.794103\n",
      "[606]\ttrain-rmse:3.792704\n",
      "[607]\ttrain-rmse:3.791297\n",
      "[608]\ttrain-rmse:3.789906\n",
      "[609]\ttrain-rmse:3.788527\n",
      "[610]\ttrain-rmse:3.787108\n",
      "[611]\ttrain-rmse:3.785708\n",
      "[612]\ttrain-rmse:3.784293\n",
      "[613]\ttrain-rmse:3.782904\n",
      "[614]\ttrain-rmse:3.781518\n",
      "[615]\ttrain-rmse:3.780123\n",
      "[616]\ttrain-rmse:3.778712\n",
      "[617]\ttrain-rmse:3.777331\n",
      "[618]\ttrain-rmse:3.775929\n",
      "[619]\ttrain-rmse:3.774510\n",
      "[620]\ttrain-rmse:3.773126\n",
      "[621]\ttrain-rmse:3.771735\n",
      "[622]\ttrain-rmse:3.770333\n",
      "[623]\ttrain-rmse:3.768966\n",
      "[624]\ttrain-rmse:3.767548\n",
      "[625]\ttrain-rmse:3.766162\n",
      "[626]\ttrain-rmse:3.764779\n",
      "[627]\ttrain-rmse:3.763381\n",
      "[628]\ttrain-rmse:3.761995\n",
      "[629]\ttrain-rmse:3.760613\n",
      "[630]\ttrain-rmse:3.759213\n",
      "[631]\ttrain-rmse:3.757824\n",
      "[632]\ttrain-rmse:3.756434\n",
      "[633]\ttrain-rmse:3.755048\n",
      "[634]\ttrain-rmse:3.753666\n",
      "[635]\ttrain-rmse:3.752306\n",
      "[636]\ttrain-rmse:3.750906\n",
      "[637]\ttrain-rmse:3.749543\n",
      "[638]\ttrain-rmse:3.748172\n",
      "[639]\ttrain-rmse:3.746771\n",
      "[640]\ttrain-rmse:3.745397\n",
      "[641]\ttrain-rmse:3.744021\n",
      "[642]\ttrain-rmse:3.742624\n",
      "[643]\ttrain-rmse:3.741249\n",
      "[644]\ttrain-rmse:3.739885\n",
      "[645]\ttrain-rmse:3.738497\n",
      "[646]\ttrain-rmse:3.737140\n",
      "[647]\ttrain-rmse:3.735768\n",
      "[648]\ttrain-rmse:3.734365\n",
      "[649]\ttrain-rmse:3.733005\n",
      "[650]\ttrain-rmse:3.731637\n",
      "[651]\ttrain-rmse:3.730258\n",
      "[652]\ttrain-rmse:3.728898\n",
      "[653]\ttrain-rmse:3.727530\n",
      "[654]\ttrain-rmse:3.726139\n",
      "[655]\ttrain-rmse:3.724779\n",
      "[656]\ttrain-rmse:3.723415\n",
      "[657]\ttrain-rmse:3.722048\n",
      "[658]\ttrain-rmse:3.720690\n",
      "[659]\ttrain-rmse:3.719329\n",
      "[660]\ttrain-rmse:3.717941\n",
      "[661]\ttrain-rmse:3.716600\n",
      "[662]\ttrain-rmse:3.715240\n",
      "[663]\ttrain-rmse:3.713851\n",
      "[664]\ttrain-rmse:3.712504\n",
      "[665]\ttrain-rmse:3.711152\n",
      "[666]\ttrain-rmse:3.709753\n",
      "[667]\ttrain-rmse:3.708423\n",
      "[668]\ttrain-rmse:3.707063\n",
      "[669]\ttrain-rmse:3.705707\n",
      "[670]\ttrain-rmse:3.704373\n",
      "[671]\ttrain-rmse:3.703027\n",
      "[672]\ttrain-rmse:3.701651\n",
      "[673]\ttrain-rmse:3.700293\n",
      "[674]\ttrain-rmse:3.698960\n",
      "[675]\ttrain-rmse:3.697589\n",
      "[676]\ttrain-rmse:3.696225\n",
      "[677]\ttrain-rmse:3.694888\n",
      "[678]\ttrain-rmse:3.693527\n",
      "[679]\ttrain-rmse:3.692187\n",
      "[680]\ttrain-rmse:3.690828\n",
      "[681]\ttrain-rmse:3.689491\n",
      "[682]\ttrain-rmse:3.688146\n",
      "[683]\ttrain-rmse:3.686810\n",
      "[684]\ttrain-rmse:3.685446\n",
      "[685]\ttrain-rmse:3.684084\n",
      "[686]\ttrain-rmse:3.682768\n",
      "[687]\ttrain-rmse:3.681401\n",
      "[688]\ttrain-rmse:3.680050\n",
      "[689]\ttrain-rmse:3.678732\n",
      "[690]\ttrain-rmse:3.677384\n",
      "[691]\ttrain-rmse:3.676040\n",
      "[692]\ttrain-rmse:3.674711\n",
      "[693]\ttrain-rmse:3.673376\n",
      "[694]\ttrain-rmse:3.672028\n",
      "[695]\ttrain-rmse:3.670703\n",
      "[696]\ttrain-rmse:3.669366\n",
      "[697]\ttrain-rmse:3.668025\n",
      "[698]\ttrain-rmse:3.666706\n",
      "[699]\ttrain-rmse:3.665356\n",
      "[700]\ttrain-rmse:3.664027\n",
      "[701]\ttrain-rmse:3.662698\n",
      "[702]\ttrain-rmse:3.661373\n",
      "[703]\ttrain-rmse:3.660026\n",
      "[704]\ttrain-rmse:3.658714\n",
      "[705]\ttrain-rmse:3.657394\n",
      "[706]\ttrain-rmse:3.656062\n",
      "[707]\ttrain-rmse:3.654740\n",
      "[708]\ttrain-rmse:3.653412\n",
      "[709]\ttrain-rmse:3.652071\n",
      "[710]\ttrain-rmse:3.650758\n",
      "[711]\ttrain-rmse:3.649448\n",
      "[712]\ttrain-rmse:3.648099\n",
      "[713]\ttrain-rmse:3.646786\n",
      "[714]\ttrain-rmse:3.645482\n",
      "[715]\ttrain-rmse:3.644166\n",
      "[716]\ttrain-rmse:3.642855\n",
      "[717]\ttrain-rmse:3.641539\n",
      "[718]\ttrain-rmse:3.640229\n",
      "[719]\ttrain-rmse:3.638905\n",
      "[720]\ttrain-rmse:3.637604\n",
      "[721]\ttrain-rmse:3.636276\n",
      "[722]\ttrain-rmse:3.634959\n",
      "[723]\ttrain-rmse:3.633651\n",
      "[724]\ttrain-rmse:3.632347\n",
      "[725]\ttrain-rmse:3.631022\n",
      "[726]\ttrain-rmse:3.629716\n",
      "[727]\ttrain-rmse:3.628422\n",
      "[728]\ttrain-rmse:3.627115\n",
      "[729]\ttrain-rmse:3.625786\n",
      "[730]\ttrain-rmse:3.624496\n",
      "[731]\ttrain-rmse:3.623189\n",
      "[732]\ttrain-rmse:3.621866\n",
      "[733]\ttrain-rmse:3.620578\n",
      "[734]\ttrain-rmse:3.619267\n",
      "[735]\ttrain-rmse:3.617947\n",
      "[736]\ttrain-rmse:3.616636\n",
      "[737]\ttrain-rmse:3.615339\n",
      "[738]\ttrain-rmse:3.614029\n",
      "[739]\ttrain-rmse:3.612742\n",
      "[740]\ttrain-rmse:3.611451\n",
      "[741]\ttrain-rmse:3.610153\n",
      "[742]\ttrain-rmse:3.608844\n",
      "[743]\ttrain-rmse:3.607555\n",
      "[744]\ttrain-rmse:3.606273\n",
      "[745]\ttrain-rmse:3.604967\n",
      "[746]\ttrain-rmse:3.603676\n",
      "[747]\ttrain-rmse:3.602377\n",
      "[748]\ttrain-rmse:3.601086\n",
      "[749]\ttrain-rmse:3.599778\n",
      "[750]\ttrain-rmse:3.598495\n",
      "[751]\ttrain-rmse:3.597186\n",
      "[752]\ttrain-rmse:3.595900\n",
      "[753]\ttrain-rmse:3.594619\n",
      "[754]\ttrain-rmse:3.593325\n",
      "[755]\ttrain-rmse:3.592054\n",
      "[756]\ttrain-rmse:3.590753\n",
      "[757]\ttrain-rmse:3.589463\n",
      "[758]\ttrain-rmse:3.588193\n",
      "[759]\ttrain-rmse:3.586920\n",
      "[760]\ttrain-rmse:3.585639\n",
      "[761]\ttrain-rmse:3.584349\n",
      "[762]\ttrain-rmse:3.583085\n",
      "[763]\ttrain-rmse:3.581797\n",
      "[764]\ttrain-rmse:3.580504\n",
      "[765]\ttrain-rmse:3.579234\n",
      "[766]\ttrain-rmse:3.577943\n",
      "[767]\ttrain-rmse:3.576653\n",
      "[768]\ttrain-rmse:3.575380\n",
      "[769]\ttrain-rmse:3.574122\n",
      "[770]\ttrain-rmse:3.572845\n",
      "[771]\ttrain-rmse:3.571550\n",
      "[772]\ttrain-rmse:3.570269\n",
      "[773]\ttrain-rmse:3.568994\n",
      "[774]\ttrain-rmse:3.567721\n",
      "[775]\ttrain-rmse:3.566450\n",
      "[776]\ttrain-rmse:3.565180\n",
      "[777]\ttrain-rmse:3.563907\n",
      "[778]\ttrain-rmse:3.562646\n",
      "[779]\ttrain-rmse:3.561384\n",
      "[780]\ttrain-rmse:3.560110\n",
      "[781]\ttrain-rmse:3.558838\n",
      "[782]\ttrain-rmse:3.557568\n",
      "[783]\ttrain-rmse:3.556314\n",
      "[784]\ttrain-rmse:3.555046\n",
      "[785]\ttrain-rmse:3.553792\n",
      "[786]\ttrain-rmse:3.552523\n",
      "[787]\ttrain-rmse:3.551252\n",
      "[788]\ttrain-rmse:3.549995\n",
      "[789]\ttrain-rmse:3.548745\n",
      "[790]\ttrain-rmse:3.547482\n",
      "[791]\ttrain-rmse:3.546219\n",
      "[792]\ttrain-rmse:3.544965\n",
      "[793]\ttrain-rmse:3.543706\n",
      "[794]\ttrain-rmse:3.542436\n",
      "[795]\ttrain-rmse:3.541183\n",
      "[796]\ttrain-rmse:3.539953\n",
      "[797]\ttrain-rmse:3.538687\n",
      "[798]\ttrain-rmse:3.537426\n",
      "[799]\ttrain-rmse:3.536187\n",
      "[800]\ttrain-rmse:3.534922\n",
      "[801]\ttrain-rmse:3.533655\n",
      "[802]\ttrain-rmse:3.532409\n",
      "[803]\ttrain-rmse:3.531169\n",
      "[804]\ttrain-rmse:3.529907\n",
      "[805]\ttrain-rmse:3.528651\n",
      "[806]\ttrain-rmse:3.527406\n",
      "[807]\ttrain-rmse:3.526158\n",
      "[808]\ttrain-rmse:3.524901\n",
      "[809]\ttrain-rmse:3.523646\n",
      "[810]\ttrain-rmse:3.522423\n",
      "[811]\ttrain-rmse:3.521166\n",
      "[812]\ttrain-rmse:3.519931\n",
      "[813]\ttrain-rmse:3.518708\n",
      "[814]\ttrain-rmse:3.517458\n",
      "[815]\ttrain-rmse:3.516202\n",
      "[816]\ttrain-rmse:3.514977\n",
      "[817]\ttrain-rmse:3.513735\n",
      "[818]\ttrain-rmse:3.512496\n",
      "[819]\ttrain-rmse:3.511258\n",
      "[820]\ttrain-rmse:3.510032\n",
      "[821]\ttrain-rmse:3.508773\n",
      "[822]\ttrain-rmse:3.507550\n",
      "[823]\ttrain-rmse:3.506318\n",
      "[824]\ttrain-rmse:3.505086\n",
      "[825]\ttrain-rmse:3.503837\n",
      "[826]\ttrain-rmse:3.502615\n",
      "[827]\ttrain-rmse:3.501374\n",
      "[828]\ttrain-rmse:3.500120\n",
      "[829]\ttrain-rmse:3.498898\n",
      "[830]\ttrain-rmse:3.497674\n",
      "[831]\ttrain-rmse:3.496446\n",
      "[832]\ttrain-rmse:3.495209\n",
      "[833]\ttrain-rmse:3.493983\n",
      "[834]\ttrain-rmse:3.492785\n",
      "[835]\ttrain-rmse:3.491534\n",
      "[836]\ttrain-rmse:3.490291\n",
      "[837]\ttrain-rmse:3.489086\n",
      "[838]\ttrain-rmse:3.487868\n",
      "[839]\ttrain-rmse:3.486644\n",
      "[840]\ttrain-rmse:3.485420\n",
      "[841]\ttrain-rmse:3.484212\n",
      "[842]\ttrain-rmse:3.482978\n",
      "[843]\ttrain-rmse:3.481729\n",
      "[844]\ttrain-rmse:3.480528\n",
      "[845]\ttrain-rmse:3.479308\n",
      "[846]\ttrain-rmse:3.478095\n",
      "[847]\ttrain-rmse:3.476872\n",
      "[848]\ttrain-rmse:3.475662\n",
      "[849]\ttrain-rmse:3.474442\n",
      "[850]\ttrain-rmse:3.473216\n",
      "[851]\ttrain-rmse:3.472010\n",
      "[852]\ttrain-rmse:3.470793\n",
      "[853]\ttrain-rmse:3.469587\n",
      "[854]\ttrain-rmse:3.468379\n",
      "[855]\ttrain-rmse:3.467158\n",
      "[856]\ttrain-rmse:3.465949\n",
      "[857]\ttrain-rmse:3.464736\n",
      "[858]\ttrain-rmse:3.463529\n",
      "[859]\ttrain-rmse:3.462327\n",
      "[860]\ttrain-rmse:3.461110\n",
      "[861]\ttrain-rmse:3.459914\n",
      "[862]\ttrain-rmse:3.458715\n",
      "[863]\ttrain-rmse:3.457517\n",
      "[864]\ttrain-rmse:3.456292\n",
      "[865]\ttrain-rmse:3.455108\n",
      "[866]\ttrain-rmse:3.453905\n",
      "[867]\ttrain-rmse:3.452702\n",
      "[868]\ttrain-rmse:3.451479\n",
      "[869]\ttrain-rmse:3.450297\n",
      "[870]\ttrain-rmse:3.449111\n",
      "[871]\ttrain-rmse:3.447907\n",
      "[872]\ttrain-rmse:3.446689\n",
      "[873]\ttrain-rmse:3.445493\n",
      "[874]\ttrain-rmse:3.444305\n",
      "[875]\ttrain-rmse:3.443109\n",
      "[876]\ttrain-rmse:3.441910\n",
      "[877]\ttrain-rmse:3.440717\n",
      "[878]\ttrain-rmse:3.439507\n",
      "[879]\ttrain-rmse:3.438322\n",
      "[880]\ttrain-rmse:3.437130\n",
      "[881]\ttrain-rmse:3.435961\n",
      "[882]\ttrain-rmse:3.434740\n",
      "[883]\ttrain-rmse:3.433553\n",
      "[884]\ttrain-rmse:3.432378\n",
      "[885]\ttrain-rmse:3.431180\n",
      "[886]\ttrain-rmse:3.429979\n",
      "[887]\ttrain-rmse:3.428799\n",
      "[888]\ttrain-rmse:3.427623\n",
      "[889]\ttrain-rmse:3.426409\n",
      "[890]\ttrain-rmse:3.425231\n",
      "[891]\ttrain-rmse:3.424051\n",
      "[892]\ttrain-rmse:3.422874\n",
      "[893]\ttrain-rmse:3.421678\n",
      "[894]\ttrain-rmse:3.420494\n",
      "[895]\ttrain-rmse:3.419314\n",
      "[896]\ttrain-rmse:3.418148\n",
      "[897]\ttrain-rmse:3.416937\n",
      "[898]\ttrain-rmse:3.415761\n",
      "[899]\ttrain-rmse:3.414582\n",
      "[900]\ttrain-rmse:3.413387\n",
      "[901]\ttrain-rmse:3.412217\n",
      "[902]\ttrain-rmse:3.411046\n",
      "[903]\ttrain-rmse:3.409860\n",
      "[904]\ttrain-rmse:3.408668\n",
      "[905]\ttrain-rmse:3.407497\n",
      "[906]\ttrain-rmse:3.406339\n",
      "[907]\ttrain-rmse:3.405151\n",
      "[908]\ttrain-rmse:3.403980\n",
      "[909]\ttrain-rmse:3.402814\n",
      "[910]\ttrain-rmse:3.401664\n",
      "[911]\ttrain-rmse:3.400489\n",
      "[912]\ttrain-rmse:3.399314\n",
      "[913]\ttrain-rmse:3.398144\n",
      "[914]\ttrain-rmse:3.396984\n",
      "[915]\ttrain-rmse:3.395813\n",
      "[916]\ttrain-rmse:3.394643\n",
      "[917]\ttrain-rmse:3.393475\n",
      "[918]\ttrain-rmse:3.392294\n",
      "[919]\ttrain-rmse:3.391124\n",
      "[920]\ttrain-rmse:3.389946\n",
      "[921]\ttrain-rmse:3.388785\n",
      "[922]\ttrain-rmse:3.387626\n",
      "[923]\ttrain-rmse:3.386459\n",
      "[924]\ttrain-rmse:3.385285\n",
      "[925]\ttrain-rmse:3.384151\n",
      "[926]\ttrain-rmse:3.382975\n",
      "[927]\ttrain-rmse:3.381809\n",
      "[928]\ttrain-rmse:3.380644\n",
      "[929]\ttrain-rmse:3.379498\n",
      "[930]\ttrain-rmse:3.378344\n",
      "[931]\ttrain-rmse:3.377179\n",
      "[932]\ttrain-rmse:3.376012\n",
      "[933]\ttrain-rmse:3.374871\n",
      "[934]\ttrain-rmse:3.373704\n",
      "[935]\ttrain-rmse:3.372538\n",
      "[936]\ttrain-rmse:3.371381\n",
      "[937]\ttrain-rmse:3.370251\n",
      "[938]\ttrain-rmse:3.369093\n",
      "[939]\ttrain-rmse:3.367928\n",
      "[940]\ttrain-rmse:3.366787\n",
      "[941]\ttrain-rmse:3.365642\n",
      "[942]\ttrain-rmse:3.364488\n",
      "[943]\ttrain-rmse:3.363327\n",
      "[944]\ttrain-rmse:3.362188\n",
      "[945]\ttrain-rmse:3.361054\n",
      "[946]\ttrain-rmse:3.359890\n",
      "[947]\ttrain-rmse:3.358737\n",
      "[948]\ttrain-rmse:3.357598\n",
      "[949]\ttrain-rmse:3.356459\n",
      "[950]\ttrain-rmse:3.355301\n",
      "[951]\ttrain-rmse:3.354154\n",
      "[952]\ttrain-rmse:3.353015\n",
      "[953]\ttrain-rmse:3.351887\n",
      "[954]\ttrain-rmse:3.350734\n",
      "[955]\ttrain-rmse:3.349597\n",
      "[956]\ttrain-rmse:3.348480\n",
      "[957]\ttrain-rmse:3.347356\n",
      "[958]\ttrain-rmse:3.346224\n",
      "[959]\ttrain-rmse:3.345081\n",
      "[960]\ttrain-rmse:3.343951\n",
      "[961]\ttrain-rmse:3.342816\n",
      "[962]\ttrain-rmse:3.341674\n",
      "[963]\ttrain-rmse:3.340532\n",
      "[964]\ttrain-rmse:3.339405\n",
      "[965]\ttrain-rmse:3.338284\n",
      "[966]\ttrain-rmse:3.337150\n",
      "[967]\ttrain-rmse:3.335998\n",
      "[968]\ttrain-rmse:3.334880\n",
      "[969]\ttrain-rmse:3.333761\n",
      "[970]\ttrain-rmse:3.332646\n",
      "[971]\ttrain-rmse:3.331503\n",
      "[972]\ttrain-rmse:3.330381\n",
      "[973]\ttrain-rmse:3.329243\n",
      "[974]\ttrain-rmse:3.328114\n",
      "[975]\ttrain-rmse:3.326971\n",
      "[976]\ttrain-rmse:3.325857\n",
      "[977]\ttrain-rmse:3.324746\n",
      "[978]\ttrain-rmse:3.323622\n",
      "[979]\ttrain-rmse:3.322487\n",
      "[980]\ttrain-rmse:3.321374\n",
      "[981]\ttrain-rmse:3.320252\n",
      "[982]\ttrain-rmse:3.319137\n",
      "[983]\ttrain-rmse:3.318004\n",
      "[984]\ttrain-rmse:3.316892\n",
      "[985]\ttrain-rmse:3.315776\n",
      "[986]\ttrain-rmse:3.314664\n",
      "[987]\ttrain-rmse:3.313540\n",
      "[988]\ttrain-rmse:3.312441\n",
      "[989]\ttrain-rmse:3.311309\n",
      "[990]\ttrain-rmse:3.310196\n",
      "[991]\ttrain-rmse:3.309069\n",
      "[992]\ttrain-rmse:3.307968\n",
      "[993]\ttrain-rmse:3.306854\n",
      "[994]\ttrain-rmse:3.305748\n",
      "[995]\ttrain-rmse:3.304623\n",
      "[996]\ttrain-rmse:3.303505\n",
      "[997]\ttrain-rmse:3.302406\n",
      "[998]\ttrain-rmse:3.301305\n",
      "[999]\ttrain-rmse:3.300191\n",
      "[1000]\ttrain-rmse:3.299064\n",
      "[1001]\ttrain-rmse:3.297969\n",
      "[1002]\ttrain-rmse:3.296869\n",
      "[1003]\ttrain-rmse:3.295752\n",
      "[1004]\ttrain-rmse:3.294631\n",
      "[1005]\ttrain-rmse:3.293556\n",
      "[1006]\ttrain-rmse:3.292441\n",
      "[1007]\ttrain-rmse:3.291337\n",
      "[1008]\ttrain-rmse:3.290211\n",
      "[1009]\ttrain-rmse:3.289128\n",
      "[1010]\ttrain-rmse:3.288031\n",
      "[1011]\ttrain-rmse:3.286922\n",
      "[1012]\ttrain-rmse:3.285806\n",
      "[1013]\ttrain-rmse:3.284724\n",
      "[1014]\ttrain-rmse:3.283631\n",
      "[1015]\ttrain-rmse:3.282529\n",
      "[1016]\ttrain-rmse:3.281409\n",
      "[1017]\ttrain-rmse:3.280324\n",
      "[1018]\ttrain-rmse:3.279227\n",
      "[1019]\ttrain-rmse:3.278141\n",
      "[1020]\ttrain-rmse:3.277031\n",
      "[1021]\ttrain-rmse:3.275947\n",
      "[1022]\ttrain-rmse:3.274860\n",
      "[1023]\ttrain-rmse:3.273773\n",
      "[1024]\ttrain-rmse:3.272656\n",
      "[1025]\ttrain-rmse:3.271558\n",
      "[1026]\ttrain-rmse:3.270475\n",
      "[1027]\ttrain-rmse:3.269385\n",
      "[1028]\ttrain-rmse:3.268295\n",
      "[1029]\ttrain-rmse:3.267207\n",
      "[1030]\ttrain-rmse:3.266115\n",
      "[1031]\ttrain-rmse:3.265037\n",
      "[1032]\ttrain-rmse:3.263952\n",
      "[1033]\ttrain-rmse:3.262863\n",
      "[1034]\ttrain-rmse:3.261790\n",
      "[1035]\ttrain-rmse:3.260697\n",
      "[1036]\ttrain-rmse:3.259629\n",
      "[1037]\ttrain-rmse:3.258535\n",
      "[1038]\ttrain-rmse:3.257460\n",
      "[1039]\ttrain-rmse:3.256376\n",
      "[1040]\ttrain-rmse:3.255305\n",
      "[1041]\ttrain-rmse:3.254209\n",
      "[1042]\ttrain-rmse:3.253147\n",
      "[1043]\ttrain-rmse:3.252058\n",
      "[1044]\ttrain-rmse:3.250987\n",
      "[1045]\ttrain-rmse:3.249902\n",
      "[1046]\ttrain-rmse:3.248834\n",
      "[1047]\ttrain-rmse:3.247751\n",
      "[1048]\ttrain-rmse:3.246693\n",
      "[1049]\ttrain-rmse:3.245605\n",
      "[1050]\ttrain-rmse:3.244531\n",
      "[1051]\ttrain-rmse:3.243458\n",
      "[1052]\ttrain-rmse:3.242384\n",
      "[1053]\ttrain-rmse:3.241307\n",
      "[1054]\ttrain-rmse:3.240235\n",
      "[1055]\ttrain-rmse:3.239161\n",
      "[1056]\ttrain-rmse:3.238084\n",
      "[1057]\ttrain-rmse:3.237023\n",
      "[1058]\ttrain-rmse:3.235938\n",
      "[1059]\ttrain-rmse:3.234869\n",
      "[1060]\ttrain-rmse:3.233800\n",
      "[1061]\ttrain-rmse:3.232742\n",
      "[1062]\ttrain-rmse:3.231671\n",
      "[1063]\ttrain-rmse:3.230604\n",
      "[1064]\ttrain-rmse:3.229532\n",
      "[1065]\ttrain-rmse:3.228462\n",
      "[1066]\ttrain-rmse:3.227413\n",
      "[1067]\ttrain-rmse:3.226339\n",
      "[1068]\ttrain-rmse:3.225269\n",
      "[1069]\ttrain-rmse:3.224224\n",
      "[1070]\ttrain-rmse:3.223160\n",
      "[1071]\ttrain-rmse:3.222085\n",
      "[1072]\ttrain-rmse:3.221029\n",
      "[1073]\ttrain-rmse:3.219971\n",
      "[1074]\ttrain-rmse:3.218926\n",
      "[1075]\ttrain-rmse:3.217868\n",
      "[1076]\ttrain-rmse:3.216803\n",
      "[1077]\ttrain-rmse:3.215733\n",
      "[1078]\ttrain-rmse:3.214689\n",
      "[1079]\ttrain-rmse:3.213633\n",
      "[1080]\ttrain-rmse:3.212579\n",
      "[1081]\ttrain-rmse:3.211519\n",
      "[1082]\ttrain-rmse:3.210465\n",
      "[1083]\ttrain-rmse:3.209424\n",
      "[1084]\ttrain-rmse:3.208371\n",
      "[1085]\ttrain-rmse:3.207334\n",
      "[1086]\ttrain-rmse:3.206269\n",
      "[1087]\ttrain-rmse:3.205225\n",
      "[1088]\ttrain-rmse:3.204198\n",
      "[1089]\ttrain-rmse:3.203146\n",
      "[1090]\ttrain-rmse:3.202092\n",
      "[1091]\ttrain-rmse:3.201043\n",
      "[1092]\ttrain-rmse:3.199997\n",
      "[1093]\ttrain-rmse:3.198941\n",
      "[1094]\ttrain-rmse:3.197913\n",
      "[1095]\ttrain-rmse:3.196870\n",
      "[1096]\ttrain-rmse:3.195836\n",
      "[1097]\ttrain-rmse:3.194782\n",
      "[1098]\ttrain-rmse:3.193741\n",
      "[1099]\ttrain-rmse:3.192704\n",
      "[1100]\ttrain-rmse:3.191665\n",
      "[1101]\ttrain-rmse:3.190625\n",
      "[1102]\ttrain-rmse:3.189567\n",
      "[1103]\ttrain-rmse:3.188522\n",
      "[1104]\ttrain-rmse:3.187485\n",
      "[1105]\ttrain-rmse:3.186450\n",
      "[1106]\ttrain-rmse:3.185409\n",
      "[1107]\ttrain-rmse:3.184375\n",
      "[1108]\ttrain-rmse:3.183332\n",
      "[1109]\ttrain-rmse:3.182301\n",
      "[1110]\ttrain-rmse:3.181277\n",
      "[1111]\ttrain-rmse:3.180236\n",
      "[1112]\ttrain-rmse:3.179195\n",
      "[1113]\ttrain-rmse:3.178161\n",
      "[1114]\ttrain-rmse:3.177131\n",
      "[1115]\ttrain-rmse:3.176101\n",
      "[1116]\ttrain-rmse:3.175081\n",
      "[1117]\ttrain-rmse:3.174045\n",
      "[1118]\ttrain-rmse:3.173022\n",
      "[1119]\ttrain-rmse:3.172000\n",
      "[1120]\ttrain-rmse:3.170965\n",
      "[1121]\ttrain-rmse:3.169937\n",
      "[1122]\ttrain-rmse:3.168905\n",
      "[1123]\ttrain-rmse:3.167875\n",
      "[1124]\ttrain-rmse:3.166862\n",
      "[1125]\ttrain-rmse:3.165842\n",
      "[1126]\ttrain-rmse:3.164791\n",
      "[1127]\ttrain-rmse:3.163778\n",
      "[1128]\ttrain-rmse:3.162746\n",
      "[1129]\ttrain-rmse:3.161738\n",
      "[1130]\ttrain-rmse:3.160722\n",
      "[1131]\ttrain-rmse:3.159689\n",
      "[1132]\ttrain-rmse:3.158684\n",
      "[1133]\ttrain-rmse:3.157668\n",
      "[1134]\ttrain-rmse:3.156651\n",
      "[1135]\ttrain-rmse:3.155634\n",
      "[1136]\ttrain-rmse:3.154616\n",
      "[1137]\ttrain-rmse:3.153608\n",
      "[1138]\ttrain-rmse:3.152593\n",
      "[1139]\ttrain-rmse:3.151567\n",
      "[1140]\ttrain-rmse:3.150537\n",
      "[1141]\ttrain-rmse:3.149535\n",
      "[1142]\ttrain-rmse:3.148528\n",
      "[1143]\ttrain-rmse:3.147533\n",
      "[1144]\ttrain-rmse:3.146526\n",
      "[1145]\ttrain-rmse:3.145509\n",
      "[1146]\ttrain-rmse:3.144497\n",
      "[1147]\ttrain-rmse:3.143488\n",
      "[1148]\ttrain-rmse:3.142491\n",
      "[1149]\ttrain-rmse:3.141467\n",
      "[1150]\ttrain-rmse:3.140469\n",
      "[1151]\ttrain-rmse:3.139467\n",
      "[1152]\ttrain-rmse:3.138442\n",
      "[1153]\ttrain-rmse:3.137447\n",
      "[1154]\ttrain-rmse:3.136426\n",
      "[1155]\ttrain-rmse:3.135446\n",
      "[1156]\ttrain-rmse:3.134436\n",
      "[1157]\ttrain-rmse:3.133432\n",
      "[1158]\ttrain-rmse:3.132422\n",
      "[1159]\ttrain-rmse:3.131413\n",
      "[1160]\ttrain-rmse:3.130424\n",
      "[1161]\ttrain-rmse:3.129429\n",
      "[1162]\ttrain-rmse:3.128434\n",
      "[1163]\ttrain-rmse:3.127424\n",
      "[1164]\ttrain-rmse:3.126416\n",
      "[1165]\ttrain-rmse:3.125419\n",
      "[1166]\ttrain-rmse:3.124418\n",
      "[1167]\ttrain-rmse:3.123437\n",
      "[1168]\ttrain-rmse:3.122430\n",
      "[1169]\ttrain-rmse:3.121437\n",
      "[1170]\ttrain-rmse:3.120443\n",
      "[1171]\ttrain-rmse:3.119447\n",
      "[1172]\ttrain-rmse:3.118466\n",
      "[1173]\ttrain-rmse:3.117446\n",
      "[1174]\ttrain-rmse:3.116478\n",
      "[1175]\ttrain-rmse:3.115476\n",
      "[1176]\ttrain-rmse:3.114484\n",
      "[1177]\ttrain-rmse:3.113508\n",
      "[1178]\ttrain-rmse:3.112522\n",
      "[1179]\ttrain-rmse:3.111538\n",
      "[1180]\ttrain-rmse:3.110531\n",
      "[1181]\ttrain-rmse:3.109551\n",
      "[1182]\ttrain-rmse:3.108564\n",
      "[1183]\ttrain-rmse:3.107566\n",
      "[1184]\ttrain-rmse:3.106597\n",
      "[1185]\ttrain-rmse:3.105603\n",
      "[1186]\ttrain-rmse:3.104620\n",
      "[1187]\ttrain-rmse:3.103634\n",
      "[1188]\ttrain-rmse:3.102645\n",
      "[1189]\ttrain-rmse:3.101669\n",
      "[1190]\ttrain-rmse:3.100688\n",
      "[1191]\ttrain-rmse:3.099705\n",
      "[1192]\ttrain-rmse:3.098716\n",
      "[1193]\ttrain-rmse:3.097730\n",
      "[1194]\ttrain-rmse:3.096780\n",
      "[1195]\ttrain-rmse:3.095788\n",
      "[1196]\ttrain-rmse:3.094812\n",
      "[1197]\ttrain-rmse:3.093830\n",
      "[1198]\ttrain-rmse:3.092861\n",
      "[1199]\ttrain-rmse:3.091887\n",
      "[1200]\ttrain-rmse:3.090903\n",
      "[1201]\ttrain-rmse:3.089914\n",
      "[1202]\ttrain-rmse:3.088934\n",
      "[1203]\ttrain-rmse:3.087964\n",
      "[1204]\ttrain-rmse:3.086998\n",
      "[1205]\ttrain-rmse:3.086013\n",
      "[1206]\ttrain-rmse:3.085041\n",
      "[1207]\ttrain-rmse:3.084066\n",
      "[1208]\ttrain-rmse:3.083086\n",
      "[1209]\ttrain-rmse:3.082121\n",
      "[1210]\ttrain-rmse:3.081142\n",
      "[1211]\ttrain-rmse:3.080173\n",
      "[1212]\ttrain-rmse:3.079191\n",
      "[1213]\ttrain-rmse:3.078233\n",
      "[1214]\ttrain-rmse:3.077265\n",
      "[1215]\ttrain-rmse:3.076286\n",
      "[1216]\ttrain-rmse:3.075335\n",
      "[1217]\ttrain-rmse:3.074354\n",
      "[1218]\ttrain-rmse:3.073392\n",
      "[1219]\ttrain-rmse:3.072433\n",
      "[1220]\ttrain-rmse:3.071463\n",
      "[1221]\ttrain-rmse:3.070506\n",
      "[1222]\ttrain-rmse:3.069543\n",
      "[1223]\ttrain-rmse:3.068591\n",
      "[1224]\ttrain-rmse:3.067622\n",
      "[1225]\ttrain-rmse:3.066666\n",
      "[1226]\ttrain-rmse:3.065719\n",
      "[1227]\ttrain-rmse:3.064743\n",
      "[1228]\ttrain-rmse:3.063796\n",
      "[1229]\ttrain-rmse:3.062839\n",
      "[1230]\ttrain-rmse:3.061869\n",
      "[1231]\ttrain-rmse:3.060916\n",
      "[1232]\ttrain-rmse:3.059954\n",
      "[1233]\ttrain-rmse:3.058988\n",
      "[1234]\ttrain-rmse:3.058050\n",
      "[1235]\ttrain-rmse:3.057095\n",
      "[1236]\ttrain-rmse:3.056143\n",
      "[1237]\ttrain-rmse:3.055182\n",
      "[1238]\ttrain-rmse:3.054235\n",
      "[1239]\ttrain-rmse:3.053276\n",
      "[1240]\ttrain-rmse:3.052316\n",
      "[1241]\ttrain-rmse:3.051368\n",
      "[1242]\ttrain-rmse:3.050417\n",
      "[1243]\ttrain-rmse:3.049471\n",
      "[1244]\ttrain-rmse:3.048511\n",
      "[1245]\ttrain-rmse:3.047572\n",
      "[1246]\ttrain-rmse:3.046634\n",
      "[1247]\ttrain-rmse:3.045676\n",
      "[1248]\ttrain-rmse:3.044729\n",
      "[1249]\ttrain-rmse:3.043770\n",
      "[1250]\ttrain-rmse:3.042819\n",
      "[1251]\ttrain-rmse:3.041885\n",
      "[1252]\ttrain-rmse:3.040932\n",
      "[1253]\ttrain-rmse:3.039985\n",
      "[1254]\ttrain-rmse:3.039035\n",
      "[1255]\ttrain-rmse:3.038094\n",
      "[1256]\ttrain-rmse:3.037150\n",
      "[1257]\ttrain-rmse:3.036208\n",
      "[1258]\ttrain-rmse:3.035278\n",
      "[1259]\ttrain-rmse:3.034333\n",
      "[1260]\ttrain-rmse:3.033398\n",
      "[1261]\ttrain-rmse:3.032442\n",
      "[1262]\ttrain-rmse:3.031502\n",
      "[1263]\ttrain-rmse:3.030571\n",
      "[1264]\ttrain-rmse:3.029640\n",
      "[1265]\ttrain-rmse:3.028693\n",
      "[1266]\ttrain-rmse:3.027757\n",
      "[1267]\ttrain-rmse:3.026822\n",
      "[1268]\ttrain-rmse:3.025890\n",
      "[1269]\ttrain-rmse:3.024964\n",
      "[1270]\ttrain-rmse:3.024029\n",
      "[1271]\ttrain-rmse:3.023102\n",
      "[1272]\ttrain-rmse:3.022160\n",
      "[1273]\ttrain-rmse:3.021222\n",
      "[1274]\ttrain-rmse:3.020301\n",
      "[1275]\ttrain-rmse:3.019354\n",
      "[1276]\ttrain-rmse:3.018423\n",
      "[1277]\ttrain-rmse:3.017498\n",
      "[1278]\ttrain-rmse:3.016564\n",
      "[1279]\ttrain-rmse:3.015644\n",
      "[1280]\ttrain-rmse:3.014718\n",
      "[1281]\ttrain-rmse:3.013800\n",
      "[1282]\ttrain-rmse:3.012871\n",
      "[1283]\ttrain-rmse:3.011940\n",
      "[1284]\ttrain-rmse:3.011004\n",
      "[1285]\ttrain-rmse:3.010084\n",
      "[1286]\ttrain-rmse:3.009151\n",
      "[1287]\ttrain-rmse:3.008234\n",
      "[1288]\ttrain-rmse:3.007319\n",
      "[1289]\ttrain-rmse:3.006389\n",
      "[1290]\ttrain-rmse:3.005471\n",
      "[1291]\ttrain-rmse:3.004539\n",
      "[1292]\ttrain-rmse:3.003616\n",
      "[1293]\ttrain-rmse:3.002706\n",
      "[1294]\ttrain-rmse:3.001800\n",
      "[1295]\ttrain-rmse:3.000882\n",
      "[1296]\ttrain-rmse:2.999963\n",
      "[1297]\ttrain-rmse:2.999028\n",
      "[1298]\ttrain-rmse:2.998125\n",
      "[1299]\ttrain-rmse:2.997203\n",
      "[1300]\ttrain-rmse:2.996305\n",
      "[1301]\ttrain-rmse:2.995381\n",
      "[1302]\ttrain-rmse:2.994458\n",
      "[1303]\ttrain-rmse:2.993541\n",
      "[1304]\ttrain-rmse:2.992633\n",
      "[1305]\ttrain-rmse:2.991718\n",
      "[1306]\ttrain-rmse:2.990816\n",
      "[1307]\ttrain-rmse:2.989884\n",
      "[1308]\ttrain-rmse:2.988965\n",
      "[1309]\ttrain-rmse:2.988055\n",
      "[1310]\ttrain-rmse:2.987146\n",
      "[1311]\ttrain-rmse:2.986241\n",
      "[1312]\ttrain-rmse:2.985339\n",
      "[1313]\ttrain-rmse:2.984416\n",
      "[1314]\ttrain-rmse:2.983514\n",
      "[1315]\ttrain-rmse:2.982609\n",
      "[1316]\ttrain-rmse:2.981702\n",
      "[1317]\ttrain-rmse:2.980797\n",
      "[1318]\ttrain-rmse:2.979905\n",
      "[1319]\ttrain-rmse:2.978992\n",
      "[1320]\ttrain-rmse:2.978089\n",
      "[1321]\ttrain-rmse:2.977200\n",
      "[1322]\ttrain-rmse:2.976302\n",
      "[1323]\ttrain-rmse:2.975402\n",
      "[1324]\ttrain-rmse:2.974498\n",
      "[1325]\ttrain-rmse:2.973604\n",
      "[1326]\ttrain-rmse:2.972692\n",
      "[1327]\ttrain-rmse:2.971793\n",
      "[1328]\ttrain-rmse:2.970898\n",
      "[1329]\ttrain-rmse:2.970001\n",
      "[1330]\ttrain-rmse:2.969101\n",
      "[1331]\ttrain-rmse:2.968204\n",
      "[1332]\ttrain-rmse:2.967309\n",
      "[1333]\ttrain-rmse:2.966401\n",
      "[1334]\ttrain-rmse:2.965502\n",
      "[1335]\ttrain-rmse:2.964613\n",
      "[1336]\ttrain-rmse:2.963725\n",
      "[1337]\ttrain-rmse:2.962833\n",
      "[1338]\ttrain-rmse:2.961928\n",
      "[1339]\ttrain-rmse:2.961038\n",
      "[1340]\ttrain-rmse:2.960159\n",
      "[1341]\ttrain-rmse:2.959259\n",
      "[1342]\ttrain-rmse:2.958366\n",
      "[1343]\ttrain-rmse:2.957469\n",
      "[1344]\ttrain-rmse:2.956578\n",
      "[1345]\ttrain-rmse:2.955697\n",
      "[1346]\ttrain-rmse:2.954815\n",
      "[1347]\ttrain-rmse:2.953916\n",
      "[1348]\ttrain-rmse:2.953025\n",
      "[1349]\ttrain-rmse:2.952141\n",
      "[1350]\ttrain-rmse:2.951254\n",
      "[1351]\ttrain-rmse:2.950372\n",
      "[1352]\ttrain-rmse:2.949488\n",
      "[1353]\ttrain-rmse:2.948591\n",
      "[1354]\ttrain-rmse:2.947719\n",
      "[1355]\ttrain-rmse:2.946840\n",
      "[1356]\ttrain-rmse:2.945952\n",
      "[1357]\ttrain-rmse:2.945073\n",
      "[1358]\ttrain-rmse:2.944193\n",
      "[1359]\ttrain-rmse:2.943303\n",
      "[1360]\ttrain-rmse:2.942429\n",
      "[1361]\ttrain-rmse:2.941547\n",
      "[1362]\ttrain-rmse:2.940663\n",
      "[1363]\ttrain-rmse:2.939798\n",
      "[1364]\ttrain-rmse:2.938922\n",
      "[1365]\ttrain-rmse:2.938051\n",
      "[1366]\ttrain-rmse:2.937183\n",
      "[1367]\ttrain-rmse:2.936316\n",
      "[1368]\ttrain-rmse:2.935435\n",
      "[1369]\ttrain-rmse:2.934572\n",
      "[1370]\ttrain-rmse:2.933692\n",
      "[1371]\ttrain-rmse:2.932815\n",
      "[1372]\ttrain-rmse:2.931933\n",
      "[1373]\ttrain-rmse:2.931061\n",
      "[1374]\ttrain-rmse:2.930193\n",
      "[1375]\ttrain-rmse:2.929336\n",
      "[1376]\ttrain-rmse:2.928460\n",
      "[1377]\ttrain-rmse:2.927598\n",
      "[1378]\ttrain-rmse:2.926722\n",
      "[1379]\ttrain-rmse:2.925845\n",
      "[1380]\ttrain-rmse:2.924992\n",
      "[1381]\ttrain-rmse:2.924115\n",
      "[1382]\ttrain-rmse:2.923240\n",
      "[1383]\ttrain-rmse:2.922381\n",
      "[1384]\ttrain-rmse:2.921518\n",
      "[1385]\ttrain-rmse:2.920642\n",
      "[1386]\ttrain-rmse:2.919782\n",
      "[1387]\ttrain-rmse:2.918913\n",
      "[1388]\ttrain-rmse:2.918038\n",
      "[1389]\ttrain-rmse:2.917186\n",
      "[1390]\ttrain-rmse:2.916325\n",
      "[1391]\ttrain-rmse:2.915449\n",
      "[1392]\ttrain-rmse:2.914596\n",
      "[1393]\ttrain-rmse:2.913723\n",
      "[1394]\ttrain-rmse:2.912859\n",
      "[1395]\ttrain-rmse:2.912005\n",
      "[1396]\ttrain-rmse:2.911146\n",
      "[1397]\ttrain-rmse:2.910294\n",
      "[1398]\ttrain-rmse:2.909433\n",
      "[1399]\ttrain-rmse:2.908559\n",
      "[1400]\ttrain-rmse:2.907711\n",
      "[1401]\ttrain-rmse:2.906848\n",
      "[1402]\ttrain-rmse:2.905992\n",
      "[1403]\ttrain-rmse:2.905147\n",
      "[1404]\ttrain-rmse:2.904288\n",
      "[1405]\ttrain-rmse:2.903416\n",
      "[1406]\ttrain-rmse:2.902567\n",
      "[1407]\ttrain-rmse:2.901709\n",
      "[1408]\ttrain-rmse:2.900845\n",
      "[1409]\ttrain-rmse:2.900000\n",
      "[1410]\ttrain-rmse:2.899153\n",
      "[1411]\ttrain-rmse:2.898296\n",
      "[1412]\ttrain-rmse:2.897448\n",
      "[1413]\ttrain-rmse:2.896591\n",
      "[1414]\ttrain-rmse:2.895721\n",
      "[1415]\ttrain-rmse:2.894889\n",
      "[1416]\ttrain-rmse:2.894036\n",
      "[1417]\ttrain-rmse:2.893187\n",
      "[1418]\ttrain-rmse:2.892341\n",
      "[1419]\ttrain-rmse:2.891492\n",
      "[1420]\ttrain-rmse:2.890635\n",
      "[1421]\ttrain-rmse:2.889797\n",
      "[1422]\ttrain-rmse:2.888949\n",
      "[1423]\ttrain-rmse:2.888089\n",
      "[1424]\ttrain-rmse:2.887261\n",
      "[1425]\ttrain-rmse:2.886413\n",
      "[1426]\ttrain-rmse:2.885561\n",
      "[1427]\ttrain-rmse:2.884736\n",
      "[1428]\ttrain-rmse:2.883888\n",
      "[1429]\ttrain-rmse:2.883045\n",
      "[1430]\ttrain-rmse:2.882221\n",
      "[1431]\ttrain-rmse:2.881367\n",
      "[1432]\ttrain-rmse:2.880526\n",
      "[1433]\ttrain-rmse:2.879694\n",
      "[1434]\ttrain-rmse:2.878841\n",
      "[1435]\ttrain-rmse:2.878005\n",
      "[1436]\ttrain-rmse:2.877172\n",
      "[1437]\ttrain-rmse:2.876338\n",
      "[1438]\ttrain-rmse:2.875485\n",
      "[1439]\ttrain-rmse:2.874661\n",
      "[1440]\ttrain-rmse:2.873817\n",
      "[1441]\ttrain-rmse:2.872978\n",
      "[1442]\ttrain-rmse:2.872155\n",
      "[1443]\ttrain-rmse:2.871303\n",
      "[1444]\ttrain-rmse:2.870463\n",
      "[1445]\ttrain-rmse:2.869644\n",
      "[1446]\ttrain-rmse:2.868805\n",
      "[1447]\ttrain-rmse:2.867958\n",
      "[1448]\ttrain-rmse:2.867154\n",
      "[1449]\ttrain-rmse:2.866311\n",
      "[1450]\ttrain-rmse:2.865460\n",
      "[1451]\ttrain-rmse:2.864645\n",
      "[1452]\ttrain-rmse:2.863799\n",
      "[1453]\ttrain-rmse:2.862973\n",
      "[1454]\ttrain-rmse:2.862170\n",
      "[1455]\ttrain-rmse:2.861342\n",
      "[1456]\ttrain-rmse:2.860507\n",
      "[1457]\ttrain-rmse:2.859686\n",
      "[1458]\ttrain-rmse:2.858861\n",
      "[1459]\ttrain-rmse:2.858025\n",
      "[1460]\ttrain-rmse:2.857217\n",
      "[1461]\ttrain-rmse:2.856385\n",
      "[1462]\ttrain-rmse:2.855545\n",
      "[1463]\ttrain-rmse:2.854731\n",
      "[1464]\ttrain-rmse:2.853907\n",
      "[1465]\ttrain-rmse:2.853085\n",
      "[1466]\ttrain-rmse:2.852259\n",
      "[1467]\ttrain-rmse:2.851432\n",
      "[1468]\ttrain-rmse:2.850602\n",
      "[1469]\ttrain-rmse:2.849792\n",
      "[1470]\ttrain-rmse:2.848971\n",
      "[1471]\ttrain-rmse:2.848154\n",
      "[1472]\ttrain-rmse:2.847341\n",
      "[1473]\ttrain-rmse:2.846526\n",
      "[1474]\ttrain-rmse:2.845705\n",
      "[1475]\ttrain-rmse:2.844886\n",
      "[1476]\ttrain-rmse:2.844077\n",
      "[1477]\ttrain-rmse:2.843263\n",
      "[1478]\ttrain-rmse:2.842454\n",
      "[1479]\ttrain-rmse:2.841651\n",
      "[1480]\ttrain-rmse:2.840822\n",
      "[1481]\ttrain-rmse:2.840012\n",
      "[1482]\ttrain-rmse:2.839203\n",
      "[1483]\ttrain-rmse:2.838384\n",
      "[1484]\ttrain-rmse:2.837564\n",
      "[1485]\ttrain-rmse:2.836759\n",
      "[1486]\ttrain-rmse:2.835947\n",
      "[1487]\ttrain-rmse:2.835125\n",
      "[1488]\ttrain-rmse:2.834321\n",
      "[1489]\ttrain-rmse:2.833527\n",
      "[1490]\ttrain-rmse:2.832712\n",
      "[1491]\ttrain-rmse:2.831906\n",
      "[1492]\ttrain-rmse:2.831102\n",
      "[1493]\ttrain-rmse:2.830293\n",
      "[1494]\ttrain-rmse:2.829489\n",
      "[1495]\ttrain-rmse:2.828700\n",
      "[1496]\ttrain-rmse:2.827880\n",
      "[1497]\ttrain-rmse:2.827067\n",
      "[1498]\ttrain-rmse:2.826279\n",
      "[1499]\ttrain-rmse:2.825478\n",
      "[1500]\ttrain-rmse:2.824660\n",
      "[1501]\ttrain-rmse:2.823874\n",
      "[1502]\ttrain-rmse:2.823072\n",
      "[1503]\ttrain-rmse:2.822264\n",
      "[1504]\ttrain-rmse:2.821468\n",
      "[1505]\ttrain-rmse:2.820669\n",
      "[1506]\ttrain-rmse:2.819861\n",
      "[1507]\ttrain-rmse:2.819062\n",
      "[1508]\ttrain-rmse:2.818271\n",
      "[1509]\ttrain-rmse:2.817472\n",
      "[1510]\ttrain-rmse:2.816675\n",
      "[1511]\ttrain-rmse:2.815886\n",
      "[1512]\ttrain-rmse:2.815088\n",
      "[1513]\ttrain-rmse:2.814287\n",
      "[1514]\ttrain-rmse:2.813496\n",
      "[1515]\ttrain-rmse:2.812706\n",
      "[1516]\ttrain-rmse:2.811916\n",
      "[1517]\ttrain-rmse:2.811129\n",
      "[1518]\ttrain-rmse:2.810333\n",
      "[1519]\ttrain-rmse:2.809522\n",
      "[1520]\ttrain-rmse:2.808735\n",
      "[1521]\ttrain-rmse:2.807963\n",
      "[1522]\ttrain-rmse:2.807166\n",
      "[1523]\ttrain-rmse:2.806368\n",
      "[1524]\ttrain-rmse:2.805592\n",
      "[1525]\ttrain-rmse:2.804794\n",
      "[1526]\ttrain-rmse:2.804000\n",
      "[1527]\ttrain-rmse:2.803224\n",
      "[1528]\ttrain-rmse:2.802441\n",
      "[1529]\ttrain-rmse:2.801644\n",
      "[1530]\ttrain-rmse:2.800864\n",
      "[1531]\ttrain-rmse:2.800082\n",
      "[1532]\ttrain-rmse:2.799286\n",
      "[1533]\ttrain-rmse:2.798500\n",
      "[1534]\ttrain-rmse:2.797724\n",
      "[1535]\ttrain-rmse:2.796933\n",
      "[1536]\ttrain-rmse:2.796157\n",
      "[1537]\ttrain-rmse:2.795377\n",
      "[1538]\ttrain-rmse:2.794596\n",
      "[1539]\ttrain-rmse:2.793804\n",
      "[1540]\ttrain-rmse:2.793023\n",
      "[1541]\ttrain-rmse:2.792250\n",
      "[1542]\ttrain-rmse:2.791453\n",
      "[1543]\ttrain-rmse:2.790661\n",
      "[1544]\ttrain-rmse:2.789894\n",
      "[1545]\ttrain-rmse:2.789121\n",
      "[1546]\ttrain-rmse:2.788335\n",
      "[1547]\ttrain-rmse:2.787563\n",
      "[1548]\ttrain-rmse:2.786783\n",
      "[1549]\ttrain-rmse:2.786006\n",
      "[1550]\ttrain-rmse:2.785232\n",
      "[1551]\ttrain-rmse:2.784462\n",
      "[1552]\ttrain-rmse:2.783683\n",
      "[1553]\ttrain-rmse:2.782913\n",
      "[1554]\ttrain-rmse:2.782144\n",
      "[1555]\ttrain-rmse:2.781368\n",
      "[1556]\ttrain-rmse:2.780590\n",
      "[1557]\ttrain-rmse:2.779826\n",
      "[1558]\ttrain-rmse:2.779057\n",
      "[1559]\ttrain-rmse:2.778284\n",
      "[1560]\ttrain-rmse:2.777528\n",
      "[1561]\ttrain-rmse:2.776750\n",
      "[1562]\ttrain-rmse:2.775972\n",
      "[1563]\ttrain-rmse:2.775205\n",
      "[1564]\ttrain-rmse:2.774452\n",
      "[1565]\ttrain-rmse:2.773682\n",
      "[1566]\ttrain-rmse:2.772909\n",
      "[1567]\ttrain-rmse:2.772144\n",
      "[1568]\ttrain-rmse:2.771384\n",
      "[1569]\ttrain-rmse:2.770617\n",
      "[1570]\ttrain-rmse:2.769848\n",
      "[1571]\ttrain-rmse:2.769089\n",
      "[1572]\ttrain-rmse:2.768314\n",
      "[1573]\ttrain-rmse:2.767563\n",
      "[1574]\ttrain-rmse:2.766796\n",
      "[1575]\ttrain-rmse:2.766029\n",
      "[1576]\ttrain-rmse:2.765262\n",
      "[1577]\ttrain-rmse:2.764509\n",
      "[1578]\ttrain-rmse:2.763741\n",
      "[1579]\ttrain-rmse:2.762979\n",
      "[1580]\ttrain-rmse:2.762229\n",
      "[1581]\ttrain-rmse:2.761466\n",
      "[1582]\ttrain-rmse:2.760701\n",
      "[1583]\ttrain-rmse:2.759947\n",
      "[1584]\ttrain-rmse:2.759195\n",
      "[1585]\ttrain-rmse:2.758419\n",
      "[1586]\ttrain-rmse:2.757665\n",
      "[1587]\ttrain-rmse:2.756906\n",
      "[1588]\ttrain-rmse:2.756145\n",
      "[1589]\ttrain-rmse:2.755384\n",
      "[1590]\ttrain-rmse:2.754631\n",
      "[1591]\ttrain-rmse:2.753894\n",
      "[1592]\ttrain-rmse:2.753129\n",
      "[1593]\ttrain-rmse:2.752366\n",
      "[1594]\ttrain-rmse:2.751621\n",
      "[1595]\ttrain-rmse:2.750859\n",
      "[1596]\ttrain-rmse:2.750106\n",
      "[1597]\ttrain-rmse:2.749358\n",
      "[1598]\ttrain-rmse:2.748618\n",
      "[1599]\ttrain-rmse:2.747865\n",
      "[1600]\ttrain-rmse:2.747107\n",
      "[1601]\ttrain-rmse:2.746361\n",
      "[1602]\ttrain-rmse:2.745610\n",
      "[1603]\ttrain-rmse:2.744840\n",
      "[1604]\ttrain-rmse:2.744079\n",
      "[1605]\ttrain-rmse:2.743346\n",
      "[1606]\ttrain-rmse:2.742594\n",
      "[1607]\ttrain-rmse:2.741828\n",
      "[1608]\ttrain-rmse:2.741089\n",
      "[1609]\ttrain-rmse:2.740353\n",
      "[1610]\ttrain-rmse:2.739604\n",
      "[1611]\ttrain-rmse:2.738858\n",
      "[1612]\ttrain-rmse:2.738127\n",
      "[1613]\ttrain-rmse:2.737387\n",
      "[1614]\ttrain-rmse:2.736640\n",
      "[1615]\ttrain-rmse:2.735904\n",
      "[1616]\ttrain-rmse:2.735172\n",
      "[1617]\ttrain-rmse:2.734416\n",
      "[1618]\ttrain-rmse:2.733673\n",
      "[1619]\ttrain-rmse:2.732927\n",
      "[1620]\ttrain-rmse:2.732209\n",
      "[1621]\ttrain-rmse:2.731462\n",
      "[1622]\ttrain-rmse:2.730713\n",
      "[1623]\ttrain-rmse:2.729975\n",
      "[1624]\ttrain-rmse:2.729234\n",
      "[1625]\ttrain-rmse:2.728492\n",
      "[1626]\ttrain-rmse:2.727754\n",
      "[1627]\ttrain-rmse:2.727013\n",
      "[1628]\ttrain-rmse:2.726285\n",
      "[1629]\ttrain-rmse:2.725548\n",
      "[1630]\ttrain-rmse:2.724807\n",
      "[1631]\ttrain-rmse:2.724060\n",
      "[1632]\ttrain-rmse:2.723328\n",
      "[1633]\ttrain-rmse:2.722605\n",
      "[1634]\ttrain-rmse:2.721878\n",
      "[1635]\ttrain-rmse:2.721133\n",
      "[1636]\ttrain-rmse:2.720399\n",
      "[1637]\ttrain-rmse:2.719663\n",
      "[1638]\ttrain-rmse:2.718921\n",
      "[1639]\ttrain-rmse:2.718213\n",
      "[1640]\ttrain-rmse:2.717481\n",
      "[1641]\ttrain-rmse:2.716751\n",
      "[1642]\ttrain-rmse:2.716021\n",
      "[1643]\ttrain-rmse:2.715292\n",
      "[1644]\ttrain-rmse:2.714569\n",
      "[1645]\ttrain-rmse:2.713836\n",
      "[1646]\ttrain-rmse:2.713103\n",
      "[1647]\ttrain-rmse:2.712381\n",
      "[1648]\ttrain-rmse:2.711662\n",
      "[1649]\ttrain-rmse:2.710921\n",
      "[1650]\ttrain-rmse:2.710211\n",
      "[1651]\ttrain-rmse:2.709475\n",
      "[1652]\ttrain-rmse:2.708751\n",
      "[1653]\ttrain-rmse:2.708017\n",
      "[1654]\ttrain-rmse:2.707302\n",
      "[1655]\ttrain-rmse:2.706592\n",
      "[1656]\ttrain-rmse:2.705855\n",
      "[1657]\ttrain-rmse:2.705126\n",
      "[1658]\ttrain-rmse:2.704398\n",
      "[1659]\ttrain-rmse:2.703678\n",
      "[1660]\ttrain-rmse:2.702947\n",
      "[1661]\ttrain-rmse:2.702228\n",
      "[1662]\ttrain-rmse:2.701510\n",
      "[1663]\ttrain-rmse:2.700789\n",
      "[1664]\ttrain-rmse:2.700070\n",
      "[1665]\ttrain-rmse:2.699345\n",
      "[1666]\ttrain-rmse:2.698633\n",
      "[1667]\ttrain-rmse:2.697913\n",
      "[1668]\ttrain-rmse:2.697192\n",
      "[1669]\ttrain-rmse:2.696478\n",
      "[1670]\ttrain-rmse:2.695773\n",
      "[1671]\ttrain-rmse:2.695045\n",
      "[1672]\ttrain-rmse:2.694331\n",
      "[1673]\ttrain-rmse:2.693617\n",
      "[1674]\ttrain-rmse:2.692904\n",
      "[1675]\ttrain-rmse:2.692181\n",
      "[1676]\ttrain-rmse:2.691475\n",
      "[1677]\ttrain-rmse:2.690763\n",
      "[1678]\ttrain-rmse:2.690040\n",
      "[1679]\ttrain-rmse:2.689328\n",
      "[1680]\ttrain-rmse:2.688624\n",
      "[1681]\ttrain-rmse:2.687930\n",
      "[1682]\ttrain-rmse:2.687212\n",
      "[1683]\ttrain-rmse:2.686503\n",
      "[1684]\ttrain-rmse:2.685798\n",
      "[1685]\ttrain-rmse:2.685088\n",
      "[1686]\ttrain-rmse:2.684382\n",
      "[1687]\ttrain-rmse:2.683664\n",
      "[1688]\ttrain-rmse:2.682968\n",
      "[1689]\ttrain-rmse:2.682267\n",
      "[1690]\ttrain-rmse:2.681550\n",
      "[1691]\ttrain-rmse:2.680842\n",
      "[1692]\ttrain-rmse:2.680147\n",
      "[1693]\ttrain-rmse:2.679434\n",
      "[1694]\ttrain-rmse:2.678730\n",
      "[1695]\ttrain-rmse:2.678025\n",
      "[1696]\ttrain-rmse:2.677325\n",
      "[1697]\ttrain-rmse:2.676620\n",
      "[1698]\ttrain-rmse:2.675910\n",
      "[1699]\ttrain-rmse:2.675203\n",
      "[1700]\ttrain-rmse:2.674508\n",
      "[1701]\ttrain-rmse:2.673806\n",
      "[1702]\ttrain-rmse:2.673105\n",
      "[1703]\ttrain-rmse:2.672405\n",
      "[1704]\ttrain-rmse:2.671724\n",
      "[1705]\ttrain-rmse:2.671017\n",
      "[1706]\ttrain-rmse:2.670309\n",
      "[1707]\ttrain-rmse:2.669613\n",
      "[1708]\ttrain-rmse:2.668919\n",
      "[1709]\ttrain-rmse:2.668221\n",
      "[1710]\ttrain-rmse:2.667510\n",
      "[1711]\ttrain-rmse:2.666820\n",
      "[1712]\ttrain-rmse:2.666126\n",
      "[1713]\ttrain-rmse:2.665414\n",
      "[1714]\ttrain-rmse:2.664719\n",
      "[1715]\ttrain-rmse:2.664023\n",
      "[1716]\ttrain-rmse:2.663327\n",
      "[1717]\ttrain-rmse:2.662628\n",
      "[1718]\ttrain-rmse:2.661929\n",
      "[1719]\ttrain-rmse:2.661243\n",
      "[1720]\ttrain-rmse:2.660555\n",
      "[1721]\ttrain-rmse:2.659852\n",
      "[1722]\ttrain-rmse:2.659154\n",
      "[1723]\ttrain-rmse:2.658475\n",
      "[1724]\ttrain-rmse:2.657795\n",
      "[1725]\ttrain-rmse:2.657106\n",
      "[1726]\ttrain-rmse:2.656419\n",
      "[1727]\ttrain-rmse:2.655736\n",
      "[1728]\ttrain-rmse:2.655040\n",
      "[1729]\ttrain-rmse:2.654345\n",
      "[1730]\ttrain-rmse:2.653666\n",
      "[1731]\ttrain-rmse:2.652976\n",
      "[1732]\ttrain-rmse:2.652294\n",
      "[1733]\ttrain-rmse:2.651599\n",
      "[1734]\ttrain-rmse:2.650922\n",
      "[1735]\ttrain-rmse:2.650229\n",
      "[1736]\ttrain-rmse:2.649549\n",
      "[1737]\ttrain-rmse:2.648864\n",
      "[1738]\ttrain-rmse:2.648197\n",
      "[1739]\ttrain-rmse:2.647514\n",
      "[1740]\ttrain-rmse:2.646817\n",
      "[1741]\ttrain-rmse:2.646125\n",
      "[1742]\ttrain-rmse:2.645447\n",
      "[1743]\ttrain-rmse:2.644759\n",
      "[1744]\ttrain-rmse:2.644096\n",
      "[1745]\ttrain-rmse:2.643405\n",
      "[1746]\ttrain-rmse:2.642734\n",
      "[1747]\ttrain-rmse:2.642061\n",
      "[1748]\ttrain-rmse:2.641371\n",
      "[1749]\ttrain-rmse:2.640692\n",
      "[1750]\ttrain-rmse:2.640007\n",
      "[1751]\ttrain-rmse:2.639333\n",
      "[1752]\ttrain-rmse:2.638652\n",
      "[1753]\ttrain-rmse:2.637981\n",
      "[1754]\ttrain-rmse:2.637318\n",
      "[1755]\ttrain-rmse:2.636635\n",
      "[1756]\ttrain-rmse:2.635955\n",
      "[1757]\ttrain-rmse:2.635276\n",
      "[1758]\ttrain-rmse:2.634602\n",
      "[1759]\ttrain-rmse:2.633931\n",
      "[1760]\ttrain-rmse:2.633267\n",
      "[1761]\ttrain-rmse:2.632592\n",
      "[1762]\ttrain-rmse:2.631922\n",
      "[1763]\ttrain-rmse:2.631249\n",
      "[1764]\ttrain-rmse:2.630576\n",
      "[1765]\ttrain-rmse:2.629898\n",
      "[1766]\ttrain-rmse:2.629228\n",
      "[1767]\ttrain-rmse:2.628544\n",
      "[1768]\ttrain-rmse:2.627879\n",
      "[1769]\ttrain-rmse:2.627200\n",
      "[1770]\ttrain-rmse:2.626533\n",
      "[1771]\ttrain-rmse:2.625871\n",
      "[1772]\ttrain-rmse:2.625198\n",
      "[1773]\ttrain-rmse:2.624528\n",
      "[1774]\ttrain-rmse:2.623862\n",
      "[1775]\ttrain-rmse:2.623201\n",
      "[1776]\ttrain-rmse:2.622536\n",
      "[1777]\ttrain-rmse:2.621881\n",
      "[1778]\ttrain-rmse:2.621208\n",
      "[1779]\ttrain-rmse:2.620554\n",
      "[1780]\ttrain-rmse:2.619892\n",
      "[1781]\ttrain-rmse:2.619233\n",
      "[1782]\ttrain-rmse:2.618562\n",
      "[1783]\ttrain-rmse:2.617901\n",
      "[1784]\ttrain-rmse:2.617244\n",
      "[1785]\ttrain-rmse:2.616569\n",
      "[1786]\ttrain-rmse:2.615909\n",
      "[1787]\ttrain-rmse:2.615248\n",
      "[1788]\ttrain-rmse:2.614594\n",
      "[1789]\ttrain-rmse:2.613931\n",
      "[1790]\ttrain-rmse:2.613279\n",
      "[1791]\ttrain-rmse:2.612606\n",
      "[1792]\ttrain-rmse:2.611952\n",
      "[1793]\ttrain-rmse:2.611282\n",
      "[1794]\ttrain-rmse:2.610632\n",
      "[1795]\ttrain-rmse:2.609982\n",
      "[1796]\ttrain-rmse:2.609329\n",
      "[1797]\ttrain-rmse:2.608668\n",
      "[1798]\ttrain-rmse:2.608021\n",
      "[1799]\ttrain-rmse:2.607363\n",
      "[1800]\ttrain-rmse:2.606712\n",
      "[1801]\ttrain-rmse:2.606059\n",
      "[1802]\ttrain-rmse:2.605397\n",
      "[1803]\ttrain-rmse:2.604742\n",
      "[1804]\ttrain-rmse:2.604094\n",
      "[1805]\ttrain-rmse:2.603440\n",
      "[1806]\ttrain-rmse:2.602786\n",
      "[1807]\ttrain-rmse:2.602143\n",
      "[1808]\ttrain-rmse:2.601494\n",
      "[1809]\ttrain-rmse:2.600846\n",
      "[1810]\ttrain-rmse:2.600188\n",
      "[1811]\ttrain-rmse:2.599542\n",
      "[1812]\ttrain-rmse:2.598891\n",
      "[1813]\ttrain-rmse:2.598243\n",
      "[1814]\ttrain-rmse:2.597575\n",
      "[1815]\ttrain-rmse:2.596931\n",
      "[1816]\ttrain-rmse:2.596286\n",
      "[1817]\ttrain-rmse:2.595641\n",
      "[1818]\ttrain-rmse:2.594997\n",
      "[1819]\ttrain-rmse:2.594358\n",
      "[1820]\ttrain-rmse:2.593708\n",
      "[1821]\ttrain-rmse:2.593051\n",
      "[1822]\ttrain-rmse:2.592404\n",
      "[1823]\ttrain-rmse:2.591757\n",
      "[1824]\ttrain-rmse:2.591125\n",
      "[1825]\ttrain-rmse:2.590476\n",
      "[1826]\ttrain-rmse:2.589837\n",
      "[1827]\ttrain-rmse:2.589188\n",
      "[1828]\ttrain-rmse:2.588553\n",
      "[1829]\ttrain-rmse:2.587907\n",
      "[1830]\ttrain-rmse:2.587267\n",
      "[1831]\ttrain-rmse:2.586606\n",
      "[1832]\ttrain-rmse:2.585973\n",
      "[1833]\ttrain-rmse:2.585319\n",
      "[1834]\ttrain-rmse:2.584678\n",
      "[1835]\ttrain-rmse:2.584046\n",
      "[1836]\ttrain-rmse:2.583401\n",
      "[1837]\ttrain-rmse:2.582760\n",
      "[1838]\ttrain-rmse:2.582127\n",
      "[1839]\ttrain-rmse:2.581490\n",
      "[1840]\ttrain-rmse:2.580843\n",
      "[1841]\ttrain-rmse:2.580206\n",
      "[1842]\ttrain-rmse:2.579569\n",
      "[1843]\ttrain-rmse:2.578928\n",
      "[1844]\ttrain-rmse:2.578290\n",
      "[1845]\ttrain-rmse:2.577664\n",
      "[1846]\ttrain-rmse:2.577013\n",
      "[1847]\ttrain-rmse:2.576382\n",
      "[1848]\ttrain-rmse:2.575745\n",
      "[1849]\ttrain-rmse:2.575109\n",
      "[1850]\ttrain-rmse:2.574476\n",
      "[1851]\ttrain-rmse:2.573852\n",
      "[1852]\ttrain-rmse:2.573221\n",
      "[1853]\ttrain-rmse:2.572586\n",
      "[1854]\ttrain-rmse:2.571959\n",
      "[1855]\ttrain-rmse:2.571328\n",
      "[1856]\ttrain-rmse:2.570694\n",
      "[1857]\ttrain-rmse:2.570064\n",
      "[1858]\ttrain-rmse:2.569441\n",
      "[1859]\ttrain-rmse:2.568814\n",
      "[1860]\ttrain-rmse:2.568192\n",
      "[1861]\ttrain-rmse:2.567557\n",
      "[1862]\ttrain-rmse:2.566931\n",
      "[1863]\ttrain-rmse:2.566303\n",
      "[1864]\ttrain-rmse:2.565678\n",
      "[1865]\ttrain-rmse:2.565042\n",
      "[1866]\ttrain-rmse:2.564418\n",
      "[1867]\ttrain-rmse:2.563791\n",
      "[1868]\ttrain-rmse:2.563168\n",
      "[1869]\ttrain-rmse:2.562552\n",
      "[1870]\ttrain-rmse:2.561929\n",
      "[1871]\ttrain-rmse:2.561303\n",
      "[1872]\ttrain-rmse:2.560695\n",
      "[1873]\ttrain-rmse:2.560071\n",
      "[1874]\ttrain-rmse:2.559453\n",
      "[1875]\ttrain-rmse:2.558831\n",
      "[1876]\ttrain-rmse:2.558215\n",
      "[1877]\ttrain-rmse:2.557598\n",
      "[1878]\ttrain-rmse:2.556984\n",
      "[1879]\ttrain-rmse:2.556370\n",
      "[1880]\ttrain-rmse:2.555748\n",
      "[1881]\ttrain-rmse:2.555131\n",
      "[1882]\ttrain-rmse:2.554521\n",
      "[1883]\ttrain-rmse:2.553897\n",
      "[1884]\ttrain-rmse:2.553275\n",
      "[1885]\ttrain-rmse:2.552658\n",
      "[1886]\ttrain-rmse:2.552039\n",
      "[1887]\ttrain-rmse:2.551425\n",
      "[1888]\ttrain-rmse:2.550802\n",
      "[1889]\ttrain-rmse:2.550186\n",
      "[1890]\ttrain-rmse:2.549577\n",
      "[1891]\ttrain-rmse:2.548964\n",
      "[1892]\ttrain-rmse:2.548344\n",
      "[1893]\ttrain-rmse:2.547724\n",
      "[1894]\ttrain-rmse:2.547104\n",
      "[1895]\ttrain-rmse:2.546494\n",
      "[1896]\ttrain-rmse:2.545884\n",
      "[1897]\ttrain-rmse:2.545270\n",
      "[1898]\ttrain-rmse:2.544664\n",
      "[1899]\ttrain-rmse:2.544065\n",
      "[1900]\ttrain-rmse:2.543461\n",
      "[1901]\ttrain-rmse:2.542828\n",
      "[1902]\ttrain-rmse:2.542222\n",
      "[1903]\ttrain-rmse:2.541618\n",
      "[1904]\ttrain-rmse:2.541005\n",
      "[1905]\ttrain-rmse:2.540404\n",
      "[1906]\ttrain-rmse:2.539804\n",
      "[1907]\ttrain-rmse:2.539194\n",
      "[1908]\ttrain-rmse:2.538592\n",
      "[1909]\ttrain-rmse:2.537986\n",
      "[1910]\ttrain-rmse:2.537376\n",
      "[1911]\ttrain-rmse:2.536772\n",
      "[1912]\ttrain-rmse:2.536171\n",
      "[1913]\ttrain-rmse:2.535558\n",
      "[1914]\ttrain-rmse:2.534963\n",
      "[1915]\ttrain-rmse:2.534364\n",
      "[1916]\ttrain-rmse:2.533747\n",
      "[1917]\ttrain-rmse:2.533143\n",
      "[1918]\ttrain-rmse:2.532537\n",
      "[1919]\ttrain-rmse:2.531946\n",
      "[1920]\ttrain-rmse:2.531350\n",
      "[1921]\ttrain-rmse:2.530745\n",
      "[1922]\ttrain-rmse:2.530151\n",
      "[1923]\ttrain-rmse:2.529544\n",
      "[1924]\ttrain-rmse:2.528949\n",
      "[1925]\ttrain-rmse:2.528351\n",
      "[1926]\ttrain-rmse:2.527743\n",
      "[1927]\ttrain-rmse:2.527148\n",
      "[1928]\ttrain-rmse:2.526536\n",
      "[1929]\ttrain-rmse:2.525949\n",
      "[1930]\ttrain-rmse:2.525348\n",
      "[1931]\ttrain-rmse:2.524749\n",
      "[1932]\ttrain-rmse:2.524144\n",
      "[1933]\ttrain-rmse:2.523551\n",
      "[1934]\ttrain-rmse:2.522961\n",
      "[1935]\ttrain-rmse:2.522359\n",
      "[1936]\ttrain-rmse:2.521751\n",
      "[1937]\ttrain-rmse:2.521165\n",
      "[1938]\ttrain-rmse:2.520581\n",
      "[1939]\ttrain-rmse:2.519991\n",
      "[1940]\ttrain-rmse:2.519395\n",
      "[1941]\ttrain-rmse:2.518808\n",
      "[1942]\ttrain-rmse:2.518204\n",
      "[1943]\ttrain-rmse:2.517629\n",
      "[1944]\ttrain-rmse:2.517027\n",
      "[1945]\ttrain-rmse:2.516447\n",
      "[1946]\ttrain-rmse:2.515831\n",
      "[1947]\ttrain-rmse:2.515245\n",
      "[1948]\ttrain-rmse:2.514661\n",
      "[1949]\ttrain-rmse:2.514058\n",
      "[1950]\ttrain-rmse:2.513476\n",
      "[1951]\ttrain-rmse:2.512892\n",
      "[1952]\ttrain-rmse:2.512296\n",
      "[1953]\ttrain-rmse:2.511704\n",
      "[1954]\ttrain-rmse:2.511111\n",
      "[1955]\ttrain-rmse:2.510535\n",
      "[1956]\ttrain-rmse:2.509948\n",
      "[1957]\ttrain-rmse:2.509364\n",
      "[1958]\ttrain-rmse:2.508779\n",
      "[1959]\ttrain-rmse:2.508198\n",
      "[1960]\ttrain-rmse:2.507613\n",
      "[1961]\ttrain-rmse:2.507027\n",
      "[1962]\ttrain-rmse:2.506443\n",
      "[1963]\ttrain-rmse:2.505854\n",
      "[1964]\ttrain-rmse:2.505286\n",
      "[1965]\ttrain-rmse:2.504700\n",
      "[1966]\ttrain-rmse:2.504124\n",
      "[1967]\ttrain-rmse:2.503539\n",
      "[1968]\ttrain-rmse:2.502946\n",
      "[1969]\ttrain-rmse:2.502356\n",
      "[1970]\ttrain-rmse:2.501778\n",
      "[1971]\ttrain-rmse:2.501204\n",
      "[1972]\ttrain-rmse:2.500640\n",
      "[1973]\ttrain-rmse:2.500060\n",
      "[1974]\ttrain-rmse:2.499478\n",
      "[1975]\ttrain-rmse:2.498893\n",
      "[1976]\ttrain-rmse:2.498321\n",
      "[1977]\ttrain-rmse:2.497736\n",
      "[1978]\ttrain-rmse:2.497169\n",
      "[1979]\ttrain-rmse:2.496600\n",
      "[1980]\ttrain-rmse:2.496025\n",
      "[1981]\ttrain-rmse:2.495448\n",
      "[1982]\ttrain-rmse:2.494863\n",
      "[1983]\ttrain-rmse:2.494288\n",
      "[1984]\ttrain-rmse:2.493717\n",
      "[1985]\ttrain-rmse:2.493138\n",
      "[1986]\ttrain-rmse:2.492563\n",
      "[1987]\ttrain-rmse:2.492000\n",
      "[1988]\ttrain-rmse:2.491434\n",
      "[1989]\ttrain-rmse:2.490857\n",
      "[1990]\ttrain-rmse:2.490282\n",
      "[1991]\ttrain-rmse:2.489701\n",
      "[1992]\ttrain-rmse:2.489132\n",
      "[1993]\ttrain-rmse:2.488553\n",
      "[1994]\ttrain-rmse:2.487987\n",
      "[1995]\ttrain-rmse:2.487428\n",
      "[1996]\ttrain-rmse:2.486851\n",
      "[1997]\ttrain-rmse:2.486273\n",
      "[1998]\ttrain-rmse:2.485703\n",
      "[1999]\ttrain-rmse:2.485140\n",
      "[2000]\ttrain-rmse:2.484571\n",
      "[2001]\ttrain-rmse:2.484004\n",
      "[2002]\ttrain-rmse:2.483437\n",
      "[2003]\ttrain-rmse:2.482872\n",
      "[2004]\ttrain-rmse:2.482300\n",
      "[2005]\ttrain-rmse:2.481729\n",
      "[2006]\ttrain-rmse:2.481169\n",
      "[2007]\ttrain-rmse:2.480580\n",
      "[2008]\ttrain-rmse:2.480014\n",
      "[2009]\ttrain-rmse:2.479456\n",
      "[2010]\ttrain-rmse:2.478894\n",
      "[2011]\ttrain-rmse:2.478330\n",
      "[2012]\ttrain-rmse:2.477765\n",
      "[2013]\ttrain-rmse:2.477201\n",
      "[2014]\ttrain-rmse:2.476638\n",
      "[2015]\ttrain-rmse:2.476074\n",
      "[2016]\ttrain-rmse:2.475513\n",
      "[2017]\ttrain-rmse:2.474951\n",
      "[2018]\ttrain-rmse:2.474388\n",
      "[2019]\ttrain-rmse:2.473823\n",
      "[2020]\ttrain-rmse:2.473270\n",
      "[2021]\ttrain-rmse:2.472705\n",
      "[2022]\ttrain-rmse:2.472149\n",
      "[2023]\ttrain-rmse:2.471591\n",
      "[2024]\ttrain-rmse:2.471023\n",
      "[2025]\ttrain-rmse:2.470453\n",
      "[2026]\ttrain-rmse:2.469896\n",
      "[2027]\ttrain-rmse:2.469334\n",
      "[2028]\ttrain-rmse:2.468762\n",
      "[2029]\ttrain-rmse:2.468224\n",
      "[2030]\ttrain-rmse:2.467665\n",
      "[2031]\ttrain-rmse:2.467118\n",
      "[2032]\ttrain-rmse:2.466564\n",
      "[2033]\ttrain-rmse:2.465988\n",
      "[2034]\ttrain-rmse:2.465440\n",
      "[2035]\ttrain-rmse:2.464881\n",
      "[2036]\ttrain-rmse:2.464324\n",
      "[2037]\ttrain-rmse:2.463772\n",
      "[2038]\ttrain-rmse:2.463213\n",
      "[2039]\ttrain-rmse:2.462669\n",
      "[2040]\ttrain-rmse:2.462125\n",
      "[2041]\ttrain-rmse:2.461570\n",
      "[2042]\ttrain-rmse:2.461012\n",
      "[2043]\ttrain-rmse:2.460457\n",
      "[2044]\ttrain-rmse:2.459919\n",
      "[2045]\ttrain-rmse:2.459380\n",
      "[2046]\ttrain-rmse:2.458829\n",
      "[2047]\ttrain-rmse:2.458274\n",
      "[2048]\ttrain-rmse:2.457711\n",
      "[2049]\ttrain-rmse:2.457170\n",
      "[2050]\ttrain-rmse:2.456629\n",
      "[2051]\ttrain-rmse:2.456068\n",
      "[2052]\ttrain-rmse:2.455522\n",
      "[2053]\ttrain-rmse:2.454970\n",
      "[2054]\ttrain-rmse:2.454431\n",
      "[2055]\ttrain-rmse:2.453876\n",
      "[2056]\ttrain-rmse:2.453332\n",
      "[2057]\ttrain-rmse:2.452791\n",
      "[2058]\ttrain-rmse:2.452247\n",
      "[2059]\ttrain-rmse:2.451699\n",
      "[2060]\ttrain-rmse:2.451151\n",
      "[2061]\ttrain-rmse:2.450605\n",
      "[2062]\ttrain-rmse:2.450060\n",
      "[2063]\ttrain-rmse:2.449514\n",
      "[2064]\ttrain-rmse:2.448956\n",
      "[2065]\ttrain-rmse:2.448407\n",
      "[2066]\ttrain-rmse:2.447882\n",
      "[2067]\ttrain-rmse:2.447345\n",
      "[2068]\ttrain-rmse:2.446792\n",
      "[2069]\ttrain-rmse:2.446242\n",
      "[2070]\ttrain-rmse:2.445713\n",
      "[2071]\ttrain-rmse:2.445176\n",
      "[2072]\ttrain-rmse:2.444629\n",
      "[2073]\ttrain-rmse:2.444090\n",
      "[2074]\ttrain-rmse:2.443538\n",
      "[2075]\ttrain-rmse:2.443000\n",
      "[2076]\ttrain-rmse:2.442461\n",
      "[2077]\ttrain-rmse:2.441936\n",
      "[2078]\ttrain-rmse:2.441397\n",
      "[2079]\ttrain-rmse:2.440853\n",
      "[2080]\ttrain-rmse:2.440314\n",
      "[2081]\ttrain-rmse:2.439759\n",
      "[2082]\ttrain-rmse:2.439220\n",
      "[2083]\ttrain-rmse:2.438690\n",
      "[2084]\ttrain-rmse:2.438171\n",
      "[2085]\ttrain-rmse:2.437625\n",
      "[2086]\ttrain-rmse:2.437091\n",
      "[2087]\ttrain-rmse:2.436561\n",
      "[2088]\ttrain-rmse:2.436035\n",
      "[2089]\ttrain-rmse:2.435500\n",
      "[2090]\ttrain-rmse:2.434968\n",
      "[2091]\ttrain-rmse:2.434437\n",
      "[2092]\ttrain-rmse:2.433909\n",
      "[2093]\ttrain-rmse:2.433380\n",
      "[2094]\ttrain-rmse:2.432845\n",
      "[2095]\ttrain-rmse:2.432312\n",
      "[2096]\ttrain-rmse:2.431772\n",
      "[2097]\ttrain-rmse:2.431247\n",
      "[2098]\ttrain-rmse:2.430714\n",
      "[2099]\ttrain-rmse:2.430185\n",
      "[2100]\ttrain-rmse:2.429651\n",
      "[2101]\ttrain-rmse:2.429114\n",
      "[2102]\ttrain-rmse:2.428568\n",
      "[2103]\ttrain-rmse:2.428031\n",
      "[2104]\ttrain-rmse:2.427509\n",
      "[2105]\ttrain-rmse:2.426985\n",
      "[2106]\ttrain-rmse:2.426455\n",
      "[2107]\ttrain-rmse:2.425916\n",
      "[2108]\ttrain-rmse:2.425398\n",
      "[2109]\ttrain-rmse:2.424879\n",
      "[2110]\ttrain-rmse:2.424355\n",
      "[2111]\ttrain-rmse:2.423823\n",
      "[2112]\ttrain-rmse:2.423309\n",
      "[2113]\ttrain-rmse:2.422789\n",
      "[2114]\ttrain-rmse:2.422257\n",
      "[2115]\ttrain-rmse:2.421735\n",
      "[2116]\ttrain-rmse:2.421211\n",
      "[2117]\ttrain-rmse:2.420685\n",
      "[2118]\ttrain-rmse:2.420146\n",
      "[2119]\ttrain-rmse:2.419628\n",
      "[2120]\ttrain-rmse:2.419112\n",
      "[2121]\ttrain-rmse:2.418593\n",
      "[2122]\ttrain-rmse:2.418078\n",
      "[2123]\ttrain-rmse:2.417539\n",
      "[2124]\ttrain-rmse:2.417022\n",
      "[2125]\ttrain-rmse:2.416513\n",
      "[2126]\ttrain-rmse:2.415996\n",
      "[2127]\ttrain-rmse:2.415488\n",
      "[2128]\ttrain-rmse:2.414948\n",
      "[2129]\ttrain-rmse:2.414432\n",
      "[2130]\ttrain-rmse:2.413908\n",
      "[2131]\ttrain-rmse:2.413391\n",
      "[2132]\ttrain-rmse:2.412881\n",
      "[2133]\ttrain-rmse:2.412350\n",
      "[2134]\ttrain-rmse:2.411838\n",
      "[2135]\ttrain-rmse:2.411314\n",
      "[2136]\ttrain-rmse:2.410811\n",
      "[2137]\ttrain-rmse:2.410298\n",
      "[2138]\ttrain-rmse:2.409774\n",
      "[2139]\ttrain-rmse:2.409245\n",
      "[2140]\ttrain-rmse:2.408727\n",
      "[2141]\ttrain-rmse:2.408221\n",
      "[2142]\ttrain-rmse:2.407706\n",
      "[2143]\ttrain-rmse:2.407190\n",
      "[2144]\ttrain-rmse:2.406677\n",
      "[2145]\ttrain-rmse:2.406163\n",
      "[2146]\ttrain-rmse:2.405656\n",
      "[2147]\ttrain-rmse:2.405129\n",
      "[2148]\ttrain-rmse:2.404626\n",
      "[2149]\ttrain-rmse:2.404116\n",
      "[2150]\ttrain-rmse:2.403605\n",
      "[2151]\ttrain-rmse:2.403096\n",
      "[2152]\ttrain-rmse:2.402590\n",
      "[2153]\ttrain-rmse:2.402088\n",
      "[2154]\ttrain-rmse:2.401574\n",
      "[2155]\ttrain-rmse:2.401074\n",
      "[2156]\ttrain-rmse:2.400559\n",
      "[2157]\ttrain-rmse:2.400054\n",
      "[2158]\ttrain-rmse:2.399540\n",
      "[2159]\ttrain-rmse:2.399032\n",
      "[2160]\ttrain-rmse:2.398529\n",
      "[2161]\ttrain-rmse:2.398016\n",
      "[2162]\ttrain-rmse:2.397516\n",
      "[2163]\ttrain-rmse:2.397002\n",
      "[2164]\ttrain-rmse:2.396501\n",
      "[2165]\ttrain-rmse:2.395997\n",
      "[2166]\ttrain-rmse:2.395496\n",
      "[2167]\ttrain-rmse:2.394990\n",
      "[2168]\ttrain-rmse:2.394477\n",
      "[2169]\ttrain-rmse:2.393967\n",
      "[2170]\ttrain-rmse:2.393468\n",
      "[2171]\ttrain-rmse:2.392967\n",
      "[2172]\ttrain-rmse:2.392468\n",
      "[2173]\ttrain-rmse:2.391971\n",
      "[2174]\ttrain-rmse:2.391461\n",
      "[2175]\ttrain-rmse:2.390960\n",
      "[2176]\ttrain-rmse:2.390458\n",
      "[2177]\ttrain-rmse:2.389952\n",
      "[2178]\ttrain-rmse:2.389461\n",
      "[2179]\ttrain-rmse:2.388967\n",
      "[2180]\ttrain-rmse:2.388460\n",
      "[2181]\ttrain-rmse:2.387969\n",
      "[2182]\ttrain-rmse:2.387472\n",
      "[2183]\ttrain-rmse:2.386976\n",
      "[2184]\ttrain-rmse:2.386476\n",
      "[2185]\ttrain-rmse:2.385980\n",
      "[2186]\ttrain-rmse:2.385469\n",
      "[2187]\ttrain-rmse:2.384976\n",
      "[2188]\ttrain-rmse:2.384485\n",
      "[2189]\ttrain-rmse:2.383987\n",
      "[2190]\ttrain-rmse:2.383496\n",
      "[2191]\ttrain-rmse:2.382986\n",
      "[2192]\ttrain-rmse:2.382483\n",
      "[2193]\ttrain-rmse:2.381978\n",
      "[2194]\ttrain-rmse:2.381483\n",
      "[2195]\ttrain-rmse:2.380985\n",
      "[2196]\ttrain-rmse:2.380495\n",
      "[2197]\ttrain-rmse:2.380010\n",
      "[2198]\ttrain-rmse:2.379523\n",
      "[2199]\ttrain-rmse:2.379015\n",
      "[2200]\ttrain-rmse:2.378533\n",
      "[2201]\ttrain-rmse:2.378037\n",
      "[2202]\ttrain-rmse:2.377543\n",
      "[2203]\ttrain-rmse:2.377059\n",
      "[2204]\ttrain-rmse:2.376563\n",
      "[2205]\ttrain-rmse:2.376061\n",
      "[2206]\ttrain-rmse:2.375566\n",
      "[2207]\ttrain-rmse:2.375078\n",
      "[2208]\ttrain-rmse:2.374594\n",
      "[2209]\ttrain-rmse:2.374098\n",
      "[2210]\ttrain-rmse:2.373602\n",
      "[2211]\ttrain-rmse:2.373115\n",
      "[2212]\ttrain-rmse:2.372636\n",
      "[2213]\ttrain-rmse:2.372146\n",
      "[2214]\ttrain-rmse:2.371672\n",
      "[2215]\ttrain-rmse:2.371189\n",
      "[2216]\ttrain-rmse:2.370704\n",
      "[2217]\ttrain-rmse:2.370217\n",
      "[2218]\ttrain-rmse:2.369724\n",
      "[2219]\ttrain-rmse:2.369246\n",
      "[2220]\ttrain-rmse:2.368754\n",
      "[2221]\ttrain-rmse:2.368273\n",
      "[2222]\ttrain-rmse:2.367779\n",
      "[2223]\ttrain-rmse:2.367295\n",
      "[2224]\ttrain-rmse:2.366806\n",
      "[2225]\ttrain-rmse:2.366329\n",
      "[2226]\ttrain-rmse:2.365842\n",
      "[2227]\ttrain-rmse:2.365346\n",
      "[2228]\ttrain-rmse:2.364861\n",
      "[2229]\ttrain-rmse:2.364385\n",
      "[2230]\ttrain-rmse:2.363903\n",
      "[2231]\ttrain-rmse:2.363427\n",
      "[2232]\ttrain-rmse:2.362928\n",
      "[2233]\ttrain-rmse:2.362453\n",
      "[2234]\ttrain-rmse:2.361973\n",
      "[2235]\ttrain-rmse:2.361493\n",
      "[2236]\ttrain-rmse:2.361009\n",
      "[2237]\ttrain-rmse:2.360525\n",
      "[2238]\ttrain-rmse:2.360043\n",
      "[2239]\ttrain-rmse:2.359569\n",
      "[2240]\ttrain-rmse:2.359082\n",
      "[2241]\ttrain-rmse:2.358590\n",
      "[2242]\ttrain-rmse:2.358106\n",
      "[2243]\ttrain-rmse:2.357618\n",
      "[2244]\ttrain-rmse:2.357139\n",
      "[2245]\ttrain-rmse:2.356660\n",
      "[2246]\ttrain-rmse:2.356178\n",
      "[2247]\ttrain-rmse:2.355702\n",
      "[2248]\ttrain-rmse:2.355221\n",
      "[2249]\ttrain-rmse:2.354741\n",
      "[2250]\ttrain-rmse:2.354267\n",
      "[2251]\ttrain-rmse:2.353784\n",
      "[2252]\ttrain-rmse:2.353309\n",
      "[2253]\ttrain-rmse:2.352838\n",
      "[2254]\ttrain-rmse:2.352371\n",
      "[2255]\ttrain-rmse:2.351898\n",
      "[2256]\ttrain-rmse:2.351423\n",
      "[2257]\ttrain-rmse:2.350963\n",
      "[2258]\ttrain-rmse:2.350477\n",
      "[2259]\ttrain-rmse:2.350000\n",
      "[2260]\ttrain-rmse:2.349524\n",
      "[2261]\ttrain-rmse:2.349048\n",
      "[2262]\ttrain-rmse:2.348593\n",
      "[2263]\ttrain-rmse:2.348133\n",
      "[2264]\ttrain-rmse:2.347664\n",
      "[2265]\ttrain-rmse:2.347189\n",
      "[2266]\ttrain-rmse:2.346714\n",
      "[2267]\ttrain-rmse:2.346246\n",
      "[2268]\ttrain-rmse:2.345781\n",
      "[2269]\ttrain-rmse:2.345322\n",
      "[2270]\ttrain-rmse:2.344846\n",
      "[2271]\ttrain-rmse:2.344380\n",
      "[2272]\ttrain-rmse:2.343905\n",
      "[2273]\ttrain-rmse:2.343435\n",
      "[2274]\ttrain-rmse:2.342964\n",
      "[2275]\ttrain-rmse:2.342505\n",
      "[2276]\ttrain-rmse:2.342041\n",
      "[2277]\ttrain-rmse:2.341569\n",
      "[2278]\ttrain-rmse:2.341097\n",
      "[2279]\ttrain-rmse:2.340632\n",
      "[2280]\ttrain-rmse:2.340166\n",
      "[2281]\ttrain-rmse:2.339678\n",
      "[2282]\ttrain-rmse:2.339225\n",
      "[2283]\ttrain-rmse:2.338764\n",
      "[2284]\ttrain-rmse:2.338300\n",
      "[2285]\ttrain-rmse:2.337839\n",
      "[2286]\ttrain-rmse:2.337372\n",
      "[2287]\ttrain-rmse:2.336906\n",
      "[2288]\ttrain-rmse:2.336444\n",
      "[2289]\ttrain-rmse:2.335976\n",
      "[2290]\ttrain-rmse:2.335520\n",
      "[2291]\ttrain-rmse:2.335069\n",
      "[2292]\ttrain-rmse:2.334619\n",
      "[2293]\ttrain-rmse:2.334158\n",
      "[2294]\ttrain-rmse:2.333705\n",
      "[2295]\ttrain-rmse:2.333252\n",
      "[2296]\ttrain-rmse:2.332791\n",
      "[2297]\ttrain-rmse:2.332332\n",
      "[2298]\ttrain-rmse:2.331872\n",
      "[2299]\ttrain-rmse:2.331404\n",
      "[2300]\ttrain-rmse:2.330939\n",
      "[2301]\ttrain-rmse:2.330489\n",
      "[2302]\ttrain-rmse:2.330025\n",
      "[2303]\ttrain-rmse:2.329577\n",
      "[2304]\ttrain-rmse:2.329128\n",
      "[2305]\ttrain-rmse:2.328681\n",
      "[2306]\ttrain-rmse:2.328212\n",
      "[2307]\ttrain-rmse:2.327761\n",
      "[2308]\ttrain-rmse:2.327292\n",
      "[2309]\ttrain-rmse:2.326839\n",
      "[2310]\ttrain-rmse:2.326384\n",
      "[2311]\ttrain-rmse:2.325931\n",
      "[2312]\ttrain-rmse:2.325478\n",
      "[2313]\ttrain-rmse:2.325031\n",
      "[2314]\ttrain-rmse:2.324579\n",
      "[2315]\ttrain-rmse:2.324115\n",
      "[2316]\ttrain-rmse:2.323652\n",
      "[2317]\ttrain-rmse:2.323195\n",
      "[2318]\ttrain-rmse:2.322737\n",
      "[2319]\ttrain-rmse:2.322281\n",
      "[2320]\ttrain-rmse:2.321839\n",
      "[2321]\ttrain-rmse:2.321386\n",
      "[2322]\ttrain-rmse:2.320936\n",
      "[2323]\ttrain-rmse:2.320487\n",
      "[2324]\ttrain-rmse:2.320028\n",
      "[2325]\ttrain-rmse:2.319576\n",
      "[2326]\ttrain-rmse:2.319144\n",
      "[2327]\ttrain-rmse:2.318687\n",
      "[2328]\ttrain-rmse:2.318231\n",
      "[2329]\ttrain-rmse:2.317788\n",
      "[2330]\ttrain-rmse:2.317342\n",
      "[2331]\ttrain-rmse:2.316877\n",
      "[2332]\ttrain-rmse:2.316432\n",
      "[2333]\ttrain-rmse:2.315989\n",
      "[2334]\ttrain-rmse:2.315542\n",
      "[2335]\ttrain-rmse:2.315096\n",
      "[2336]\ttrain-rmse:2.314652\n",
      "[2337]\ttrain-rmse:2.314206\n",
      "[2338]\ttrain-rmse:2.313757\n",
      "[2339]\ttrain-rmse:2.313322\n",
      "[2340]\ttrain-rmse:2.312876\n",
      "[2341]\ttrain-rmse:2.312431\n",
      "[2342]\ttrain-rmse:2.311980\n",
      "[2343]\ttrain-rmse:2.311537\n",
      "[2344]\ttrain-rmse:2.311095\n",
      "[2345]\ttrain-rmse:2.310658\n",
      "[2346]\ttrain-rmse:2.310211\n",
      "[2347]\ttrain-rmse:2.309758\n",
      "[2348]\ttrain-rmse:2.309312\n",
      "[2349]\ttrain-rmse:2.308875\n",
      "[2350]\ttrain-rmse:2.308443\n",
      "[2351]\ttrain-rmse:2.307997\n",
      "[2352]\ttrain-rmse:2.307559\n",
      "[2353]\ttrain-rmse:2.307117\n",
      "[2354]\ttrain-rmse:2.306675\n",
      "[2355]\ttrain-rmse:2.306235\n",
      "[2356]\ttrain-rmse:2.305807\n",
      "[2357]\ttrain-rmse:2.305358\n",
      "[2358]\ttrain-rmse:2.304917\n",
      "[2359]\ttrain-rmse:2.304475\n",
      "[2360]\ttrain-rmse:2.304040\n",
      "[2361]\ttrain-rmse:2.303609\n",
      "[2362]\ttrain-rmse:2.303171\n",
      "[2363]\ttrain-rmse:2.302725\n",
      "[2364]\ttrain-rmse:2.302281\n",
      "[2365]\ttrain-rmse:2.301844\n",
      "[2366]\ttrain-rmse:2.301403\n",
      "[2367]\ttrain-rmse:2.300970\n",
      "[2368]\ttrain-rmse:2.300545\n",
      "[2369]\ttrain-rmse:2.300102\n",
      "[2370]\ttrain-rmse:2.299671\n",
      "[2371]\ttrain-rmse:2.299227\n",
      "[2372]\ttrain-rmse:2.298798\n",
      "[2373]\ttrain-rmse:2.298365\n",
      "[2374]\ttrain-rmse:2.297936\n",
      "[2375]\ttrain-rmse:2.297508\n",
      "[2376]\ttrain-rmse:2.297078\n",
      "[2377]\ttrain-rmse:2.296648\n",
      "[2378]\ttrain-rmse:2.296216\n",
      "[2379]\ttrain-rmse:2.295777\n",
      "[2380]\ttrain-rmse:2.295352\n",
      "[2381]\ttrain-rmse:2.294928\n",
      "[2382]\ttrain-rmse:2.294502\n",
      "[2383]\ttrain-rmse:2.294073\n",
      "[2384]\ttrain-rmse:2.293630\n",
      "[2385]\ttrain-rmse:2.293197\n",
      "[2386]\ttrain-rmse:2.292776\n",
      "[2387]\ttrain-rmse:2.292340\n",
      "[2388]\ttrain-rmse:2.291909\n",
      "[2389]\ttrain-rmse:2.291464\n",
      "[2390]\ttrain-rmse:2.291030\n",
      "[2391]\ttrain-rmse:2.290584\n",
      "[2392]\ttrain-rmse:2.290153\n",
      "[2393]\ttrain-rmse:2.289712\n",
      "[2394]\ttrain-rmse:2.289284\n",
      "[2395]\ttrain-rmse:2.288864\n",
      "[2396]\ttrain-rmse:2.288433\n",
      "[2397]\ttrain-rmse:2.288007\n",
      "[2398]\ttrain-rmse:2.287582\n",
      "[2399]\ttrain-rmse:2.287152\n",
      "[2400]\ttrain-rmse:2.286712\n",
      "[2401]\ttrain-rmse:2.286293\n",
      "[2402]\ttrain-rmse:2.285873\n",
      "[2403]\ttrain-rmse:2.285450\n",
      "[2404]\ttrain-rmse:2.285016\n",
      "[2405]\ttrain-rmse:2.284592\n",
      "[2406]\ttrain-rmse:2.284164\n",
      "[2407]\ttrain-rmse:2.283733\n",
      "[2408]\ttrain-rmse:2.283297\n",
      "[2409]\ttrain-rmse:2.282880\n",
      "[2410]\ttrain-rmse:2.282452\n",
      "[2411]\ttrain-rmse:2.282035\n",
      "[2412]\ttrain-rmse:2.281620\n",
      "[2413]\ttrain-rmse:2.281199\n",
      "[2414]\ttrain-rmse:2.280768\n",
      "[2415]\ttrain-rmse:2.280338\n",
      "[2416]\ttrain-rmse:2.279923\n",
      "[2417]\ttrain-rmse:2.279516\n",
      "[2418]\ttrain-rmse:2.279082\n",
      "[2419]\ttrain-rmse:2.278651\n",
      "[2420]\ttrain-rmse:2.278233\n",
      "[2421]\ttrain-rmse:2.277807\n",
      "[2422]\ttrain-rmse:2.277381\n",
      "[2423]\ttrain-rmse:2.276962\n",
      "[2424]\ttrain-rmse:2.276539\n",
      "[2425]\ttrain-rmse:2.276108\n",
      "[2426]\ttrain-rmse:2.275692\n",
      "[2427]\ttrain-rmse:2.275284\n",
      "[2428]\ttrain-rmse:2.274863\n",
      "[2429]\ttrain-rmse:2.274443\n",
      "[2430]\ttrain-rmse:2.274035\n",
      "[2431]\ttrain-rmse:2.273623\n",
      "[2432]\ttrain-rmse:2.273210\n",
      "[2433]\ttrain-rmse:2.272794\n",
      "[2434]\ttrain-rmse:2.272367\n",
      "[2435]\ttrain-rmse:2.271959\n",
      "[2436]\ttrain-rmse:2.271547\n",
      "[2437]\ttrain-rmse:2.271126\n",
      "[2438]\ttrain-rmse:2.270709\n",
      "[2439]\ttrain-rmse:2.270301\n",
      "[2440]\ttrain-rmse:2.269887\n",
      "[2441]\ttrain-rmse:2.269476\n",
      "[2442]\ttrain-rmse:2.269067\n",
      "[2443]\ttrain-rmse:2.268654\n",
      "[2444]\ttrain-rmse:2.268250\n",
      "[2445]\ttrain-rmse:2.267847\n",
      "[2446]\ttrain-rmse:2.267427\n",
      "[2447]\ttrain-rmse:2.267022\n",
      "[2448]\ttrain-rmse:2.266611\n",
      "[2449]\ttrain-rmse:2.266206\n",
      "[2450]\ttrain-rmse:2.265775\n",
      "[2451]\ttrain-rmse:2.265367\n",
      "[2452]\ttrain-rmse:2.264964\n",
      "[2453]\ttrain-rmse:2.264551\n",
      "[2454]\ttrain-rmse:2.264135\n",
      "[2455]\ttrain-rmse:2.263716\n",
      "[2456]\ttrain-rmse:2.263301\n",
      "[2457]\ttrain-rmse:2.262886\n",
      "[2458]\ttrain-rmse:2.262480\n",
      "[2459]\ttrain-rmse:2.262078\n",
      "[2460]\ttrain-rmse:2.261674\n",
      "[2461]\ttrain-rmse:2.261269\n",
      "[2462]\ttrain-rmse:2.260860\n",
      "[2463]\ttrain-rmse:2.260459\n",
      "[2464]\ttrain-rmse:2.260041\n",
      "[2465]\ttrain-rmse:2.259645\n",
      "[2466]\ttrain-rmse:2.259234\n",
      "[2467]\ttrain-rmse:2.258830\n",
      "[2468]\ttrain-rmse:2.258422\n",
      "[2469]\ttrain-rmse:2.258015\n",
      "[2470]\ttrain-rmse:2.257611\n",
      "[2471]\ttrain-rmse:2.257209\n",
      "[2472]\ttrain-rmse:2.256794\n",
      "[2473]\ttrain-rmse:2.256385\n",
      "[2474]\ttrain-rmse:2.255988\n",
      "[2475]\ttrain-rmse:2.255581\n",
      "[2476]\ttrain-rmse:2.255180\n",
      "[2477]\ttrain-rmse:2.254782\n",
      "[2478]\ttrain-rmse:2.254382\n",
      "[2479]\ttrain-rmse:2.253980\n",
      "[2480]\ttrain-rmse:2.253578\n",
      "[2481]\ttrain-rmse:2.253178\n",
      "[2482]\ttrain-rmse:2.252773\n",
      "[2483]\ttrain-rmse:2.252386\n",
      "[2484]\ttrain-rmse:2.251981\n",
      "[2485]\ttrain-rmse:2.251588\n",
      "[2486]\ttrain-rmse:2.251194\n",
      "[2487]\ttrain-rmse:2.250781\n",
      "[2488]\ttrain-rmse:2.250386\n",
      "[2489]\ttrain-rmse:2.249991\n",
      "[2490]\ttrain-rmse:2.249581\n",
      "[2491]\ttrain-rmse:2.249187\n",
      "[2492]\ttrain-rmse:2.248785\n",
      "[2493]\ttrain-rmse:2.248386\n",
      "[2494]\ttrain-rmse:2.247983\n",
      "[2495]\ttrain-rmse:2.247579\n",
      "[2496]\ttrain-rmse:2.247187\n",
      "[2497]\ttrain-rmse:2.246787\n",
      "[2498]\ttrain-rmse:2.246394\n",
      "[2499]\ttrain-rmse:2.245992\n",
      "[2500]\ttrain-rmse:2.245604\n",
      "[2501]\ttrain-rmse:2.245202\n",
      "[2502]\ttrain-rmse:2.244800\n",
      "[2503]\ttrain-rmse:2.244407\n",
      "[2504]\ttrain-rmse:2.244019\n",
      "[2505]\ttrain-rmse:2.243614\n",
      "[2506]\ttrain-rmse:2.243224\n",
      "[2507]\ttrain-rmse:2.242831\n",
      "[2508]\ttrain-rmse:2.242424\n",
      "[2509]\ttrain-rmse:2.242026\n",
      "[2510]\ttrain-rmse:2.241627\n",
      "[2511]\ttrain-rmse:2.241241\n",
      "[2512]\ttrain-rmse:2.240855\n",
      "[2513]\ttrain-rmse:2.240464\n",
      "[2514]\ttrain-rmse:2.240077\n",
      "[2515]\ttrain-rmse:2.239682\n",
      "[2516]\ttrain-rmse:2.239297\n",
      "[2517]\ttrain-rmse:2.238898\n",
      "[2518]\ttrain-rmse:2.238507\n",
      "[2519]\ttrain-rmse:2.238100\n",
      "[2520]\ttrain-rmse:2.237710\n",
      "[2521]\ttrain-rmse:2.237317\n",
      "[2522]\ttrain-rmse:2.236937\n",
      "[2523]\ttrain-rmse:2.236559\n",
      "[2524]\ttrain-rmse:2.236164\n",
      "[2525]\ttrain-rmse:2.235768\n",
      "[2526]\ttrain-rmse:2.235386\n",
      "[2527]\ttrain-rmse:2.235011\n",
      "[2528]\ttrain-rmse:2.234623\n",
      "[2529]\ttrain-rmse:2.234228\n",
      "[2530]\ttrain-rmse:2.233839\n",
      "[2531]\ttrain-rmse:2.233462\n",
      "[2532]\ttrain-rmse:2.233074\n",
      "[2533]\ttrain-rmse:2.232675\n",
      "[2534]\ttrain-rmse:2.232290\n",
      "[2535]\ttrain-rmse:2.231907\n",
      "[2536]\ttrain-rmse:2.231516\n",
      "[2537]\ttrain-rmse:2.231136\n",
      "[2538]\ttrain-rmse:2.230757\n",
      "[2539]\ttrain-rmse:2.230376\n",
      "[2540]\ttrain-rmse:2.229985\n",
      "[2541]\ttrain-rmse:2.229606\n",
      "[2542]\ttrain-rmse:2.229214\n",
      "[2543]\ttrain-rmse:2.228834\n",
      "[2544]\ttrain-rmse:2.228448\n",
      "[2545]\ttrain-rmse:2.228068\n",
      "[2546]\ttrain-rmse:2.227679\n",
      "[2547]\ttrain-rmse:2.227304\n",
      "[2548]\ttrain-rmse:2.226919\n",
      "[2549]\ttrain-rmse:2.226537\n",
      "[2550]\ttrain-rmse:2.226168\n",
      "[2551]\ttrain-rmse:2.225775\n",
      "[2552]\ttrain-rmse:2.225396\n",
      "[2553]\ttrain-rmse:2.225013\n",
      "[2554]\ttrain-rmse:2.224638\n",
      "[2555]\ttrain-rmse:2.224256\n",
      "[2556]\ttrain-rmse:2.223874\n",
      "[2557]\ttrain-rmse:2.223490\n",
      "[2558]\ttrain-rmse:2.223104\n",
      "[2559]\ttrain-rmse:2.222721\n",
      "[2560]\ttrain-rmse:2.222354\n",
      "[2561]\ttrain-rmse:2.221969\n",
      "[2562]\ttrain-rmse:2.221592\n",
      "[2563]\ttrain-rmse:2.221211\n",
      "[2564]\ttrain-rmse:2.220834\n",
      "[2565]\ttrain-rmse:2.220454\n",
      "[2566]\ttrain-rmse:2.220079\n",
      "[2567]\ttrain-rmse:2.219695\n",
      "[2568]\ttrain-rmse:2.219315\n",
      "[2569]\ttrain-rmse:2.218946\n",
      "[2570]\ttrain-rmse:2.218573\n",
      "[2571]\ttrain-rmse:2.218197\n",
      "[2572]\ttrain-rmse:2.217828\n",
      "[2573]\ttrain-rmse:2.217454\n",
      "[2574]\ttrain-rmse:2.217087\n",
      "[2575]\ttrain-rmse:2.216698\n",
      "[2576]\ttrain-rmse:2.216325\n",
      "[2577]\ttrain-rmse:2.215938\n",
      "[2578]\ttrain-rmse:2.215567\n",
      "[2579]\ttrain-rmse:2.215193\n",
      "[2580]\ttrain-rmse:2.214828\n",
      "[2581]\ttrain-rmse:2.214449\n",
      "[2582]\ttrain-rmse:2.214077\n",
      "[2583]\ttrain-rmse:2.213718\n",
      "[2584]\ttrain-rmse:2.213343\n",
      "[2585]\ttrain-rmse:2.212962\n",
      "[2586]\ttrain-rmse:2.212595\n",
      "[2587]\ttrain-rmse:2.212217\n",
      "[2588]\ttrain-rmse:2.211846\n",
      "[2589]\ttrain-rmse:2.211469\n",
      "[2590]\ttrain-rmse:2.211104\n",
      "[2591]\ttrain-rmse:2.210737\n",
      "[2592]\ttrain-rmse:2.210358\n",
      "[2593]\ttrain-rmse:2.209991\n",
      "[2594]\ttrain-rmse:2.209625\n",
      "[2595]\ttrain-rmse:2.209245\n",
      "[2596]\ttrain-rmse:2.208875\n",
      "[2597]\ttrain-rmse:2.208503\n",
      "[2598]\ttrain-rmse:2.208125\n",
      "[2599]\ttrain-rmse:2.207762\n",
      "[2600]\ttrain-rmse:2.207385\n",
      "[2601]\ttrain-rmse:2.207012\n",
      "[2602]\ttrain-rmse:2.206658\n",
      "[2603]\ttrain-rmse:2.206292\n",
      "[2604]\ttrain-rmse:2.205922\n",
      "[2605]\ttrain-rmse:2.205556\n",
      "[2606]\ttrain-rmse:2.205191\n",
      "[2607]\ttrain-rmse:2.204829\n",
      "[2608]\ttrain-rmse:2.204453\n",
      "[2609]\ttrain-rmse:2.204091\n",
      "[2610]\ttrain-rmse:2.203729\n",
      "[2611]\ttrain-rmse:2.203373\n",
      "[2612]\ttrain-rmse:2.203012\n",
      "[2613]\ttrain-rmse:2.202647\n",
      "[2614]\ttrain-rmse:2.202270\n",
      "[2615]\ttrain-rmse:2.201911\n",
      "[2616]\ttrain-rmse:2.201544\n",
      "[2617]\ttrain-rmse:2.201182\n",
      "[2618]\ttrain-rmse:2.200812\n",
      "[2619]\ttrain-rmse:2.200461\n",
      "[2620]\ttrain-rmse:2.200098\n",
      "[2621]\ttrain-rmse:2.199731\n",
      "[2622]\ttrain-rmse:2.199375\n",
      "[2623]\ttrain-rmse:2.199005\n",
      "[2624]\ttrain-rmse:2.198643\n",
      "[2625]\ttrain-rmse:2.198278\n",
      "[2626]\ttrain-rmse:2.197926\n",
      "[2627]\ttrain-rmse:2.197577\n",
      "[2628]\ttrain-rmse:2.197215\n",
      "[2629]\ttrain-rmse:2.196847\n",
      "[2630]\ttrain-rmse:2.196489\n",
      "[2631]\ttrain-rmse:2.196134\n",
      "[2632]\ttrain-rmse:2.195774\n",
      "[2633]\ttrain-rmse:2.195408\n",
      "[2634]\ttrain-rmse:2.195060\n",
      "[2635]\ttrain-rmse:2.194702\n",
      "[2636]\ttrain-rmse:2.194346\n",
      "[2637]\ttrain-rmse:2.193986\n",
      "[2638]\ttrain-rmse:2.193642\n",
      "[2639]\ttrain-rmse:2.193283\n",
      "[2640]\ttrain-rmse:2.192915\n",
      "[2641]\ttrain-rmse:2.192551\n",
      "[2642]\ttrain-rmse:2.192186\n",
      "[2643]\ttrain-rmse:2.191827\n",
      "[2644]\ttrain-rmse:2.191479\n",
      "[2645]\ttrain-rmse:2.191131\n",
      "[2646]\ttrain-rmse:2.190786\n",
      "[2647]\ttrain-rmse:2.190442\n",
      "[2648]\ttrain-rmse:2.190081\n",
      "[2649]\ttrain-rmse:2.189725\n",
      "[2650]\ttrain-rmse:2.189368\n",
      "[2651]\ttrain-rmse:2.189017\n",
      "[2652]\ttrain-rmse:2.188660\n",
      "[2653]\ttrain-rmse:2.188302\n",
      "[2654]\ttrain-rmse:2.187933\n",
      "[2655]\ttrain-rmse:2.187587\n",
      "[2656]\ttrain-rmse:2.187238\n",
      "[2657]\ttrain-rmse:2.186889\n",
      "[2658]\ttrain-rmse:2.186539\n",
      "[2659]\ttrain-rmse:2.186186\n",
      "[2660]\ttrain-rmse:2.185846\n",
      "[2661]\ttrain-rmse:2.185499\n",
      "[2662]\ttrain-rmse:2.185151\n",
      "[2663]\ttrain-rmse:2.184805\n",
      "[2664]\ttrain-rmse:2.184461\n",
      "[2665]\ttrain-rmse:2.184115\n",
      "[2666]\ttrain-rmse:2.183764\n",
      "[2667]\ttrain-rmse:2.183427\n",
      "[2668]\ttrain-rmse:2.183064\n",
      "[2669]\ttrain-rmse:2.182722\n",
      "[2670]\ttrain-rmse:2.182371\n",
      "[2671]\ttrain-rmse:2.182027\n",
      "[2672]\ttrain-rmse:2.181678\n",
      "[2673]\ttrain-rmse:2.181333\n",
      "[2674]\ttrain-rmse:2.180980\n",
      "[2675]\ttrain-rmse:2.180628\n",
      "[2676]\ttrain-rmse:2.180276\n",
      "[2677]\ttrain-rmse:2.179929\n",
      "[2678]\ttrain-rmse:2.179581\n",
      "[2679]\ttrain-rmse:2.179241\n",
      "[2680]\ttrain-rmse:2.178896\n",
      "[2681]\ttrain-rmse:2.178549\n",
      "[2682]\ttrain-rmse:2.178209\n",
      "[2683]\ttrain-rmse:2.177865\n",
      "[2684]\ttrain-rmse:2.177519\n",
      "[2685]\ttrain-rmse:2.177175\n",
      "[2686]\ttrain-rmse:2.176833\n",
      "[2687]\ttrain-rmse:2.176490\n",
      "[2688]\ttrain-rmse:2.176154\n",
      "[2689]\ttrain-rmse:2.175808\n",
      "[2690]\ttrain-rmse:2.175468\n",
      "[2691]\ttrain-rmse:2.175115\n",
      "[2692]\ttrain-rmse:2.174785\n",
      "[2693]\ttrain-rmse:2.174434\n",
      "[2694]\ttrain-rmse:2.174105\n",
      "[2695]\ttrain-rmse:2.173768\n",
      "[2696]\ttrain-rmse:2.173425\n",
      "[2697]\ttrain-rmse:2.173083\n",
      "[2698]\ttrain-rmse:2.172737\n",
      "[2699]\ttrain-rmse:2.172400\n",
      "[2700]\ttrain-rmse:2.172044\n",
      "[2701]\ttrain-rmse:2.171710\n",
      "[2702]\ttrain-rmse:2.171360\n",
      "[2703]\ttrain-rmse:2.171024\n",
      "[2704]\ttrain-rmse:2.170692\n",
      "[2705]\ttrain-rmse:2.170346\n",
      "[2706]\ttrain-rmse:2.170014\n",
      "[2707]\ttrain-rmse:2.169682\n",
      "[2708]\ttrain-rmse:2.169341\n",
      "[2709]\ttrain-rmse:2.168993\n",
      "[2710]\ttrain-rmse:2.168657\n",
      "[2711]\ttrain-rmse:2.168330\n",
      "[2712]\ttrain-rmse:2.167992\n",
      "[2713]\ttrain-rmse:2.167649\n",
      "[2714]\ttrain-rmse:2.167305\n",
      "[2715]\ttrain-rmse:2.166969\n",
      "[2716]\ttrain-rmse:2.166615\n",
      "[2717]\ttrain-rmse:2.166287\n",
      "[2718]\ttrain-rmse:2.165951\n",
      "[2719]\ttrain-rmse:2.165607\n",
      "[2720]\ttrain-rmse:2.165276\n",
      "[2721]\ttrain-rmse:2.164934\n",
      "[2722]\ttrain-rmse:2.164589\n",
      "[2723]\ttrain-rmse:2.164241\n",
      "[2724]\ttrain-rmse:2.163903\n",
      "[2725]\ttrain-rmse:2.163573\n",
      "[2726]\ttrain-rmse:2.163240\n",
      "[2727]\ttrain-rmse:2.162898\n",
      "[2728]\ttrain-rmse:2.162560\n",
      "[2729]\ttrain-rmse:2.162221\n",
      "[2730]\ttrain-rmse:2.161883\n",
      "[2731]\ttrain-rmse:2.161554\n",
      "[2732]\ttrain-rmse:2.161223\n",
      "[2733]\ttrain-rmse:2.160879\n",
      "[2734]\ttrain-rmse:2.160553\n",
      "[2735]\ttrain-rmse:2.160224\n",
      "[2736]\ttrain-rmse:2.159892\n",
      "[2737]\ttrain-rmse:2.159567\n",
      "[2738]\ttrain-rmse:2.159229\n",
      "[2739]\ttrain-rmse:2.158905\n",
      "[2740]\ttrain-rmse:2.158568\n",
      "[2741]\ttrain-rmse:2.158249\n",
      "[2742]\ttrain-rmse:2.157911\n",
      "[2743]\ttrain-rmse:2.157587\n",
      "[2744]\ttrain-rmse:2.157255\n",
      "[2745]\ttrain-rmse:2.156916\n",
      "[2746]\ttrain-rmse:2.156588\n",
      "[2747]\ttrain-rmse:2.156260\n",
      "[2748]\ttrain-rmse:2.155939\n",
      "[2749]\ttrain-rmse:2.155613\n",
      "[2750]\ttrain-rmse:2.155290\n",
      "[2751]\ttrain-rmse:2.154956\n",
      "[2752]\ttrain-rmse:2.154625\n",
      "[2753]\ttrain-rmse:2.154307\n",
      "[2754]\ttrain-rmse:2.153975\n",
      "[2755]\ttrain-rmse:2.153645\n",
      "[2756]\ttrain-rmse:2.153319\n",
      "[2757]\ttrain-rmse:2.153003\n",
      "[2758]\ttrain-rmse:2.152670\n",
      "[2759]\ttrain-rmse:2.152343\n",
      "[2760]\ttrain-rmse:2.152017\n",
      "[2761]\ttrain-rmse:2.151698\n",
      "[2762]\ttrain-rmse:2.151378\n",
      "[2763]\ttrain-rmse:2.151050\n",
      "[2764]\ttrain-rmse:2.150719\n",
      "[2765]\ttrain-rmse:2.150379\n",
      "[2766]\ttrain-rmse:2.150057\n",
      "[2767]\ttrain-rmse:2.149737\n",
      "[2768]\ttrain-rmse:2.149418\n",
      "[2769]\ttrain-rmse:2.149094\n",
      "[2770]\ttrain-rmse:2.148769\n",
      "[2771]\ttrain-rmse:2.148444\n",
      "[2772]\ttrain-rmse:2.148114\n",
      "[2773]\ttrain-rmse:2.147796\n",
      "[2774]\ttrain-rmse:2.147480\n",
      "[2775]\ttrain-rmse:2.147149\n",
      "[2776]\ttrain-rmse:2.146829\n",
      "[2777]\ttrain-rmse:2.146519\n",
      "[2778]\ttrain-rmse:2.146177\n",
      "[2779]\ttrain-rmse:2.145855\n",
      "[2780]\ttrain-rmse:2.145533\n",
      "[2781]\ttrain-rmse:2.145206\n",
      "[2782]\ttrain-rmse:2.144881\n",
      "[2783]\ttrain-rmse:2.144550\n",
      "[2784]\ttrain-rmse:2.144231\n",
      "[2785]\ttrain-rmse:2.143921\n",
      "[2786]\ttrain-rmse:2.143601\n",
      "[2787]\ttrain-rmse:2.143286\n",
      "[2788]\ttrain-rmse:2.142963\n",
      "[2789]\ttrain-rmse:2.142639\n",
      "[2790]\ttrain-rmse:2.142320\n",
      "[2791]\ttrain-rmse:2.142000\n",
      "[2792]\ttrain-rmse:2.141676\n",
      "[2793]\ttrain-rmse:2.141369\n",
      "[2794]\ttrain-rmse:2.141065\n",
      "[2795]\ttrain-rmse:2.140755\n",
      "[2796]\ttrain-rmse:2.140439\n",
      "[2797]\ttrain-rmse:2.140122\n",
      "[2798]\ttrain-rmse:2.139809\n",
      "[2799]\ttrain-rmse:2.139491\n",
      "[2800]\ttrain-rmse:2.139174\n",
      "[2801]\ttrain-rmse:2.138863\n",
      "[2802]\ttrain-rmse:2.138550\n",
      "[2803]\ttrain-rmse:2.138212\n",
      "[2804]\ttrain-rmse:2.137885\n",
      "[2805]\ttrain-rmse:2.137578\n",
      "[2806]\ttrain-rmse:2.137255\n",
      "[2807]\ttrain-rmse:2.136942\n",
      "[2808]\ttrain-rmse:2.136628\n",
      "[2809]\ttrain-rmse:2.136316\n",
      "[2810]\ttrain-rmse:2.136018\n",
      "[2811]\ttrain-rmse:2.135694\n",
      "[2812]\ttrain-rmse:2.135372\n",
      "[2813]\ttrain-rmse:2.135060\n",
      "[2814]\ttrain-rmse:2.134761\n",
      "[2815]\ttrain-rmse:2.134436\n",
      "[2816]\ttrain-rmse:2.134127\n",
      "[2817]\ttrain-rmse:2.133818\n",
      "[2818]\ttrain-rmse:2.133507\n",
      "[2819]\ttrain-rmse:2.133200\n",
      "[2820]\ttrain-rmse:2.132889\n",
      "[2821]\ttrain-rmse:2.132580\n",
      "[2822]\ttrain-rmse:2.132270\n",
      "[2823]\ttrain-rmse:2.131961\n",
      "[2824]\ttrain-rmse:2.131652\n",
      "[2825]\ttrain-rmse:2.131347\n",
      "[2826]\ttrain-rmse:2.131026\n",
      "[2827]\ttrain-rmse:2.130724\n",
      "[2828]\ttrain-rmse:2.130409\n",
      "[2829]\ttrain-rmse:2.130103\n",
      "[2830]\ttrain-rmse:2.129790\n",
      "[2831]\ttrain-rmse:2.129478\n",
      "[2832]\ttrain-rmse:2.129176\n",
      "[2833]\ttrain-rmse:2.128877\n",
      "[2834]\ttrain-rmse:2.128577\n",
      "[2835]\ttrain-rmse:2.128266\n",
      "[2836]\ttrain-rmse:2.127961\n",
      "[2837]\ttrain-rmse:2.127667\n",
      "[2838]\ttrain-rmse:2.127357\n",
      "[2839]\ttrain-rmse:2.127046\n",
      "[2840]\ttrain-rmse:2.126738\n",
      "[2841]\ttrain-rmse:2.126420\n",
      "[2842]\ttrain-rmse:2.126103\n",
      "[2843]\ttrain-rmse:2.125808\n",
      "[2844]\ttrain-rmse:2.125500\n",
      "[2845]\ttrain-rmse:2.125189\n",
      "[2846]\ttrain-rmse:2.124866\n",
      "[2847]\ttrain-rmse:2.124563\n",
      "[2848]\ttrain-rmse:2.124265\n",
      "[2849]\ttrain-rmse:2.123955\n",
      "[2850]\ttrain-rmse:2.123655\n",
      "[2851]\ttrain-rmse:2.123331\n",
      "[2852]\ttrain-rmse:2.123036\n",
      "[2853]\ttrain-rmse:2.122740\n",
      "[2854]\ttrain-rmse:2.122437\n",
      "[2855]\ttrain-rmse:2.122127\n",
      "[2856]\ttrain-rmse:2.121825\n",
      "[2857]\ttrain-rmse:2.121513\n",
      "[2858]\ttrain-rmse:2.121203\n",
      "[2859]\ttrain-rmse:2.120906\n",
      "[2860]\ttrain-rmse:2.120600\n",
      "[2861]\ttrain-rmse:2.120296\n",
      "[2862]\ttrain-rmse:2.119991\n",
      "[2863]\ttrain-rmse:2.119688\n",
      "[2864]\ttrain-rmse:2.119379\n",
      "[2865]\ttrain-rmse:2.119088\n",
      "[2866]\ttrain-rmse:2.118786\n",
      "[2867]\ttrain-rmse:2.118492\n",
      "[2868]\ttrain-rmse:2.118198\n",
      "[2869]\ttrain-rmse:2.117899\n",
      "[2870]\ttrain-rmse:2.117589\n",
      "[2871]\ttrain-rmse:2.117285\n",
      "[2872]\ttrain-rmse:2.116981\n",
      "[2873]\ttrain-rmse:2.116681\n",
      "[2874]\ttrain-rmse:2.116385\n",
      "[2875]\ttrain-rmse:2.116086\n",
      "[2876]\ttrain-rmse:2.115793\n",
      "[2877]\ttrain-rmse:2.115491\n",
      "[2878]\ttrain-rmse:2.115193\n",
      "[2879]\ttrain-rmse:2.114878\n",
      "[2880]\ttrain-rmse:2.114586\n",
      "[2881]\ttrain-rmse:2.114278\n",
      "[2882]\ttrain-rmse:2.113981\n",
      "[2883]\ttrain-rmse:2.113683\n",
      "[2884]\ttrain-rmse:2.113391\n",
      "[2885]\ttrain-rmse:2.113091\n",
      "[2886]\ttrain-rmse:2.112790\n",
      "[2887]\ttrain-rmse:2.112488\n",
      "[2888]\ttrain-rmse:2.112190\n",
      "[2889]\ttrain-rmse:2.111905\n",
      "[2890]\ttrain-rmse:2.111604\n",
      "[2891]\ttrain-rmse:2.111308\n",
      "[2892]\ttrain-rmse:2.111003\n",
      "[2893]\ttrain-rmse:2.110706\n",
      "[2894]\ttrain-rmse:2.110393\n",
      "[2895]\ttrain-rmse:2.110109\n",
      "[2896]\ttrain-rmse:2.109819\n",
      "[2897]\ttrain-rmse:2.109521\n",
      "[2898]\ttrain-rmse:2.109240\n",
      "[2899]\ttrain-rmse:2.108941\n",
      "[2900]\ttrain-rmse:2.108640\n",
      "[2901]\ttrain-rmse:2.108357\n",
      "[2902]\ttrain-rmse:2.108055\n",
      "[2903]\ttrain-rmse:2.107750\n",
      "[2904]\ttrain-rmse:2.107466\n",
      "[2905]\ttrain-rmse:2.107178\n",
      "[2906]\ttrain-rmse:2.106888\n",
      "[2907]\ttrain-rmse:2.106600\n",
      "[2908]\ttrain-rmse:2.106302\n",
      "[2909]\ttrain-rmse:2.106015\n",
      "[2910]\ttrain-rmse:2.105723\n",
      "[2911]\ttrain-rmse:2.105427\n",
      "[2912]\ttrain-rmse:2.105140\n",
      "[2913]\ttrain-rmse:2.104831\n",
      "[2914]\ttrain-rmse:2.104542\n",
      "[2915]\ttrain-rmse:2.104253\n",
      "[2916]\ttrain-rmse:2.103964\n",
      "[2917]\ttrain-rmse:2.103681\n",
      "[2918]\ttrain-rmse:2.103380\n",
      "[2919]\ttrain-rmse:2.103093\n",
      "[2920]\ttrain-rmse:2.102809\n",
      "[2921]\ttrain-rmse:2.102512\n",
      "[2922]\ttrain-rmse:2.102225\n",
      "[2923]\ttrain-rmse:2.101935\n",
      "[2924]\ttrain-rmse:2.101653\n",
      "[2925]\ttrain-rmse:2.101368\n",
      "[2926]\ttrain-rmse:2.101066\n",
      "[2927]\ttrain-rmse:2.100789\n",
      "[2928]\ttrain-rmse:2.100501\n",
      "[2929]\ttrain-rmse:2.100217\n",
      "[2930]\ttrain-rmse:2.099935\n",
      "[2931]\ttrain-rmse:2.099649\n",
      "[2932]\ttrain-rmse:2.099370\n",
      "[2933]\ttrain-rmse:2.099082\n",
      "[2934]\ttrain-rmse:2.098793\n",
      "[2935]\ttrain-rmse:2.098516\n",
      "[2936]\ttrain-rmse:2.098222\n",
      "[2937]\ttrain-rmse:2.097941\n",
      "[2938]\ttrain-rmse:2.097650\n",
      "[2939]\ttrain-rmse:2.097360\n",
      "[2940]\ttrain-rmse:2.097076\n",
      "[2941]\ttrain-rmse:2.096801\n",
      "[2942]\ttrain-rmse:2.096522\n",
      "[2943]\ttrain-rmse:2.096237\n",
      "[2944]\ttrain-rmse:2.095951\n",
      "[2945]\ttrain-rmse:2.095671\n",
      "[2946]\ttrain-rmse:2.095385\n",
      "[2947]\ttrain-rmse:2.095102\n",
      "[2948]\ttrain-rmse:2.094816\n",
      "[2949]\ttrain-rmse:2.094537\n",
      "[2950]\ttrain-rmse:2.094254\n",
      "[2951]\ttrain-rmse:2.093972\n",
      "[2952]\ttrain-rmse:2.093693\n",
      "[2953]\ttrain-rmse:2.093418\n",
      "[2954]\ttrain-rmse:2.093138\n",
      "[2955]\ttrain-rmse:2.092864\n",
      "[2956]\ttrain-rmse:2.092578\n",
      "[2957]\ttrain-rmse:2.092291\n",
      "[2958]\ttrain-rmse:2.091997\n",
      "[2959]\ttrain-rmse:2.091711\n",
      "[2960]\ttrain-rmse:2.091428\n",
      "[2961]\ttrain-rmse:2.091153\n",
      "[2962]\ttrain-rmse:2.090870\n",
      "[2963]\ttrain-rmse:2.090588\n",
      "[2964]\ttrain-rmse:2.090312\n",
      "[2965]\ttrain-rmse:2.090022\n",
      "[2966]\ttrain-rmse:2.089742\n",
      "[2967]\ttrain-rmse:2.089475\n",
      "[2968]\ttrain-rmse:2.089199\n",
      "[2969]\ttrain-rmse:2.088919\n",
      "[2970]\ttrain-rmse:2.088640\n",
      "[2971]\ttrain-rmse:2.088354\n",
      "[2972]\ttrain-rmse:2.088086\n",
      "[2973]\ttrain-rmse:2.087812\n",
      "[2974]\ttrain-rmse:2.087530\n",
      "[2975]\ttrain-rmse:2.087251\n",
      "[2976]\ttrain-rmse:2.086977\n",
      "[2977]\ttrain-rmse:2.086707\n",
      "[2978]\ttrain-rmse:2.086429\n",
      "[2979]\ttrain-rmse:2.086163\n",
      "[2980]\ttrain-rmse:2.085886\n",
      "[2981]\ttrain-rmse:2.085597\n",
      "[2982]\ttrain-rmse:2.085323\n",
      "[2983]\ttrain-rmse:2.085038\n",
      "[2984]\ttrain-rmse:2.084770\n",
      "[2985]\ttrain-rmse:2.084498\n",
      "[2986]\ttrain-rmse:2.084227\n",
      "[2987]\ttrain-rmse:2.083932\n",
      "[2988]\ttrain-rmse:2.083653\n",
      "[2989]\ttrain-rmse:2.083386\n",
      "[2990]\ttrain-rmse:2.083100\n",
      "[2991]\ttrain-rmse:2.082833\n",
      "[2992]\ttrain-rmse:2.082563\n",
      "[2993]\ttrain-rmse:2.082283\n",
      "[2994]\ttrain-rmse:2.082011\n",
      "[2995]\ttrain-rmse:2.081738\n",
      "[2996]\ttrain-rmse:2.081465\n",
      "[2997]\ttrain-rmse:2.081202\n",
      "[2998]\ttrain-rmse:2.080925\n",
      "[2999]\ttrain-rmse:2.080657\n",
      "[3000]\ttrain-rmse:2.080378\n",
      "[3001]\ttrain-rmse:2.080104\n",
      "[3002]\ttrain-rmse:2.079839\n",
      "[3003]\ttrain-rmse:2.079571\n",
      "[3004]\ttrain-rmse:2.079295\n",
      "[3005]\ttrain-rmse:2.079023\n",
      "[3006]\ttrain-rmse:2.078762\n",
      "[3007]\ttrain-rmse:2.078490\n",
      "[3008]\ttrain-rmse:2.078224\n",
      "[3009]\ttrain-rmse:2.077944\n",
      "[3010]\ttrain-rmse:2.077668\n",
      "[3011]\ttrain-rmse:2.077397\n",
      "[3012]\ttrain-rmse:2.077124\n",
      "[3013]\ttrain-rmse:2.076856\n",
      "[3014]\ttrain-rmse:2.076583\n",
      "[3015]\ttrain-rmse:2.076319\n",
      "[3016]\ttrain-rmse:2.076049\n",
      "[3017]\ttrain-rmse:2.075782\n",
      "[3018]\ttrain-rmse:2.075526\n",
      "[3019]\ttrain-rmse:2.075260\n",
      "[3020]\ttrain-rmse:2.074996\n",
      "[3021]\ttrain-rmse:2.074728\n",
      "[3022]\ttrain-rmse:2.074460\n",
      "[3023]\ttrain-rmse:2.074198\n",
      "[3024]\ttrain-rmse:2.073935\n",
      "[3025]\ttrain-rmse:2.073676\n",
      "[3026]\ttrain-rmse:2.073412\n",
      "[3027]\ttrain-rmse:2.073144\n",
      "[3028]\ttrain-rmse:2.072884\n",
      "[3029]\ttrain-rmse:2.072618\n",
      "[3030]\ttrain-rmse:2.072353\n",
      "[3031]\ttrain-rmse:2.072088\n",
      "[3032]\ttrain-rmse:2.071833\n",
      "[3033]\ttrain-rmse:2.071573\n",
      "[3034]\ttrain-rmse:2.071308\n",
      "[3035]\ttrain-rmse:2.071037\n",
      "[3036]\ttrain-rmse:2.070764\n",
      "[3037]\ttrain-rmse:2.070497\n",
      "[3038]\ttrain-rmse:2.070222\n",
      "[3039]\ttrain-rmse:2.069964\n",
      "[3040]\ttrain-rmse:2.069705\n",
      "[3041]\ttrain-rmse:2.069432\n",
      "[3042]\ttrain-rmse:2.069161\n",
      "[3043]\ttrain-rmse:2.068894\n",
      "[3044]\ttrain-rmse:2.068629\n",
      "[3045]\ttrain-rmse:2.068376\n",
      "[3046]\ttrain-rmse:2.068101\n",
      "[3047]\ttrain-rmse:2.067846\n",
      "[3048]\ttrain-rmse:2.067584\n",
      "[3049]\ttrain-rmse:2.067325\n",
      "[3050]\ttrain-rmse:2.067061\n",
      "[3051]\ttrain-rmse:2.066804\n",
      "[3052]\ttrain-rmse:2.066536\n",
      "[3053]\ttrain-rmse:2.066283\n",
      "[3054]\ttrain-rmse:2.066016\n",
      "[3055]\ttrain-rmse:2.065755\n",
      "[3056]\ttrain-rmse:2.065486\n",
      "[3057]\ttrain-rmse:2.065218\n",
      "[3058]\ttrain-rmse:2.064960\n",
      "[3059]\ttrain-rmse:2.064698\n",
      "[3060]\ttrain-rmse:2.064439\n",
      "[3061]\ttrain-rmse:2.064187\n",
      "[3062]\ttrain-rmse:2.063930\n",
      "[3063]\ttrain-rmse:2.063650\n",
      "[3064]\ttrain-rmse:2.063395\n",
      "[3065]\ttrain-rmse:2.063135\n",
      "[3066]\ttrain-rmse:2.062874\n",
      "[3067]\ttrain-rmse:2.062603\n",
      "[3068]\ttrain-rmse:2.062346\n",
      "[3069]\ttrain-rmse:2.062089\n",
      "[3070]\ttrain-rmse:2.061830\n",
      "[3071]\ttrain-rmse:2.061576\n",
      "[3072]\ttrain-rmse:2.061316\n",
      "[3073]\ttrain-rmse:2.061066\n",
      "[3074]\ttrain-rmse:2.060808\n",
      "[3075]\ttrain-rmse:2.060549\n",
      "[3076]\ttrain-rmse:2.060300\n",
      "[3077]\ttrain-rmse:2.060048\n",
      "[3078]\ttrain-rmse:2.059795\n",
      "[3079]\ttrain-rmse:2.059526\n",
      "[3080]\ttrain-rmse:2.059277\n",
      "[3081]\ttrain-rmse:2.059027\n",
      "[3082]\ttrain-rmse:2.058774\n",
      "[3083]\ttrain-rmse:2.058516\n",
      "[3084]\ttrain-rmse:2.058266\n",
      "[3085]\ttrain-rmse:2.058013\n",
      "[3086]\ttrain-rmse:2.057763\n",
      "[3087]\ttrain-rmse:2.057515\n",
      "[3088]\ttrain-rmse:2.057264\n",
      "[3089]\ttrain-rmse:2.057013\n",
      "[3090]\ttrain-rmse:2.056758\n",
      "[3091]\ttrain-rmse:2.056505\n",
      "[3092]\ttrain-rmse:2.056257\n",
      "[3093]\ttrain-rmse:2.056000\n",
      "[3094]\ttrain-rmse:2.055742\n",
      "[3095]\ttrain-rmse:2.055479\n",
      "[3096]\ttrain-rmse:2.055228\n",
      "[3097]\ttrain-rmse:2.054980\n",
      "[3098]\ttrain-rmse:2.054716\n",
      "[3099]\ttrain-rmse:2.054457\n",
      "[3100]\ttrain-rmse:2.054207\n",
      "[3101]\ttrain-rmse:2.053952\n",
      "[3102]\ttrain-rmse:2.053698\n",
      "[3103]\ttrain-rmse:2.053448\n",
      "[3104]\ttrain-rmse:2.053209\n",
      "[3105]\ttrain-rmse:2.052960\n",
      "[3106]\ttrain-rmse:2.052704\n",
      "[3107]\ttrain-rmse:2.052453\n",
      "[3108]\ttrain-rmse:2.052188\n",
      "[3109]\ttrain-rmse:2.051939\n",
      "[3110]\ttrain-rmse:2.051692\n",
      "[3111]\ttrain-rmse:2.051433\n",
      "[3112]\ttrain-rmse:2.051188\n",
      "[3113]\ttrain-rmse:2.050941\n",
      "[3114]\ttrain-rmse:2.050693\n",
      "[3115]\ttrain-rmse:2.050439\n",
      "[3116]\ttrain-rmse:2.050190\n",
      "[3117]\ttrain-rmse:2.049942\n",
      "[3118]\ttrain-rmse:2.049691\n",
      "[3119]\ttrain-rmse:2.049446\n",
      "[3120]\ttrain-rmse:2.049209\n",
      "[3121]\ttrain-rmse:2.048957\n",
      "[3122]\ttrain-rmse:2.048701\n",
      "[3123]\ttrain-rmse:2.048456\n",
      "[3124]\ttrain-rmse:2.048213\n",
      "[3125]\ttrain-rmse:2.047955\n",
      "[3126]\ttrain-rmse:2.047709\n",
      "[3127]\ttrain-rmse:2.047467\n",
      "[3128]\ttrain-rmse:2.047214\n",
      "[3129]\ttrain-rmse:2.046966\n",
      "[3130]\ttrain-rmse:2.046716\n",
      "[3131]\ttrain-rmse:2.046482\n",
      "[3132]\ttrain-rmse:2.046235\n",
      "[3133]\ttrain-rmse:2.045983\n",
      "[3134]\ttrain-rmse:2.045739\n",
      "[3135]\ttrain-rmse:2.045485\n",
      "[3136]\ttrain-rmse:2.045250\n",
      "[3137]\ttrain-rmse:2.045013\n",
      "[3138]\ttrain-rmse:2.044763\n",
      "[3139]\ttrain-rmse:2.044525\n",
      "[3140]\ttrain-rmse:2.044278\n",
      "[3141]\ttrain-rmse:2.044033\n",
      "[3142]\ttrain-rmse:2.043788\n",
      "[3143]\ttrain-rmse:2.043543\n",
      "[3144]\ttrain-rmse:2.043288\n",
      "[3145]\ttrain-rmse:2.043051\n",
      "[3146]\ttrain-rmse:2.042804\n",
      "[3147]\ttrain-rmse:2.042547\n",
      "[3148]\ttrain-rmse:2.042302\n",
      "[3149]\ttrain-rmse:2.042044\n",
      "[3150]\ttrain-rmse:2.041796\n",
      "[3151]\ttrain-rmse:2.041562\n",
      "[3152]\ttrain-rmse:2.041327\n",
      "[3153]\ttrain-rmse:2.041087\n",
      "[3154]\ttrain-rmse:2.040850\n",
      "[3155]\ttrain-rmse:2.040617\n",
      "[3156]\ttrain-rmse:2.040374\n",
      "[3157]\ttrain-rmse:2.040143\n",
      "[3158]\ttrain-rmse:2.039908\n",
      "[3159]\ttrain-rmse:2.039660\n",
      "[3160]\ttrain-rmse:2.039411\n",
      "[3161]\ttrain-rmse:2.039169\n",
      "[3162]\ttrain-rmse:2.038930\n",
      "[3163]\ttrain-rmse:2.038687\n",
      "[3164]\ttrain-rmse:2.038453\n",
      "[3165]\ttrain-rmse:2.038213\n",
      "[3166]\ttrain-rmse:2.037964\n",
      "[3167]\ttrain-rmse:2.037729\n",
      "[3168]\ttrain-rmse:2.037495\n",
      "[3169]\ttrain-rmse:2.037251\n",
      "[3170]\ttrain-rmse:2.037010\n",
      "[3171]\ttrain-rmse:2.036776\n",
      "[3172]\ttrain-rmse:2.036537\n",
      "[3173]\ttrain-rmse:2.036306\n",
      "[3174]\ttrain-rmse:2.036068\n",
      "[3175]\ttrain-rmse:2.035827\n",
      "[3176]\ttrain-rmse:2.035589\n",
      "[3177]\ttrain-rmse:2.035346\n",
      "[3178]\ttrain-rmse:2.035102\n",
      "[3179]\ttrain-rmse:2.034868\n",
      "[3180]\ttrain-rmse:2.034630\n",
      "[3181]\ttrain-rmse:2.034384\n",
      "[3182]\ttrain-rmse:2.034151\n",
      "[3183]\ttrain-rmse:2.033916\n",
      "[3184]\ttrain-rmse:2.033686\n",
      "[3185]\ttrain-rmse:2.033459\n",
      "[3186]\ttrain-rmse:2.033233\n",
      "[3187]\ttrain-rmse:2.033004\n",
      "[3188]\ttrain-rmse:2.032764\n",
      "[3189]\ttrain-rmse:2.032537\n",
      "[3190]\ttrain-rmse:2.032296\n",
      "[3191]\ttrain-rmse:2.032066\n",
      "[3192]\ttrain-rmse:2.031840\n",
      "[3193]\ttrain-rmse:2.031595\n",
      "[3194]\ttrain-rmse:2.031369\n",
      "[3195]\ttrain-rmse:2.031142\n",
      "[3196]\ttrain-rmse:2.030910\n",
      "[3197]\ttrain-rmse:2.030682\n",
      "[3198]\ttrain-rmse:2.030453\n",
      "[3199]\ttrain-rmse:2.030227\n",
      "[3200]\ttrain-rmse:2.029994\n",
      "[3201]\ttrain-rmse:2.029759\n",
      "[3202]\ttrain-rmse:2.029529\n",
      "[3203]\ttrain-rmse:2.029299\n",
      "[3204]\ttrain-rmse:2.029069\n",
      "[3205]\ttrain-rmse:2.028835\n",
      "[3206]\ttrain-rmse:2.028602\n",
      "[3207]\ttrain-rmse:2.028378\n",
      "[3208]\ttrain-rmse:2.028145\n",
      "[3209]\ttrain-rmse:2.027896\n",
      "[3210]\ttrain-rmse:2.027662\n",
      "[3211]\ttrain-rmse:2.027436\n",
      "[3212]\ttrain-rmse:2.027211\n",
      "[3213]\ttrain-rmse:2.026986\n",
      "[3214]\ttrain-rmse:2.026761\n",
      "[3215]\ttrain-rmse:2.026522\n",
      "[3216]\ttrain-rmse:2.026283\n",
      "[3217]\ttrain-rmse:2.026052\n",
      "[3218]\ttrain-rmse:2.025823\n",
      "[3219]\ttrain-rmse:2.025602\n",
      "[3220]\ttrain-rmse:2.025374\n",
      "[3221]\ttrain-rmse:2.025151\n",
      "[3222]\ttrain-rmse:2.024912\n",
      "[3223]\ttrain-rmse:2.024676\n",
      "[3224]\ttrain-rmse:2.024446\n",
      "[3225]\ttrain-rmse:2.024214\n",
      "[3226]\ttrain-rmse:2.023986\n",
      "[3227]\ttrain-rmse:2.023761\n",
      "[3228]\ttrain-rmse:2.023539\n",
      "[3229]\ttrain-rmse:2.023304\n",
      "[3230]\ttrain-rmse:2.023068\n",
      "[3231]\ttrain-rmse:2.022840\n",
      "[3232]\ttrain-rmse:2.022614\n",
      "[3233]\ttrain-rmse:2.022380\n",
      "[3234]\ttrain-rmse:2.022148\n",
      "[3235]\ttrain-rmse:2.021916\n",
      "[3236]\ttrain-rmse:2.021688\n",
      "[3237]\ttrain-rmse:2.021461\n",
      "[3238]\ttrain-rmse:2.021237\n",
      "[3239]\ttrain-rmse:2.021008\n",
      "[3240]\ttrain-rmse:2.020786\n",
      "[3241]\ttrain-rmse:2.020561\n",
      "[3242]\ttrain-rmse:2.020335\n",
      "[3243]\ttrain-rmse:2.020108\n",
      "[3244]\ttrain-rmse:2.019886\n",
      "[3245]\ttrain-rmse:2.019665\n",
      "[3246]\ttrain-rmse:2.019434\n",
      "[3247]\ttrain-rmse:2.019215\n",
      "[3248]\ttrain-rmse:2.018997\n",
      "[3249]\ttrain-rmse:2.018764\n",
      "[3250]\ttrain-rmse:2.018544\n",
      "[3251]\ttrain-rmse:2.018315\n",
      "[3252]\ttrain-rmse:2.018095\n",
      "[3253]\ttrain-rmse:2.017873\n",
      "[3254]\ttrain-rmse:2.017644\n",
      "[3255]\ttrain-rmse:2.017418\n",
      "[3256]\ttrain-rmse:2.017202\n",
      "[3257]\ttrain-rmse:2.016977\n",
      "[3258]\ttrain-rmse:2.016747\n",
      "[3259]\ttrain-rmse:2.016525\n",
      "[3260]\ttrain-rmse:2.016306\n",
      "[3261]\ttrain-rmse:2.016076\n",
      "[3262]\ttrain-rmse:2.015852\n",
      "[3263]\ttrain-rmse:2.015640\n",
      "[3264]\ttrain-rmse:2.015421\n",
      "[3265]\ttrain-rmse:2.015195\n",
      "[3266]\ttrain-rmse:2.014968\n",
      "[3267]\ttrain-rmse:2.014750\n",
      "[3268]\ttrain-rmse:2.014534\n",
      "[3269]\ttrain-rmse:2.014318\n",
      "[3270]\ttrain-rmse:2.014108\n",
      "[3271]\ttrain-rmse:2.013897\n",
      "[3272]\ttrain-rmse:2.013670\n",
      "[3273]\ttrain-rmse:2.013445\n",
      "[3274]\ttrain-rmse:2.013224\n",
      "[3275]\ttrain-rmse:2.013010\n",
      "[3276]\ttrain-rmse:2.012795\n",
      "[3277]\ttrain-rmse:2.012579\n",
      "[3278]\ttrain-rmse:2.012364\n",
      "[3279]\ttrain-rmse:2.012149\n",
      "[3280]\ttrain-rmse:2.011923\n",
      "[3281]\ttrain-rmse:2.011697\n",
      "[3282]\ttrain-rmse:2.011472\n",
      "[3283]\ttrain-rmse:2.011254\n",
      "[3284]\ttrain-rmse:2.011039\n",
      "[3285]\ttrain-rmse:2.010828\n",
      "[3286]\ttrain-rmse:2.010602\n",
      "[3287]\ttrain-rmse:2.010376\n",
      "[3288]\ttrain-rmse:2.010155\n",
      "[3289]\ttrain-rmse:2.009930\n",
      "[3290]\ttrain-rmse:2.009718\n",
      "[3291]\ttrain-rmse:2.009514\n",
      "[3292]\ttrain-rmse:2.009292\n",
      "[3293]\ttrain-rmse:2.009073\n",
      "[3294]\ttrain-rmse:2.008860\n",
      "[3295]\ttrain-rmse:2.008643\n",
      "[3296]\ttrain-rmse:2.008429\n",
      "[3297]\ttrain-rmse:2.008213\n",
      "[3298]\ttrain-rmse:2.007994\n",
      "[3299]\ttrain-rmse:2.007770\n",
      "[3300]\ttrain-rmse:2.007555\n",
      "[3301]\ttrain-rmse:2.007343\n",
      "[3302]\ttrain-rmse:2.007138\n",
      "[3303]\ttrain-rmse:2.006910\n",
      "[3304]\ttrain-rmse:2.006701\n",
      "[3305]\ttrain-rmse:2.006488\n",
      "[3306]\ttrain-rmse:2.006277\n",
      "[3307]\ttrain-rmse:2.006060\n",
      "[3308]\ttrain-rmse:2.005845\n",
      "[3309]\ttrain-rmse:2.005628\n",
      "[3310]\ttrain-rmse:2.005404\n",
      "[3311]\ttrain-rmse:2.005187\n",
      "[3312]\ttrain-rmse:2.004982\n",
      "[3313]\ttrain-rmse:2.004773\n",
      "[3314]\ttrain-rmse:2.004561\n",
      "[3315]\ttrain-rmse:2.004349\n",
      "[3316]\ttrain-rmse:2.004133\n",
      "[3317]\ttrain-rmse:2.003916\n",
      "[3318]\ttrain-rmse:2.003703\n",
      "[3319]\ttrain-rmse:2.003494\n",
      "[3320]\ttrain-rmse:2.003268\n",
      "[3321]\ttrain-rmse:2.003055\n",
      "[3322]\ttrain-rmse:2.002843\n",
      "[3323]\ttrain-rmse:2.002633\n",
      "[3324]\ttrain-rmse:2.002419\n",
      "[3325]\ttrain-rmse:2.002211\n",
      "[3326]\ttrain-rmse:2.002006\n",
      "[3327]\ttrain-rmse:2.001799\n",
      "[3328]\ttrain-rmse:2.001590\n",
      "[3329]\ttrain-rmse:2.001383\n",
      "[3330]\ttrain-rmse:2.001166\n",
      "[3331]\ttrain-rmse:2.000957\n",
      "[3332]\ttrain-rmse:2.000746\n",
      "[3333]\ttrain-rmse:2.000544\n",
      "[3334]\ttrain-rmse:2.000323\n",
      "[3335]\ttrain-rmse:2.000118\n",
      "[3336]\ttrain-rmse:1.999911\n",
      "[3337]\ttrain-rmse:1.999688\n",
      "[3338]\ttrain-rmse:1.999481\n",
      "[3339]\ttrain-rmse:1.999283\n",
      "[3340]\ttrain-rmse:1.999064\n",
      "[3341]\ttrain-rmse:1.998852\n",
      "[3342]\ttrain-rmse:1.998642\n",
      "[3343]\ttrain-rmse:1.998430\n",
      "[3344]\ttrain-rmse:1.998234\n",
      "[3345]\ttrain-rmse:1.998024\n",
      "[3346]\ttrain-rmse:1.997819\n",
      "[3347]\ttrain-rmse:1.997606\n",
      "[3348]\ttrain-rmse:1.997405\n",
      "[3349]\ttrain-rmse:1.997190\n",
      "[3350]\ttrain-rmse:1.996984\n",
      "[3351]\ttrain-rmse:1.996776\n",
      "[3352]\ttrain-rmse:1.996572\n",
      "[3353]\ttrain-rmse:1.996373\n",
      "[3354]\ttrain-rmse:1.996172\n",
      "[3355]\ttrain-rmse:1.995964\n",
      "[3356]\ttrain-rmse:1.995757\n",
      "[3357]\ttrain-rmse:1.995553\n",
      "[3358]\ttrain-rmse:1.995347\n",
      "[3359]\ttrain-rmse:1.995145\n",
      "[3360]\ttrain-rmse:1.994942\n",
      "[3361]\ttrain-rmse:1.994739\n",
      "[3362]\ttrain-rmse:1.994523\n",
      "[3363]\ttrain-rmse:1.994328\n",
      "[3364]\ttrain-rmse:1.994119\n",
      "[3365]\ttrain-rmse:1.993912\n",
      "[3366]\ttrain-rmse:1.993712\n",
      "[3367]\ttrain-rmse:1.993507\n",
      "[3368]\ttrain-rmse:1.993309\n",
      "[3369]\ttrain-rmse:1.993105\n",
      "[3370]\ttrain-rmse:1.992898\n",
      "[3371]\ttrain-rmse:1.992697\n",
      "[3372]\ttrain-rmse:1.992492\n",
      "[3373]\ttrain-rmse:1.992289\n",
      "[3374]\ttrain-rmse:1.992080\n",
      "[3375]\ttrain-rmse:1.991880\n",
      "[3376]\ttrain-rmse:1.991671\n",
      "[3377]\ttrain-rmse:1.991476\n",
      "[3378]\ttrain-rmse:1.991272\n",
      "[3379]\ttrain-rmse:1.991071\n",
      "[3380]\ttrain-rmse:1.990866\n",
      "[3381]\ttrain-rmse:1.990664\n",
      "[3382]\ttrain-rmse:1.990466\n",
      "[3383]\ttrain-rmse:1.990271\n",
      "[3384]\ttrain-rmse:1.990073\n",
      "[3385]\ttrain-rmse:1.989871\n",
      "[3386]\ttrain-rmse:1.989666\n",
      "[3387]\ttrain-rmse:1.989451\n",
      "[3388]\ttrain-rmse:1.989263\n",
      "[3389]\ttrain-rmse:1.989058\n",
      "[3390]\ttrain-rmse:1.988856\n",
      "[3391]\ttrain-rmse:1.988663\n",
      "[3392]\ttrain-rmse:1.988459\n",
      "[3393]\ttrain-rmse:1.988257\n",
      "[3394]\ttrain-rmse:1.988053\n",
      "[3395]\ttrain-rmse:1.987846\n",
      "[3396]\ttrain-rmse:1.987660\n",
      "[3397]\ttrain-rmse:1.987462\n",
      "[3398]\ttrain-rmse:1.987259\n",
      "[3399]\ttrain-rmse:1.987070\n",
      "[3400]\ttrain-rmse:1.986869\n",
      "[3401]\ttrain-rmse:1.986657\n",
      "[3402]\ttrain-rmse:1.986441\n",
      "[3403]\ttrain-rmse:1.986242\n",
      "[3404]\ttrain-rmse:1.986045\n",
      "[3405]\ttrain-rmse:1.985841\n",
      "[3406]\ttrain-rmse:1.985640\n",
      "[3407]\ttrain-rmse:1.985450\n",
      "[3408]\ttrain-rmse:1.985256\n",
      "[3409]\ttrain-rmse:1.985060\n",
      "[3410]\ttrain-rmse:1.984863\n",
      "[3411]\ttrain-rmse:1.984670\n",
      "[3412]\ttrain-rmse:1.984473\n",
      "[3413]\ttrain-rmse:1.984282\n",
      "[3414]\ttrain-rmse:1.984085\n",
      "[3415]\ttrain-rmse:1.983898\n",
      "[3416]\ttrain-rmse:1.983707\n",
      "[3417]\ttrain-rmse:1.983492\n",
      "[3418]\ttrain-rmse:1.983297\n",
      "[3419]\ttrain-rmse:1.983091\n",
      "[3420]\ttrain-rmse:1.982895\n",
      "[3421]\ttrain-rmse:1.982701\n",
      "[3422]\ttrain-rmse:1.982502\n",
      "[3423]\ttrain-rmse:1.982320\n",
      "[3424]\ttrain-rmse:1.982116\n",
      "[3425]\ttrain-rmse:1.981918\n",
      "[3426]\ttrain-rmse:1.981718\n",
      "[3427]\ttrain-rmse:1.981524\n",
      "[3428]\ttrain-rmse:1.981334\n",
      "[3429]\ttrain-rmse:1.981140\n",
      "[3430]\ttrain-rmse:1.980951\n",
      "[3431]\ttrain-rmse:1.980758\n",
      "[3432]\ttrain-rmse:1.980561\n",
      "[3433]\ttrain-rmse:1.980370\n",
      "[3434]\ttrain-rmse:1.980166\n",
      "[3435]\ttrain-rmse:1.979979\n",
      "[3436]\ttrain-rmse:1.979770\n",
      "[3437]\ttrain-rmse:1.979567\n",
      "[3438]\ttrain-rmse:1.979370\n",
      "[3439]\ttrain-rmse:1.979166\n",
      "[3440]\ttrain-rmse:1.978962\n",
      "[3441]\ttrain-rmse:1.978769\n",
      "[3442]\ttrain-rmse:1.978576\n",
      "[3443]\ttrain-rmse:1.978387\n",
      "[3444]\ttrain-rmse:1.978195\n",
      "[3445]\ttrain-rmse:1.978004\n",
      "[3446]\ttrain-rmse:1.977816\n",
      "[3447]\ttrain-rmse:1.977623\n",
      "[3448]\ttrain-rmse:1.977431\n",
      "[3449]\ttrain-rmse:1.977236\n",
      "[3450]\ttrain-rmse:1.977053\n",
      "[3451]\ttrain-rmse:1.976861\n",
      "[3452]\ttrain-rmse:1.976675\n",
      "[3453]\ttrain-rmse:1.976492\n",
      "[3454]\ttrain-rmse:1.976295\n",
      "[3455]\ttrain-rmse:1.976100\n",
      "[3456]\ttrain-rmse:1.975913\n",
      "[3457]\ttrain-rmse:1.975714\n",
      "[3458]\ttrain-rmse:1.975518\n",
      "[3459]\ttrain-rmse:1.975314\n",
      "[3460]\ttrain-rmse:1.975126\n",
      "[3461]\ttrain-rmse:1.974930\n",
      "[3462]\ttrain-rmse:1.974749\n",
      "[3463]\ttrain-rmse:1.974558\n",
      "[3464]\ttrain-rmse:1.974372\n",
      "[3465]\ttrain-rmse:1.974178\n",
      "[3466]\ttrain-rmse:1.973988\n",
      "[3467]\ttrain-rmse:1.973801\n",
      "[3468]\ttrain-rmse:1.973605\n",
      "[3469]\ttrain-rmse:1.973418\n",
      "[3470]\ttrain-rmse:1.973237\n",
      "[3471]\ttrain-rmse:1.973049\n",
      "[3472]\ttrain-rmse:1.972867\n",
      "[3473]\ttrain-rmse:1.972674\n",
      "[3474]\ttrain-rmse:1.972492\n",
      "[3475]\ttrain-rmse:1.972306\n",
      "[3476]\ttrain-rmse:1.972119\n",
      "[3477]\ttrain-rmse:1.971930\n",
      "[3478]\ttrain-rmse:1.971748\n",
      "[3479]\ttrain-rmse:1.971563\n",
      "[3480]\ttrain-rmse:1.971382\n",
      "[3481]\ttrain-rmse:1.971200\n",
      "[3482]\ttrain-rmse:1.971020\n",
      "[3483]\ttrain-rmse:1.970831\n",
      "[3484]\ttrain-rmse:1.970639\n",
      "[3485]\ttrain-rmse:1.970457\n",
      "[3486]\ttrain-rmse:1.970269\n",
      "[3487]\ttrain-rmse:1.970080\n",
      "[3488]\ttrain-rmse:1.969895\n",
      "[3489]\ttrain-rmse:1.969705\n",
      "[3490]\ttrain-rmse:1.969512\n",
      "[3491]\ttrain-rmse:1.969334\n",
      "[3492]\ttrain-rmse:1.969144\n",
      "[3493]\ttrain-rmse:1.968956\n",
      "[3494]\ttrain-rmse:1.968773\n",
      "[3495]\ttrain-rmse:1.968591\n",
      "[3496]\ttrain-rmse:1.968397\n",
      "[3497]\ttrain-rmse:1.968214\n",
      "[3498]\ttrain-rmse:1.968032\n",
      "[3499]\ttrain-rmse:1.967853\n",
      "[3500]\ttrain-rmse:1.967676\n",
      "[3501]\ttrain-rmse:1.967483\n",
      "[3502]\ttrain-rmse:1.967291\n",
      "[3503]\ttrain-rmse:1.967101\n",
      "[3504]\ttrain-rmse:1.966915\n",
      "[3505]\ttrain-rmse:1.966743\n",
      "[3506]\ttrain-rmse:1.966554\n",
      "[3507]\ttrain-rmse:1.966372\n",
      "[3508]\ttrain-rmse:1.966176\n",
      "[3509]\ttrain-rmse:1.965982\n",
      "[3510]\ttrain-rmse:1.965798\n",
      "[3511]\ttrain-rmse:1.965617\n",
      "[3512]\ttrain-rmse:1.965443\n",
      "[3513]\ttrain-rmse:1.965260\n",
      "[3514]\ttrain-rmse:1.965077\n",
      "[3515]\ttrain-rmse:1.964893\n",
      "[3516]\ttrain-rmse:1.964706\n",
      "[3517]\ttrain-rmse:1.964519\n",
      "[3518]\ttrain-rmse:1.964331\n",
      "[3519]\ttrain-rmse:1.964146\n",
      "[3520]\ttrain-rmse:1.963963\n",
      "[3521]\ttrain-rmse:1.963778\n",
      "[3522]\ttrain-rmse:1.963587\n",
      "[3523]\ttrain-rmse:1.963398\n",
      "[3524]\ttrain-rmse:1.963222\n",
      "[3525]\ttrain-rmse:1.963045\n",
      "[3526]\ttrain-rmse:1.962869\n",
      "[3527]\ttrain-rmse:1.962691\n",
      "[3528]\ttrain-rmse:1.962510\n",
      "[3529]\ttrain-rmse:1.962334\n",
      "[3530]\ttrain-rmse:1.962151\n",
      "[3531]\ttrain-rmse:1.961954\n",
      "[3532]\ttrain-rmse:1.961781\n",
      "[3533]\ttrain-rmse:1.961597\n",
      "[3534]\ttrain-rmse:1.961423\n",
      "[3535]\ttrain-rmse:1.961240\n",
      "[3536]\ttrain-rmse:1.961052\n",
      "[3537]\ttrain-rmse:1.960878\n",
      "[3538]\ttrain-rmse:1.960703\n",
      "[3539]\ttrain-rmse:1.960518\n",
      "[3540]\ttrain-rmse:1.960330\n",
      "[3541]\ttrain-rmse:1.960151\n",
      "[3542]\ttrain-rmse:1.959988\n",
      "[3543]\ttrain-rmse:1.959816\n",
      "[3544]\ttrain-rmse:1.959635\n",
      "[3545]\ttrain-rmse:1.959460\n",
      "[3546]\ttrain-rmse:1.959283\n",
      "[3547]\ttrain-rmse:1.959105\n",
      "[3548]\ttrain-rmse:1.958919\n",
      "[3549]\ttrain-rmse:1.958739\n",
      "[3550]\ttrain-rmse:1.958575\n",
      "[3551]\ttrain-rmse:1.958395\n",
      "[3552]\ttrain-rmse:1.958219\n",
      "[3553]\ttrain-rmse:1.958040\n",
      "[3554]\ttrain-rmse:1.957860\n",
      "[3555]\ttrain-rmse:1.957680\n",
      "[3556]\ttrain-rmse:1.957493\n",
      "[3557]\ttrain-rmse:1.957311\n",
      "[3558]\ttrain-rmse:1.957140\n",
      "[3559]\ttrain-rmse:1.956974\n",
      "[3560]\ttrain-rmse:1.956796\n",
      "[3561]\ttrain-rmse:1.956616\n",
      "[3562]\ttrain-rmse:1.956435\n",
      "[3563]\ttrain-rmse:1.956261\n",
      "[3564]\ttrain-rmse:1.956076\n",
      "[3565]\ttrain-rmse:1.955893\n",
      "[3566]\ttrain-rmse:1.955716\n",
      "[3567]\ttrain-rmse:1.955542\n",
      "[3568]\ttrain-rmse:1.955360\n",
      "[3569]\ttrain-rmse:1.955187\n",
      "[3570]\ttrain-rmse:1.955017\n",
      "[3571]\ttrain-rmse:1.954832\n",
      "[3572]\ttrain-rmse:1.954660\n",
      "[3573]\ttrain-rmse:1.954486\n",
      "[3574]\ttrain-rmse:1.954308\n",
      "[3575]\ttrain-rmse:1.954125\n",
      "[3576]\ttrain-rmse:1.953955\n",
      "[3577]\ttrain-rmse:1.953783\n",
      "[3578]\ttrain-rmse:1.953608\n",
      "[3579]\ttrain-rmse:1.953437\n",
      "[3580]\ttrain-rmse:1.953256\n",
      "[3581]\ttrain-rmse:1.953082\n",
      "[3582]\ttrain-rmse:1.952899\n",
      "[3583]\ttrain-rmse:1.952733\n",
      "[3584]\ttrain-rmse:1.952556\n",
      "[3585]\ttrain-rmse:1.952380\n",
      "[3586]\ttrain-rmse:1.952210\n",
      "[3587]\ttrain-rmse:1.952038\n",
      "[3588]\ttrain-rmse:1.951871\n",
      "[3589]\ttrain-rmse:1.951689\n",
      "[3590]\ttrain-rmse:1.951516\n",
      "[3591]\ttrain-rmse:1.951351\n",
      "[3592]\ttrain-rmse:1.951186\n",
      "[3593]\ttrain-rmse:1.951018\n",
      "[3594]\ttrain-rmse:1.950847\n",
      "[3595]\ttrain-rmse:1.950677\n",
      "[3596]\ttrain-rmse:1.950510\n",
      "[3597]\ttrain-rmse:1.950337\n",
      "[3598]\ttrain-rmse:1.950162\n",
      "[3599]\ttrain-rmse:1.949997\n",
      "[3600]\ttrain-rmse:1.949826\n",
      "[3601]\ttrain-rmse:1.949659\n",
      "[3602]\ttrain-rmse:1.949482\n",
      "[3603]\ttrain-rmse:1.949307\n",
      "[3604]\ttrain-rmse:1.949122\n",
      "[3605]\ttrain-rmse:1.948954\n",
      "[3606]\ttrain-rmse:1.948792\n",
      "[3607]\ttrain-rmse:1.948624\n",
      "[3608]\ttrain-rmse:1.948452\n",
      "[3609]\ttrain-rmse:1.948282\n",
      "[3610]\ttrain-rmse:1.948104\n",
      "[3611]\ttrain-rmse:1.947936\n",
      "[3612]\ttrain-rmse:1.947761\n",
      "[3613]\ttrain-rmse:1.947598\n",
      "[3614]\ttrain-rmse:1.947433\n",
      "[3615]\ttrain-rmse:1.947265\n",
      "[3616]\ttrain-rmse:1.947090\n",
      "[3617]\ttrain-rmse:1.946934\n",
      "[3618]\ttrain-rmse:1.946757\n",
      "[3619]\ttrain-rmse:1.946577\n",
      "[3620]\ttrain-rmse:1.946407\n",
      "[3621]\ttrain-rmse:1.946247\n",
      "[3622]\ttrain-rmse:1.946080\n",
      "[3623]\ttrain-rmse:1.945906\n",
      "[3624]\ttrain-rmse:1.945729\n",
      "[3625]\ttrain-rmse:1.945559\n",
      "[3626]\ttrain-rmse:1.945402\n",
      "[3627]\ttrain-rmse:1.945230\n",
      "[3628]\ttrain-rmse:1.945062\n",
      "[3629]\ttrain-rmse:1.944895\n",
      "[3630]\ttrain-rmse:1.944727\n",
      "[3631]\ttrain-rmse:1.944558\n",
      "[3632]\ttrain-rmse:1.944396\n",
      "[3633]\ttrain-rmse:1.944229\n",
      "[3634]\ttrain-rmse:1.944060\n",
      "[3635]\ttrain-rmse:1.943901\n",
      "[3636]\ttrain-rmse:1.943731\n",
      "[3637]\ttrain-rmse:1.943572\n",
      "[3638]\ttrain-rmse:1.943406\n",
      "[3639]\ttrain-rmse:1.943238\n",
      "[3640]\ttrain-rmse:1.943081\n",
      "[3641]\ttrain-rmse:1.942910\n",
      "[3642]\ttrain-rmse:1.942745\n",
      "[3643]\ttrain-rmse:1.942582\n",
      "[3644]\ttrain-rmse:1.942423\n",
      "[3645]\ttrain-rmse:1.942254\n",
      "[3646]\ttrain-rmse:1.942091\n",
      "[3647]\ttrain-rmse:1.941931\n",
      "[3648]\ttrain-rmse:1.941773\n",
      "[3649]\ttrain-rmse:1.941603\n",
      "[3650]\ttrain-rmse:1.941442\n",
      "[3651]\ttrain-rmse:1.941274\n",
      "[3652]\ttrain-rmse:1.941116\n",
      "[3653]\ttrain-rmse:1.940946\n",
      "[3654]\ttrain-rmse:1.940785\n",
      "[3655]\ttrain-rmse:1.940613\n",
      "[3656]\ttrain-rmse:1.940444\n",
      "[3657]\ttrain-rmse:1.940288\n",
      "[3658]\ttrain-rmse:1.940125\n",
      "[3659]\ttrain-rmse:1.939962\n",
      "[3660]\ttrain-rmse:1.939793\n",
      "[3661]\ttrain-rmse:1.939622\n",
      "[3662]\ttrain-rmse:1.939445\n",
      "[3663]\ttrain-rmse:1.939269\n",
      "[3664]\ttrain-rmse:1.939105\n",
      "[3665]\ttrain-rmse:1.938958\n",
      "[3666]\ttrain-rmse:1.938796\n",
      "[3667]\ttrain-rmse:1.938627\n",
      "[3668]\ttrain-rmse:1.938468\n",
      "[3669]\ttrain-rmse:1.938309\n",
      "[3670]\ttrain-rmse:1.938149\n",
      "[3671]\ttrain-rmse:1.937986\n",
      "[3672]\ttrain-rmse:1.937826\n",
      "[3673]\ttrain-rmse:1.937661\n",
      "[3674]\ttrain-rmse:1.937500\n",
      "[3675]\ttrain-rmse:1.937343\n",
      "[3676]\ttrain-rmse:1.937183\n",
      "[3677]\ttrain-rmse:1.937011\n",
      "[3678]\ttrain-rmse:1.936843\n",
      "[3679]\ttrain-rmse:1.936696\n",
      "[3680]\ttrain-rmse:1.936530\n",
      "[3681]\ttrain-rmse:1.936370\n",
      "[3682]\ttrain-rmse:1.936207\n",
      "[3683]\ttrain-rmse:1.936049\n",
      "[3684]\ttrain-rmse:1.935882\n",
      "[3685]\ttrain-rmse:1.935725\n",
      "[3686]\ttrain-rmse:1.935562\n",
      "[3687]\ttrain-rmse:1.935408\n",
      "[3688]\ttrain-rmse:1.935251\n",
      "[3689]\ttrain-rmse:1.935093\n",
      "[3690]\ttrain-rmse:1.934934\n",
      "[3691]\ttrain-rmse:1.934774\n",
      "[3692]\ttrain-rmse:1.934618\n",
      "[3693]\ttrain-rmse:1.934451\n",
      "[3694]\ttrain-rmse:1.934284\n",
      "[3695]\ttrain-rmse:1.934119\n",
      "[3696]\ttrain-rmse:1.933957\n",
      "[3697]\ttrain-rmse:1.933799\n",
      "[3698]\ttrain-rmse:1.933653\n",
      "[3699]\ttrain-rmse:1.933485\n",
      "[3700]\ttrain-rmse:1.933321\n",
      "[3701]\ttrain-rmse:1.933151\n",
      "[3702]\ttrain-rmse:1.932987\n",
      "[3703]\ttrain-rmse:1.932827\n",
      "[3704]\ttrain-rmse:1.932673\n",
      "[3705]\ttrain-rmse:1.932517\n",
      "[3706]\ttrain-rmse:1.932366\n",
      "[3707]\ttrain-rmse:1.932208\n",
      "[3708]\ttrain-rmse:1.932048\n",
      "[3709]\ttrain-rmse:1.931892\n",
      "[3710]\ttrain-rmse:1.931733\n",
      "[3711]\ttrain-rmse:1.931577\n",
      "[3712]\ttrain-rmse:1.931419\n",
      "[3713]\ttrain-rmse:1.931257\n",
      "[3714]\ttrain-rmse:1.931106\n",
      "[3715]\ttrain-rmse:1.930946\n",
      "[3716]\ttrain-rmse:1.930794\n",
      "[3717]\ttrain-rmse:1.930639\n",
      "[3718]\ttrain-rmse:1.930485\n",
      "[3719]\ttrain-rmse:1.930331\n",
      "[3720]\ttrain-rmse:1.930175\n",
      "[3721]\ttrain-rmse:1.930019\n",
      "[3722]\ttrain-rmse:1.929870\n",
      "[3723]\ttrain-rmse:1.929709\n",
      "[3724]\ttrain-rmse:1.929552\n",
      "[3725]\ttrain-rmse:1.929388\n",
      "[3726]\ttrain-rmse:1.929229\n",
      "[3727]\ttrain-rmse:1.929070\n",
      "[3728]\ttrain-rmse:1.928917\n",
      "[3729]\ttrain-rmse:1.928756\n",
      "[3730]\ttrain-rmse:1.928607\n",
      "[3731]\ttrain-rmse:1.928444\n",
      "[3732]\ttrain-rmse:1.928289\n",
      "[3733]\ttrain-rmse:1.928132\n",
      "[3734]\ttrain-rmse:1.927983\n",
      "[3735]\ttrain-rmse:1.927824\n",
      "[3736]\ttrain-rmse:1.927667\n",
      "[3737]\ttrain-rmse:1.927522\n",
      "[3738]\ttrain-rmse:1.927366\n",
      "[3739]\ttrain-rmse:1.927220\n",
      "[3740]\ttrain-rmse:1.927063\n",
      "[3741]\ttrain-rmse:1.926911\n",
      "[3742]\ttrain-rmse:1.926761\n",
      "[3743]\ttrain-rmse:1.926606\n",
      "[3744]\ttrain-rmse:1.926457\n",
      "[3745]\ttrain-rmse:1.926311\n",
      "[3746]\ttrain-rmse:1.926157\n",
      "[3747]\ttrain-rmse:1.926000\n",
      "[3748]\ttrain-rmse:1.925848\n",
      "[3749]\ttrain-rmse:1.925699\n",
      "[3750]\ttrain-rmse:1.925548\n",
      "[3751]\ttrain-rmse:1.925387\n",
      "[3752]\ttrain-rmse:1.925227\n",
      "[3753]\ttrain-rmse:1.925068\n",
      "[3754]\ttrain-rmse:1.924925\n",
      "[3755]\ttrain-rmse:1.924777\n",
      "[3756]\ttrain-rmse:1.924623\n",
      "[3757]\ttrain-rmse:1.924468\n",
      "[3758]\ttrain-rmse:1.924310\n",
      "[3759]\ttrain-rmse:1.924172\n",
      "[3760]\ttrain-rmse:1.924021\n",
      "[3761]\ttrain-rmse:1.923866\n",
      "[3762]\ttrain-rmse:1.923719\n",
      "[3763]\ttrain-rmse:1.923576\n",
      "[3764]\ttrain-rmse:1.923419\n",
      "[3765]\ttrain-rmse:1.923266\n",
      "[3766]\ttrain-rmse:1.923121\n",
      "[3767]\ttrain-rmse:1.922973\n",
      "[3768]\ttrain-rmse:1.922824\n",
      "[3769]\ttrain-rmse:1.922668\n",
      "[3770]\ttrain-rmse:1.922511\n",
      "[3771]\ttrain-rmse:1.922349\n",
      "[3772]\ttrain-rmse:1.922197\n",
      "[3773]\ttrain-rmse:1.922048\n",
      "[3774]\ttrain-rmse:1.921890\n",
      "[3775]\ttrain-rmse:1.921751\n",
      "[3776]\ttrain-rmse:1.921606\n",
      "[3777]\ttrain-rmse:1.921447\n",
      "[3778]\ttrain-rmse:1.921299\n",
      "[3779]\ttrain-rmse:1.921148\n",
      "[3780]\ttrain-rmse:1.921000\n",
      "[3781]\ttrain-rmse:1.920851\n",
      "[3782]\ttrain-rmse:1.920706\n",
      "[3783]\ttrain-rmse:1.920565\n",
      "[3784]\ttrain-rmse:1.920410\n",
      "[3785]\ttrain-rmse:1.920260\n",
      "[3786]\ttrain-rmse:1.920115\n",
      "[3787]\ttrain-rmse:1.919962\n",
      "[3788]\ttrain-rmse:1.919818\n",
      "[3789]\ttrain-rmse:1.919659\n",
      "[3790]\ttrain-rmse:1.919510\n",
      "[3791]\ttrain-rmse:1.919367\n",
      "[3792]\ttrain-rmse:1.919218\n",
      "[3793]\ttrain-rmse:1.919069\n",
      "[3794]\ttrain-rmse:1.918915\n",
      "[3795]\ttrain-rmse:1.918762\n",
      "[3796]\ttrain-rmse:1.918612\n",
      "[3797]\ttrain-rmse:1.918466\n",
      "[3798]\ttrain-rmse:1.918326\n",
      "[3799]\ttrain-rmse:1.918185\n",
      "[3800]\ttrain-rmse:1.918044\n",
      "[3801]\ttrain-rmse:1.917894\n",
      "[3802]\ttrain-rmse:1.917748\n",
      "[3803]\ttrain-rmse:1.917596\n",
      "[3804]\ttrain-rmse:1.917446\n",
      "[3805]\ttrain-rmse:1.917296\n",
      "[3806]\ttrain-rmse:1.917148\n",
      "[3807]\ttrain-rmse:1.916993\n",
      "[3808]\ttrain-rmse:1.916850\n",
      "[3809]\ttrain-rmse:1.916705\n",
      "[3810]\ttrain-rmse:1.916553\n",
      "[3811]\ttrain-rmse:1.916413\n",
      "[3812]\ttrain-rmse:1.916264\n",
      "[3813]\ttrain-rmse:1.916116\n",
      "[3814]\ttrain-rmse:1.915978\n",
      "[3815]\ttrain-rmse:1.915839\n",
      "[3816]\ttrain-rmse:1.915695\n",
      "[3817]\ttrain-rmse:1.915557\n",
      "[3818]\ttrain-rmse:1.915411\n",
      "[3819]\ttrain-rmse:1.915261\n",
      "[3820]\ttrain-rmse:1.915109\n",
      "[3821]\ttrain-rmse:1.914965\n",
      "[3822]\ttrain-rmse:1.914827\n",
      "[3823]\ttrain-rmse:1.914681\n",
      "[3824]\ttrain-rmse:1.914542\n",
      "[3825]\ttrain-rmse:1.914402\n",
      "[3826]\ttrain-rmse:1.914265\n",
      "[3827]\ttrain-rmse:1.914117\n",
      "[3828]\ttrain-rmse:1.913973\n",
      "[3829]\ttrain-rmse:1.913831\n",
      "[3830]\ttrain-rmse:1.913689\n",
      "[3831]\ttrain-rmse:1.913540\n",
      "[3832]\ttrain-rmse:1.913406\n",
      "[3833]\ttrain-rmse:1.913272\n",
      "[3834]\ttrain-rmse:1.913125\n",
      "[3835]\ttrain-rmse:1.912985\n",
      "[3836]\ttrain-rmse:1.912849\n",
      "[3837]\ttrain-rmse:1.912711\n",
      "[3838]\ttrain-rmse:1.912566\n",
      "[3839]\ttrain-rmse:1.912424\n",
      "[3840]\ttrain-rmse:1.912278\n",
      "[3841]\ttrain-rmse:1.912138\n",
      "[3842]\ttrain-rmse:1.912005\n",
      "[3843]\ttrain-rmse:1.911866\n",
      "[3844]\ttrain-rmse:1.911722\n",
      "[3845]\ttrain-rmse:1.911589\n",
      "[3846]\ttrain-rmse:1.911444\n",
      "[3847]\ttrain-rmse:1.911305\n",
      "[3848]\ttrain-rmse:1.911166\n",
      "[3849]\ttrain-rmse:1.911021\n",
      "[3850]\ttrain-rmse:1.910875\n",
      "[3851]\ttrain-rmse:1.910732\n",
      "[3852]\ttrain-rmse:1.910595\n",
      "[3853]\ttrain-rmse:1.910460\n",
      "[3854]\ttrain-rmse:1.910309\n",
      "[3855]\ttrain-rmse:1.910173\n",
      "[3856]\ttrain-rmse:1.910035\n",
      "[3857]\ttrain-rmse:1.909904\n",
      "[3858]\ttrain-rmse:1.909763\n",
      "[3859]\ttrain-rmse:1.909620\n",
      "[3860]\ttrain-rmse:1.909474\n",
      "[3861]\ttrain-rmse:1.909336\n",
      "[3862]\ttrain-rmse:1.909193\n",
      "[3863]\ttrain-rmse:1.909056\n",
      "[3864]\ttrain-rmse:1.908915\n",
      "[3865]\ttrain-rmse:1.908777\n",
      "[3866]\ttrain-rmse:1.908631\n",
      "[3867]\ttrain-rmse:1.908492\n",
      "[3868]\ttrain-rmse:1.908348\n",
      "[3869]\ttrain-rmse:1.908213\n",
      "[3870]\ttrain-rmse:1.908074\n",
      "[3871]\ttrain-rmse:1.907937\n",
      "[3872]\ttrain-rmse:1.907805\n",
      "[3873]\ttrain-rmse:1.907661\n",
      "[3874]\ttrain-rmse:1.907524\n",
      "[3875]\ttrain-rmse:1.907384\n",
      "[3876]\ttrain-rmse:1.907253\n",
      "[3877]\ttrain-rmse:1.907103\n",
      "[3878]\ttrain-rmse:1.906957\n",
      "[3879]\ttrain-rmse:1.906824\n",
      "[3880]\ttrain-rmse:1.906690\n",
      "[3881]\ttrain-rmse:1.906555\n",
      "[3882]\ttrain-rmse:1.906413\n",
      "[3883]\ttrain-rmse:1.906276\n",
      "[3884]\ttrain-rmse:1.906142\n",
      "[3885]\ttrain-rmse:1.905996\n",
      "[3886]\ttrain-rmse:1.905863\n",
      "[3887]\ttrain-rmse:1.905731\n",
      "[3888]\ttrain-rmse:1.905593\n",
      "[3889]\ttrain-rmse:1.905463\n",
      "[3890]\ttrain-rmse:1.905330\n",
      "[3891]\ttrain-rmse:1.905199\n",
      "[3892]\ttrain-rmse:1.905059\n",
      "[3893]\ttrain-rmse:1.904931\n",
      "[3894]\ttrain-rmse:1.904789\n",
      "[3895]\ttrain-rmse:1.904658\n",
      "[3896]\ttrain-rmse:1.904514\n",
      "[3897]\ttrain-rmse:1.904364\n",
      "[3898]\ttrain-rmse:1.904238\n",
      "[3899]\ttrain-rmse:1.904101\n",
      "[3900]\ttrain-rmse:1.903967\n",
      "[3901]\ttrain-rmse:1.903830\n",
      "[3902]\ttrain-rmse:1.903694\n",
      "[3903]\ttrain-rmse:1.903556\n",
      "[3904]\ttrain-rmse:1.903424\n",
      "[3905]\ttrain-rmse:1.903294\n",
      "[3906]\ttrain-rmse:1.903160\n",
      "[3907]\ttrain-rmse:1.903024\n",
      "[3908]\ttrain-rmse:1.902892\n",
      "[3909]\ttrain-rmse:1.902750\n",
      "[3910]\ttrain-rmse:1.902617\n",
      "[3911]\ttrain-rmse:1.902473\n",
      "[3912]\ttrain-rmse:1.902341\n",
      "[3913]\ttrain-rmse:1.902212\n",
      "[3914]\ttrain-rmse:1.902081\n",
      "[3915]\ttrain-rmse:1.901950\n",
      "[3916]\ttrain-rmse:1.901821\n",
      "[3917]\ttrain-rmse:1.901691\n",
      "[3918]\ttrain-rmse:1.901549\n",
      "[3919]\ttrain-rmse:1.901415\n",
      "[3920]\ttrain-rmse:1.901279\n",
      "[3921]\ttrain-rmse:1.901140\n",
      "[3922]\ttrain-rmse:1.901012\n",
      "[3923]\ttrain-rmse:1.900875\n",
      "[3924]\ttrain-rmse:1.900751\n",
      "[3925]\ttrain-rmse:1.900617\n",
      "[3926]\ttrain-rmse:1.900493\n",
      "[3927]\ttrain-rmse:1.900347\n",
      "[3928]\ttrain-rmse:1.900212\n",
      "[3929]\ttrain-rmse:1.900082\n",
      "[3930]\ttrain-rmse:1.899950\n",
      "[3931]\ttrain-rmse:1.899818\n",
      "[3932]\ttrain-rmse:1.899685\n",
      "[3933]\ttrain-rmse:1.899555\n",
      "[3934]\ttrain-rmse:1.899431\n",
      "[3935]\ttrain-rmse:1.899312\n",
      "[3936]\ttrain-rmse:1.899186\n",
      "[3937]\ttrain-rmse:1.899054\n",
      "[3938]\ttrain-rmse:1.898923\n",
      "[3939]\ttrain-rmse:1.898792\n",
      "[3940]\ttrain-rmse:1.898655\n",
      "[3941]\ttrain-rmse:1.898519\n",
      "[3942]\ttrain-rmse:1.898389\n",
      "[3943]\ttrain-rmse:1.898251\n",
      "[3944]\ttrain-rmse:1.898121\n",
      "[3945]\ttrain-rmse:1.898002\n",
      "[3946]\ttrain-rmse:1.897862\n",
      "[3947]\ttrain-rmse:1.897731\n",
      "[3948]\ttrain-rmse:1.897599\n",
      "[3949]\ttrain-rmse:1.897465\n",
      "[3950]\ttrain-rmse:1.897326\n",
      "[3951]\ttrain-rmse:1.897204\n",
      "[3952]\ttrain-rmse:1.897073\n",
      "[3953]\ttrain-rmse:1.896934\n",
      "[3954]\ttrain-rmse:1.896807\n",
      "[3955]\ttrain-rmse:1.896678\n",
      "[3956]\ttrain-rmse:1.896550\n",
      "[3957]\ttrain-rmse:1.896428\n",
      "[3958]\ttrain-rmse:1.896305\n",
      "[3959]\ttrain-rmse:1.896165\n",
      "[3960]\ttrain-rmse:1.896033\n",
      "[3961]\ttrain-rmse:1.895902\n",
      "[3962]\ttrain-rmse:1.895777\n",
      "[3963]\ttrain-rmse:1.895649\n",
      "[3964]\ttrain-rmse:1.895526\n",
      "[3965]\ttrain-rmse:1.895406\n",
      "[3966]\ttrain-rmse:1.895273\n",
      "[3967]\ttrain-rmse:1.895141\n",
      "[3968]\ttrain-rmse:1.895008\n",
      "[3969]\ttrain-rmse:1.894887\n",
      "[3970]\ttrain-rmse:1.894769\n",
      "[3971]\ttrain-rmse:1.894634\n",
      "[3972]\ttrain-rmse:1.894514\n",
      "[3973]\ttrain-rmse:1.894378\n",
      "[3974]\ttrain-rmse:1.894259\n",
      "[3975]\ttrain-rmse:1.894122\n",
      "[3976]\ttrain-rmse:1.893990\n",
      "[3977]\ttrain-rmse:1.893860\n",
      "[3978]\ttrain-rmse:1.893730\n",
      "[3979]\ttrain-rmse:1.893600\n",
      "[3980]\ttrain-rmse:1.893469\n",
      "[3981]\ttrain-rmse:1.893344\n",
      "[3982]\ttrain-rmse:1.893214\n",
      "[3983]\ttrain-rmse:1.893096\n",
      "[3984]\ttrain-rmse:1.892967\n",
      "[3985]\ttrain-rmse:1.892850\n",
      "[3986]\ttrain-rmse:1.892721\n",
      "[3987]\ttrain-rmse:1.892584\n",
      "[3988]\ttrain-rmse:1.892444\n",
      "[3989]\ttrain-rmse:1.892318\n",
      "[3990]\ttrain-rmse:1.892198\n",
      "[3991]\ttrain-rmse:1.892076\n",
      "[3992]\ttrain-rmse:1.891947\n",
      "[3993]\ttrain-rmse:1.891824\n",
      "[3994]\ttrain-rmse:1.891698\n",
      "[3995]\ttrain-rmse:1.891573\n",
      "[3996]\ttrain-rmse:1.891452\n",
      "[3997]\ttrain-rmse:1.891327\n",
      "[3998]\ttrain-rmse:1.891209\n",
      "[3999]\ttrain-rmse:1.891094\n",
      "[4000]\ttrain-rmse:1.890963\n",
      "[4001]\ttrain-rmse:1.890834\n",
      "[4002]\ttrain-rmse:1.890718\n",
      "[4003]\ttrain-rmse:1.890605\n",
      "[4004]\ttrain-rmse:1.890485\n",
      "[4005]\ttrain-rmse:1.890356\n",
      "[4006]\ttrain-rmse:1.890224\n",
      "[4007]\ttrain-rmse:1.890101\n",
      "[4008]\ttrain-rmse:1.889976\n",
      "[4009]\ttrain-rmse:1.889847\n",
      "[4010]\ttrain-rmse:1.889731\n",
      "[4011]\ttrain-rmse:1.889610\n",
      "[4012]\ttrain-rmse:1.889482\n",
      "[4013]\ttrain-rmse:1.889356\n",
      "[4014]\ttrain-rmse:1.889236\n",
      "[4015]\ttrain-rmse:1.889114\n",
      "[4016]\ttrain-rmse:1.888988\n",
      "[4017]\ttrain-rmse:1.888871\n",
      "[4018]\ttrain-rmse:1.888754\n",
      "[4019]\ttrain-rmse:1.888620\n",
      "[4020]\ttrain-rmse:1.888491\n",
      "[4021]\ttrain-rmse:1.888364\n",
      "[4022]\ttrain-rmse:1.888246\n",
      "[4023]\ttrain-rmse:1.888114\n",
      "[4024]\ttrain-rmse:1.887995\n",
      "[4025]\ttrain-rmse:1.887874\n",
      "[4026]\ttrain-rmse:1.887748\n",
      "[4027]\ttrain-rmse:1.887630\n",
      "[4028]\ttrain-rmse:1.887513\n",
      "[4029]\ttrain-rmse:1.887377\n",
      "[4030]\ttrain-rmse:1.887261\n",
      "[4031]\ttrain-rmse:1.887130\n",
      "[4032]\ttrain-rmse:1.887001\n",
      "[4033]\ttrain-rmse:1.886880\n",
      "[4034]\ttrain-rmse:1.886759\n",
      "[4035]\ttrain-rmse:1.886640\n",
      "[4036]\ttrain-rmse:1.886510\n",
      "[4037]\ttrain-rmse:1.886388\n",
      "[4038]\ttrain-rmse:1.886262\n",
      "[4039]\ttrain-rmse:1.886134\n",
      "[4040]\ttrain-rmse:1.886012\n",
      "[4041]\ttrain-rmse:1.885894\n",
      "[4042]\ttrain-rmse:1.885756\n",
      "[4043]\ttrain-rmse:1.885637\n",
      "[4044]\ttrain-rmse:1.885518\n",
      "[4045]\ttrain-rmse:1.885398\n",
      "[4046]\ttrain-rmse:1.885263\n",
      "[4047]\ttrain-rmse:1.885136\n",
      "[4048]\ttrain-rmse:1.885018\n",
      "[4049]\ttrain-rmse:1.884901\n",
      "[4050]\ttrain-rmse:1.884778\n",
      "[4051]\ttrain-rmse:1.884648\n",
      "[4052]\ttrain-rmse:1.884527\n",
      "[4053]\ttrain-rmse:1.884400\n",
      "[4054]\ttrain-rmse:1.884282\n",
      "[4055]\ttrain-rmse:1.884169\n",
      "[4056]\ttrain-rmse:1.884041\n",
      "[4057]\ttrain-rmse:1.883923\n",
      "[4058]\ttrain-rmse:1.883806\n",
      "[4059]\ttrain-rmse:1.883684\n",
      "[4060]\ttrain-rmse:1.883554\n",
      "[4061]\ttrain-rmse:1.883423\n",
      "[4062]\ttrain-rmse:1.883301\n",
      "[4063]\ttrain-rmse:1.883193\n",
      "[4064]\ttrain-rmse:1.883073\n",
      "[4065]\ttrain-rmse:1.882950\n",
      "[4066]\ttrain-rmse:1.882836\n",
      "[4067]\ttrain-rmse:1.882716\n",
      "[4068]\ttrain-rmse:1.882601\n",
      "[4069]\ttrain-rmse:1.882488\n",
      "[4070]\ttrain-rmse:1.882365\n",
      "[4071]\ttrain-rmse:1.882249\n",
      "[4072]\ttrain-rmse:1.882132\n",
      "[4073]\ttrain-rmse:1.882021\n",
      "[4074]\ttrain-rmse:1.881905\n",
      "[4075]\ttrain-rmse:1.881786\n",
      "[4076]\ttrain-rmse:1.881669\n",
      "[4077]\ttrain-rmse:1.881546\n",
      "[4078]\ttrain-rmse:1.881430\n",
      "[4079]\ttrain-rmse:1.881307\n",
      "[4080]\ttrain-rmse:1.881198\n",
      "[4081]\ttrain-rmse:1.881061\n",
      "[4082]\ttrain-rmse:1.880943\n",
      "[4083]\ttrain-rmse:1.880824\n",
      "[4084]\ttrain-rmse:1.880708\n",
      "[4085]\ttrain-rmse:1.880587\n",
      "[4086]\ttrain-rmse:1.880463\n",
      "[4087]\ttrain-rmse:1.880340\n",
      "[4088]\ttrain-rmse:1.880224\n",
      "[4089]\ttrain-rmse:1.880102\n",
      "[4090]\ttrain-rmse:1.879987\n",
      "[4091]\ttrain-rmse:1.879866\n",
      "[4092]\ttrain-rmse:1.879749\n",
      "[4093]\ttrain-rmse:1.879633\n",
      "[4094]\ttrain-rmse:1.879513\n",
      "[4095]\ttrain-rmse:1.879398\n",
      "[4096]\ttrain-rmse:1.879283\n",
      "[4097]\ttrain-rmse:1.879176\n",
      "[4098]\ttrain-rmse:1.879061\n",
      "[4099]\ttrain-rmse:1.878947\n",
      "[4100]\ttrain-rmse:1.878835\n",
      "[4101]\ttrain-rmse:1.878716\n",
      "[4102]\ttrain-rmse:1.878599\n",
      "[4103]\ttrain-rmse:1.878480\n",
      "[4104]\ttrain-rmse:1.878354\n",
      "[4105]\ttrain-rmse:1.878241\n",
      "[4106]\ttrain-rmse:1.878128\n",
      "[4107]\ttrain-rmse:1.878006\n",
      "[4108]\ttrain-rmse:1.877896\n",
      "[4109]\ttrain-rmse:1.877780\n",
      "[4110]\ttrain-rmse:1.877663\n",
      "[4111]\ttrain-rmse:1.877553\n",
      "[4112]\ttrain-rmse:1.877438\n",
      "[4113]\ttrain-rmse:1.877328\n",
      "[4114]\ttrain-rmse:1.877214\n",
      "[4115]\ttrain-rmse:1.877091\n",
      "[4116]\ttrain-rmse:1.876972\n",
      "[4117]\ttrain-rmse:1.876854\n",
      "[4118]\ttrain-rmse:1.876736\n",
      "[4119]\ttrain-rmse:1.876628\n",
      "[4120]\ttrain-rmse:1.876512\n",
      "[4121]\ttrain-rmse:1.876397\n",
      "[4122]\ttrain-rmse:1.876278\n",
      "[4123]\ttrain-rmse:1.876157\n",
      "[4124]\ttrain-rmse:1.876046\n",
      "[4125]\ttrain-rmse:1.875927\n",
      "[4126]\ttrain-rmse:1.875808\n",
      "[4127]\ttrain-rmse:1.875689\n",
      "[4128]\ttrain-rmse:1.875574\n",
      "[4129]\ttrain-rmse:1.875459\n",
      "[4130]\ttrain-rmse:1.875349\n",
      "[4131]\ttrain-rmse:1.875243\n",
      "[4132]\ttrain-rmse:1.875129\n",
      "[4133]\ttrain-rmse:1.875016\n",
      "[4134]\ttrain-rmse:1.874903\n",
      "[4135]\ttrain-rmse:1.874776\n",
      "[4136]\ttrain-rmse:1.874663\n",
      "[4137]\ttrain-rmse:1.874555\n",
      "[4138]\ttrain-rmse:1.874437\n",
      "[4139]\ttrain-rmse:1.874327\n",
      "[4140]\ttrain-rmse:1.874209\n",
      "[4141]\ttrain-rmse:1.874097\n",
      "[4142]\ttrain-rmse:1.873978\n",
      "[4143]\ttrain-rmse:1.873867\n",
      "[4144]\ttrain-rmse:1.873760\n",
      "[4145]\ttrain-rmse:1.873642\n",
      "[4146]\ttrain-rmse:1.873530\n",
      "[4147]\ttrain-rmse:1.873424\n",
      "[4148]\ttrain-rmse:1.873310\n",
      "[4149]\ttrain-rmse:1.873194\n",
      "[4150]\ttrain-rmse:1.873080\n",
      "[4151]\ttrain-rmse:1.872956\n",
      "[4152]\ttrain-rmse:1.872840\n",
      "[4153]\ttrain-rmse:1.872729\n",
      "[4154]\ttrain-rmse:1.872612\n",
      "[4155]\ttrain-rmse:1.872496\n",
      "[4156]\ttrain-rmse:1.872390\n",
      "[4157]\ttrain-rmse:1.872283\n",
      "[4158]\ttrain-rmse:1.872170\n",
      "[4159]\ttrain-rmse:1.872060\n",
      "[4160]\ttrain-rmse:1.871949\n",
      "[4161]\ttrain-rmse:1.871838\n",
      "[4162]\ttrain-rmse:1.871734\n",
      "[4163]\ttrain-rmse:1.871620\n",
      "[4164]\ttrain-rmse:1.871514\n",
      "[4165]\ttrain-rmse:1.871405\n",
      "[4166]\ttrain-rmse:1.871288\n",
      "[4167]\ttrain-rmse:1.871181\n",
      "[4168]\ttrain-rmse:1.871071\n",
      "[4169]\ttrain-rmse:1.870967\n",
      "[4170]\ttrain-rmse:1.870857\n",
      "[4171]\ttrain-rmse:1.870740\n",
      "[4172]\ttrain-rmse:1.870636\n",
      "[4173]\ttrain-rmse:1.870530\n",
      "[4174]\ttrain-rmse:1.870424\n",
      "[4175]\ttrain-rmse:1.870314\n",
      "[4176]\ttrain-rmse:1.870202\n",
      "[4177]\ttrain-rmse:1.870085\n",
      "[4178]\ttrain-rmse:1.869985\n",
      "[4179]\ttrain-rmse:1.869871\n",
      "[4180]\ttrain-rmse:1.869769\n",
      "[4181]\ttrain-rmse:1.869660\n",
      "[4182]\ttrain-rmse:1.869546\n",
      "[4183]\ttrain-rmse:1.869434\n",
      "[4184]\ttrain-rmse:1.869327\n",
      "[4185]\ttrain-rmse:1.869207\n",
      "[4186]\ttrain-rmse:1.869098\n",
      "[4187]\ttrain-rmse:1.868981\n",
      "[4188]\ttrain-rmse:1.868876\n",
      "[4189]\ttrain-rmse:1.868762\n",
      "[4190]\ttrain-rmse:1.868647\n",
      "[4191]\ttrain-rmse:1.868543\n",
      "[4192]\ttrain-rmse:1.868431\n",
      "[4193]\ttrain-rmse:1.868311\n",
      "[4194]\ttrain-rmse:1.868207\n",
      "[4195]\ttrain-rmse:1.868108\n",
      "[4196]\ttrain-rmse:1.868000\n",
      "[4197]\ttrain-rmse:1.867895\n",
      "[4198]\ttrain-rmse:1.867783\n",
      "[4199]\ttrain-rmse:1.867682\n",
      "[4200]\ttrain-rmse:1.867579\n",
      "[4201]\ttrain-rmse:1.867471\n",
      "[4202]\ttrain-rmse:1.867362\n",
      "[4203]\ttrain-rmse:1.867245\n",
      "[4204]\ttrain-rmse:1.867134\n",
      "[4205]\ttrain-rmse:1.867034\n",
      "[4206]\ttrain-rmse:1.866922\n",
      "[4207]\ttrain-rmse:1.866817\n",
      "[4208]\ttrain-rmse:1.866704\n",
      "[4209]\ttrain-rmse:1.866607\n",
      "[4210]\ttrain-rmse:1.866489\n",
      "[4211]\ttrain-rmse:1.866385\n",
      "[4212]\ttrain-rmse:1.866271\n",
      "[4213]\ttrain-rmse:1.866161\n",
      "[4214]\ttrain-rmse:1.866043\n",
      "[4215]\ttrain-rmse:1.865935\n",
      "[4216]\ttrain-rmse:1.865829\n",
      "[4217]\ttrain-rmse:1.865729\n",
      "[4218]\ttrain-rmse:1.865613\n",
      "[4219]\ttrain-rmse:1.865512\n",
      "[4220]\ttrain-rmse:1.865401\n",
      "[4221]\ttrain-rmse:1.865288\n",
      "[4222]\ttrain-rmse:1.865186\n",
      "[4223]\ttrain-rmse:1.865090\n",
      "[4224]\ttrain-rmse:1.864982\n",
      "[4225]\ttrain-rmse:1.864866\n",
      "[4226]\ttrain-rmse:1.864760\n",
      "[4227]\ttrain-rmse:1.864653\n",
      "[4228]\ttrain-rmse:1.864551\n",
      "[4229]\ttrain-rmse:1.864447\n",
      "[4230]\ttrain-rmse:1.864341\n",
      "[4231]\ttrain-rmse:1.864237\n",
      "[4232]\ttrain-rmse:1.864135\n",
      "[4233]\ttrain-rmse:1.864027\n",
      "[4234]\ttrain-rmse:1.863921\n",
      "[4235]\ttrain-rmse:1.863809\n",
      "[4236]\ttrain-rmse:1.863703\n",
      "[4237]\ttrain-rmse:1.863604\n",
      "[4238]\ttrain-rmse:1.863495\n",
      "[4239]\ttrain-rmse:1.863396\n",
      "[4240]\ttrain-rmse:1.863288\n",
      "[4241]\ttrain-rmse:1.863188\n",
      "[4242]\ttrain-rmse:1.863075\n",
      "[4243]\ttrain-rmse:1.862970\n",
      "[4244]\ttrain-rmse:1.862871\n",
      "[4245]\ttrain-rmse:1.862773\n",
      "[4246]\ttrain-rmse:1.862674\n",
      "[4247]\ttrain-rmse:1.862557\n",
      "[4248]\ttrain-rmse:1.862446\n",
      "[4249]\ttrain-rmse:1.862341\n",
      "[4250]\ttrain-rmse:1.862234\n",
      "[4251]\ttrain-rmse:1.862135\n",
      "[4252]\ttrain-rmse:1.862033\n",
      "[4253]\ttrain-rmse:1.861931\n",
      "[4254]\ttrain-rmse:1.861831\n",
      "[4255]\ttrain-rmse:1.861725\n",
      "[4256]\ttrain-rmse:1.861618\n",
      "[4257]\ttrain-rmse:1.861523\n",
      "[4258]\ttrain-rmse:1.861424\n",
      "[4259]\ttrain-rmse:1.861314\n",
      "[4260]\ttrain-rmse:1.861214\n",
      "[4261]\ttrain-rmse:1.861108\n",
      "[4262]\ttrain-rmse:1.861009\n",
      "[4263]\ttrain-rmse:1.860904\n",
      "[4264]\ttrain-rmse:1.860803\n",
      "[4265]\ttrain-rmse:1.860705\n",
      "[4266]\ttrain-rmse:1.860600\n",
      "[4267]\ttrain-rmse:1.860493\n",
      "[4268]\ttrain-rmse:1.860400\n",
      "[4269]\ttrain-rmse:1.860299\n",
      "[4270]\ttrain-rmse:1.860200\n",
      "[4271]\ttrain-rmse:1.860101\n",
      "[4272]\ttrain-rmse:1.860007\n",
      "[4273]\ttrain-rmse:1.859903\n",
      "[4274]\ttrain-rmse:1.859804\n",
      "[4275]\ttrain-rmse:1.859703\n",
      "[4276]\ttrain-rmse:1.859603\n",
      "[4277]\ttrain-rmse:1.859499\n",
      "[4278]\ttrain-rmse:1.859394\n",
      "[4279]\ttrain-rmse:1.859289\n",
      "[4280]\ttrain-rmse:1.859189\n",
      "[4281]\ttrain-rmse:1.859080\n",
      "[4282]\ttrain-rmse:1.858981\n",
      "[4283]\ttrain-rmse:1.858881\n",
      "[4284]\ttrain-rmse:1.858778\n",
      "[4285]\ttrain-rmse:1.858675\n",
      "[4286]\ttrain-rmse:1.858569\n",
      "[4287]\ttrain-rmse:1.858473\n",
      "[4288]\ttrain-rmse:1.858374\n",
      "[4289]\ttrain-rmse:1.858271\n",
      "[4290]\ttrain-rmse:1.858173\n",
      "[4291]\ttrain-rmse:1.858068\n",
      "[4292]\ttrain-rmse:1.857965\n",
      "[4293]\ttrain-rmse:1.857866\n",
      "[4294]\ttrain-rmse:1.857766\n",
      "[4295]\ttrain-rmse:1.857677\n",
      "[4296]\ttrain-rmse:1.857581\n",
      "[4297]\ttrain-rmse:1.857479\n",
      "[4298]\ttrain-rmse:1.857372\n",
      "[4299]\ttrain-rmse:1.857276\n",
      "[4300]\ttrain-rmse:1.857165\n",
      "[4301]\ttrain-rmse:1.857059\n",
      "[4302]\ttrain-rmse:1.856967\n",
      "[4303]\ttrain-rmse:1.856863\n",
      "[4304]\ttrain-rmse:1.856760\n",
      "[4305]\ttrain-rmse:1.856658\n",
      "[4306]\ttrain-rmse:1.856554\n",
      "[4307]\ttrain-rmse:1.856454\n",
      "[4308]\ttrain-rmse:1.856355\n",
      "[4309]\ttrain-rmse:1.856252\n",
      "[4310]\ttrain-rmse:1.856149\n",
      "[4311]\ttrain-rmse:1.856054\n",
      "[4312]\ttrain-rmse:1.855950\n",
      "[4313]\ttrain-rmse:1.855852\n",
      "[4314]\ttrain-rmse:1.855754\n",
      "[4315]\ttrain-rmse:1.855652\n",
      "[4316]\ttrain-rmse:1.855550\n",
      "[4317]\ttrain-rmse:1.855450\n",
      "[4318]\ttrain-rmse:1.855349\n",
      "[4319]\ttrain-rmse:1.855253\n",
      "[4320]\ttrain-rmse:1.855147\n",
      "[4321]\ttrain-rmse:1.855044\n",
      "[4322]\ttrain-rmse:1.854941\n",
      "[4323]\ttrain-rmse:1.854852\n",
      "[4324]\ttrain-rmse:1.854744\n",
      "[4325]\ttrain-rmse:1.854641\n",
      "[4326]\ttrain-rmse:1.854545\n",
      "[4327]\ttrain-rmse:1.854447\n",
      "[4328]\ttrain-rmse:1.854356\n",
      "[4329]\ttrain-rmse:1.854253\n",
      "[4330]\ttrain-rmse:1.854161\n",
      "[4331]\ttrain-rmse:1.854067\n",
      "[4332]\ttrain-rmse:1.853966\n",
      "[4333]\ttrain-rmse:1.853870\n",
      "[4334]\ttrain-rmse:1.853759\n",
      "[4335]\ttrain-rmse:1.853658\n",
      "[4336]\ttrain-rmse:1.853558\n",
      "[4337]\ttrain-rmse:1.853457\n",
      "[4338]\ttrain-rmse:1.853358\n",
      "[4339]\ttrain-rmse:1.853266\n",
      "[4340]\ttrain-rmse:1.853171\n",
      "[4341]\ttrain-rmse:1.853076\n",
      "[4342]\ttrain-rmse:1.852970\n",
      "[4343]\ttrain-rmse:1.852876\n",
      "[4344]\ttrain-rmse:1.852782\n",
      "[4345]\ttrain-rmse:1.852688\n",
      "[4346]\ttrain-rmse:1.852589\n",
      "[4347]\ttrain-rmse:1.852499\n",
      "[4348]\ttrain-rmse:1.852399\n",
      "[4349]\ttrain-rmse:1.852305\n",
      "[4350]\ttrain-rmse:1.852203\n",
      "[4351]\ttrain-rmse:1.852107\n",
      "[4352]\ttrain-rmse:1.852012\n",
      "[4353]\ttrain-rmse:1.851926\n",
      "[4354]\ttrain-rmse:1.851831\n",
      "[4355]\ttrain-rmse:1.851728\n",
      "[4356]\ttrain-rmse:1.851624\n",
      "[4357]\ttrain-rmse:1.851522\n",
      "[4358]\ttrain-rmse:1.851431\n",
      "[4359]\ttrain-rmse:1.851329\n",
      "[4360]\ttrain-rmse:1.851224\n",
      "[4361]\ttrain-rmse:1.851127\n",
      "[4362]\ttrain-rmse:1.851026\n",
      "[4363]\ttrain-rmse:1.850925\n",
      "[4364]\ttrain-rmse:1.850826\n",
      "[4365]\ttrain-rmse:1.850732\n",
      "[4366]\ttrain-rmse:1.850635\n",
      "[4367]\ttrain-rmse:1.850541\n",
      "[4368]\ttrain-rmse:1.850441\n",
      "[4369]\ttrain-rmse:1.850336\n",
      "[4370]\ttrain-rmse:1.850244\n",
      "[4371]\ttrain-rmse:1.850148\n",
      "[4372]\ttrain-rmse:1.850054\n",
      "[4373]\ttrain-rmse:1.849957\n",
      "[4374]\ttrain-rmse:1.849870\n",
      "[4375]\ttrain-rmse:1.849773\n",
      "[4376]\ttrain-rmse:1.849680\n",
      "[4377]\ttrain-rmse:1.849580\n",
      "[4378]\ttrain-rmse:1.849486\n",
      "[4379]\ttrain-rmse:1.849392\n",
      "[4380]\ttrain-rmse:1.849304\n",
      "[4381]\ttrain-rmse:1.849214\n",
      "[4382]\ttrain-rmse:1.849113\n",
      "[4383]\ttrain-rmse:1.849013\n",
      "[4384]\ttrain-rmse:1.848917\n",
      "[4385]\ttrain-rmse:1.848821\n",
      "[4386]\ttrain-rmse:1.848723\n",
      "[4387]\ttrain-rmse:1.848632\n",
      "[4388]\ttrain-rmse:1.848541\n",
      "[4389]\ttrain-rmse:1.848451\n",
      "[4390]\ttrain-rmse:1.848364\n",
      "[4391]\ttrain-rmse:1.848266\n",
      "[4392]\ttrain-rmse:1.848165\n",
      "[4393]\ttrain-rmse:1.848073\n",
      "[4394]\ttrain-rmse:1.847974\n",
      "[4395]\ttrain-rmse:1.847879\n",
      "[4396]\ttrain-rmse:1.847786\n",
      "[4397]\ttrain-rmse:1.847695\n",
      "[4398]\ttrain-rmse:1.847602\n",
      "[4399]\ttrain-rmse:1.847513\n",
      "[4400]\ttrain-rmse:1.847416\n",
      "[4401]\ttrain-rmse:1.847322\n",
      "[4402]\ttrain-rmse:1.847232\n",
      "[4403]\ttrain-rmse:1.847140\n",
      "[4404]\ttrain-rmse:1.847051\n",
      "[4405]\ttrain-rmse:1.846956\n",
      "[4406]\ttrain-rmse:1.846852\n",
      "[4407]\ttrain-rmse:1.846763\n",
      "[4408]\ttrain-rmse:1.846673\n",
      "[4409]\ttrain-rmse:1.846585\n",
      "[4410]\ttrain-rmse:1.846499\n",
      "[4411]\ttrain-rmse:1.846410\n",
      "[4412]\ttrain-rmse:1.846316\n",
      "[4413]\ttrain-rmse:1.846219\n",
      "[4414]\ttrain-rmse:1.846130\n",
      "[4415]\ttrain-rmse:1.846039\n",
      "[4416]\ttrain-rmse:1.845950\n",
      "[4417]\ttrain-rmse:1.845865\n",
      "[4418]\ttrain-rmse:1.845773\n",
      "[4419]\ttrain-rmse:1.845677\n",
      "[4420]\ttrain-rmse:1.845589\n",
      "[4421]\ttrain-rmse:1.845498\n",
      "[4422]\ttrain-rmse:1.845403\n",
      "[4423]\ttrain-rmse:1.845310\n",
      "[4424]\ttrain-rmse:1.845220\n",
      "[4425]\ttrain-rmse:1.845129\n",
      "[4426]\ttrain-rmse:1.845043\n",
      "[4427]\ttrain-rmse:1.844952\n",
      "[4428]\ttrain-rmse:1.844858\n",
      "[4429]\ttrain-rmse:1.844770\n",
      "[4430]\ttrain-rmse:1.844680\n",
      "[4431]\ttrain-rmse:1.844588\n",
      "[4432]\ttrain-rmse:1.844495\n",
      "[4433]\ttrain-rmse:1.844409\n",
      "[4434]\ttrain-rmse:1.844317\n",
      "[4435]\ttrain-rmse:1.844228\n",
      "[4436]\ttrain-rmse:1.844141\n",
      "[4437]\ttrain-rmse:1.844059\n",
      "[4438]\ttrain-rmse:1.843975\n",
      "[4439]\ttrain-rmse:1.843889\n",
      "[4440]\ttrain-rmse:1.843801\n",
      "[4441]\ttrain-rmse:1.843711\n",
      "[4442]\ttrain-rmse:1.843626\n",
      "[4443]\ttrain-rmse:1.843538\n",
      "[4444]\ttrain-rmse:1.843456\n",
      "[4445]\ttrain-rmse:1.843368\n",
      "[4446]\ttrain-rmse:1.843279\n",
      "[4447]\ttrain-rmse:1.843181\n",
      "[4448]\ttrain-rmse:1.843087\n",
      "[4449]\ttrain-rmse:1.842992\n",
      "[4450]\ttrain-rmse:1.842903\n",
      "[4451]\ttrain-rmse:1.842819\n",
      "[4452]\ttrain-rmse:1.842722\n",
      "[4453]\ttrain-rmse:1.842639\n",
      "[4454]\ttrain-rmse:1.842552\n",
      "[4455]\ttrain-rmse:1.842453\n",
      "[4456]\ttrain-rmse:1.842358\n",
      "[4457]\ttrain-rmse:1.842268\n",
      "[4458]\ttrain-rmse:1.842185\n",
      "[4459]\ttrain-rmse:1.842093\n",
      "[4460]\ttrain-rmse:1.842001\n",
      "[4461]\ttrain-rmse:1.841922\n",
      "[4462]\ttrain-rmse:1.841831\n",
      "[4463]\ttrain-rmse:1.841740\n",
      "[4464]\ttrain-rmse:1.841656\n",
      "[4465]\ttrain-rmse:1.841570\n",
      "[4466]\ttrain-rmse:1.841480\n",
      "[4467]\ttrain-rmse:1.841387\n",
      "[4468]\ttrain-rmse:1.841303\n",
      "[4469]\ttrain-rmse:1.841216\n",
      "[4470]\ttrain-rmse:1.841119\n",
      "[4471]\ttrain-rmse:1.841038\n",
      "[4472]\ttrain-rmse:1.840949\n",
      "[4473]\ttrain-rmse:1.840857\n",
      "[4474]\ttrain-rmse:1.840771\n",
      "[4475]\ttrain-rmse:1.840691\n",
      "[4476]\ttrain-rmse:1.840598\n",
      "[4477]\ttrain-rmse:1.840513\n",
      "[4478]\ttrain-rmse:1.840427\n",
      "[4479]\ttrain-rmse:1.840338\n",
      "[4480]\ttrain-rmse:1.840246\n",
      "[4481]\ttrain-rmse:1.840154\n",
      "[4482]\ttrain-rmse:1.840064\n",
      "[4483]\ttrain-rmse:1.839978\n",
      "[4484]\ttrain-rmse:1.839893\n",
      "[4485]\ttrain-rmse:1.839804\n",
      "[4486]\ttrain-rmse:1.839722\n",
      "[4487]\ttrain-rmse:1.839630\n",
      "[4488]\ttrain-rmse:1.839546\n",
      "[4489]\ttrain-rmse:1.839454\n",
      "[4490]\ttrain-rmse:1.839364\n",
      "[4491]\ttrain-rmse:1.839277\n",
      "[4492]\ttrain-rmse:1.839189\n",
      "[4493]\ttrain-rmse:1.839102\n",
      "[4494]\ttrain-rmse:1.839014\n",
      "[4495]\ttrain-rmse:1.838926\n",
      "[4496]\ttrain-rmse:1.838837\n",
      "[4497]\ttrain-rmse:1.838750\n",
      "[4498]\ttrain-rmse:1.838665\n",
      "[4499]\ttrain-rmse:1.838583\n",
      "[4500]\ttrain-rmse:1.838496\n",
      "[4501]\ttrain-rmse:1.838410\n",
      "[4502]\ttrain-rmse:1.838327\n",
      "[4503]\ttrain-rmse:1.838244\n",
      "[4504]\ttrain-rmse:1.838158\n",
      "[4505]\ttrain-rmse:1.838078\n",
      "[4506]\ttrain-rmse:1.837995\n",
      "[4507]\ttrain-rmse:1.837906\n",
      "[4508]\ttrain-rmse:1.837824\n",
      "[4509]\ttrain-rmse:1.837736\n",
      "[4510]\ttrain-rmse:1.837642\n",
      "[4511]\ttrain-rmse:1.837551\n",
      "[4512]\ttrain-rmse:1.837468\n",
      "[4513]\ttrain-rmse:1.837383\n",
      "[4514]\ttrain-rmse:1.837292\n",
      "[4515]\ttrain-rmse:1.837206\n",
      "[4516]\ttrain-rmse:1.837126\n",
      "[4517]\ttrain-rmse:1.837038\n",
      "[4518]\ttrain-rmse:1.836956\n",
      "[4519]\ttrain-rmse:1.836867\n",
      "[4520]\ttrain-rmse:1.836772\n",
      "[4521]\ttrain-rmse:1.836680\n",
      "[4522]\ttrain-rmse:1.836591\n",
      "[4523]\ttrain-rmse:1.836514\n",
      "[4524]\ttrain-rmse:1.836428\n",
      "[4525]\ttrain-rmse:1.836334\n",
      "[4526]\ttrain-rmse:1.836251\n",
      "[4527]\ttrain-rmse:1.836162\n",
      "[4528]\ttrain-rmse:1.836079\n",
      "[4529]\ttrain-rmse:1.835994\n",
      "[4530]\ttrain-rmse:1.835903\n",
      "[4531]\ttrain-rmse:1.835824\n",
      "[4532]\ttrain-rmse:1.835753\n",
      "[4533]\ttrain-rmse:1.835670\n",
      "[4534]\ttrain-rmse:1.835586\n",
      "[4535]\ttrain-rmse:1.835503\n",
      "[4536]\ttrain-rmse:1.835411\n",
      "[4537]\ttrain-rmse:1.835325\n",
      "[4538]\ttrain-rmse:1.835251\n",
      "[4539]\ttrain-rmse:1.835173\n",
      "[4540]\ttrain-rmse:1.835091\n",
      "[4541]\ttrain-rmse:1.835003\n",
      "[4542]\ttrain-rmse:1.834919\n",
      "[4543]\ttrain-rmse:1.834842\n",
      "[4544]\ttrain-rmse:1.834761\n",
      "[4545]\ttrain-rmse:1.834682\n",
      "[4546]\ttrain-rmse:1.834602\n",
      "[4547]\ttrain-rmse:1.834526\n",
      "[4548]\ttrain-rmse:1.834444\n",
      "[4549]\ttrain-rmse:1.834362\n",
      "[4550]\ttrain-rmse:1.834275\n",
      "[4551]\ttrain-rmse:1.834198\n",
      "[4552]\ttrain-rmse:1.834120\n",
      "[4553]\ttrain-rmse:1.834043\n",
      "[4554]\ttrain-rmse:1.833967\n",
      "[4555]\ttrain-rmse:1.833880\n",
      "[4556]\ttrain-rmse:1.833796\n",
      "[4557]\ttrain-rmse:1.833714\n",
      "[4558]\ttrain-rmse:1.833628\n",
      "[4559]\ttrain-rmse:1.833544\n",
      "[4560]\ttrain-rmse:1.833455\n",
      "[4561]\ttrain-rmse:1.833371\n",
      "[4562]\ttrain-rmse:1.833298\n",
      "[4563]\ttrain-rmse:1.833217\n",
      "[4564]\ttrain-rmse:1.833133\n",
      "[4565]\ttrain-rmse:1.833055\n",
      "[4566]\ttrain-rmse:1.832974\n",
      "[4567]\ttrain-rmse:1.832886\n",
      "[4568]\ttrain-rmse:1.832806\n",
      "[4569]\ttrain-rmse:1.832721\n",
      "[4570]\ttrain-rmse:1.832633\n",
      "[4571]\ttrain-rmse:1.832538\n",
      "[4572]\ttrain-rmse:1.832465\n",
      "[4573]\ttrain-rmse:1.832385\n",
      "[4574]\ttrain-rmse:1.832302\n",
      "[4575]\ttrain-rmse:1.832217\n",
      "[4576]\ttrain-rmse:1.832135\n",
      "[4577]\ttrain-rmse:1.832051\n",
      "[4578]\ttrain-rmse:1.831972\n",
      "[4579]\ttrain-rmse:1.831891\n",
      "[4580]\ttrain-rmse:1.831793\n",
      "[4581]\ttrain-rmse:1.831711\n",
      "[4582]\ttrain-rmse:1.831630\n",
      "[4583]\ttrain-rmse:1.831545\n",
      "[4584]\ttrain-rmse:1.831463\n",
      "[4585]\ttrain-rmse:1.831379\n",
      "[4586]\ttrain-rmse:1.831289\n",
      "[4587]\ttrain-rmse:1.831215\n",
      "[4588]\ttrain-rmse:1.831135\n",
      "[4589]\ttrain-rmse:1.831057\n",
      "[4590]\ttrain-rmse:1.830980\n",
      "[4591]\ttrain-rmse:1.830892\n",
      "[4592]\ttrain-rmse:1.830810\n",
      "[4593]\ttrain-rmse:1.830732\n",
      "[4594]\ttrain-rmse:1.830655\n",
      "[4595]\ttrain-rmse:1.830572\n",
      "[4596]\ttrain-rmse:1.830494\n",
      "[4597]\ttrain-rmse:1.830406\n",
      "[4598]\ttrain-rmse:1.830321\n",
      "[4599]\ttrain-rmse:1.830241\n",
      "[4600]\ttrain-rmse:1.830158\n",
      "[4601]\ttrain-rmse:1.830065\n",
      "[4602]\ttrain-rmse:1.829980\n",
      "[4603]\ttrain-rmse:1.829901\n",
      "[4604]\ttrain-rmse:1.829824\n",
      "[4605]\ttrain-rmse:1.829740\n",
      "[4606]\ttrain-rmse:1.829659\n",
      "[4607]\ttrain-rmse:1.829583\n",
      "[4608]\ttrain-rmse:1.829500\n",
      "[4609]\ttrain-rmse:1.829418\n",
      "[4610]\ttrain-rmse:1.829337\n",
      "[4611]\ttrain-rmse:1.829259\n",
      "[4612]\ttrain-rmse:1.829173\n",
      "[4613]\ttrain-rmse:1.829094\n",
      "[4614]\ttrain-rmse:1.829003\n",
      "[4615]\ttrain-rmse:1.828912\n",
      "[4616]\ttrain-rmse:1.828839\n",
      "[4617]\ttrain-rmse:1.828757\n",
      "[4618]\ttrain-rmse:1.828677\n",
      "[4619]\ttrain-rmse:1.828589\n",
      "[4620]\ttrain-rmse:1.828510\n",
      "[4621]\ttrain-rmse:1.828429\n",
      "[4622]\ttrain-rmse:1.828357\n",
      "[4623]\ttrain-rmse:1.828272\n",
      "[4624]\ttrain-rmse:1.828187\n",
      "[4625]\ttrain-rmse:1.828107\n",
      "[4626]\ttrain-rmse:1.828033\n",
      "[4627]\ttrain-rmse:1.827941\n",
      "[4628]\ttrain-rmse:1.827861\n",
      "[4629]\ttrain-rmse:1.827784\n",
      "[4630]\ttrain-rmse:1.827705\n",
      "[4631]\ttrain-rmse:1.827634\n",
      "[4632]\ttrain-rmse:1.827552\n",
      "[4633]\ttrain-rmse:1.827474\n",
      "[4634]\ttrain-rmse:1.827393\n",
      "[4635]\ttrain-rmse:1.827314\n",
      "[4636]\ttrain-rmse:1.827233\n",
      "[4637]\ttrain-rmse:1.827156\n",
      "[4638]\ttrain-rmse:1.827081\n",
      "[4639]\ttrain-rmse:1.827006\n",
      "[4640]\ttrain-rmse:1.826931\n",
      "[4641]\ttrain-rmse:1.826854\n",
      "[4642]\ttrain-rmse:1.826775\n",
      "[4643]\ttrain-rmse:1.826700\n",
      "[4644]\ttrain-rmse:1.826617\n",
      "[4645]\ttrain-rmse:1.826536\n",
      "[4646]\ttrain-rmse:1.826458\n",
      "[4647]\ttrain-rmse:1.826383\n",
      "[4648]\ttrain-rmse:1.826308\n",
      "[4649]\ttrain-rmse:1.826230\n",
      "[4650]\ttrain-rmse:1.826154\n",
      "[4651]\ttrain-rmse:1.826074\n",
      "[4652]\ttrain-rmse:1.825996\n",
      "[4653]\ttrain-rmse:1.825925\n",
      "[4654]\ttrain-rmse:1.825851\n",
      "[4655]\ttrain-rmse:1.825768\n",
      "[4656]\ttrain-rmse:1.825685\n",
      "[4657]\ttrain-rmse:1.825609\n",
      "[4658]\ttrain-rmse:1.825537\n",
      "[4659]\ttrain-rmse:1.825460\n",
      "[4660]\ttrain-rmse:1.825390\n",
      "[4661]\ttrain-rmse:1.825301\n",
      "[4662]\ttrain-rmse:1.825215\n",
      "[4663]\ttrain-rmse:1.825135\n",
      "[4664]\ttrain-rmse:1.825058\n",
      "[4665]\ttrain-rmse:1.824985\n",
      "[4666]\ttrain-rmse:1.824908\n",
      "[4667]\ttrain-rmse:1.824835\n",
      "[4668]\ttrain-rmse:1.824756\n",
      "[4669]\ttrain-rmse:1.824685\n",
      "[4670]\ttrain-rmse:1.824606\n",
      "[4671]\ttrain-rmse:1.824528\n",
      "[4672]\ttrain-rmse:1.824455\n",
      "[4673]\ttrain-rmse:1.824384\n",
      "[4674]\ttrain-rmse:1.824312\n",
      "[4675]\ttrain-rmse:1.824235\n",
      "[4676]\ttrain-rmse:1.824156\n",
      "[4677]\ttrain-rmse:1.824077\n",
      "[4678]\ttrain-rmse:1.823996\n",
      "[4679]\ttrain-rmse:1.823922\n",
      "[4680]\ttrain-rmse:1.823853\n",
      "[4681]\ttrain-rmse:1.823784\n",
      "[4682]\ttrain-rmse:1.823704\n",
      "[4683]\ttrain-rmse:1.823630\n",
      "[4684]\ttrain-rmse:1.823551\n",
      "[4685]\ttrain-rmse:1.823473\n",
      "[4686]\ttrain-rmse:1.823392\n",
      "[4687]\ttrain-rmse:1.823320\n",
      "[4688]\ttrain-rmse:1.823249\n",
      "[4689]\ttrain-rmse:1.823181\n",
      "[4690]\ttrain-rmse:1.823111\n",
      "[4691]\ttrain-rmse:1.823032\n",
      "[4692]\ttrain-rmse:1.822949\n",
      "[4693]\ttrain-rmse:1.822873\n",
      "[4694]\ttrain-rmse:1.822797\n",
      "[4695]\ttrain-rmse:1.822727\n",
      "[4696]\ttrain-rmse:1.822651\n",
      "[4697]\ttrain-rmse:1.822576\n",
      "[4698]\ttrain-rmse:1.822502\n",
      "[4699]\ttrain-rmse:1.822424\n",
      "[4700]\ttrain-rmse:1.822344\n",
      "[4701]\ttrain-rmse:1.822264\n",
      "[4702]\ttrain-rmse:1.822188\n",
      "[4703]\ttrain-rmse:1.822108\n",
      "[4704]\ttrain-rmse:1.822038\n",
      "[4705]\ttrain-rmse:1.821968\n",
      "[4706]\ttrain-rmse:1.821887\n",
      "[4707]\ttrain-rmse:1.821814\n",
      "[4708]\ttrain-rmse:1.821745\n",
      "[4709]\ttrain-rmse:1.821671\n",
      "[4710]\ttrain-rmse:1.821590\n",
      "[4711]\ttrain-rmse:1.821517\n",
      "[4712]\ttrain-rmse:1.821440\n",
      "[4713]\ttrain-rmse:1.821363\n",
      "[4714]\ttrain-rmse:1.821290\n",
      "[4715]\ttrain-rmse:1.821221\n",
      "[4716]\ttrain-rmse:1.821144\n",
      "[4717]\ttrain-rmse:1.821071\n",
      "[4718]\ttrain-rmse:1.821003\n",
      "[4719]\ttrain-rmse:1.820928\n",
      "[4720]\ttrain-rmse:1.820853\n",
      "[4721]\ttrain-rmse:1.820787\n",
      "[4722]\ttrain-rmse:1.820709\n",
      "[4723]\ttrain-rmse:1.820626\n",
      "[4724]\ttrain-rmse:1.820554\n",
      "[4725]\ttrain-rmse:1.820487\n",
      "[4726]\ttrain-rmse:1.820413\n",
      "[4727]\ttrain-rmse:1.820335\n",
      "[4728]\ttrain-rmse:1.820263\n",
      "[4729]\ttrain-rmse:1.820176\n",
      "[4730]\ttrain-rmse:1.820102\n",
      "[4731]\ttrain-rmse:1.820028\n",
      "[4732]\ttrain-rmse:1.819959\n",
      "[4733]\ttrain-rmse:1.819888\n",
      "[4734]\ttrain-rmse:1.819815\n",
      "[4735]\ttrain-rmse:1.819741\n",
      "[4736]\ttrain-rmse:1.819669\n",
      "[4737]\ttrain-rmse:1.819595\n",
      "[4738]\ttrain-rmse:1.819521\n",
      "[4739]\ttrain-rmse:1.819445\n",
      "[4740]\ttrain-rmse:1.819369\n",
      "[4741]\ttrain-rmse:1.819297\n",
      "[4742]\ttrain-rmse:1.819225\n",
      "[4743]\ttrain-rmse:1.819147\n",
      "[4744]\ttrain-rmse:1.819073\n",
      "[4745]\ttrain-rmse:1.819000\n",
      "[4746]\ttrain-rmse:1.818926\n",
      "[4747]\ttrain-rmse:1.818850\n",
      "[4748]\ttrain-rmse:1.818779\n",
      "[4749]\ttrain-rmse:1.818700\n",
      "[4750]\ttrain-rmse:1.818632\n",
      "[4751]\ttrain-rmse:1.818559\n",
      "[4752]\ttrain-rmse:1.818491\n",
      "[4753]\ttrain-rmse:1.818428\n",
      "[4754]\ttrain-rmse:1.818361\n",
      "[4755]\ttrain-rmse:1.818285\n",
      "[4756]\ttrain-rmse:1.818208\n",
      "[4757]\ttrain-rmse:1.818135\n",
      "[4758]\ttrain-rmse:1.818060\n",
      "[4759]\ttrain-rmse:1.817980\n",
      "[4760]\ttrain-rmse:1.817908\n",
      "[4761]\ttrain-rmse:1.817832\n",
      "[4762]\ttrain-rmse:1.817750\n",
      "[4763]\ttrain-rmse:1.817679\n",
      "[4764]\ttrain-rmse:1.817610\n",
      "[4765]\ttrain-rmse:1.817546\n",
      "[4766]\ttrain-rmse:1.817472\n",
      "[4767]\ttrain-rmse:1.817397\n",
      "[4768]\ttrain-rmse:1.817330\n",
      "[4769]\ttrain-rmse:1.817263\n",
      "[4770]\ttrain-rmse:1.817192\n",
      "[4771]\ttrain-rmse:1.817116\n",
      "[4772]\ttrain-rmse:1.817045\n",
      "[4773]\ttrain-rmse:1.816972\n",
      "[4774]\ttrain-rmse:1.816911\n",
      "[4775]\ttrain-rmse:1.816844\n",
      "[4776]\ttrain-rmse:1.816773\n",
      "[4777]\ttrain-rmse:1.816707\n",
      "[4778]\ttrain-rmse:1.816628\n",
      "[4779]\ttrain-rmse:1.816557\n",
      "[4780]\ttrain-rmse:1.816480\n",
      "[4781]\ttrain-rmse:1.816411\n",
      "[4782]\ttrain-rmse:1.816341\n",
      "[4783]\ttrain-rmse:1.816262\n",
      "[4784]\ttrain-rmse:1.816190\n",
      "[4785]\ttrain-rmse:1.816114\n",
      "[4786]\ttrain-rmse:1.816054\n",
      "[4787]\ttrain-rmse:1.815979\n",
      "[4788]\ttrain-rmse:1.815913\n",
      "[4789]\ttrain-rmse:1.815849\n",
      "[4790]\ttrain-rmse:1.815786\n",
      "[4791]\ttrain-rmse:1.815710\n",
      "[4792]\ttrain-rmse:1.815647\n",
      "[4793]\ttrain-rmse:1.815580\n",
      "[4794]\ttrain-rmse:1.815508\n",
      "[4795]\ttrain-rmse:1.815441\n",
      "[4796]\ttrain-rmse:1.815371\n",
      "[4797]\ttrain-rmse:1.815302\n",
      "[4798]\ttrain-rmse:1.815237\n",
      "[4799]\ttrain-rmse:1.815162\n",
      "[4800]\ttrain-rmse:1.815080\n",
      "[4801]\ttrain-rmse:1.815014\n",
      "[4802]\ttrain-rmse:1.814936\n",
      "[4803]\ttrain-rmse:1.814877\n",
      "[4804]\ttrain-rmse:1.814798\n",
      "[4805]\ttrain-rmse:1.814730\n",
      "[4806]\ttrain-rmse:1.814663\n",
      "[4807]\ttrain-rmse:1.814590\n",
      "[4808]\ttrain-rmse:1.814520\n",
      "[4809]\ttrain-rmse:1.814452\n",
      "[4810]\ttrain-rmse:1.814390\n",
      "[4811]\ttrain-rmse:1.814318\n",
      "[4812]\ttrain-rmse:1.814245\n",
      "[4813]\ttrain-rmse:1.814174\n",
      "[4814]\ttrain-rmse:1.814112\n",
      "[4815]\ttrain-rmse:1.814047\n",
      "[4816]\ttrain-rmse:1.813987\n",
      "[4817]\ttrain-rmse:1.813920\n",
      "[4818]\ttrain-rmse:1.813845\n",
      "[4819]\ttrain-rmse:1.813774\n",
      "[4820]\ttrain-rmse:1.813698\n",
      "[4821]\ttrain-rmse:1.813627\n",
      "[4822]\ttrain-rmse:1.813560\n",
      "[4823]\ttrain-rmse:1.813494\n",
      "[4824]\ttrain-rmse:1.813426\n",
      "[4825]\ttrain-rmse:1.813365\n",
      "[4826]\ttrain-rmse:1.813292\n",
      "[4827]\ttrain-rmse:1.813226\n",
      "[4828]\ttrain-rmse:1.813155\n",
      "[4829]\ttrain-rmse:1.813088\n",
      "[4830]\ttrain-rmse:1.813021\n",
      "[4831]\ttrain-rmse:1.812957\n",
      "[4832]\ttrain-rmse:1.812891\n",
      "[4833]\ttrain-rmse:1.812819\n",
      "[4834]\ttrain-rmse:1.812749\n",
      "[4835]\ttrain-rmse:1.812683\n",
      "[4836]\ttrain-rmse:1.812618\n",
      "[4837]\ttrain-rmse:1.812557\n",
      "[4838]\ttrain-rmse:1.812481\n",
      "[4839]\ttrain-rmse:1.812411\n",
      "[4840]\ttrain-rmse:1.812347\n",
      "[4841]\ttrain-rmse:1.812280\n",
      "[4842]\ttrain-rmse:1.812217\n",
      "[4843]\ttrain-rmse:1.812155\n",
      "[4844]\ttrain-rmse:1.812088\n",
      "[4845]\ttrain-rmse:1.812017\n",
      "[4846]\ttrain-rmse:1.811946\n",
      "[4847]\ttrain-rmse:1.811886\n",
      "[4848]\ttrain-rmse:1.811819\n",
      "[4849]\ttrain-rmse:1.811751\n",
      "[4850]\ttrain-rmse:1.811682\n",
      "[4851]\ttrain-rmse:1.811603\n",
      "[4852]\ttrain-rmse:1.811540\n",
      "[4853]\ttrain-rmse:1.811470\n",
      "[4854]\ttrain-rmse:1.811403\n",
      "[4855]\ttrain-rmse:1.811330\n",
      "[4856]\ttrain-rmse:1.811262\n",
      "[4857]\ttrain-rmse:1.811190\n",
      "[4858]\ttrain-rmse:1.811122\n",
      "[4859]\ttrain-rmse:1.811058\n",
      "[4860]\ttrain-rmse:1.810993\n",
      "[4861]\ttrain-rmse:1.810926\n",
      "[4862]\ttrain-rmse:1.810850\n",
      "[4863]\ttrain-rmse:1.810794\n",
      "[4864]\ttrain-rmse:1.810710\n",
      "[4865]\ttrain-rmse:1.810644\n",
      "[4866]\ttrain-rmse:1.810579\n",
      "[4867]\ttrain-rmse:1.810516\n",
      "[4868]\ttrain-rmse:1.810451\n",
      "[4869]\ttrain-rmse:1.810382\n",
      "[4870]\ttrain-rmse:1.810320\n",
      "[4871]\ttrain-rmse:1.810261\n",
      "[4872]\ttrain-rmse:1.810193\n",
      "[4873]\ttrain-rmse:1.810127\n",
      "[4874]\ttrain-rmse:1.810067\n",
      "[4875]\ttrain-rmse:1.809997\n",
      "[4876]\ttrain-rmse:1.809926\n",
      "[4877]\ttrain-rmse:1.809861\n",
      "[4878]\ttrain-rmse:1.809798\n",
      "[4879]\ttrain-rmse:1.809726\n",
      "[4880]\ttrain-rmse:1.809666\n",
      "[4881]\ttrain-rmse:1.809601\n",
      "[4882]\ttrain-rmse:1.809527\n",
      "[4883]\ttrain-rmse:1.809455\n",
      "[4884]\ttrain-rmse:1.809394\n",
      "[4885]\ttrain-rmse:1.809325\n",
      "[4886]\ttrain-rmse:1.809263\n",
      "[4887]\ttrain-rmse:1.809206\n",
      "[4888]\ttrain-rmse:1.809139\n",
      "[4889]\ttrain-rmse:1.809070\n",
      "[4890]\ttrain-rmse:1.809009\n",
      "[4891]\ttrain-rmse:1.808942\n",
      "[4892]\ttrain-rmse:1.808879\n",
      "[4893]\ttrain-rmse:1.808813\n",
      "[4894]\ttrain-rmse:1.808747\n",
      "[4895]\ttrain-rmse:1.808670\n",
      "[4896]\ttrain-rmse:1.808604\n",
      "[4897]\ttrain-rmse:1.808539\n",
      "[4898]\ttrain-rmse:1.808475\n",
      "[4899]\ttrain-rmse:1.808417\n",
      "[4900]\ttrain-rmse:1.808347\n",
      "[4901]\ttrain-rmse:1.808286\n",
      "[4902]\ttrain-rmse:1.808226\n",
      "[4903]\ttrain-rmse:1.808152\n",
      "[4904]\ttrain-rmse:1.808083\n",
      "[4905]\ttrain-rmse:1.808027\n",
      "[4906]\ttrain-rmse:1.807961\n",
      "[4907]\ttrain-rmse:1.807892\n",
      "[4908]\ttrain-rmse:1.807829\n",
      "[4909]\ttrain-rmse:1.807769\n",
      "[4910]\ttrain-rmse:1.807705\n",
      "[4911]\ttrain-rmse:1.807636\n",
      "[4912]\ttrain-rmse:1.807563\n",
      "[4913]\ttrain-rmse:1.807494\n",
      "[4914]\ttrain-rmse:1.807432\n",
      "[4915]\ttrain-rmse:1.807370\n",
      "[4916]\ttrain-rmse:1.807303\n",
      "[4917]\ttrain-rmse:1.807240\n",
      "[4918]\ttrain-rmse:1.807176\n",
      "[4919]\ttrain-rmse:1.807111\n",
      "[4920]\ttrain-rmse:1.807044\n",
      "[4921]\ttrain-rmse:1.806981\n",
      "[4922]\ttrain-rmse:1.806914\n",
      "[4923]\ttrain-rmse:1.806851\n",
      "[4924]\ttrain-rmse:1.806790\n",
      "[4925]\ttrain-rmse:1.806723\n",
      "[4926]\ttrain-rmse:1.806658\n",
      "[4927]\ttrain-rmse:1.806596\n",
      "[4928]\ttrain-rmse:1.806537\n",
      "[4929]\ttrain-rmse:1.806479\n",
      "[4930]\ttrain-rmse:1.806414\n",
      "[4931]\ttrain-rmse:1.806344\n",
      "[4932]\ttrain-rmse:1.806277\n",
      "[4933]\ttrain-rmse:1.806215\n",
      "[4934]\ttrain-rmse:1.806152\n",
      "[4935]\ttrain-rmse:1.806082\n",
      "[4936]\ttrain-rmse:1.806017\n",
      "[4937]\ttrain-rmse:1.805954\n",
      "[4938]\ttrain-rmse:1.805885\n",
      "[4939]\ttrain-rmse:1.805824\n",
      "[4940]\ttrain-rmse:1.805763\n",
      "[4941]\ttrain-rmse:1.805699\n",
      "[4942]\ttrain-rmse:1.805632\n",
      "[4943]\ttrain-rmse:1.805574\n",
      "[4944]\ttrain-rmse:1.805508\n",
      "[4945]\ttrain-rmse:1.805452\n",
      "[4946]\ttrain-rmse:1.805388\n",
      "[4947]\ttrain-rmse:1.805314\n",
      "[4948]\ttrain-rmse:1.805255\n",
      "[4949]\ttrain-rmse:1.805201\n",
      "[4950]\ttrain-rmse:1.805138\n",
      "[4951]\ttrain-rmse:1.805073\n",
      "[4952]\ttrain-rmse:1.805012\n",
      "[4953]\ttrain-rmse:1.804947\n",
      "[4954]\ttrain-rmse:1.804884\n",
      "[4955]\ttrain-rmse:1.804822\n",
      "[4956]\ttrain-rmse:1.804763\n",
      "[4957]\ttrain-rmse:1.804702\n",
      "[4958]\ttrain-rmse:1.804645\n",
      "[4959]\ttrain-rmse:1.804583\n",
      "[4960]\ttrain-rmse:1.804515\n",
      "[4961]\ttrain-rmse:1.804443\n",
      "[4962]\ttrain-rmse:1.804381\n",
      "[4963]\ttrain-rmse:1.804313\n",
      "[4964]\ttrain-rmse:1.804252\n",
      "[4965]\ttrain-rmse:1.804192\n",
      "[4966]\ttrain-rmse:1.804121\n",
      "[4967]\ttrain-rmse:1.804061\n",
      "[4968]\ttrain-rmse:1.804001\n",
      "[4969]\ttrain-rmse:1.803933\n",
      "[4970]\ttrain-rmse:1.803871\n",
      "[4971]\ttrain-rmse:1.803803\n",
      "[4972]\ttrain-rmse:1.803746\n",
      "[4973]\ttrain-rmse:1.803692\n",
      "[4974]\ttrain-rmse:1.803632\n",
      "[4975]\ttrain-rmse:1.803574\n",
      "[4976]\ttrain-rmse:1.803513\n",
      "[4977]\ttrain-rmse:1.803448\n",
      "[4978]\ttrain-rmse:1.803381\n",
      "[4979]\ttrain-rmse:1.803316\n",
      "[4980]\ttrain-rmse:1.803251\n",
      "[4981]\ttrain-rmse:1.803190\n",
      "[4982]\ttrain-rmse:1.803124\n",
      "[4983]\ttrain-rmse:1.803059\n",
      "[4984]\ttrain-rmse:1.802995\n",
      "[4985]\ttrain-rmse:1.802934\n",
      "[4986]\ttrain-rmse:1.802877\n",
      "[4987]\ttrain-rmse:1.802822\n",
      "[4988]\ttrain-rmse:1.802759\n",
      "[4989]\ttrain-rmse:1.802698\n",
      "[4990]\ttrain-rmse:1.802628\n",
      "[4991]\ttrain-rmse:1.802565\n",
      "[4992]\ttrain-rmse:1.802495\n",
      "[4993]\ttrain-rmse:1.802434\n",
      "[4994]\ttrain-rmse:1.802364\n",
      "[4995]\ttrain-rmse:1.802306\n",
      "[4996]\ttrain-rmse:1.802244\n",
      "[4997]\ttrain-rmse:1.802183\n",
      "[4998]\ttrain-rmse:1.802114\n",
      "[4999]\ttrain-rmse:1.802044\n",
      "[5000]\ttrain-rmse:1.801982\n",
      "[5001]\ttrain-rmse:1.801925\n",
      "[5002]\ttrain-rmse:1.801864\n",
      "[5003]\ttrain-rmse:1.801800\n",
      "[5004]\ttrain-rmse:1.801740\n",
      "[5005]\ttrain-rmse:1.801679\n",
      "[5006]\ttrain-rmse:1.801620\n",
      "[5007]\ttrain-rmse:1.801558\n",
      "[5008]\ttrain-rmse:1.801495\n",
      "[5009]\ttrain-rmse:1.801437\n",
      "[5010]\ttrain-rmse:1.801369\n",
      "[5011]\ttrain-rmse:1.801317\n",
      "[5012]\ttrain-rmse:1.801255\n",
      "[5013]\ttrain-rmse:1.801190\n",
      "[5014]\ttrain-rmse:1.801138\n",
      "[5015]\ttrain-rmse:1.801074\n",
      "[5016]\ttrain-rmse:1.801016\n",
      "[5017]\ttrain-rmse:1.800958\n",
      "[5018]\ttrain-rmse:1.800898\n",
      "[5019]\ttrain-rmse:1.800843\n",
      "[5020]\ttrain-rmse:1.800783\n",
      "[5021]\ttrain-rmse:1.800720\n",
      "[5022]\ttrain-rmse:1.800664\n",
      "[5023]\ttrain-rmse:1.800607\n",
      "[5024]\ttrain-rmse:1.800544\n",
      "[5025]\ttrain-rmse:1.800491\n",
      "[5026]\ttrain-rmse:1.800428\n",
      "[5027]\ttrain-rmse:1.800360\n",
      "[5028]\ttrain-rmse:1.800300\n",
      "[5029]\ttrain-rmse:1.800240\n",
      "[5030]\ttrain-rmse:1.800184\n",
      "[5031]\ttrain-rmse:1.800127\n",
      "[5032]\ttrain-rmse:1.800071\n",
      "[5033]\ttrain-rmse:1.800014\n",
      "[5034]\ttrain-rmse:1.799956\n",
      "[5035]\ttrain-rmse:1.799907\n",
      "[5036]\ttrain-rmse:1.799850\n",
      "[5037]\ttrain-rmse:1.799788\n",
      "[5038]\ttrain-rmse:1.799728\n",
      "[5039]\ttrain-rmse:1.799668\n",
      "[5040]\ttrain-rmse:1.799614\n",
      "[5041]\ttrain-rmse:1.799564\n",
      "[5042]\ttrain-rmse:1.799500\n",
      "[5043]\ttrain-rmse:1.799438\n",
      "[5044]\ttrain-rmse:1.799376\n",
      "[5045]\ttrain-rmse:1.799319\n",
      "[5046]\ttrain-rmse:1.799263\n",
      "[5047]\ttrain-rmse:1.799198\n",
      "[5048]\ttrain-rmse:1.799130\n",
      "[5049]\ttrain-rmse:1.799074\n",
      "[5050]\ttrain-rmse:1.799020\n",
      "[5051]\ttrain-rmse:1.798957\n",
      "[5052]\ttrain-rmse:1.798891\n",
      "[5053]\ttrain-rmse:1.798828\n",
      "[5054]\ttrain-rmse:1.798770\n",
      "[5055]\ttrain-rmse:1.798709\n",
      "[5056]\ttrain-rmse:1.798657\n",
      "[5057]\ttrain-rmse:1.798595\n",
      "[5058]\ttrain-rmse:1.798536\n",
      "[5059]\ttrain-rmse:1.798484\n",
      "[5060]\ttrain-rmse:1.798426\n",
      "[5061]\ttrain-rmse:1.798369\n",
      "[5062]\ttrain-rmse:1.798312\n",
      "[5063]\ttrain-rmse:1.798250\n",
      "[5064]\ttrain-rmse:1.798183\n",
      "[5065]\ttrain-rmse:1.798130\n",
      "[5066]\ttrain-rmse:1.798073\n",
      "[5067]\ttrain-rmse:1.798015\n",
      "[5068]\ttrain-rmse:1.797959\n",
      "[5069]\ttrain-rmse:1.797901\n",
      "[5070]\ttrain-rmse:1.797839\n",
      "[5071]\ttrain-rmse:1.797779\n",
      "[5072]\ttrain-rmse:1.797722\n",
      "[5073]\ttrain-rmse:1.797664\n",
      "[5074]\ttrain-rmse:1.797608\n",
      "[5075]\ttrain-rmse:1.797548\n",
      "[5076]\ttrain-rmse:1.797499\n",
      "[5077]\ttrain-rmse:1.797443\n",
      "[5078]\ttrain-rmse:1.797381\n",
      "[5079]\ttrain-rmse:1.797321\n",
      "[5080]\ttrain-rmse:1.797266\n",
      "[5081]\ttrain-rmse:1.797208\n",
      "[5082]\ttrain-rmse:1.797151\n",
      "[5083]\ttrain-rmse:1.797099\n",
      "[5084]\ttrain-rmse:1.797038\n",
      "[5085]\ttrain-rmse:1.796978\n",
      "[5086]\ttrain-rmse:1.796928\n",
      "[5087]\ttrain-rmse:1.796873\n",
      "[5088]\ttrain-rmse:1.796810\n",
      "[5089]\ttrain-rmse:1.796761\n",
      "[5090]\ttrain-rmse:1.796709\n",
      "[5091]\ttrain-rmse:1.796646\n",
      "[5092]\ttrain-rmse:1.796594\n",
      "[5093]\ttrain-rmse:1.796536\n",
      "[5094]\ttrain-rmse:1.796476\n",
      "[5095]\ttrain-rmse:1.796420\n",
      "[5096]\ttrain-rmse:1.796369\n",
      "[5097]\ttrain-rmse:1.796315\n",
      "[5098]\ttrain-rmse:1.796262\n",
      "[5099]\ttrain-rmse:1.796205\n",
      "[5100]\ttrain-rmse:1.796148\n",
      "[5101]\ttrain-rmse:1.796090\n",
      "[5102]\ttrain-rmse:1.796035\n",
      "[5103]\ttrain-rmse:1.795982\n",
      "[5104]\ttrain-rmse:1.795924\n",
      "[5105]\ttrain-rmse:1.795863\n",
      "[5106]\ttrain-rmse:1.795810\n",
      "[5107]\ttrain-rmse:1.795749\n",
      "[5108]\ttrain-rmse:1.795691\n",
      "[5109]\ttrain-rmse:1.795632\n",
      "[5110]\ttrain-rmse:1.795576\n",
      "[5111]\ttrain-rmse:1.795518\n",
      "[5112]\ttrain-rmse:1.795464\n",
      "[5113]\ttrain-rmse:1.795410\n",
      "[5114]\ttrain-rmse:1.795357\n",
      "[5115]\ttrain-rmse:1.795298\n",
      "[5116]\ttrain-rmse:1.795241\n",
      "[5117]\ttrain-rmse:1.795187\n",
      "[5118]\ttrain-rmse:1.795122\n",
      "[5119]\ttrain-rmse:1.795060\n",
      "[5120]\ttrain-rmse:1.795008\n",
      "[5121]\ttrain-rmse:1.794951\n",
      "[5122]\ttrain-rmse:1.794900\n",
      "[5123]\ttrain-rmse:1.794844\n",
      "[5124]\ttrain-rmse:1.794788\n",
      "[5125]\ttrain-rmse:1.794730\n",
      "[5126]\ttrain-rmse:1.794675\n",
      "[5127]\ttrain-rmse:1.794624\n",
      "[5128]\ttrain-rmse:1.794570\n",
      "[5129]\ttrain-rmse:1.794517\n",
      "[5130]\ttrain-rmse:1.794463\n",
      "[5131]\ttrain-rmse:1.794408\n",
      "[5132]\ttrain-rmse:1.794349\n",
      "[5133]\ttrain-rmse:1.794293\n",
      "[5134]\ttrain-rmse:1.794238\n",
      "[5135]\ttrain-rmse:1.794190\n",
      "[5136]\ttrain-rmse:1.794136\n",
      "[5137]\ttrain-rmse:1.794081\n",
      "[5138]\ttrain-rmse:1.794014\n",
      "[5139]\ttrain-rmse:1.793945\n",
      "[5140]\ttrain-rmse:1.793890\n",
      "[5141]\ttrain-rmse:1.793833\n",
      "[5142]\ttrain-rmse:1.793780\n",
      "[5143]\ttrain-rmse:1.793726\n",
      "[5144]\ttrain-rmse:1.793679\n",
      "[5145]\ttrain-rmse:1.793622\n",
      "[5146]\ttrain-rmse:1.793570\n",
      "[5147]\ttrain-rmse:1.793517\n",
      "[5148]\ttrain-rmse:1.793460\n",
      "[5149]\ttrain-rmse:1.793404\n",
      "[5150]\ttrain-rmse:1.793348\n",
      "[5151]\ttrain-rmse:1.793294\n",
      "[5152]\ttrain-rmse:1.793244\n",
      "[5153]\ttrain-rmse:1.793183\n",
      "[5154]\ttrain-rmse:1.793131\n",
      "[5155]\ttrain-rmse:1.793076\n",
      "[5156]\ttrain-rmse:1.793026\n",
      "[5157]\ttrain-rmse:1.792971\n",
      "[5158]\ttrain-rmse:1.792915\n",
      "[5159]\ttrain-rmse:1.792861\n",
      "[5160]\ttrain-rmse:1.792807\n",
      "[5161]\ttrain-rmse:1.792749\n",
      "[5162]\ttrain-rmse:1.792696\n",
      "[5163]\ttrain-rmse:1.792645\n",
      "[5164]\ttrain-rmse:1.792589\n",
      "[5165]\ttrain-rmse:1.792531\n",
      "[5166]\ttrain-rmse:1.792482\n",
      "[5167]\ttrain-rmse:1.792429\n",
      "[5168]\ttrain-rmse:1.792372\n",
      "[5169]\ttrain-rmse:1.792314\n",
      "[5170]\ttrain-rmse:1.792259\n",
      "[5171]\ttrain-rmse:1.792208\n",
      "[5172]\ttrain-rmse:1.792143\n",
      "[5173]\ttrain-rmse:1.792080\n",
      "[5174]\ttrain-rmse:1.792027\n",
      "[5175]\ttrain-rmse:1.791973\n",
      "[5176]\ttrain-rmse:1.791919\n",
      "[5177]\ttrain-rmse:1.791861\n",
      "[5178]\ttrain-rmse:1.791799\n",
      "[5179]\ttrain-rmse:1.791745\n",
      "[5180]\ttrain-rmse:1.791692\n",
      "[5181]\ttrain-rmse:1.791639\n",
      "[5182]\ttrain-rmse:1.791586\n",
      "[5183]\ttrain-rmse:1.791538\n",
      "[5184]\ttrain-rmse:1.791488\n",
      "[5185]\ttrain-rmse:1.791437\n",
      "[5186]\ttrain-rmse:1.791379\n",
      "[5187]\ttrain-rmse:1.791330\n",
      "[5188]\ttrain-rmse:1.791272\n",
      "[5189]\ttrain-rmse:1.791214\n",
      "[5190]\ttrain-rmse:1.791162\n",
      "[5191]\ttrain-rmse:1.791101\n",
      "[5192]\ttrain-rmse:1.791052\n",
      "[5193]\ttrain-rmse:1.790992\n",
      "[5194]\ttrain-rmse:1.790934\n",
      "[5195]\ttrain-rmse:1.790883\n",
      "[5196]\ttrain-rmse:1.790833\n",
      "[5197]\ttrain-rmse:1.790775\n",
      "[5198]\ttrain-rmse:1.790720\n",
      "[5199]\ttrain-rmse:1.790667\n",
      "[5200]\ttrain-rmse:1.790615\n",
      "[5201]\ttrain-rmse:1.790566\n",
      "[5202]\ttrain-rmse:1.790516\n",
      "[5203]\ttrain-rmse:1.790469\n",
      "[5204]\ttrain-rmse:1.790417\n",
      "[5205]\ttrain-rmse:1.790356\n",
      "[5206]\ttrain-rmse:1.790303\n",
      "[5207]\ttrain-rmse:1.790245\n",
      "[5208]\ttrain-rmse:1.790197\n",
      "[5209]\ttrain-rmse:1.790146\n",
      "[5210]\ttrain-rmse:1.790090\n",
      "[5211]\ttrain-rmse:1.790030\n",
      "[5212]\ttrain-rmse:1.789978\n",
      "[5213]\ttrain-rmse:1.789920\n",
      "[5214]\ttrain-rmse:1.789870\n",
      "[5215]\ttrain-rmse:1.789820\n",
      "[5216]\ttrain-rmse:1.789770\n",
      "[5217]\ttrain-rmse:1.789707\n",
      "[5218]\ttrain-rmse:1.789652\n",
      "[5219]\ttrain-rmse:1.789603\n",
      "[5220]\ttrain-rmse:1.789553\n",
      "[5221]\ttrain-rmse:1.789493\n",
      "[5222]\ttrain-rmse:1.789437\n",
      "[5223]\ttrain-rmse:1.789393\n",
      "[5224]\ttrain-rmse:1.789333\n",
      "[5225]\ttrain-rmse:1.789285\n",
      "[5226]\ttrain-rmse:1.789234\n",
      "[5227]\ttrain-rmse:1.789186\n",
      "[5228]\ttrain-rmse:1.789130\n",
      "[5229]\ttrain-rmse:1.789075\n",
      "[5230]\ttrain-rmse:1.789026\n",
      "[5231]\ttrain-rmse:1.788967\n",
      "[5232]\ttrain-rmse:1.788915\n",
      "[5233]\ttrain-rmse:1.788858\n",
      "[5234]\ttrain-rmse:1.788800\n",
      "[5235]\ttrain-rmse:1.788748\n",
      "[5236]\ttrain-rmse:1.788693\n",
      "[5237]\ttrain-rmse:1.788642\n",
      "[5238]\ttrain-rmse:1.788593\n",
      "[5239]\ttrain-rmse:1.788538\n",
      "[5240]\ttrain-rmse:1.788488\n",
      "[5241]\ttrain-rmse:1.788434\n",
      "[5242]\ttrain-rmse:1.788382\n",
      "[5243]\ttrain-rmse:1.788324\n",
      "[5244]\ttrain-rmse:1.788263\n",
      "[5245]\ttrain-rmse:1.788206\n",
      "[5246]\ttrain-rmse:1.788158\n",
      "[5247]\ttrain-rmse:1.788109\n",
      "[5248]\ttrain-rmse:1.788053\n",
      "[5249]\ttrain-rmse:1.788001\n",
      "[5250]\ttrain-rmse:1.787944\n",
      "[5251]\ttrain-rmse:1.787894\n",
      "[5252]\ttrain-rmse:1.787842\n",
      "[5253]\ttrain-rmse:1.787794\n",
      "[5254]\ttrain-rmse:1.787753\n",
      "[5255]\ttrain-rmse:1.787708\n",
      "[5256]\ttrain-rmse:1.787655\n",
      "[5257]\ttrain-rmse:1.787601\n",
      "[5258]\ttrain-rmse:1.787551\n",
      "[5259]\ttrain-rmse:1.787504\n",
      "[5260]\ttrain-rmse:1.787454\n",
      "[5261]\ttrain-rmse:1.787397\n",
      "[5262]\ttrain-rmse:1.787344\n",
      "[5263]\ttrain-rmse:1.787294\n",
      "[5264]\ttrain-rmse:1.787239\n",
      "[5265]\ttrain-rmse:1.787190\n",
      "[5266]\ttrain-rmse:1.787136\n",
      "[5267]\ttrain-rmse:1.787083\n",
      "[5268]\ttrain-rmse:1.787026\n",
      "[5269]\ttrain-rmse:1.786966\n",
      "[5270]\ttrain-rmse:1.786918\n",
      "[5271]\ttrain-rmse:1.786867\n",
      "[5272]\ttrain-rmse:1.786820\n",
      "[5273]\ttrain-rmse:1.786766\n",
      "[5274]\ttrain-rmse:1.786715\n",
      "[5275]\ttrain-rmse:1.786666\n",
      "[5276]\ttrain-rmse:1.786613\n",
      "[5277]\ttrain-rmse:1.786564\n",
      "[5278]\ttrain-rmse:1.786515\n",
      "[5279]\ttrain-rmse:1.786467\n",
      "[5280]\ttrain-rmse:1.786414\n",
      "[5281]\ttrain-rmse:1.786363\n",
      "[5282]\ttrain-rmse:1.786301\n",
      "[5283]\ttrain-rmse:1.786252\n",
      "[5284]\ttrain-rmse:1.786196\n",
      "[5285]\ttrain-rmse:1.786145\n",
      "[5286]\ttrain-rmse:1.786096\n",
      "[5287]\ttrain-rmse:1.786052\n",
      "[5288]\ttrain-rmse:1.786007\n",
      "[5289]\ttrain-rmse:1.785963\n",
      "[5290]\ttrain-rmse:1.785922\n",
      "[5291]\ttrain-rmse:1.785877\n",
      "[5292]\ttrain-rmse:1.785814\n",
      "[5293]\ttrain-rmse:1.785766\n",
      "[5294]\ttrain-rmse:1.785715\n",
      "[5295]\ttrain-rmse:1.785661\n",
      "[5296]\ttrain-rmse:1.785604\n",
      "[5297]\ttrain-rmse:1.785553\n",
      "[5298]\ttrain-rmse:1.785499\n",
      "[5299]\ttrain-rmse:1.785453\n",
      "[5300]\ttrain-rmse:1.785396\n",
      "[5301]\ttrain-rmse:1.785349\n",
      "[5302]\ttrain-rmse:1.785293\n",
      "[5303]\ttrain-rmse:1.785243\n",
      "[5304]\ttrain-rmse:1.785194\n",
      "[5305]\ttrain-rmse:1.785149\n",
      "[5306]\ttrain-rmse:1.785097\n",
      "[5307]\ttrain-rmse:1.785046\n",
      "[5308]\ttrain-rmse:1.784999\n",
      "[5309]\ttrain-rmse:1.784950\n",
      "[5310]\ttrain-rmse:1.784899\n",
      "[5311]\ttrain-rmse:1.784847\n",
      "[5312]\ttrain-rmse:1.784801\n",
      "[5313]\ttrain-rmse:1.784741\n",
      "[5314]\ttrain-rmse:1.784687\n",
      "[5315]\ttrain-rmse:1.784631\n",
      "[5316]\ttrain-rmse:1.784571\n",
      "[5317]\ttrain-rmse:1.784523\n",
      "[5318]\ttrain-rmse:1.784475\n",
      "[5319]\ttrain-rmse:1.784421\n",
      "[5320]\ttrain-rmse:1.784379\n",
      "[5321]\ttrain-rmse:1.784339\n",
      "[5322]\ttrain-rmse:1.784289\n",
      "[5323]\ttrain-rmse:1.784242\n",
      "[5324]\ttrain-rmse:1.784196\n",
      "[5325]\ttrain-rmse:1.784145\n",
      "[5326]\ttrain-rmse:1.784099\n",
      "[5327]\ttrain-rmse:1.784051\n",
      "[5328]\ttrain-rmse:1.784006\n",
      "[5329]\ttrain-rmse:1.783957\n",
      "[5330]\ttrain-rmse:1.783911\n",
      "[5331]\ttrain-rmse:1.783862\n",
      "[5332]\ttrain-rmse:1.783812\n",
      "[5333]\ttrain-rmse:1.783760\n",
      "[5334]\ttrain-rmse:1.783704\n",
      "[5335]\ttrain-rmse:1.783662\n",
      "[5336]\ttrain-rmse:1.783615\n",
      "[5337]\ttrain-rmse:1.783561\n",
      "[5338]\ttrain-rmse:1.783515\n",
      "[5339]\ttrain-rmse:1.783469\n",
      "[5340]\ttrain-rmse:1.783417\n",
      "[5341]\ttrain-rmse:1.783377\n",
      "[5342]\ttrain-rmse:1.783322\n",
      "[5343]\ttrain-rmse:1.783271\n",
      "[5344]\ttrain-rmse:1.783227\n",
      "[5345]\ttrain-rmse:1.783179\n",
      "[5346]\ttrain-rmse:1.783133\n",
      "[5347]\ttrain-rmse:1.783086\n",
      "[5348]\ttrain-rmse:1.783036\n",
      "[5349]\ttrain-rmse:1.782993\n",
      "[5350]\ttrain-rmse:1.782941\n",
      "[5351]\ttrain-rmse:1.782890\n",
      "[5352]\ttrain-rmse:1.782843\n",
      "[5353]\ttrain-rmse:1.782794\n",
      "[5354]\ttrain-rmse:1.782743\n",
      "[5355]\ttrain-rmse:1.782693\n",
      "[5356]\ttrain-rmse:1.782636\n",
      "[5357]\ttrain-rmse:1.782585\n",
      "[5358]\ttrain-rmse:1.782541\n",
      "[5359]\ttrain-rmse:1.782498\n",
      "[5360]\ttrain-rmse:1.782455\n",
      "[5361]\ttrain-rmse:1.782413\n",
      "[5362]\ttrain-rmse:1.782363\n",
      "[5363]\ttrain-rmse:1.782319\n",
      "[5364]\ttrain-rmse:1.782268\n",
      "[5365]\ttrain-rmse:1.782220\n",
      "[5366]\ttrain-rmse:1.782168\n",
      "[5367]\ttrain-rmse:1.782125\n",
      "[5368]\ttrain-rmse:1.782075\n",
      "[5369]\ttrain-rmse:1.782022\n",
      "[5370]\ttrain-rmse:1.781975\n",
      "[5371]\ttrain-rmse:1.781927\n",
      "[5372]\ttrain-rmse:1.781876\n",
      "[5373]\ttrain-rmse:1.781830\n",
      "[5374]\ttrain-rmse:1.781772\n",
      "[5375]\ttrain-rmse:1.781730\n",
      "[5376]\ttrain-rmse:1.781679\n",
      "[5377]\ttrain-rmse:1.781633\n",
      "[5378]\ttrain-rmse:1.781579\n",
      "[5379]\ttrain-rmse:1.781532\n",
      "[5380]\ttrain-rmse:1.781492\n",
      "[5381]\ttrain-rmse:1.781451\n",
      "[5382]\ttrain-rmse:1.781404\n",
      "[5383]\ttrain-rmse:1.781360\n",
      "[5384]\ttrain-rmse:1.781314\n",
      "[5385]\ttrain-rmse:1.781259\n",
      "[5386]\ttrain-rmse:1.781213\n",
      "[5387]\ttrain-rmse:1.781155\n",
      "[5388]\ttrain-rmse:1.781108\n",
      "[5389]\ttrain-rmse:1.781060\n",
      "[5390]\ttrain-rmse:1.781011\n",
      "[5391]\ttrain-rmse:1.780966\n",
      "[5392]\ttrain-rmse:1.780922\n",
      "[5393]\ttrain-rmse:1.780868\n",
      "[5394]\ttrain-rmse:1.780820\n",
      "[5395]\ttrain-rmse:1.780769\n",
      "[5396]\ttrain-rmse:1.780720\n",
      "[5397]\ttrain-rmse:1.780677\n",
      "[5398]\ttrain-rmse:1.780633\n",
      "[5399]\ttrain-rmse:1.780589\n",
      "[5400]\ttrain-rmse:1.780540\n",
      "[5401]\ttrain-rmse:1.780490\n",
      "[5402]\ttrain-rmse:1.780446\n",
      "[5403]\ttrain-rmse:1.780399\n",
      "[5404]\ttrain-rmse:1.780349\n",
      "[5405]\ttrain-rmse:1.780306\n",
      "[5406]\ttrain-rmse:1.780259\n",
      "[5407]\ttrain-rmse:1.780212\n",
      "[5408]\ttrain-rmse:1.780173\n",
      "[5409]\ttrain-rmse:1.780124\n",
      "[5410]\ttrain-rmse:1.780076\n",
      "[5411]\ttrain-rmse:1.780031\n",
      "[5412]\ttrain-rmse:1.779987\n",
      "[5413]\ttrain-rmse:1.779937\n",
      "[5414]\ttrain-rmse:1.779890\n",
      "[5415]\ttrain-rmse:1.779842\n",
      "[5416]\ttrain-rmse:1.779795\n",
      "[5417]\ttrain-rmse:1.779755\n",
      "[5418]\ttrain-rmse:1.779703\n",
      "[5419]\ttrain-rmse:1.779657\n",
      "[5420]\ttrain-rmse:1.779615\n",
      "[5421]\ttrain-rmse:1.779570\n",
      "[5422]\ttrain-rmse:1.779528\n",
      "[5423]\ttrain-rmse:1.779486\n",
      "[5424]\ttrain-rmse:1.779442\n",
      "[5425]\ttrain-rmse:1.779393\n",
      "[5426]\ttrain-rmse:1.779344\n",
      "[5427]\ttrain-rmse:1.779297\n",
      "[5428]\ttrain-rmse:1.779261\n",
      "[5429]\ttrain-rmse:1.779218\n",
      "[5430]\ttrain-rmse:1.779161\n",
      "[5431]\ttrain-rmse:1.779109\n",
      "[5432]\ttrain-rmse:1.779063\n",
      "[5433]\ttrain-rmse:1.779014\n",
      "[5434]\ttrain-rmse:1.778972\n",
      "[5435]\ttrain-rmse:1.778921\n",
      "[5436]\ttrain-rmse:1.778880\n",
      "[5437]\ttrain-rmse:1.778840\n",
      "[5438]\ttrain-rmse:1.778788\n",
      "[5439]\ttrain-rmse:1.778738\n",
      "[5440]\ttrain-rmse:1.778697\n",
      "[5441]\ttrain-rmse:1.778658\n",
      "[5442]\ttrain-rmse:1.778613\n",
      "[5443]\ttrain-rmse:1.778564\n",
      "[5444]\ttrain-rmse:1.778512\n",
      "[5445]\ttrain-rmse:1.778464\n",
      "[5446]\ttrain-rmse:1.778426\n",
      "[5447]\ttrain-rmse:1.778378\n",
      "[5448]\ttrain-rmse:1.778327\n",
      "[5449]\ttrain-rmse:1.778281\n",
      "[5450]\ttrain-rmse:1.778242\n",
      "[5451]\ttrain-rmse:1.778203\n",
      "[5452]\ttrain-rmse:1.778153\n",
      "[5453]\ttrain-rmse:1.778108\n",
      "[5454]\ttrain-rmse:1.778068\n",
      "[5455]\ttrain-rmse:1.778029\n",
      "[5456]\ttrain-rmse:1.777986\n",
      "[5457]\ttrain-rmse:1.777943\n",
      "[5458]\ttrain-rmse:1.777898\n",
      "[5459]\ttrain-rmse:1.777856\n",
      "[5460]\ttrain-rmse:1.777815\n",
      "[5461]\ttrain-rmse:1.777771\n",
      "[5462]\ttrain-rmse:1.777728\n",
      "[5463]\ttrain-rmse:1.777684\n",
      "[5464]\ttrain-rmse:1.777630\n",
      "[5465]\ttrain-rmse:1.777585\n",
      "[5466]\ttrain-rmse:1.777539\n",
      "[5467]\ttrain-rmse:1.777484\n",
      "[5468]\ttrain-rmse:1.777444\n",
      "[5469]\ttrain-rmse:1.777400\n",
      "[5470]\ttrain-rmse:1.777363\n",
      "[5471]\ttrain-rmse:1.777317\n",
      "[5472]\ttrain-rmse:1.777272\n",
      "[5473]\ttrain-rmse:1.777234\n",
      "[5474]\ttrain-rmse:1.777185\n",
      "[5475]\ttrain-rmse:1.777149\n",
      "[5476]\ttrain-rmse:1.777101\n",
      "[5477]\ttrain-rmse:1.777060\n",
      "[5478]\ttrain-rmse:1.777013\n",
      "[5479]\ttrain-rmse:1.776964\n",
      "[5480]\ttrain-rmse:1.776923\n",
      "[5481]\ttrain-rmse:1.776873\n",
      "[5482]\ttrain-rmse:1.776821\n",
      "[5483]\ttrain-rmse:1.776780\n",
      "[5484]\ttrain-rmse:1.776736\n",
      "[5485]\ttrain-rmse:1.776694\n",
      "[5486]\ttrain-rmse:1.776655\n",
      "[5487]\ttrain-rmse:1.776614\n",
      "[5488]\ttrain-rmse:1.776578\n",
      "[5489]\ttrain-rmse:1.776535\n",
      "[5490]\ttrain-rmse:1.776493\n",
      "[5491]\ttrain-rmse:1.776455\n",
      "[5492]\ttrain-rmse:1.776400\n",
      "[5493]\ttrain-rmse:1.776355\n",
      "[5494]\ttrain-rmse:1.776301\n",
      "[5495]\ttrain-rmse:1.776264\n",
      "[5496]\ttrain-rmse:1.776229\n",
      "[5497]\ttrain-rmse:1.776183\n",
      "[5498]\ttrain-rmse:1.776137\n",
      "[5499]\ttrain-rmse:1.776092\n",
      "[5500]\ttrain-rmse:1.776053\n",
      "[5501]\ttrain-rmse:1.776007\n",
      "[5502]\ttrain-rmse:1.775964\n",
      "[5503]\ttrain-rmse:1.775920\n",
      "[5504]\ttrain-rmse:1.775875\n",
      "[5505]\ttrain-rmse:1.775832\n",
      "[5506]\ttrain-rmse:1.775791\n",
      "[5507]\ttrain-rmse:1.775746\n",
      "[5508]\ttrain-rmse:1.775702\n",
      "[5509]\ttrain-rmse:1.775659\n",
      "[5510]\ttrain-rmse:1.775612\n",
      "[5511]\ttrain-rmse:1.775568\n",
      "[5512]\ttrain-rmse:1.775522\n",
      "[5513]\ttrain-rmse:1.775481\n",
      "[5514]\ttrain-rmse:1.775433\n",
      "[5515]\ttrain-rmse:1.775392\n",
      "[5516]\ttrain-rmse:1.775349\n",
      "[5517]\ttrain-rmse:1.775309\n",
      "[5518]\ttrain-rmse:1.775268\n",
      "[5519]\ttrain-rmse:1.775232\n",
      "[5520]\ttrain-rmse:1.775192\n",
      "[5521]\ttrain-rmse:1.775151\n",
      "[5522]\ttrain-rmse:1.775100\n",
      "[5523]\ttrain-rmse:1.775061\n",
      "[5524]\ttrain-rmse:1.775018\n",
      "[5525]\ttrain-rmse:1.774980\n",
      "[5526]\ttrain-rmse:1.774935\n",
      "[5527]\ttrain-rmse:1.774890\n",
      "[5528]\ttrain-rmse:1.774842\n",
      "[5529]\ttrain-rmse:1.774796\n",
      "[5530]\ttrain-rmse:1.774754\n",
      "[5531]\ttrain-rmse:1.774709\n",
      "[5532]\ttrain-rmse:1.774662\n",
      "[5533]\ttrain-rmse:1.774624\n",
      "[5534]\ttrain-rmse:1.774581\n",
      "[5535]\ttrain-rmse:1.774539\n",
      "[5536]\ttrain-rmse:1.774504\n",
      "[5537]\ttrain-rmse:1.774454\n",
      "[5538]\ttrain-rmse:1.774402\n",
      "[5539]\ttrain-rmse:1.774366\n",
      "[5540]\ttrain-rmse:1.774322\n",
      "[5541]\ttrain-rmse:1.774284\n",
      "[5542]\ttrain-rmse:1.774246\n",
      "[5543]\ttrain-rmse:1.774199\n",
      "[5544]\ttrain-rmse:1.774161\n",
      "[5545]\ttrain-rmse:1.774118\n",
      "[5546]\ttrain-rmse:1.774077\n",
      "[5547]\ttrain-rmse:1.774025\n",
      "[5548]\ttrain-rmse:1.773983\n",
      "[5549]\ttrain-rmse:1.773939\n",
      "[5550]\ttrain-rmse:1.773905\n",
      "[5551]\ttrain-rmse:1.773855\n",
      "[5552]\ttrain-rmse:1.773819\n",
      "[5553]\ttrain-rmse:1.773774\n",
      "[5554]\ttrain-rmse:1.773736\n",
      "[5555]\ttrain-rmse:1.773699\n",
      "[5556]\ttrain-rmse:1.773659\n",
      "[5557]\ttrain-rmse:1.773620\n",
      "[5558]\ttrain-rmse:1.773583\n",
      "[5559]\ttrain-rmse:1.773535\n",
      "[5560]\ttrain-rmse:1.773495\n",
      "[5561]\ttrain-rmse:1.773454\n",
      "[5562]\ttrain-rmse:1.773419\n",
      "[5563]\ttrain-rmse:1.773381\n",
      "[5564]\ttrain-rmse:1.773338\n",
      "[5565]\ttrain-rmse:1.773298\n",
      "[5566]\ttrain-rmse:1.773247\n",
      "[5567]\ttrain-rmse:1.773197\n",
      "[5568]\ttrain-rmse:1.773159\n",
      "[5569]\ttrain-rmse:1.773118\n",
      "[5570]\ttrain-rmse:1.773070\n",
      "[5571]\ttrain-rmse:1.773037\n",
      "[5572]\ttrain-rmse:1.773002\n",
      "[5573]\ttrain-rmse:1.772955\n",
      "[5574]\ttrain-rmse:1.772920\n",
      "[5575]\ttrain-rmse:1.772876\n",
      "[5576]\ttrain-rmse:1.772835\n",
      "[5577]\ttrain-rmse:1.772800\n",
      "[5578]\ttrain-rmse:1.772759\n",
      "[5579]\ttrain-rmse:1.772719\n",
      "[5580]\ttrain-rmse:1.772682\n",
      "[5581]\ttrain-rmse:1.772642\n",
      "[5582]\ttrain-rmse:1.772608\n",
      "[5583]\ttrain-rmse:1.772570\n",
      "[5584]\ttrain-rmse:1.772527\n",
      "[5585]\ttrain-rmse:1.772486\n",
      "[5586]\ttrain-rmse:1.772444\n",
      "[5587]\ttrain-rmse:1.772400\n",
      "[5588]\ttrain-rmse:1.772351\n",
      "[5589]\ttrain-rmse:1.772312\n",
      "[5590]\ttrain-rmse:1.772272\n",
      "[5591]\ttrain-rmse:1.772229\n",
      "[5592]\ttrain-rmse:1.772191\n",
      "[5593]\ttrain-rmse:1.772153\n",
      "[5594]\ttrain-rmse:1.772114\n",
      "[5595]\ttrain-rmse:1.772073\n",
      "[5596]\ttrain-rmse:1.772032\n",
      "[5597]\ttrain-rmse:1.771989\n",
      "[5598]\ttrain-rmse:1.771953\n",
      "[5599]\ttrain-rmse:1.771910\n",
      "[5600]\ttrain-rmse:1.771874\n",
      "[5601]\ttrain-rmse:1.771836\n",
      "[5602]\ttrain-rmse:1.771789\n",
      "[5603]\ttrain-rmse:1.771749\n",
      "[5604]\ttrain-rmse:1.771708\n",
      "[5605]\ttrain-rmse:1.771667\n",
      "[5606]\ttrain-rmse:1.771626\n",
      "[5607]\ttrain-rmse:1.771587\n",
      "[5608]\ttrain-rmse:1.771553\n",
      "[5609]\ttrain-rmse:1.771515\n",
      "[5610]\ttrain-rmse:1.771473\n",
      "[5611]\ttrain-rmse:1.771434\n",
      "[5612]\ttrain-rmse:1.771393\n",
      "[5613]\ttrain-rmse:1.771354\n",
      "[5614]\ttrain-rmse:1.771310\n",
      "[5615]\ttrain-rmse:1.771268\n",
      "[5616]\ttrain-rmse:1.771226\n",
      "[5617]\ttrain-rmse:1.771183\n",
      "[5618]\ttrain-rmse:1.771144\n",
      "[5619]\ttrain-rmse:1.771102\n",
      "[5620]\ttrain-rmse:1.771063\n",
      "[5621]\ttrain-rmse:1.771023\n",
      "[5622]\ttrain-rmse:1.770974\n",
      "[5623]\ttrain-rmse:1.770936\n",
      "[5624]\ttrain-rmse:1.770900\n",
      "[5625]\ttrain-rmse:1.770862\n",
      "[5626]\ttrain-rmse:1.770824\n",
      "[5627]\ttrain-rmse:1.770781\n",
      "[5628]\ttrain-rmse:1.770737\n",
      "[5629]\ttrain-rmse:1.770696\n",
      "[5630]\ttrain-rmse:1.770659\n",
      "[5631]\ttrain-rmse:1.770626\n",
      "[5632]\ttrain-rmse:1.770583\n",
      "[5633]\ttrain-rmse:1.770541\n",
      "[5634]\ttrain-rmse:1.770504\n",
      "[5635]\ttrain-rmse:1.770460\n",
      "[5636]\ttrain-rmse:1.770419\n",
      "[5637]\ttrain-rmse:1.770372\n",
      "[5638]\ttrain-rmse:1.770333\n",
      "[5639]\ttrain-rmse:1.770286\n",
      "[5640]\ttrain-rmse:1.770251\n",
      "[5641]\ttrain-rmse:1.770210\n",
      "[5642]\ttrain-rmse:1.770169\n",
      "[5643]\ttrain-rmse:1.770135\n",
      "[5644]\ttrain-rmse:1.770093\n",
      "[5645]\ttrain-rmse:1.770053\n",
      "[5646]\ttrain-rmse:1.770005\n",
      "[5647]\ttrain-rmse:1.769962\n",
      "[5648]\ttrain-rmse:1.769925\n",
      "[5649]\ttrain-rmse:1.769883\n",
      "[5650]\ttrain-rmse:1.769846\n",
      "[5651]\ttrain-rmse:1.769806\n",
      "[5652]\ttrain-rmse:1.769768\n",
      "[5653]\ttrain-rmse:1.769725\n",
      "[5654]\ttrain-rmse:1.769682\n",
      "[5655]\ttrain-rmse:1.769637\n",
      "[5656]\ttrain-rmse:1.769599\n",
      "[5657]\ttrain-rmse:1.769562\n",
      "[5658]\ttrain-rmse:1.769522\n",
      "[5659]\ttrain-rmse:1.769484\n",
      "[5660]\ttrain-rmse:1.769446\n",
      "[5661]\ttrain-rmse:1.769408\n",
      "[5662]\ttrain-rmse:1.769367\n",
      "[5663]\ttrain-rmse:1.769319\n",
      "[5664]\ttrain-rmse:1.769280\n",
      "[5665]\ttrain-rmse:1.769234\n",
      "[5666]\ttrain-rmse:1.769198\n",
      "[5667]\ttrain-rmse:1.769156\n",
      "[5668]\ttrain-rmse:1.769117\n",
      "[5669]\ttrain-rmse:1.769076\n",
      "[5670]\ttrain-rmse:1.769025\n",
      "[5671]\ttrain-rmse:1.768986\n",
      "[5672]\ttrain-rmse:1.768945\n",
      "[5673]\ttrain-rmse:1.768908\n",
      "[5674]\ttrain-rmse:1.768874\n",
      "[5675]\ttrain-rmse:1.768832\n",
      "[5676]\ttrain-rmse:1.768790\n",
      "[5677]\ttrain-rmse:1.768750\n",
      "[5678]\ttrain-rmse:1.768707\n",
      "[5679]\ttrain-rmse:1.768671\n",
      "[5680]\ttrain-rmse:1.768626\n",
      "[5681]\ttrain-rmse:1.768589\n",
      "[5682]\ttrain-rmse:1.768548\n",
      "[5683]\ttrain-rmse:1.768506\n",
      "[5684]\ttrain-rmse:1.768474\n",
      "[5685]\ttrain-rmse:1.768435\n",
      "[5686]\ttrain-rmse:1.768403\n",
      "[5687]\ttrain-rmse:1.768360\n",
      "[5688]\ttrain-rmse:1.768325\n",
      "[5689]\ttrain-rmse:1.768285\n",
      "[5690]\ttrain-rmse:1.768241\n",
      "[5691]\ttrain-rmse:1.768202\n",
      "[5692]\ttrain-rmse:1.768165\n",
      "[5693]\ttrain-rmse:1.768123\n",
      "[5694]\ttrain-rmse:1.768089\n",
      "[5695]\ttrain-rmse:1.768046\n",
      "[5696]\ttrain-rmse:1.768003\n",
      "[5697]\ttrain-rmse:1.767959\n",
      "[5698]\ttrain-rmse:1.767914\n",
      "[5699]\ttrain-rmse:1.767877\n",
      "[5700]\ttrain-rmse:1.767841\n",
      "[5701]\ttrain-rmse:1.767795\n",
      "[5702]\ttrain-rmse:1.767757\n",
      "[5703]\ttrain-rmse:1.767717\n",
      "[5704]\ttrain-rmse:1.767679\n",
      "[5705]\ttrain-rmse:1.767638\n",
      "[5706]\ttrain-rmse:1.767591\n",
      "[5707]\ttrain-rmse:1.767551\n",
      "[5708]\ttrain-rmse:1.767511\n",
      "[5709]\ttrain-rmse:1.767474\n",
      "[5710]\ttrain-rmse:1.767434\n",
      "[5711]\ttrain-rmse:1.767395\n",
      "[5712]\ttrain-rmse:1.767365\n",
      "[5713]\ttrain-rmse:1.767320\n",
      "[5714]\ttrain-rmse:1.767282\n",
      "[5715]\ttrain-rmse:1.767245\n",
      "[5716]\ttrain-rmse:1.767209\n",
      "[5717]\ttrain-rmse:1.767177\n",
      "[5718]\ttrain-rmse:1.767143\n",
      "[5719]\ttrain-rmse:1.767095\n",
      "[5720]\ttrain-rmse:1.767061\n",
      "[5721]\ttrain-rmse:1.767027\n",
      "[5722]\ttrain-rmse:1.766989\n",
      "[5723]\ttrain-rmse:1.766951\n",
      "[5724]\ttrain-rmse:1.766903\n",
      "[5725]\ttrain-rmse:1.766861\n",
      "[5726]\ttrain-rmse:1.766821\n",
      "[5727]\ttrain-rmse:1.766782\n",
      "[5728]\ttrain-rmse:1.766749\n",
      "[5729]\ttrain-rmse:1.766708\n",
      "[5730]\ttrain-rmse:1.766668\n",
      "[5731]\ttrain-rmse:1.766629\n",
      "[5732]\ttrain-rmse:1.766587\n",
      "[5733]\ttrain-rmse:1.766550\n",
      "[5734]\ttrain-rmse:1.766511\n",
      "[5735]\ttrain-rmse:1.766463\n",
      "[5736]\ttrain-rmse:1.766421\n",
      "[5737]\ttrain-rmse:1.766382\n",
      "[5738]\ttrain-rmse:1.766346\n",
      "[5739]\ttrain-rmse:1.766311\n",
      "[5740]\ttrain-rmse:1.766278\n",
      "[5741]\ttrain-rmse:1.766243\n",
      "[5742]\ttrain-rmse:1.766204\n",
      "[5743]\ttrain-rmse:1.766169\n",
      "[5744]\ttrain-rmse:1.766132\n",
      "[5745]\ttrain-rmse:1.766096\n",
      "[5746]\ttrain-rmse:1.766056\n",
      "[5747]\ttrain-rmse:1.766016\n",
      "[5748]\ttrain-rmse:1.765979\n",
      "[5749]\ttrain-rmse:1.765940\n",
      "[5750]\ttrain-rmse:1.765893\n",
      "[5751]\ttrain-rmse:1.765851\n",
      "[5752]\ttrain-rmse:1.765818\n",
      "[5753]\ttrain-rmse:1.765770\n",
      "[5754]\ttrain-rmse:1.765734\n",
      "[5755]\ttrain-rmse:1.765695\n",
      "[5756]\ttrain-rmse:1.765658\n",
      "[5757]\ttrain-rmse:1.765619\n",
      "[5758]\ttrain-rmse:1.765585\n",
      "[5759]\ttrain-rmse:1.765544\n",
      "[5760]\ttrain-rmse:1.765508\n",
      "[5761]\ttrain-rmse:1.765476\n",
      "[5762]\ttrain-rmse:1.765437\n",
      "[5763]\ttrain-rmse:1.765400\n",
      "[5764]\ttrain-rmse:1.765356\n",
      "[5765]\ttrain-rmse:1.765317\n",
      "[5766]\ttrain-rmse:1.765285\n",
      "[5767]\ttrain-rmse:1.765239\n",
      "[5768]\ttrain-rmse:1.765201\n",
      "[5769]\ttrain-rmse:1.765157\n",
      "[5770]\ttrain-rmse:1.765116\n",
      "[5771]\ttrain-rmse:1.765082\n",
      "[5772]\ttrain-rmse:1.765039\n",
      "[5773]\ttrain-rmse:1.765001\n",
      "[5774]\ttrain-rmse:1.764964\n",
      "[5775]\ttrain-rmse:1.764932\n",
      "[5776]\ttrain-rmse:1.764898\n",
      "[5777]\ttrain-rmse:1.764865\n",
      "[5778]\ttrain-rmse:1.764830\n",
      "[5779]\ttrain-rmse:1.764799\n",
      "[5780]\ttrain-rmse:1.764766\n",
      "[5781]\ttrain-rmse:1.764727\n",
      "[5782]\ttrain-rmse:1.764690\n",
      "[5783]\ttrain-rmse:1.764655\n",
      "[5784]\ttrain-rmse:1.764618\n",
      "[5785]\ttrain-rmse:1.764585\n",
      "[5786]\ttrain-rmse:1.764547\n",
      "[5787]\ttrain-rmse:1.764507\n",
      "[5788]\ttrain-rmse:1.764472\n",
      "[5789]\ttrain-rmse:1.764431\n",
      "[5790]\ttrain-rmse:1.764397\n",
      "[5791]\ttrain-rmse:1.764363\n",
      "[5792]\ttrain-rmse:1.764327\n",
      "[5793]\ttrain-rmse:1.764282\n",
      "[5794]\ttrain-rmse:1.764250\n",
      "[5795]\ttrain-rmse:1.764216\n",
      "[5796]\ttrain-rmse:1.764184\n",
      "[5797]\ttrain-rmse:1.764150\n",
      "[5798]\ttrain-rmse:1.764116\n",
      "[5799]\ttrain-rmse:1.764081\n",
      "[5800]\ttrain-rmse:1.764042\n",
      "[5801]\ttrain-rmse:1.764015\n",
      "[5802]\ttrain-rmse:1.763981\n",
      "[5803]\ttrain-rmse:1.763933\n",
      "[5804]\ttrain-rmse:1.763893\n",
      "[5805]\ttrain-rmse:1.763852\n",
      "[5806]\ttrain-rmse:1.763814\n",
      "[5807]\ttrain-rmse:1.763778\n",
      "[5808]\ttrain-rmse:1.763744\n",
      "[5809]\ttrain-rmse:1.763712\n",
      "[5810]\ttrain-rmse:1.763681\n",
      "[5811]\ttrain-rmse:1.763646\n",
      "[5812]\ttrain-rmse:1.763607\n",
      "[5813]\ttrain-rmse:1.763572\n",
      "[5814]\ttrain-rmse:1.763530\n",
      "[5815]\ttrain-rmse:1.763491\n",
      "[5816]\ttrain-rmse:1.763441\n",
      "[5817]\ttrain-rmse:1.763409\n",
      "[5818]\ttrain-rmse:1.763370\n",
      "[5819]\ttrain-rmse:1.763335\n",
      "[5820]\ttrain-rmse:1.763301\n",
      "[5821]\ttrain-rmse:1.763262\n",
      "[5822]\ttrain-rmse:1.763222\n",
      "[5823]\ttrain-rmse:1.763197\n",
      "[5824]\ttrain-rmse:1.763161\n",
      "[5825]\ttrain-rmse:1.763122\n",
      "[5826]\ttrain-rmse:1.763079\n",
      "[5827]\ttrain-rmse:1.763041\n",
      "[5828]\ttrain-rmse:1.763007\n",
      "[5829]\ttrain-rmse:1.762970\n",
      "[5830]\ttrain-rmse:1.762936\n",
      "[5831]\ttrain-rmse:1.762900\n",
      "[5832]\ttrain-rmse:1.762857\n",
      "[5833]\ttrain-rmse:1.762817\n",
      "[5834]\ttrain-rmse:1.762776\n",
      "[5835]\ttrain-rmse:1.762739\n",
      "[5836]\ttrain-rmse:1.762704\n",
      "[5837]\ttrain-rmse:1.762671\n",
      "[5838]\ttrain-rmse:1.762642\n",
      "[5839]\ttrain-rmse:1.762606\n",
      "[5840]\ttrain-rmse:1.762574\n",
      "[5841]\ttrain-rmse:1.762541\n",
      "[5842]\ttrain-rmse:1.762495\n",
      "[5843]\ttrain-rmse:1.762455\n",
      "[5844]\ttrain-rmse:1.762422\n",
      "[5845]\ttrain-rmse:1.762386\n",
      "[5846]\ttrain-rmse:1.762351\n",
      "[5847]\ttrain-rmse:1.762313\n",
      "[5848]\ttrain-rmse:1.762276\n",
      "[5849]\ttrain-rmse:1.762243\n",
      "[5850]\ttrain-rmse:1.762212\n",
      "[5851]\ttrain-rmse:1.762181\n",
      "[5852]\ttrain-rmse:1.762150\n",
      "[5853]\ttrain-rmse:1.762119\n",
      "[5854]\ttrain-rmse:1.762089\n",
      "[5855]\ttrain-rmse:1.762057\n",
      "[5856]\ttrain-rmse:1.762016\n",
      "[5857]\ttrain-rmse:1.761981\n",
      "[5858]\ttrain-rmse:1.761947\n",
      "[5859]\ttrain-rmse:1.761908\n",
      "[5860]\ttrain-rmse:1.761876\n",
      "[5861]\ttrain-rmse:1.761835\n",
      "[5862]\ttrain-rmse:1.761791\n",
      "[5863]\ttrain-rmse:1.761758\n",
      "[5864]\ttrain-rmse:1.761725\n",
      "[5865]\ttrain-rmse:1.761690\n",
      "[5866]\ttrain-rmse:1.761652\n",
      "[5867]\ttrain-rmse:1.761619\n",
      "[5868]\ttrain-rmse:1.761583\n",
      "[5869]\ttrain-rmse:1.761545\n",
      "[5870]\ttrain-rmse:1.761501\n",
      "[5871]\ttrain-rmse:1.761467\n",
      "[5872]\ttrain-rmse:1.761423\n",
      "[5873]\ttrain-rmse:1.761393\n",
      "[5874]\ttrain-rmse:1.761356\n",
      "[5875]\ttrain-rmse:1.761319\n",
      "[5876]\ttrain-rmse:1.761287\n",
      "[5877]\ttrain-rmse:1.761254\n",
      "[5878]\ttrain-rmse:1.761222\n",
      "[5879]\ttrain-rmse:1.761190\n",
      "[5880]\ttrain-rmse:1.761152\n",
      "[5881]\ttrain-rmse:1.761114\n",
      "[5882]\ttrain-rmse:1.761074\n",
      "[5883]\ttrain-rmse:1.761041\n",
      "[5884]\ttrain-rmse:1.760997\n",
      "[5885]\ttrain-rmse:1.760961\n",
      "[5886]\ttrain-rmse:1.760930\n",
      "[5887]\ttrain-rmse:1.760902\n",
      "[5888]\ttrain-rmse:1.760867\n",
      "[5889]\ttrain-rmse:1.760836\n",
      "[5890]\ttrain-rmse:1.760803\n",
      "[5891]\ttrain-rmse:1.760768\n",
      "[5892]\ttrain-rmse:1.760733\n",
      "[5893]\ttrain-rmse:1.760704\n",
      "[5894]\ttrain-rmse:1.760672\n",
      "[5895]\ttrain-rmse:1.760637\n",
      "[5896]\ttrain-rmse:1.760606\n",
      "[5897]\ttrain-rmse:1.760575\n",
      "[5898]\ttrain-rmse:1.760547\n",
      "[5899]\ttrain-rmse:1.760510\n",
      "[5900]\ttrain-rmse:1.760475\n",
      "[5901]\ttrain-rmse:1.760443\n",
      "[5902]\ttrain-rmse:1.760407\n",
      "[5903]\ttrain-rmse:1.760375\n",
      "[5904]\ttrain-rmse:1.760337\n",
      "[5905]\ttrain-rmse:1.760297\n",
      "[5906]\ttrain-rmse:1.760264\n",
      "[5907]\ttrain-rmse:1.760225\n",
      "[5908]\ttrain-rmse:1.760193\n",
      "[5909]\ttrain-rmse:1.760157\n",
      "[5910]\ttrain-rmse:1.760127\n",
      "[5911]\ttrain-rmse:1.760088\n",
      "[5912]\ttrain-rmse:1.760057\n",
      "[5913]\ttrain-rmse:1.760015\n",
      "[5914]\ttrain-rmse:1.759984\n",
      "[5915]\ttrain-rmse:1.759951\n",
      "[5916]\ttrain-rmse:1.759916\n",
      "[5917]\ttrain-rmse:1.759888\n",
      "[5918]\ttrain-rmse:1.759854\n",
      "[5919]\ttrain-rmse:1.759821\n",
      "[5920]\ttrain-rmse:1.759788\n",
      "[5921]\ttrain-rmse:1.759746\n",
      "[5922]\ttrain-rmse:1.759718\n",
      "[5923]\ttrain-rmse:1.759686\n",
      "[5924]\ttrain-rmse:1.759649\n",
      "[5925]\ttrain-rmse:1.759615\n",
      "[5926]\ttrain-rmse:1.759582\n",
      "[5927]\ttrain-rmse:1.759551\n",
      "[5928]\ttrain-rmse:1.759515\n",
      "[5929]\ttrain-rmse:1.759479\n",
      "[5930]\ttrain-rmse:1.759444\n",
      "[5931]\ttrain-rmse:1.759412\n",
      "[5932]\ttrain-rmse:1.759372\n",
      "[5933]\ttrain-rmse:1.759339\n",
      "[5934]\ttrain-rmse:1.759308\n",
      "[5935]\ttrain-rmse:1.759267\n",
      "[5936]\ttrain-rmse:1.759232\n",
      "[5937]\ttrain-rmse:1.759196\n",
      "[5938]\ttrain-rmse:1.759164\n",
      "[5939]\ttrain-rmse:1.759131\n",
      "[5940]\ttrain-rmse:1.759095\n",
      "[5941]\ttrain-rmse:1.759064\n",
      "[5942]\ttrain-rmse:1.759028\n",
      "[5943]\ttrain-rmse:1.758994\n",
      "[5944]\ttrain-rmse:1.758963\n",
      "[5945]\ttrain-rmse:1.758931\n",
      "[5946]\ttrain-rmse:1.758900\n",
      "[5947]\ttrain-rmse:1.758867\n",
      "[5948]\ttrain-rmse:1.758834\n",
      "[5949]\ttrain-rmse:1.758797\n",
      "[5950]\ttrain-rmse:1.758757\n",
      "[5951]\ttrain-rmse:1.758724\n",
      "[5952]\ttrain-rmse:1.758690\n",
      "[5953]\ttrain-rmse:1.758651\n",
      "[5954]\ttrain-rmse:1.758608\n",
      "[5955]\ttrain-rmse:1.758581\n",
      "[5956]\ttrain-rmse:1.758547\n",
      "[5957]\ttrain-rmse:1.758516\n",
      "[5958]\ttrain-rmse:1.758485\n",
      "[5959]\ttrain-rmse:1.758456\n",
      "[5960]\ttrain-rmse:1.758419\n",
      "[5961]\ttrain-rmse:1.758382\n",
      "[5962]\ttrain-rmse:1.758351\n",
      "[5963]\ttrain-rmse:1.758309\n",
      "[5964]\ttrain-rmse:1.758282\n",
      "[5965]\ttrain-rmse:1.758245\n",
      "[5966]\ttrain-rmse:1.758214\n",
      "[5967]\ttrain-rmse:1.758186\n",
      "[5968]\ttrain-rmse:1.758156\n",
      "[5969]\ttrain-rmse:1.758117\n",
      "[5970]\ttrain-rmse:1.758082\n",
      "[5971]\ttrain-rmse:1.758050\n",
      "[5972]\ttrain-rmse:1.758011\n",
      "[5973]\ttrain-rmse:1.757976\n",
      "[5974]\ttrain-rmse:1.757949\n",
      "[5975]\ttrain-rmse:1.757912\n",
      "[5976]\ttrain-rmse:1.757873\n",
      "[5977]\ttrain-rmse:1.757841\n",
      "[5978]\ttrain-rmse:1.757810\n",
      "[5979]\ttrain-rmse:1.757779\n",
      "[5980]\ttrain-rmse:1.757748\n",
      "[5981]\ttrain-rmse:1.757718\n",
      "[5982]\ttrain-rmse:1.757682\n",
      "[5983]\ttrain-rmse:1.757653\n",
      "[5984]\ttrain-rmse:1.757619\n",
      "[5985]\ttrain-rmse:1.757583\n",
      "[5986]\ttrain-rmse:1.757549\n",
      "[5987]\ttrain-rmse:1.757515\n",
      "[5988]\ttrain-rmse:1.757486\n",
      "[5989]\ttrain-rmse:1.757450\n",
      "[5990]\ttrain-rmse:1.757415\n",
      "[5991]\ttrain-rmse:1.757379\n",
      "[5992]\ttrain-rmse:1.757350\n",
      "[5993]\ttrain-rmse:1.757317\n",
      "[5994]\ttrain-rmse:1.757282\n",
      "[5995]\ttrain-rmse:1.757251\n",
      "[5996]\ttrain-rmse:1.757220\n",
      "[5997]\ttrain-rmse:1.757183\n",
      "[5998]\ttrain-rmse:1.757149\n",
      "[5999]\ttrain-rmse:1.757115\n",
      "[6000]\ttrain-rmse:1.757077\n",
      "[6001]\ttrain-rmse:1.757055\n",
      "[6002]\ttrain-rmse:1.757016\n",
      "[6003]\ttrain-rmse:1.756981\n",
      "[6004]\ttrain-rmse:1.756949\n",
      "[6005]\ttrain-rmse:1.756911\n",
      "[6006]\ttrain-rmse:1.756881\n",
      "[6007]\ttrain-rmse:1.756850\n",
      "[6008]\ttrain-rmse:1.756815\n",
      "[6009]\ttrain-rmse:1.756775\n",
      "[6010]\ttrain-rmse:1.756740\n",
      "[6011]\ttrain-rmse:1.756705\n",
      "[6012]\ttrain-rmse:1.756672\n",
      "[6013]\ttrain-rmse:1.756635\n",
      "[6014]\ttrain-rmse:1.756604\n",
      "[6015]\ttrain-rmse:1.756575\n",
      "[6016]\ttrain-rmse:1.756545\n",
      "[6017]\ttrain-rmse:1.756503\n",
      "[6018]\ttrain-rmse:1.756471\n",
      "[6019]\ttrain-rmse:1.756440\n",
      "[6020]\ttrain-rmse:1.756412\n",
      "[6021]\ttrain-rmse:1.756383\n",
      "[6022]\ttrain-rmse:1.756347\n",
      "[6023]\ttrain-rmse:1.756315\n",
      "[6024]\ttrain-rmse:1.756286\n",
      "[6025]\ttrain-rmse:1.756252\n",
      "[6026]\ttrain-rmse:1.756212\n",
      "[6027]\ttrain-rmse:1.756178\n",
      "[6028]\ttrain-rmse:1.756152\n",
      "[6029]\ttrain-rmse:1.756116\n",
      "[6030]\ttrain-rmse:1.756086\n",
      "[6031]\ttrain-rmse:1.756055\n",
      "[6032]\ttrain-rmse:1.756031\n",
      "[6033]\ttrain-rmse:1.755993\n",
      "[6034]\ttrain-rmse:1.755953\n",
      "[6035]\ttrain-rmse:1.755920\n",
      "[6036]\ttrain-rmse:1.755891\n",
      "[6037]\ttrain-rmse:1.755856\n",
      "[6038]\ttrain-rmse:1.755826\n",
      "[6039]\ttrain-rmse:1.755793\n",
      "[6040]\ttrain-rmse:1.755767\n",
      "[6041]\ttrain-rmse:1.755744\n",
      "[6042]\ttrain-rmse:1.755716\n",
      "[6043]\ttrain-rmse:1.755684\n",
      "[6044]\ttrain-rmse:1.755650\n",
      "[6045]\ttrain-rmse:1.755618\n",
      "[6046]\ttrain-rmse:1.755587\n",
      "[6047]\ttrain-rmse:1.755552\n",
      "[6048]\ttrain-rmse:1.755515\n",
      "[6049]\ttrain-rmse:1.755483\n",
      "[6050]\ttrain-rmse:1.755454\n",
      "[6051]\ttrain-rmse:1.755422\n",
      "[6052]\ttrain-rmse:1.755388\n",
      "[6053]\ttrain-rmse:1.755353\n",
      "[6054]\ttrain-rmse:1.755320\n",
      "[6055]\ttrain-rmse:1.755290\n",
      "[6056]\ttrain-rmse:1.755264\n",
      "[6057]\ttrain-rmse:1.755236\n",
      "[6058]\ttrain-rmse:1.755206\n",
      "[6059]\ttrain-rmse:1.755173\n",
      "[6060]\ttrain-rmse:1.755142\n",
      "[6061]\ttrain-rmse:1.755112\n",
      "[6062]\ttrain-rmse:1.755082\n",
      "[6063]\ttrain-rmse:1.755053\n",
      "[6064]\ttrain-rmse:1.755027\n",
      "[6065]\ttrain-rmse:1.754985\n",
      "[6066]\ttrain-rmse:1.754949\n",
      "[6067]\ttrain-rmse:1.754921\n",
      "[6068]\ttrain-rmse:1.754886\n",
      "[6069]\ttrain-rmse:1.754856\n",
      "[6070]\ttrain-rmse:1.754818\n",
      "[6071]\ttrain-rmse:1.754792\n",
      "[6072]\ttrain-rmse:1.754754\n",
      "[6073]\ttrain-rmse:1.754723\n",
      "[6074]\ttrain-rmse:1.754694\n",
      "[6075]\ttrain-rmse:1.754665\n",
      "[6076]\ttrain-rmse:1.754631\n",
      "[6077]\ttrain-rmse:1.754598\n",
      "[6078]\ttrain-rmse:1.754564\n",
      "[6079]\ttrain-rmse:1.754533\n",
      "[6080]\ttrain-rmse:1.754498\n",
      "[6081]\ttrain-rmse:1.754464\n",
      "[6082]\ttrain-rmse:1.754431\n",
      "[6083]\ttrain-rmse:1.754399\n",
      "[6084]\ttrain-rmse:1.754364\n",
      "[6085]\ttrain-rmse:1.754336\n",
      "[6086]\ttrain-rmse:1.754303\n",
      "[6087]\ttrain-rmse:1.754272\n",
      "[6088]\ttrain-rmse:1.754241\n",
      "[6089]\ttrain-rmse:1.754212\n",
      "[6090]\ttrain-rmse:1.754184\n",
      "[6091]\ttrain-rmse:1.754152\n",
      "[6092]\ttrain-rmse:1.754127\n",
      "[6093]\ttrain-rmse:1.754101\n",
      "[6094]\ttrain-rmse:1.754074\n",
      "[6095]\ttrain-rmse:1.754050\n",
      "[6096]\ttrain-rmse:1.754016\n",
      "[6097]\ttrain-rmse:1.753981\n",
      "[6098]\ttrain-rmse:1.753953\n",
      "[6099]\ttrain-rmse:1.753923\n",
      "[6100]\ttrain-rmse:1.753890\n",
      "[6101]\ttrain-rmse:1.753857\n",
      "[6102]\ttrain-rmse:1.753829\n",
      "[6103]\ttrain-rmse:1.753796\n",
      "[6104]\ttrain-rmse:1.753771\n",
      "[6105]\ttrain-rmse:1.753738\n",
      "[6106]\ttrain-rmse:1.753711\n",
      "[6107]\ttrain-rmse:1.753681\n",
      "[6108]\ttrain-rmse:1.753644\n",
      "[6109]\ttrain-rmse:1.753617\n",
      "[6110]\ttrain-rmse:1.753591\n",
      "[6111]\ttrain-rmse:1.753562\n",
      "[6112]\ttrain-rmse:1.753533\n",
      "[6113]\ttrain-rmse:1.753507\n",
      "[6114]\ttrain-rmse:1.753467\n",
      "[6115]\ttrain-rmse:1.753435\n",
      "[6116]\ttrain-rmse:1.753400\n",
      "[6117]\ttrain-rmse:1.753373\n",
      "[6118]\ttrain-rmse:1.753338\n",
      "[6119]\ttrain-rmse:1.753309\n",
      "[6120]\ttrain-rmse:1.753275\n",
      "[6121]\ttrain-rmse:1.753247\n",
      "[6122]\ttrain-rmse:1.753216\n",
      "[6123]\ttrain-rmse:1.753188\n",
      "[6124]\ttrain-rmse:1.753159\n",
      "[6125]\ttrain-rmse:1.753118\n",
      "[6126]\ttrain-rmse:1.753094\n",
      "[6127]\ttrain-rmse:1.753063\n",
      "[6128]\ttrain-rmse:1.753023\n",
      "[6129]\ttrain-rmse:1.752994\n",
      "[6130]\ttrain-rmse:1.752958\n",
      "[6131]\ttrain-rmse:1.752930\n",
      "[6132]\ttrain-rmse:1.752901\n",
      "[6133]\ttrain-rmse:1.752872\n",
      "[6134]\ttrain-rmse:1.752841\n",
      "[6135]\ttrain-rmse:1.752811\n",
      "[6136]\ttrain-rmse:1.752777\n",
      "[6137]\ttrain-rmse:1.752740\n",
      "[6138]\ttrain-rmse:1.752709\n",
      "[6139]\ttrain-rmse:1.752682\n",
      "[6140]\ttrain-rmse:1.752658\n",
      "[6141]\ttrain-rmse:1.752629\n",
      "[6142]\ttrain-rmse:1.752602\n",
      "[6143]\ttrain-rmse:1.752567\n",
      "[6144]\ttrain-rmse:1.752540\n",
      "[6145]\ttrain-rmse:1.752513\n",
      "[6146]\ttrain-rmse:1.752474\n",
      "[6147]\ttrain-rmse:1.752448\n",
      "[6148]\ttrain-rmse:1.752408\n",
      "[6149]\ttrain-rmse:1.752378\n",
      "[6150]\ttrain-rmse:1.752343\n",
      "[6151]\ttrain-rmse:1.752313\n",
      "[6152]\ttrain-rmse:1.752280\n",
      "[6153]\ttrain-rmse:1.752251\n",
      "[6154]\ttrain-rmse:1.752218\n",
      "[6155]\ttrain-rmse:1.752192\n",
      "[6156]\ttrain-rmse:1.752165\n",
      "[6157]\ttrain-rmse:1.752129\n",
      "[6158]\ttrain-rmse:1.752100\n",
      "[6159]\ttrain-rmse:1.752072\n",
      "[6160]\ttrain-rmse:1.752045\n",
      "[6161]\ttrain-rmse:1.752017\n",
      "[6162]\ttrain-rmse:1.751990\n",
      "[6163]\ttrain-rmse:1.751960\n",
      "[6164]\ttrain-rmse:1.751921\n",
      "[6165]\ttrain-rmse:1.751893\n",
      "[6166]\ttrain-rmse:1.751861\n",
      "[6167]\ttrain-rmse:1.751830\n",
      "[6168]\ttrain-rmse:1.751800\n",
      "[6169]\ttrain-rmse:1.751772\n",
      "[6170]\ttrain-rmse:1.751743\n",
      "[6171]\ttrain-rmse:1.751712\n",
      "[6172]\ttrain-rmse:1.751683\n",
      "[6173]\ttrain-rmse:1.751660\n",
      "[6174]\ttrain-rmse:1.751630\n",
      "[6175]\ttrain-rmse:1.751602\n",
      "[6176]\ttrain-rmse:1.751572\n",
      "[6177]\ttrain-rmse:1.751541\n",
      "[6178]\ttrain-rmse:1.751513\n",
      "[6179]\ttrain-rmse:1.751482\n",
      "[6180]\ttrain-rmse:1.751454\n",
      "[6181]\ttrain-rmse:1.751424\n",
      "[6182]\ttrain-rmse:1.751390\n",
      "[6183]\ttrain-rmse:1.751353\n",
      "[6184]\ttrain-rmse:1.751322\n",
      "[6185]\ttrain-rmse:1.751297\n",
      "[6186]\ttrain-rmse:1.751261\n",
      "[6187]\ttrain-rmse:1.751239\n",
      "[6188]\ttrain-rmse:1.751214\n",
      "[6189]\ttrain-rmse:1.751182\n",
      "[6190]\ttrain-rmse:1.751147\n",
      "[6191]\ttrain-rmse:1.751126\n",
      "[6192]\ttrain-rmse:1.751097\n",
      "[6193]\ttrain-rmse:1.751067\n",
      "[6194]\ttrain-rmse:1.751037\n",
      "[6195]\ttrain-rmse:1.751003\n",
      "[6196]\ttrain-rmse:1.750977\n",
      "[6197]\ttrain-rmse:1.750947\n",
      "[6198]\ttrain-rmse:1.750916\n",
      "[6199]\ttrain-rmse:1.750882\n",
      "[6200]\ttrain-rmse:1.750850\n",
      "[6201]\ttrain-rmse:1.750821\n",
      "[6202]\ttrain-rmse:1.750787\n",
      "[6203]\ttrain-rmse:1.750755\n",
      "[6204]\ttrain-rmse:1.750724\n",
      "[6205]\ttrain-rmse:1.750688\n",
      "[6206]\ttrain-rmse:1.750655\n",
      "[6207]\ttrain-rmse:1.750630\n",
      "[6208]\ttrain-rmse:1.750603\n",
      "[6209]\ttrain-rmse:1.750578\n",
      "[6210]\ttrain-rmse:1.750549\n",
      "[6211]\ttrain-rmse:1.750523\n",
      "[6212]\ttrain-rmse:1.750499\n",
      "[6213]\ttrain-rmse:1.750463\n",
      "[6214]\ttrain-rmse:1.750435\n",
      "[6215]\ttrain-rmse:1.750397\n",
      "[6216]\ttrain-rmse:1.750365\n",
      "[6217]\ttrain-rmse:1.750338\n",
      "[6218]\ttrain-rmse:1.750309\n",
      "[6219]\ttrain-rmse:1.750279\n",
      "[6220]\ttrain-rmse:1.750249\n",
      "[6221]\ttrain-rmse:1.750217\n",
      "[6222]\ttrain-rmse:1.750185\n",
      "[6223]\ttrain-rmse:1.750154\n",
      "[6224]\ttrain-rmse:1.750125\n",
      "[6225]\ttrain-rmse:1.750094\n",
      "[6226]\ttrain-rmse:1.750065\n",
      "[6227]\ttrain-rmse:1.750032\n",
      "[6228]\ttrain-rmse:1.750006\n",
      "[6229]\ttrain-rmse:1.749976\n",
      "[6230]\ttrain-rmse:1.749946\n",
      "[6231]\ttrain-rmse:1.749912\n",
      "[6232]\ttrain-rmse:1.749880\n",
      "[6233]\ttrain-rmse:1.749848\n",
      "[6234]\ttrain-rmse:1.749821\n",
      "[6235]\ttrain-rmse:1.749801\n",
      "[6236]\ttrain-rmse:1.749774\n",
      "[6237]\ttrain-rmse:1.749745\n",
      "[6238]\ttrain-rmse:1.749717\n",
      "[6239]\ttrain-rmse:1.749685\n",
      "[6240]\ttrain-rmse:1.749649\n",
      "[6241]\ttrain-rmse:1.749622\n",
      "[6242]\ttrain-rmse:1.749590\n",
      "[6243]\ttrain-rmse:1.749564\n",
      "[6244]\ttrain-rmse:1.749534\n",
      "[6245]\ttrain-rmse:1.749504\n",
      "[6246]\ttrain-rmse:1.749474\n",
      "[6247]\ttrain-rmse:1.749448\n",
      "[6248]\ttrain-rmse:1.749422\n",
      "[6249]\ttrain-rmse:1.749396\n",
      "[6250]\ttrain-rmse:1.749369\n",
      "[6251]\ttrain-rmse:1.749343\n",
      "[6252]\ttrain-rmse:1.749314\n",
      "[6253]\ttrain-rmse:1.749290\n",
      "[6254]\ttrain-rmse:1.749256\n",
      "[6255]\ttrain-rmse:1.749229\n",
      "[6256]\ttrain-rmse:1.749199\n",
      "[6257]\ttrain-rmse:1.749172\n",
      "[6258]\ttrain-rmse:1.749143\n",
      "[6259]\ttrain-rmse:1.749118\n",
      "[6260]\ttrain-rmse:1.749082\n",
      "[6261]\ttrain-rmse:1.749057\n",
      "[6262]\ttrain-rmse:1.749024\n",
      "[6263]\ttrain-rmse:1.748997\n",
      "[6264]\ttrain-rmse:1.748971\n",
      "[6265]\ttrain-rmse:1.748946\n",
      "[6266]\ttrain-rmse:1.748920\n",
      "[6267]\ttrain-rmse:1.748893\n",
      "[6268]\ttrain-rmse:1.748866\n",
      "[6269]\ttrain-rmse:1.748840\n",
      "[6270]\ttrain-rmse:1.748821\n",
      "[6271]\ttrain-rmse:1.748792\n",
      "[6272]\ttrain-rmse:1.748768\n",
      "[6273]\ttrain-rmse:1.748738\n",
      "[6274]\ttrain-rmse:1.748711\n",
      "[6275]\ttrain-rmse:1.748677\n",
      "[6276]\ttrain-rmse:1.748651\n",
      "[6277]\ttrain-rmse:1.748623\n",
      "[6278]\ttrain-rmse:1.748591\n",
      "[6279]\ttrain-rmse:1.748564\n",
      "[6280]\ttrain-rmse:1.748538\n",
      "[6281]\ttrain-rmse:1.748515\n",
      "[6282]\ttrain-rmse:1.748484\n",
      "[6283]\ttrain-rmse:1.748451\n",
      "[6284]\ttrain-rmse:1.748422\n",
      "[6285]\ttrain-rmse:1.748398\n",
      "[6286]\ttrain-rmse:1.748373\n",
      "[6287]\ttrain-rmse:1.748340\n",
      "[6288]\ttrain-rmse:1.748313\n",
      "[6289]\ttrain-rmse:1.748284\n",
      "[6290]\ttrain-rmse:1.748251\n",
      "[6291]\ttrain-rmse:1.748220\n",
      "[6292]\ttrain-rmse:1.748193\n",
      "[6293]\ttrain-rmse:1.748166\n",
      "[6294]\ttrain-rmse:1.748133\n",
      "[6295]\ttrain-rmse:1.748101\n",
      "[6296]\ttrain-rmse:1.748078\n",
      "[6297]\ttrain-rmse:1.748045\n",
      "[6298]\ttrain-rmse:1.748023\n",
      "[6299]\ttrain-rmse:1.747993\n",
      "[6300]\ttrain-rmse:1.747967\n",
      "[6301]\ttrain-rmse:1.747938\n",
      "[6302]\ttrain-rmse:1.747911\n",
      "[6303]\ttrain-rmse:1.747884\n",
      "[6304]\ttrain-rmse:1.747858\n",
      "[6305]\ttrain-rmse:1.747831\n",
      "[6306]\ttrain-rmse:1.747794\n",
      "[6307]\ttrain-rmse:1.747755\n",
      "[6308]\ttrain-rmse:1.747731\n",
      "[6309]\ttrain-rmse:1.747705\n",
      "[6310]\ttrain-rmse:1.747683\n",
      "[6311]\ttrain-rmse:1.747661\n",
      "[6312]\ttrain-rmse:1.747637\n",
      "[6313]\ttrain-rmse:1.747611\n",
      "[6314]\ttrain-rmse:1.747582\n",
      "[6315]\ttrain-rmse:1.747552\n",
      "[6316]\ttrain-rmse:1.747525\n",
      "[6317]\ttrain-rmse:1.747497\n",
      "[6318]\ttrain-rmse:1.747460\n",
      "[6319]\ttrain-rmse:1.747434\n",
      "[6320]\ttrain-rmse:1.747411\n",
      "[6321]\ttrain-rmse:1.747385\n",
      "[6322]\ttrain-rmse:1.747360\n",
      "[6323]\ttrain-rmse:1.747332\n",
      "[6324]\ttrain-rmse:1.747302\n",
      "[6325]\ttrain-rmse:1.747265\n",
      "[6326]\ttrain-rmse:1.747232\n",
      "[6327]\ttrain-rmse:1.747209\n",
      "[6328]\ttrain-rmse:1.747173\n",
      "[6329]\ttrain-rmse:1.747151\n",
      "[6330]\ttrain-rmse:1.747125\n",
      "[6331]\ttrain-rmse:1.747098\n",
      "[6332]\ttrain-rmse:1.747071\n",
      "[6333]\ttrain-rmse:1.747041\n",
      "[6334]\ttrain-rmse:1.747013\n",
      "[6335]\ttrain-rmse:1.746993\n",
      "[6336]\ttrain-rmse:1.746963\n",
      "[6337]\ttrain-rmse:1.746933\n",
      "[6338]\ttrain-rmse:1.746905\n",
      "[6339]\ttrain-rmse:1.746877\n",
      "[6340]\ttrain-rmse:1.746851\n",
      "[6341]\ttrain-rmse:1.746825\n",
      "[6342]\ttrain-rmse:1.746801\n",
      "[6343]\ttrain-rmse:1.746771\n",
      "[6344]\ttrain-rmse:1.746743\n",
      "[6345]\ttrain-rmse:1.746713\n",
      "[6346]\ttrain-rmse:1.746681\n",
      "[6347]\ttrain-rmse:1.746651\n",
      "[6348]\ttrain-rmse:1.746621\n",
      "[6349]\ttrain-rmse:1.746598\n",
      "[6350]\ttrain-rmse:1.746560\n",
      "[6351]\ttrain-rmse:1.746531\n",
      "[6352]\ttrain-rmse:1.746502\n",
      "[6353]\ttrain-rmse:1.746479\n",
      "[6354]\ttrain-rmse:1.746450\n",
      "[6355]\ttrain-rmse:1.746415\n",
      "[6356]\ttrain-rmse:1.746391\n",
      "[6357]\ttrain-rmse:1.746362\n",
      "[6358]\ttrain-rmse:1.746342\n",
      "[6359]\ttrain-rmse:1.746316\n",
      "[6360]\ttrain-rmse:1.746280\n",
      "[6361]\ttrain-rmse:1.746257\n",
      "[6362]\ttrain-rmse:1.746233\n",
      "[6363]\ttrain-rmse:1.746199\n",
      "[6364]\ttrain-rmse:1.746170\n",
      "[6365]\ttrain-rmse:1.746138\n",
      "[6366]\ttrain-rmse:1.746110\n",
      "[6367]\ttrain-rmse:1.746085\n",
      "[6368]\ttrain-rmse:1.746048\n",
      "[6369]\ttrain-rmse:1.746022\n",
      "[6370]\ttrain-rmse:1.745995\n",
      "[6371]\ttrain-rmse:1.745972\n",
      "[6372]\ttrain-rmse:1.745945\n",
      "[6373]\ttrain-rmse:1.745916\n",
      "[6374]\ttrain-rmse:1.745888\n",
      "[6375]\ttrain-rmse:1.745863\n",
      "[6376]\ttrain-rmse:1.745838\n",
      "[6377]\ttrain-rmse:1.745807\n",
      "[6378]\ttrain-rmse:1.745783\n",
      "[6379]\ttrain-rmse:1.745762\n",
      "[6380]\ttrain-rmse:1.745735\n",
      "[6381]\ttrain-rmse:1.745714\n",
      "[6382]\ttrain-rmse:1.745689\n",
      "[6383]\ttrain-rmse:1.745662\n",
      "[6384]\ttrain-rmse:1.745631\n",
      "[6385]\ttrain-rmse:1.745597\n",
      "[6386]\ttrain-rmse:1.745575\n",
      "[6387]\ttrain-rmse:1.745542\n",
      "[6388]\ttrain-rmse:1.745513\n",
      "[6389]\ttrain-rmse:1.745486\n",
      "[6390]\ttrain-rmse:1.745458\n",
      "[6391]\ttrain-rmse:1.745434\n",
      "[6392]\ttrain-rmse:1.745414\n",
      "[6393]\ttrain-rmse:1.745387\n",
      "[6394]\ttrain-rmse:1.745363\n",
      "[6395]\ttrain-rmse:1.745329\n",
      "[6396]\ttrain-rmse:1.745303\n",
      "[6397]\ttrain-rmse:1.745278\n",
      "[6398]\ttrain-rmse:1.745246\n",
      "[6399]\ttrain-rmse:1.745216\n",
      "[6400]\ttrain-rmse:1.745198\n",
      "[6401]\ttrain-rmse:1.745174\n",
      "[6402]\ttrain-rmse:1.745142\n",
      "[6403]\ttrain-rmse:1.745125\n",
      "[6404]\ttrain-rmse:1.745097\n",
      "[6405]\ttrain-rmse:1.745076\n",
      "[6406]\ttrain-rmse:1.745054\n",
      "[6407]\ttrain-rmse:1.745026\n",
      "[6408]\ttrain-rmse:1.744999\n",
      "[6409]\ttrain-rmse:1.744971\n",
      "[6410]\ttrain-rmse:1.744943\n",
      "[6411]\ttrain-rmse:1.744912\n",
      "[6412]\ttrain-rmse:1.744887\n",
      "[6413]\ttrain-rmse:1.744865\n",
      "[6414]\ttrain-rmse:1.744835\n",
      "[6415]\ttrain-rmse:1.744813\n",
      "[6416]\ttrain-rmse:1.744790\n",
      "[6417]\ttrain-rmse:1.744766\n",
      "[6418]\ttrain-rmse:1.744746\n",
      "[6419]\ttrain-rmse:1.744712\n",
      "[6420]\ttrain-rmse:1.744689\n",
      "[6421]\ttrain-rmse:1.744667\n",
      "[6422]\ttrain-rmse:1.744645\n",
      "[6423]\ttrain-rmse:1.744616\n",
      "[6424]\ttrain-rmse:1.744591\n",
      "[6425]\ttrain-rmse:1.744568\n",
      "[6426]\ttrain-rmse:1.744537\n",
      "[6427]\ttrain-rmse:1.744508\n",
      "[6428]\ttrain-rmse:1.744481\n",
      "[6429]\ttrain-rmse:1.744452\n",
      "[6430]\ttrain-rmse:1.744427\n",
      "[6431]\ttrain-rmse:1.744397\n",
      "[6432]\ttrain-rmse:1.744375\n",
      "[6433]\ttrain-rmse:1.744345\n",
      "[6434]\ttrain-rmse:1.744316\n",
      "[6435]\ttrain-rmse:1.744289\n",
      "[6436]\ttrain-rmse:1.744265\n",
      "[6437]\ttrain-rmse:1.744235\n",
      "[6438]\ttrain-rmse:1.744204\n",
      "[6439]\ttrain-rmse:1.744180\n",
      "[6440]\ttrain-rmse:1.744158\n",
      "[6441]\ttrain-rmse:1.744128\n",
      "[6442]\ttrain-rmse:1.744099\n",
      "[6443]\ttrain-rmse:1.744066\n",
      "[6444]\ttrain-rmse:1.744046\n",
      "[6445]\ttrain-rmse:1.744014\n",
      "[6446]\ttrain-rmse:1.743983\n",
      "[6447]\ttrain-rmse:1.743956\n",
      "[6448]\ttrain-rmse:1.743931\n",
      "[6449]\ttrain-rmse:1.743911\n",
      "[6450]\ttrain-rmse:1.743886\n",
      "[6451]\ttrain-rmse:1.743857\n",
      "[6452]\ttrain-rmse:1.743832\n",
      "[6453]\ttrain-rmse:1.743811\n",
      "[6454]\ttrain-rmse:1.743789\n",
      "[6455]\ttrain-rmse:1.743760\n",
      "[6456]\ttrain-rmse:1.743737\n",
      "[6457]\ttrain-rmse:1.743712\n",
      "[6458]\ttrain-rmse:1.743681\n",
      "[6459]\ttrain-rmse:1.743654\n",
      "[6460]\ttrain-rmse:1.743632\n",
      "[6461]\ttrain-rmse:1.743606\n",
      "[6462]\ttrain-rmse:1.743577\n",
      "[6463]\ttrain-rmse:1.743551\n",
      "[6464]\ttrain-rmse:1.743527\n",
      "[6465]\ttrain-rmse:1.743499\n",
      "[6466]\ttrain-rmse:1.743476\n",
      "[6467]\ttrain-rmse:1.743446\n",
      "[6468]\ttrain-rmse:1.743423\n",
      "[6469]\ttrain-rmse:1.743395\n",
      "[6470]\ttrain-rmse:1.743371\n",
      "[6471]\ttrain-rmse:1.743342\n",
      "[6472]\ttrain-rmse:1.743319\n",
      "[6473]\ttrain-rmse:1.743290\n",
      "[6474]\ttrain-rmse:1.743256\n",
      "[6475]\ttrain-rmse:1.743231\n",
      "[6476]\ttrain-rmse:1.743203\n",
      "[6477]\ttrain-rmse:1.743170\n",
      "[6478]\ttrain-rmse:1.743139\n",
      "[6479]\ttrain-rmse:1.743109\n",
      "[6480]\ttrain-rmse:1.743086\n",
      "[6481]\ttrain-rmse:1.743051\n",
      "[6482]\ttrain-rmse:1.743029\n",
      "[6483]\ttrain-rmse:1.743000\n",
      "[6484]\ttrain-rmse:1.742976\n",
      "[6485]\ttrain-rmse:1.742951\n",
      "[6486]\ttrain-rmse:1.742930\n",
      "[6487]\ttrain-rmse:1.742905\n",
      "[6488]\ttrain-rmse:1.742879\n",
      "[6489]\ttrain-rmse:1.742855\n",
      "[6490]\ttrain-rmse:1.742828\n",
      "[6491]\ttrain-rmse:1.742810\n",
      "[6492]\ttrain-rmse:1.742776\n",
      "[6493]\ttrain-rmse:1.742745\n",
      "[6494]\ttrain-rmse:1.742710\n",
      "[6495]\ttrain-rmse:1.742680\n",
      "[6496]\ttrain-rmse:1.742654\n",
      "[6497]\ttrain-rmse:1.742625\n",
      "[6498]\ttrain-rmse:1.742598\n",
      "[6499]\ttrain-rmse:1.742575\n",
      "[6500]\ttrain-rmse:1.742545\n",
      "[6501]\ttrain-rmse:1.742522\n",
      "[6502]\ttrain-rmse:1.742493\n",
      "[6503]\ttrain-rmse:1.742468\n",
      "[6504]\ttrain-rmse:1.742445\n",
      "[6505]\ttrain-rmse:1.742422\n",
      "[6506]\ttrain-rmse:1.742387\n",
      "[6507]\ttrain-rmse:1.742359\n",
      "[6508]\ttrain-rmse:1.742332\n",
      "[6509]\ttrain-rmse:1.742305\n",
      "[6510]\ttrain-rmse:1.742284\n",
      "[6511]\ttrain-rmse:1.742267\n",
      "[6512]\ttrain-rmse:1.742244\n",
      "[6513]\ttrain-rmse:1.742216\n",
      "[6514]\ttrain-rmse:1.742191\n",
      "[6515]\ttrain-rmse:1.742171\n",
      "[6516]\ttrain-rmse:1.742148\n",
      "[6517]\ttrain-rmse:1.742115\n",
      "[6518]\ttrain-rmse:1.742082\n",
      "[6519]\ttrain-rmse:1.742058\n",
      "[6520]\ttrain-rmse:1.742034\n",
      "[6521]\ttrain-rmse:1.742013\n",
      "[6522]\ttrain-rmse:1.741989\n",
      "[6523]\ttrain-rmse:1.741958\n",
      "[6524]\ttrain-rmse:1.741934\n",
      "[6525]\ttrain-rmse:1.741905\n",
      "[6526]\ttrain-rmse:1.741879\n",
      "[6527]\ttrain-rmse:1.741853\n",
      "[6528]\ttrain-rmse:1.741827\n",
      "[6529]\ttrain-rmse:1.741798\n",
      "[6530]\ttrain-rmse:1.741769\n",
      "[6531]\ttrain-rmse:1.741749\n",
      "[6532]\ttrain-rmse:1.741720\n",
      "[6533]\ttrain-rmse:1.741695\n",
      "[6534]\ttrain-rmse:1.741666\n",
      "[6535]\ttrain-rmse:1.741637\n",
      "[6536]\ttrain-rmse:1.741610\n",
      "[6537]\ttrain-rmse:1.741583\n",
      "[6538]\ttrain-rmse:1.741557\n",
      "[6539]\ttrain-rmse:1.741536\n",
      "[6540]\ttrain-rmse:1.741513\n",
      "[6541]\ttrain-rmse:1.741493\n",
      "[6542]\ttrain-rmse:1.741471\n",
      "[6543]\ttrain-rmse:1.741446\n",
      "[6544]\ttrain-rmse:1.741423\n",
      "[6545]\ttrain-rmse:1.741403\n",
      "[6546]\ttrain-rmse:1.741385\n",
      "[6547]\ttrain-rmse:1.741362\n",
      "[6548]\ttrain-rmse:1.741338\n",
      "[6549]\ttrain-rmse:1.741315\n",
      "[6550]\ttrain-rmse:1.741284\n",
      "[6551]\ttrain-rmse:1.741259\n",
      "[6552]\ttrain-rmse:1.741229\n",
      "[6553]\ttrain-rmse:1.741204\n",
      "[6554]\ttrain-rmse:1.741178\n",
      "[6555]\ttrain-rmse:1.741156\n",
      "[6556]\ttrain-rmse:1.741131\n",
      "[6557]\ttrain-rmse:1.741102\n",
      "[6558]\ttrain-rmse:1.741078\n",
      "[6559]\ttrain-rmse:1.741056\n",
      "[6560]\ttrain-rmse:1.741029\n",
      "[6561]\ttrain-rmse:1.740994\n",
      "[6562]\ttrain-rmse:1.740963\n",
      "[6563]\ttrain-rmse:1.740942\n",
      "[6564]\ttrain-rmse:1.740908\n",
      "[6565]\ttrain-rmse:1.740879\n",
      "[6566]\ttrain-rmse:1.740855\n",
      "[6567]\ttrain-rmse:1.740830\n",
      "[6568]\ttrain-rmse:1.740806\n",
      "[6569]\ttrain-rmse:1.740790\n",
      "[6570]\ttrain-rmse:1.740763\n",
      "[6571]\ttrain-rmse:1.740739\n",
      "[6572]\ttrain-rmse:1.740713\n",
      "[6573]\ttrain-rmse:1.740694\n",
      "[6574]\ttrain-rmse:1.740668\n",
      "[6575]\ttrain-rmse:1.740636\n",
      "[6576]\ttrain-rmse:1.740612\n",
      "[6577]\ttrain-rmse:1.740588\n",
      "[6578]\ttrain-rmse:1.740558\n",
      "[6579]\ttrain-rmse:1.740534\n",
      "[6580]\ttrain-rmse:1.740516\n",
      "[6581]\ttrain-rmse:1.740489\n",
      "[6582]\ttrain-rmse:1.740469\n",
      "[6583]\ttrain-rmse:1.740445\n",
      "[6584]\ttrain-rmse:1.740418\n",
      "[6585]\ttrain-rmse:1.740392\n",
      "[6586]\ttrain-rmse:1.740370\n",
      "[6587]\ttrain-rmse:1.740338\n",
      "[6588]\ttrain-rmse:1.740317\n",
      "[6589]\ttrain-rmse:1.740296\n",
      "[6590]\ttrain-rmse:1.740267\n",
      "[6591]\ttrain-rmse:1.740241\n",
      "[6592]\ttrain-rmse:1.740220\n",
      "[6593]\ttrain-rmse:1.740199\n",
      "[6594]\ttrain-rmse:1.740171\n",
      "[6595]\ttrain-rmse:1.740149\n",
      "[6596]\ttrain-rmse:1.740120\n",
      "[6597]\ttrain-rmse:1.740094\n",
      "[6598]\ttrain-rmse:1.740073\n",
      "[6599]\ttrain-rmse:1.740051\n",
      "[6600]\ttrain-rmse:1.740026\n",
      "[6601]\ttrain-rmse:1.740004\n",
      "[6602]\ttrain-rmse:1.739984\n",
      "[6603]\ttrain-rmse:1.739960\n",
      "[6604]\ttrain-rmse:1.739938\n",
      "[6605]\ttrain-rmse:1.739916\n",
      "[6606]\ttrain-rmse:1.739893\n",
      "[6607]\ttrain-rmse:1.739869\n",
      "[6608]\ttrain-rmse:1.739845\n",
      "[6609]\ttrain-rmse:1.739815\n",
      "[6610]\ttrain-rmse:1.739786\n",
      "[6611]\ttrain-rmse:1.739762\n",
      "[6612]\ttrain-rmse:1.739734\n",
      "[6613]\ttrain-rmse:1.739707\n",
      "[6614]\ttrain-rmse:1.739684\n",
      "[6615]\ttrain-rmse:1.739660\n",
      "[6616]\ttrain-rmse:1.739633\n",
      "[6617]\ttrain-rmse:1.739600\n",
      "[6618]\ttrain-rmse:1.739568\n",
      "[6619]\ttrain-rmse:1.739544\n",
      "[6620]\ttrain-rmse:1.739521\n",
      "[6621]\ttrain-rmse:1.739498\n",
      "[6622]\ttrain-rmse:1.739471\n",
      "[6623]\ttrain-rmse:1.739445\n",
      "[6624]\ttrain-rmse:1.739424\n",
      "[6625]\ttrain-rmse:1.739395\n",
      "[6626]\ttrain-rmse:1.739371\n",
      "[6627]\ttrain-rmse:1.739352\n",
      "[6628]\ttrain-rmse:1.739326\n",
      "[6629]\ttrain-rmse:1.739302\n",
      "[6630]\ttrain-rmse:1.739281\n",
      "[6631]\ttrain-rmse:1.739260\n",
      "[6632]\ttrain-rmse:1.739231\n",
      "[6633]\ttrain-rmse:1.739203\n",
      "[6634]\ttrain-rmse:1.739182\n",
      "[6635]\ttrain-rmse:1.739158\n",
      "[6636]\ttrain-rmse:1.739134\n",
      "[6637]\ttrain-rmse:1.739115\n",
      "[6638]\ttrain-rmse:1.739091\n",
      "[6639]\ttrain-rmse:1.739064\n",
      "[6640]\ttrain-rmse:1.739039\n",
      "[6641]\ttrain-rmse:1.739016\n",
      "[6642]\ttrain-rmse:1.738992\n",
      "[6643]\ttrain-rmse:1.738966\n",
      "[6644]\ttrain-rmse:1.738939\n",
      "[6645]\ttrain-rmse:1.738920\n",
      "[6646]\ttrain-rmse:1.738894\n",
      "[6647]\ttrain-rmse:1.738872\n",
      "[6648]\ttrain-rmse:1.738839\n",
      "[6649]\ttrain-rmse:1.738813\n",
      "[6650]\ttrain-rmse:1.738788\n",
      "[6651]\ttrain-rmse:1.738761\n",
      "[6652]\ttrain-rmse:1.738744\n",
      "[6653]\ttrain-rmse:1.738722\n",
      "[6654]\ttrain-rmse:1.738703\n",
      "[6655]\ttrain-rmse:1.738683\n",
      "[6656]\ttrain-rmse:1.738653\n",
      "[6657]\ttrain-rmse:1.738627\n",
      "[6658]\ttrain-rmse:1.738606\n",
      "[6659]\ttrain-rmse:1.738575\n",
      "[6660]\ttrain-rmse:1.738553\n",
      "[6661]\ttrain-rmse:1.738526\n",
      "[6662]\ttrain-rmse:1.738501\n",
      "[6663]\ttrain-rmse:1.738468\n",
      "[6664]\ttrain-rmse:1.738444\n",
      "[6665]\ttrain-rmse:1.738420\n",
      "[6666]\ttrain-rmse:1.738400\n",
      "[6667]\ttrain-rmse:1.738377\n",
      "[6668]\ttrain-rmse:1.738352\n",
      "[6669]\ttrain-rmse:1.738334\n",
      "[6670]\ttrain-rmse:1.738312\n",
      "[6671]\ttrain-rmse:1.738289\n",
      "[6672]\ttrain-rmse:1.738268\n",
      "[6673]\ttrain-rmse:1.738242\n",
      "[6674]\ttrain-rmse:1.738217\n",
      "[6675]\ttrain-rmse:1.738186\n",
      "[6676]\ttrain-rmse:1.738163\n",
      "[6677]\ttrain-rmse:1.738141\n",
      "[6678]\ttrain-rmse:1.738123\n",
      "[6679]\ttrain-rmse:1.738100\n",
      "[6680]\ttrain-rmse:1.738074\n",
      "[6681]\ttrain-rmse:1.738049\n",
      "[6682]\ttrain-rmse:1.738025\n",
      "[6683]\ttrain-rmse:1.737996\n",
      "[6684]\ttrain-rmse:1.737979\n",
      "[6685]\ttrain-rmse:1.737952\n",
      "[6686]\ttrain-rmse:1.737927\n",
      "[6687]\ttrain-rmse:1.737909\n",
      "[6688]\ttrain-rmse:1.737884\n",
      "[6689]\ttrain-rmse:1.737858\n",
      "[6690]\ttrain-rmse:1.737834\n",
      "[6691]\ttrain-rmse:1.737813\n",
      "[6692]\ttrain-rmse:1.737787\n",
      "[6693]\ttrain-rmse:1.737760\n",
      "[6694]\ttrain-rmse:1.737735\n",
      "[6695]\ttrain-rmse:1.737717\n",
      "[6696]\ttrain-rmse:1.737693\n",
      "[6697]\ttrain-rmse:1.737667\n",
      "[6698]\ttrain-rmse:1.737640\n",
      "[6699]\ttrain-rmse:1.737621\n",
      "[6700]\ttrain-rmse:1.737598\n",
      "[6701]\ttrain-rmse:1.737579\n",
      "[6702]\ttrain-rmse:1.737560\n",
      "[6703]\ttrain-rmse:1.737540\n",
      "[6704]\ttrain-rmse:1.737517\n",
      "[6705]\ttrain-rmse:1.737488\n",
      "[6706]\ttrain-rmse:1.737465\n",
      "[6707]\ttrain-rmse:1.737440\n",
      "[6708]\ttrain-rmse:1.737417\n",
      "[6709]\ttrain-rmse:1.737392\n",
      "[6710]\ttrain-rmse:1.737370\n",
      "[6711]\ttrain-rmse:1.737354\n",
      "[6712]\ttrain-rmse:1.737332\n",
      "[6713]\ttrain-rmse:1.737314\n",
      "[6714]\ttrain-rmse:1.737292\n",
      "[6715]\ttrain-rmse:1.737262\n",
      "[6716]\ttrain-rmse:1.737234\n",
      "[6717]\ttrain-rmse:1.737206\n",
      "[6718]\ttrain-rmse:1.737189\n",
      "[6719]\ttrain-rmse:1.737171\n",
      "[6720]\ttrain-rmse:1.737139\n",
      "[6721]\ttrain-rmse:1.737116\n",
      "[6722]\ttrain-rmse:1.737090\n",
      "[6723]\ttrain-rmse:1.737071\n",
      "[6724]\ttrain-rmse:1.737049\n",
      "[6725]\ttrain-rmse:1.737032\n",
      "[6726]\ttrain-rmse:1.737008\n",
      "[6727]\ttrain-rmse:1.736979\n",
      "[6728]\ttrain-rmse:1.736956\n",
      "[6729]\ttrain-rmse:1.736933\n",
      "[6730]\ttrain-rmse:1.736913\n",
      "[6731]\ttrain-rmse:1.736895\n",
      "[6732]\ttrain-rmse:1.736870\n",
      "[6733]\ttrain-rmse:1.736848\n",
      "[6734]\ttrain-rmse:1.736826\n",
      "[6735]\ttrain-rmse:1.736802\n",
      "[6736]\ttrain-rmse:1.736774\n",
      "[6737]\ttrain-rmse:1.736755\n",
      "[6738]\ttrain-rmse:1.736736\n",
      "[6739]\ttrain-rmse:1.736716\n",
      "[6740]\ttrain-rmse:1.736695\n",
      "[6741]\ttrain-rmse:1.736675\n",
      "[6742]\ttrain-rmse:1.736651\n",
      "[6743]\ttrain-rmse:1.736630\n",
      "[6744]\ttrain-rmse:1.736606\n",
      "[6745]\ttrain-rmse:1.736574\n",
      "[6746]\ttrain-rmse:1.736547\n",
      "[6747]\ttrain-rmse:1.736528\n",
      "[6748]\ttrain-rmse:1.736507\n",
      "[6749]\ttrain-rmse:1.736478\n",
      "[6750]\ttrain-rmse:1.736453\n",
      "[6751]\ttrain-rmse:1.736430\n",
      "[6752]\ttrain-rmse:1.736404\n",
      "[6753]\ttrain-rmse:1.736383\n",
      "[6754]\ttrain-rmse:1.736362\n",
      "[6755]\ttrain-rmse:1.736341\n",
      "[6756]\ttrain-rmse:1.736321\n",
      "[6757]\ttrain-rmse:1.736302\n",
      "[6758]\ttrain-rmse:1.736273\n",
      "[6759]\ttrain-rmse:1.736254\n",
      "[6760]\ttrain-rmse:1.736223\n",
      "[6761]\ttrain-rmse:1.736203\n",
      "[6762]\ttrain-rmse:1.736177\n",
      "[6763]\ttrain-rmse:1.736152\n",
      "[6764]\ttrain-rmse:1.736131\n",
      "[6765]\ttrain-rmse:1.736112\n",
      "[6766]\ttrain-rmse:1.736092\n",
      "[6767]\ttrain-rmse:1.736075\n",
      "[6768]\ttrain-rmse:1.736050\n",
      "[6769]\ttrain-rmse:1.736023\n",
      "[6770]\ttrain-rmse:1.735999\n",
      "[6771]\ttrain-rmse:1.735966\n",
      "[6772]\ttrain-rmse:1.735943\n",
      "[6773]\ttrain-rmse:1.735924\n",
      "[6774]\ttrain-rmse:1.735901\n",
      "[6775]\ttrain-rmse:1.735875\n",
      "[6776]\ttrain-rmse:1.735856\n",
      "[6777]\ttrain-rmse:1.735830\n",
      "[6778]\ttrain-rmse:1.735805\n",
      "[6779]\ttrain-rmse:1.735784\n",
      "[6780]\ttrain-rmse:1.735759\n",
      "[6781]\ttrain-rmse:1.735744\n",
      "[6782]\ttrain-rmse:1.735718\n",
      "[6783]\ttrain-rmse:1.735697\n",
      "[6784]\ttrain-rmse:1.735681\n",
      "[6785]\ttrain-rmse:1.735656\n",
      "[6786]\ttrain-rmse:1.735631\n",
      "[6787]\ttrain-rmse:1.735607\n",
      "[6788]\ttrain-rmse:1.735581\n",
      "[6789]\ttrain-rmse:1.735559\n",
      "[6790]\ttrain-rmse:1.735539\n",
      "[6791]\ttrain-rmse:1.735518\n",
      "[6792]\ttrain-rmse:1.735500\n",
      "[6793]\ttrain-rmse:1.735482\n",
      "[6794]\ttrain-rmse:1.735459\n",
      "[6795]\ttrain-rmse:1.735434\n",
      "[6796]\ttrain-rmse:1.735410\n",
      "[6797]\ttrain-rmse:1.735388\n",
      "[6798]\ttrain-rmse:1.735368\n",
      "[6799]\ttrain-rmse:1.735346\n",
      "[6800]\ttrain-rmse:1.735322\n",
      "[6801]\ttrain-rmse:1.735290\n",
      "[6802]\ttrain-rmse:1.735261\n",
      "[6803]\ttrain-rmse:1.735245\n",
      "[6804]\ttrain-rmse:1.735214\n",
      "[6805]\ttrain-rmse:1.735191\n",
      "[6806]\ttrain-rmse:1.735165\n",
      "[6807]\ttrain-rmse:1.735147\n",
      "[6808]\ttrain-rmse:1.735123\n",
      "[6809]\ttrain-rmse:1.735096\n",
      "[6810]\ttrain-rmse:1.735073\n",
      "[6811]\ttrain-rmse:1.735049\n",
      "[6812]\ttrain-rmse:1.735029\n",
      "[6813]\ttrain-rmse:1.735012\n",
      "[6814]\ttrain-rmse:1.734984\n",
      "[6815]\ttrain-rmse:1.734964\n",
      "[6816]\ttrain-rmse:1.734943\n",
      "[6817]\ttrain-rmse:1.734924\n",
      "[6818]\ttrain-rmse:1.734902\n",
      "[6819]\ttrain-rmse:1.734883\n",
      "[6820]\ttrain-rmse:1.734855\n",
      "[6821]\ttrain-rmse:1.734838\n",
      "[6822]\ttrain-rmse:1.734816\n",
      "[6823]\ttrain-rmse:1.734798\n",
      "[6824]\ttrain-rmse:1.734782\n",
      "[6825]\ttrain-rmse:1.734760\n",
      "[6826]\ttrain-rmse:1.734739\n",
      "[6827]\ttrain-rmse:1.734716\n",
      "[6828]\ttrain-rmse:1.734701\n",
      "[6829]\ttrain-rmse:1.734680\n",
      "[6830]\ttrain-rmse:1.734654\n",
      "[6831]\ttrain-rmse:1.734633\n",
      "[6832]\ttrain-rmse:1.734615\n",
      "[6833]\ttrain-rmse:1.734589\n",
      "[6834]\ttrain-rmse:1.734570\n",
      "[6835]\ttrain-rmse:1.734553\n",
      "[6836]\ttrain-rmse:1.734533\n",
      "[6837]\ttrain-rmse:1.734509\n",
      "[6838]\ttrain-rmse:1.734491\n",
      "[6839]\ttrain-rmse:1.734465\n",
      "[6840]\ttrain-rmse:1.734435\n",
      "[6841]\ttrain-rmse:1.734418\n",
      "[6842]\ttrain-rmse:1.734392\n",
      "[6843]\ttrain-rmse:1.734375\n",
      "[6844]\ttrain-rmse:1.734351\n",
      "[6845]\ttrain-rmse:1.734326\n",
      "[6846]\ttrain-rmse:1.734305\n",
      "[6847]\ttrain-rmse:1.734279\n",
      "[6848]\ttrain-rmse:1.734254\n",
      "[6849]\ttrain-rmse:1.734230\n",
      "[6850]\ttrain-rmse:1.734208\n",
      "[6851]\ttrain-rmse:1.734191\n",
      "[6852]\ttrain-rmse:1.734172\n",
      "[6853]\ttrain-rmse:1.734153\n",
      "[6854]\ttrain-rmse:1.734131\n",
      "[6855]\ttrain-rmse:1.734102\n",
      "[6856]\ttrain-rmse:1.734075\n",
      "[6857]\ttrain-rmse:1.734057\n",
      "[6858]\ttrain-rmse:1.734032\n",
      "[6859]\ttrain-rmse:1.734008\n",
      "[6860]\ttrain-rmse:1.733990\n",
      "[6861]\ttrain-rmse:1.733971\n",
      "[6862]\ttrain-rmse:1.733955\n",
      "[6863]\ttrain-rmse:1.733933\n",
      "[6864]\ttrain-rmse:1.733912\n",
      "[6865]\ttrain-rmse:1.733892\n",
      "[6866]\ttrain-rmse:1.733862\n",
      "[6867]\ttrain-rmse:1.733846\n",
      "[6868]\ttrain-rmse:1.733823\n",
      "[6869]\ttrain-rmse:1.733806\n",
      "[6870]\ttrain-rmse:1.733779\n",
      "[6871]\ttrain-rmse:1.733755\n",
      "[6872]\ttrain-rmse:1.733728\n",
      "[6873]\ttrain-rmse:1.733708\n",
      "[6874]\ttrain-rmse:1.733687\n",
      "[6875]\ttrain-rmse:1.733663\n",
      "[6876]\ttrain-rmse:1.733643\n",
      "[6877]\ttrain-rmse:1.733623\n",
      "[6878]\ttrain-rmse:1.733606\n",
      "[6879]\ttrain-rmse:1.733587\n",
      "[6880]\ttrain-rmse:1.733567\n",
      "[6881]\ttrain-rmse:1.733549\n",
      "[6882]\ttrain-rmse:1.733527\n",
      "[6883]\ttrain-rmse:1.733504\n",
      "[6884]\ttrain-rmse:1.733484\n",
      "[6885]\ttrain-rmse:1.733464\n",
      "[6886]\ttrain-rmse:1.733446\n",
      "[6887]\ttrain-rmse:1.733429\n",
      "[6888]\ttrain-rmse:1.733410\n",
      "[6889]\ttrain-rmse:1.733384\n",
      "[6890]\ttrain-rmse:1.733364\n",
      "[6891]\ttrain-rmse:1.733338\n",
      "[6892]\ttrain-rmse:1.733315\n",
      "[6893]\ttrain-rmse:1.733297\n",
      "[6894]\ttrain-rmse:1.733271\n",
      "[6895]\ttrain-rmse:1.733257\n",
      "[6896]\ttrain-rmse:1.733230\n",
      "[6897]\ttrain-rmse:1.733212\n",
      "[6898]\ttrain-rmse:1.733182\n",
      "[6899]\ttrain-rmse:1.733159\n",
      "[6900]\ttrain-rmse:1.733142\n",
      "[6901]\ttrain-rmse:1.733124\n",
      "[6902]\ttrain-rmse:1.733100\n",
      "[6903]\ttrain-rmse:1.733073\n",
      "[6904]\ttrain-rmse:1.733054\n",
      "[6905]\ttrain-rmse:1.733035\n",
      "[6906]\ttrain-rmse:1.733014\n",
      "[6907]\ttrain-rmse:1.732996\n",
      "[6908]\ttrain-rmse:1.732971\n",
      "[6909]\ttrain-rmse:1.732947\n",
      "[6910]\ttrain-rmse:1.732925\n",
      "[6911]\ttrain-rmse:1.732906\n",
      "[6912]\ttrain-rmse:1.732885\n",
      "[6913]\ttrain-rmse:1.732863\n",
      "[6914]\ttrain-rmse:1.732849\n",
      "[6915]\ttrain-rmse:1.732824\n",
      "[6916]\ttrain-rmse:1.732802\n",
      "[6917]\ttrain-rmse:1.732783\n",
      "[6918]\ttrain-rmse:1.732761\n",
      "[6919]\ttrain-rmse:1.732740\n",
      "[6920]\ttrain-rmse:1.732713\n",
      "[6921]\ttrain-rmse:1.732684\n",
      "[6922]\ttrain-rmse:1.732662\n",
      "[6923]\ttrain-rmse:1.732638\n",
      "[6924]\ttrain-rmse:1.732617\n",
      "[6925]\ttrain-rmse:1.732601\n",
      "[6926]\ttrain-rmse:1.732581\n",
      "[6927]\ttrain-rmse:1.732562\n",
      "[6928]\ttrain-rmse:1.732544\n",
      "[6929]\ttrain-rmse:1.732522\n",
      "[6930]\ttrain-rmse:1.732504\n",
      "[6931]\ttrain-rmse:1.732483\n",
      "[6932]\ttrain-rmse:1.732459\n",
      "[6933]\ttrain-rmse:1.732431\n",
      "[6934]\ttrain-rmse:1.732410\n",
      "[6935]\ttrain-rmse:1.732389\n",
      "[6936]\ttrain-rmse:1.732368\n",
      "[6937]\ttrain-rmse:1.732347\n",
      "[6938]\ttrain-rmse:1.732326\n",
      "[6939]\ttrain-rmse:1.732302\n",
      "[6940]\ttrain-rmse:1.732285\n",
      "[6941]\ttrain-rmse:1.732264\n",
      "[6942]\ttrain-rmse:1.732242\n",
      "[6943]\ttrain-rmse:1.732219\n",
      "[6944]\ttrain-rmse:1.732197\n",
      "[6945]\ttrain-rmse:1.732175\n",
      "[6946]\ttrain-rmse:1.732156\n",
      "[6947]\ttrain-rmse:1.732139\n",
      "[6948]\ttrain-rmse:1.732123\n",
      "[6949]\ttrain-rmse:1.732099\n",
      "[6950]\ttrain-rmse:1.732078\n",
      "[6951]\ttrain-rmse:1.732057\n",
      "[6952]\ttrain-rmse:1.732036\n",
      "[6953]\ttrain-rmse:1.732017\n",
      "[6954]\ttrain-rmse:1.731998\n",
      "[6955]\ttrain-rmse:1.731971\n",
      "[6956]\ttrain-rmse:1.731947\n",
      "[6957]\ttrain-rmse:1.731924\n",
      "[6958]\ttrain-rmse:1.731906\n",
      "[6959]\ttrain-rmse:1.731886\n",
      "[6960]\ttrain-rmse:1.731862\n",
      "[6961]\ttrain-rmse:1.731840\n",
      "[6962]\ttrain-rmse:1.731814\n",
      "[6963]\ttrain-rmse:1.731787\n",
      "[6964]\ttrain-rmse:1.731764\n",
      "[6965]\ttrain-rmse:1.731740\n",
      "[6966]\ttrain-rmse:1.731719\n",
      "[6967]\ttrain-rmse:1.731691\n",
      "[6968]\ttrain-rmse:1.731669\n",
      "[6969]\ttrain-rmse:1.731647\n",
      "[6970]\ttrain-rmse:1.731630\n",
      "[6971]\ttrain-rmse:1.731609\n",
      "[6972]\ttrain-rmse:1.731586\n",
      "[6973]\ttrain-rmse:1.731562\n",
      "[6974]\ttrain-rmse:1.731534\n",
      "[6975]\ttrain-rmse:1.731514\n",
      "[6976]\ttrain-rmse:1.731492\n",
      "[6977]\ttrain-rmse:1.731461\n",
      "[6978]\ttrain-rmse:1.731435\n",
      "[6979]\ttrain-rmse:1.731411\n",
      "[6980]\ttrain-rmse:1.731396\n",
      "[6981]\ttrain-rmse:1.731382\n",
      "[6982]\ttrain-rmse:1.731359\n",
      "[6983]\ttrain-rmse:1.731339\n",
      "[6984]\ttrain-rmse:1.731315\n",
      "[6985]\ttrain-rmse:1.731296\n",
      "[6986]\ttrain-rmse:1.731283\n",
      "[6987]\ttrain-rmse:1.731268\n",
      "[6988]\ttrain-rmse:1.731247\n",
      "[6989]\ttrain-rmse:1.731226\n",
      "[6990]\ttrain-rmse:1.731204\n",
      "[6991]\ttrain-rmse:1.731179\n",
      "[6992]\ttrain-rmse:1.731163\n",
      "[6993]\ttrain-rmse:1.731135\n",
      "[6994]\ttrain-rmse:1.731114\n",
      "[6995]\ttrain-rmse:1.731096\n",
      "[6996]\ttrain-rmse:1.731077\n",
      "[6997]\ttrain-rmse:1.731048\n",
      "[6998]\ttrain-rmse:1.731033\n",
      "[6999]\ttrain-rmse:1.731010\n",
      "[7000]\ttrain-rmse:1.730986\n",
      "[7001]\ttrain-rmse:1.730963\n",
      "[7002]\ttrain-rmse:1.730946\n",
      "[7003]\ttrain-rmse:1.730926\n",
      "[7004]\ttrain-rmse:1.730912\n",
      "[7005]\ttrain-rmse:1.730891\n",
      "[7006]\ttrain-rmse:1.730875\n",
      "[7007]\ttrain-rmse:1.730855\n",
      "[7008]\ttrain-rmse:1.730838\n",
      "[7009]\ttrain-rmse:1.730825\n",
      "[7010]\ttrain-rmse:1.730809\n",
      "[7011]\ttrain-rmse:1.730793\n",
      "[7012]\ttrain-rmse:1.730776\n",
      "[7013]\ttrain-rmse:1.730756\n",
      "[7014]\ttrain-rmse:1.730735\n",
      "[7015]\ttrain-rmse:1.730716\n",
      "[7016]\ttrain-rmse:1.730700\n",
      "[7017]\ttrain-rmse:1.730679\n",
      "[7018]\ttrain-rmse:1.730660\n",
      "[7019]\ttrain-rmse:1.730640\n",
      "[7020]\ttrain-rmse:1.730618\n",
      "[7021]\ttrain-rmse:1.730597\n",
      "[7022]\ttrain-rmse:1.730579\n",
      "[7023]\ttrain-rmse:1.730559\n",
      "[7024]\ttrain-rmse:1.730539\n",
      "[7025]\ttrain-rmse:1.730520\n",
      "[7026]\ttrain-rmse:1.730497\n",
      "[7027]\ttrain-rmse:1.730472\n",
      "[7028]\ttrain-rmse:1.730456\n",
      "[7029]\ttrain-rmse:1.730436\n",
      "[7030]\ttrain-rmse:1.730414\n",
      "[7031]\ttrain-rmse:1.730396\n",
      "[7032]\ttrain-rmse:1.730372\n",
      "[7033]\ttrain-rmse:1.730351\n",
      "[7034]\ttrain-rmse:1.730330\n",
      "[7035]\ttrain-rmse:1.730310\n",
      "[7036]\ttrain-rmse:1.730289\n",
      "[7037]\ttrain-rmse:1.730269\n",
      "[7038]\ttrain-rmse:1.730246\n",
      "[7039]\ttrain-rmse:1.730224\n",
      "[7040]\ttrain-rmse:1.730206\n",
      "[7041]\ttrain-rmse:1.730185\n",
      "[7042]\ttrain-rmse:1.730160\n",
      "[7043]\ttrain-rmse:1.730145\n",
      "[7044]\ttrain-rmse:1.730126\n",
      "[7045]\ttrain-rmse:1.730104\n",
      "[7046]\ttrain-rmse:1.730086\n",
      "[7047]\ttrain-rmse:1.730065\n",
      "[7048]\ttrain-rmse:1.730049\n",
      "[7049]\ttrain-rmse:1.730026\n",
      "[7050]\ttrain-rmse:1.729997\n",
      "[7051]\ttrain-rmse:1.729980\n",
      "[7052]\ttrain-rmse:1.729962\n",
      "[7053]\ttrain-rmse:1.729949\n",
      "[7054]\ttrain-rmse:1.729933\n",
      "[7055]\ttrain-rmse:1.729912\n",
      "[7056]\ttrain-rmse:1.729897\n",
      "[7057]\ttrain-rmse:1.729877\n",
      "[7058]\ttrain-rmse:1.729851\n",
      "[7059]\ttrain-rmse:1.729830\n",
      "[7060]\ttrain-rmse:1.729807\n",
      "[7061]\ttrain-rmse:1.729783\n",
      "[7062]\ttrain-rmse:1.729762\n",
      "[7063]\ttrain-rmse:1.729735\n",
      "[7064]\ttrain-rmse:1.729712\n",
      "[7065]\ttrain-rmse:1.729694\n",
      "[7066]\ttrain-rmse:1.729673\n",
      "[7067]\ttrain-rmse:1.729648\n",
      "[7068]\ttrain-rmse:1.729626\n",
      "[7069]\ttrain-rmse:1.729607\n",
      "[7070]\ttrain-rmse:1.729592\n",
      "[7071]\ttrain-rmse:1.729575\n",
      "[7072]\ttrain-rmse:1.729560\n",
      "[7073]\ttrain-rmse:1.729546\n",
      "[7074]\ttrain-rmse:1.729527\n",
      "[7075]\ttrain-rmse:1.729506\n",
      "[7076]\ttrain-rmse:1.729486\n",
      "[7077]\ttrain-rmse:1.729468\n",
      "[7078]\ttrain-rmse:1.729451\n",
      "[7079]\ttrain-rmse:1.729430\n",
      "[7080]\ttrain-rmse:1.729406\n",
      "[7081]\ttrain-rmse:1.729385\n",
      "[7082]\ttrain-rmse:1.729363\n",
      "[7083]\ttrain-rmse:1.729340\n",
      "[7084]\ttrain-rmse:1.729319\n",
      "[7085]\ttrain-rmse:1.729297\n",
      "[7086]\ttrain-rmse:1.729274\n",
      "[7087]\ttrain-rmse:1.729259\n",
      "[7088]\ttrain-rmse:1.729241\n",
      "[7089]\ttrain-rmse:1.729218\n",
      "[7090]\ttrain-rmse:1.729190\n",
      "[7091]\ttrain-rmse:1.729171\n",
      "[7092]\ttrain-rmse:1.729148\n",
      "[7093]\ttrain-rmse:1.729130\n",
      "[7094]\ttrain-rmse:1.729110\n",
      "[7095]\ttrain-rmse:1.729085\n",
      "[7096]\ttrain-rmse:1.729065\n",
      "[7097]\ttrain-rmse:1.729046\n",
      "[7098]\ttrain-rmse:1.729027\n",
      "[7099]\ttrain-rmse:1.729009\n",
      "[7100]\ttrain-rmse:1.728989\n",
      "[7101]\ttrain-rmse:1.728969\n",
      "[7102]\ttrain-rmse:1.728953\n",
      "[7103]\ttrain-rmse:1.728929\n",
      "[7104]\ttrain-rmse:1.728900\n",
      "[7105]\ttrain-rmse:1.728882\n",
      "[7106]\ttrain-rmse:1.728866\n",
      "[7107]\ttrain-rmse:1.728848\n",
      "[7108]\ttrain-rmse:1.728826\n",
      "[7109]\ttrain-rmse:1.728807\n",
      "[7110]\ttrain-rmse:1.728786\n",
      "[7111]\ttrain-rmse:1.728768\n",
      "[7112]\ttrain-rmse:1.728742\n",
      "[7113]\ttrain-rmse:1.728720\n",
      "[7114]\ttrain-rmse:1.728704\n",
      "[7115]\ttrain-rmse:1.728684\n",
      "[7116]\ttrain-rmse:1.728668\n",
      "[7117]\ttrain-rmse:1.728652\n",
      "[7118]\ttrain-rmse:1.728636\n",
      "[7119]\ttrain-rmse:1.728623\n",
      "[7120]\ttrain-rmse:1.728608\n",
      "[7121]\ttrain-rmse:1.728589\n",
      "[7122]\ttrain-rmse:1.728570\n",
      "[7123]\ttrain-rmse:1.728546\n",
      "[7124]\ttrain-rmse:1.728523\n",
      "[7125]\ttrain-rmse:1.728503\n",
      "[7126]\ttrain-rmse:1.728483\n",
      "[7127]\ttrain-rmse:1.728462\n",
      "[7128]\ttrain-rmse:1.728446\n",
      "[7129]\ttrain-rmse:1.728425\n",
      "[7130]\ttrain-rmse:1.728406\n",
      "[7131]\ttrain-rmse:1.728385\n",
      "[7132]\ttrain-rmse:1.728365\n",
      "[7133]\ttrain-rmse:1.728349\n",
      "[7134]\ttrain-rmse:1.728328\n",
      "[7135]\ttrain-rmse:1.728312\n",
      "[7136]\ttrain-rmse:1.728292\n",
      "[7137]\ttrain-rmse:1.728281\n",
      "[7138]\ttrain-rmse:1.728260\n",
      "[7139]\ttrain-rmse:1.728240\n",
      "[7140]\ttrain-rmse:1.728222\n",
      "[7141]\ttrain-rmse:1.728208\n",
      "[7142]\ttrain-rmse:1.728193\n",
      "[7143]\ttrain-rmse:1.728169\n",
      "[7144]\ttrain-rmse:1.728156\n",
      "[7145]\ttrain-rmse:1.728129\n",
      "[7146]\ttrain-rmse:1.728111\n",
      "[7147]\ttrain-rmse:1.728097\n",
      "[7148]\ttrain-rmse:1.728069\n",
      "[7149]\ttrain-rmse:1.728051\n",
      "[7150]\ttrain-rmse:1.728030\n",
      "[7151]\ttrain-rmse:1.728008\n",
      "[7152]\ttrain-rmse:1.727985\n",
      "[7153]\ttrain-rmse:1.727967\n",
      "[7154]\ttrain-rmse:1.727950\n",
      "[7155]\ttrain-rmse:1.727929\n",
      "[7156]\ttrain-rmse:1.727905\n",
      "[7157]\ttrain-rmse:1.727884\n",
      "[7158]\ttrain-rmse:1.727865\n",
      "[7159]\ttrain-rmse:1.727848\n",
      "[7160]\ttrain-rmse:1.727825\n",
      "[7161]\ttrain-rmse:1.727806\n",
      "[7162]\ttrain-rmse:1.727790\n",
      "[7163]\ttrain-rmse:1.727768\n",
      "[7164]\ttrain-rmse:1.727757\n",
      "[7165]\ttrain-rmse:1.727739\n",
      "[7166]\ttrain-rmse:1.727721\n",
      "[7167]\ttrain-rmse:1.727701\n",
      "[7168]\ttrain-rmse:1.727679\n",
      "[7169]\ttrain-rmse:1.727663\n",
      "[7170]\ttrain-rmse:1.727646\n",
      "[7171]\ttrain-rmse:1.727633\n",
      "[7172]\ttrain-rmse:1.727617\n",
      "[7173]\ttrain-rmse:1.727589\n",
      "[7174]\ttrain-rmse:1.727569\n",
      "[7175]\ttrain-rmse:1.727547\n",
      "[7176]\ttrain-rmse:1.727533\n",
      "[7177]\ttrain-rmse:1.727510\n",
      "[7178]\ttrain-rmse:1.727488\n",
      "[7179]\ttrain-rmse:1.727471\n",
      "[7180]\ttrain-rmse:1.727450\n",
      "[7181]\ttrain-rmse:1.727437\n",
      "[7182]\ttrain-rmse:1.727425\n",
      "[7183]\ttrain-rmse:1.727403\n",
      "[7184]\ttrain-rmse:1.727384\n",
      "[7185]\ttrain-rmse:1.727365\n",
      "[7186]\ttrain-rmse:1.727342\n",
      "[7187]\ttrain-rmse:1.727328\n",
      "[7188]\ttrain-rmse:1.727304\n",
      "[7189]\ttrain-rmse:1.727288\n",
      "[7190]\ttrain-rmse:1.727263\n",
      "[7191]\ttrain-rmse:1.727247\n",
      "[7192]\ttrain-rmse:1.727225\n",
      "[7193]\ttrain-rmse:1.727206\n",
      "[7194]\ttrain-rmse:1.727191\n",
      "[7195]\ttrain-rmse:1.727173\n",
      "[7196]\ttrain-rmse:1.727154\n",
      "[7197]\ttrain-rmse:1.727130\n",
      "[7198]\ttrain-rmse:1.727106\n",
      "[7199]\ttrain-rmse:1.727084\n",
      "[7200]\ttrain-rmse:1.727065\n",
      "[7201]\ttrain-rmse:1.727046\n",
      "[7202]\ttrain-rmse:1.727030\n",
      "[7203]\ttrain-rmse:1.727015\n",
      "[7204]\ttrain-rmse:1.726998\n",
      "[7205]\ttrain-rmse:1.726966\n",
      "[7206]\ttrain-rmse:1.726955\n",
      "[7207]\ttrain-rmse:1.726932\n",
      "[7208]\ttrain-rmse:1.726913\n",
      "[7209]\ttrain-rmse:1.726894\n",
      "[7210]\ttrain-rmse:1.726872\n",
      "[7211]\ttrain-rmse:1.726856\n",
      "[7212]\ttrain-rmse:1.726841\n",
      "[7213]\ttrain-rmse:1.726823\n",
      "[7214]\ttrain-rmse:1.726808\n",
      "[7215]\ttrain-rmse:1.726785\n",
      "[7216]\ttrain-rmse:1.726769\n",
      "[7217]\ttrain-rmse:1.726748\n",
      "[7218]\ttrain-rmse:1.726727\n",
      "[7219]\ttrain-rmse:1.726708\n",
      "[7220]\ttrain-rmse:1.726689\n",
      "[7221]\ttrain-rmse:1.726671\n",
      "[7222]\ttrain-rmse:1.726647\n",
      "[7223]\ttrain-rmse:1.726634\n",
      "[7224]\ttrain-rmse:1.726612\n",
      "[7225]\ttrain-rmse:1.726597\n",
      "[7226]\ttrain-rmse:1.726579\n",
      "[7227]\ttrain-rmse:1.726552\n",
      "[7228]\ttrain-rmse:1.726542\n",
      "[7229]\ttrain-rmse:1.726523\n",
      "[7230]\ttrain-rmse:1.726501\n",
      "[7231]\ttrain-rmse:1.726480\n",
      "[7232]\ttrain-rmse:1.726461\n",
      "[7233]\ttrain-rmse:1.726438\n",
      "[7234]\ttrain-rmse:1.726423\n",
      "[7235]\ttrain-rmse:1.726406\n",
      "[7236]\ttrain-rmse:1.726390\n",
      "[7237]\ttrain-rmse:1.726370\n",
      "[7238]\ttrain-rmse:1.726350\n",
      "[7239]\ttrain-rmse:1.726332\n",
      "[7240]\ttrain-rmse:1.726318\n",
      "[7241]\ttrain-rmse:1.726299\n",
      "[7242]\ttrain-rmse:1.726284\n",
      "[7243]\ttrain-rmse:1.726263\n",
      "[7244]\ttrain-rmse:1.726241\n",
      "[7245]\ttrain-rmse:1.726222\n",
      "[7246]\ttrain-rmse:1.726203\n",
      "[7247]\ttrain-rmse:1.726184\n",
      "[7248]\ttrain-rmse:1.726166\n",
      "[7249]\ttrain-rmse:1.726154\n",
      "[7250]\ttrain-rmse:1.726131\n",
      "[7251]\ttrain-rmse:1.726109\n",
      "[7252]\ttrain-rmse:1.726089\n",
      "[7253]\ttrain-rmse:1.726068\n",
      "[7254]\ttrain-rmse:1.726050\n",
      "[7255]\ttrain-rmse:1.726028\n",
      "[7256]\ttrain-rmse:1.726012\n",
      "[7257]\ttrain-rmse:1.726000\n",
      "[7258]\ttrain-rmse:1.725974\n",
      "[7259]\ttrain-rmse:1.725957\n",
      "[7260]\ttrain-rmse:1.725937\n",
      "[7261]\ttrain-rmse:1.725919\n",
      "[7262]\ttrain-rmse:1.725903\n",
      "[7263]\ttrain-rmse:1.725886\n",
      "[7264]\ttrain-rmse:1.725870\n",
      "[7265]\ttrain-rmse:1.725852\n",
      "[7266]\ttrain-rmse:1.725834\n",
      "[7267]\ttrain-rmse:1.725808\n",
      "[7268]\ttrain-rmse:1.725791\n",
      "[7269]\ttrain-rmse:1.725776\n",
      "[7270]\ttrain-rmse:1.725757\n",
      "[7271]\ttrain-rmse:1.725737\n",
      "[7272]\ttrain-rmse:1.725722\n",
      "[7273]\ttrain-rmse:1.725700\n",
      "[7274]\ttrain-rmse:1.725674\n",
      "[7275]\ttrain-rmse:1.725650\n",
      "[7276]\ttrain-rmse:1.725635\n",
      "[7277]\ttrain-rmse:1.725616\n",
      "[7278]\ttrain-rmse:1.725599\n",
      "[7279]\ttrain-rmse:1.725578\n",
      "[7280]\ttrain-rmse:1.725559\n",
      "[7281]\ttrain-rmse:1.725539\n",
      "[7282]\ttrain-rmse:1.725522\n",
      "[7283]\ttrain-rmse:1.725507\n",
      "[7284]\ttrain-rmse:1.725490\n",
      "[7285]\ttrain-rmse:1.725468\n",
      "[7286]\ttrain-rmse:1.725448\n",
      "[7287]\ttrain-rmse:1.725424\n",
      "[7288]\ttrain-rmse:1.725411\n",
      "[7289]\ttrain-rmse:1.725394\n",
      "[7290]\ttrain-rmse:1.725377\n",
      "[7291]\ttrain-rmse:1.725363\n",
      "[7292]\ttrain-rmse:1.725349\n",
      "[7293]\ttrain-rmse:1.725332\n",
      "[7294]\ttrain-rmse:1.725314\n",
      "[7295]\ttrain-rmse:1.725296\n",
      "[7296]\ttrain-rmse:1.725279\n",
      "[7297]\ttrain-rmse:1.725255\n",
      "[7298]\ttrain-rmse:1.725236\n",
      "[7299]\ttrain-rmse:1.725219\n",
      "[7300]\ttrain-rmse:1.725208\n",
      "[7301]\ttrain-rmse:1.725187\n",
      "[7302]\ttrain-rmse:1.725158\n",
      "[7303]\ttrain-rmse:1.725144\n",
      "[7304]\ttrain-rmse:1.725130\n",
      "[7305]\ttrain-rmse:1.725112\n",
      "[7306]\ttrain-rmse:1.725094\n",
      "[7307]\ttrain-rmse:1.725067\n",
      "[7308]\ttrain-rmse:1.725049\n",
      "[7309]\ttrain-rmse:1.725030\n",
      "[7310]\ttrain-rmse:1.725010\n",
      "[7311]\ttrain-rmse:1.724998\n",
      "[7312]\ttrain-rmse:1.724983\n",
      "[7313]\ttrain-rmse:1.724966\n",
      "[7314]\ttrain-rmse:1.724949\n",
      "[7315]\ttrain-rmse:1.724923\n",
      "[7316]\ttrain-rmse:1.724909\n",
      "[7317]\ttrain-rmse:1.724890\n",
      "[7318]\ttrain-rmse:1.724871\n",
      "[7319]\ttrain-rmse:1.724853\n",
      "[7320]\ttrain-rmse:1.724837\n",
      "[7321]\ttrain-rmse:1.724818\n",
      "[7322]\ttrain-rmse:1.724802\n",
      "[7323]\ttrain-rmse:1.724782\n",
      "[7324]\ttrain-rmse:1.724759\n",
      "[7325]\ttrain-rmse:1.724741\n",
      "[7326]\ttrain-rmse:1.724723\n",
      "[7327]\ttrain-rmse:1.724706\n",
      "[7328]\ttrain-rmse:1.724686\n",
      "[7329]\ttrain-rmse:1.724670\n",
      "[7330]\ttrain-rmse:1.724657\n",
      "[7331]\ttrain-rmse:1.724645\n",
      "[7332]\ttrain-rmse:1.724623\n",
      "[7333]\ttrain-rmse:1.724610\n",
      "[7334]\ttrain-rmse:1.724592\n",
      "[7335]\ttrain-rmse:1.724569\n",
      "[7336]\ttrain-rmse:1.724554\n",
      "[7337]\ttrain-rmse:1.724537\n",
      "[7338]\ttrain-rmse:1.724522\n",
      "[7339]\ttrain-rmse:1.724508\n",
      "[7340]\ttrain-rmse:1.724485\n",
      "[7341]\ttrain-rmse:1.724464\n",
      "[7342]\ttrain-rmse:1.724447\n",
      "[7343]\ttrain-rmse:1.724428\n",
      "[7344]\ttrain-rmse:1.724414\n",
      "[7345]\ttrain-rmse:1.724391\n",
      "[7346]\ttrain-rmse:1.724374\n",
      "[7347]\ttrain-rmse:1.724352\n",
      "[7348]\ttrain-rmse:1.724334\n",
      "[7349]\ttrain-rmse:1.724313\n",
      "[7350]\ttrain-rmse:1.724299\n",
      "[7351]\ttrain-rmse:1.724275\n",
      "[7352]\ttrain-rmse:1.724254\n",
      "[7353]\ttrain-rmse:1.724245\n",
      "[7354]\ttrain-rmse:1.724230\n",
      "[7355]\ttrain-rmse:1.724214\n",
      "[7356]\ttrain-rmse:1.724197\n",
      "[7357]\ttrain-rmse:1.724175\n",
      "[7358]\ttrain-rmse:1.724157\n",
      "[7359]\ttrain-rmse:1.724135\n",
      "[7360]\ttrain-rmse:1.724114\n",
      "[7361]\ttrain-rmse:1.724098\n",
      "[7362]\ttrain-rmse:1.724080\n",
      "[7363]\ttrain-rmse:1.724066\n",
      "[7364]\ttrain-rmse:1.724042\n",
      "[7365]\ttrain-rmse:1.724023\n",
      "[7366]\ttrain-rmse:1.724007\n",
      "[7367]\ttrain-rmse:1.723989\n",
      "[7368]\ttrain-rmse:1.723964\n",
      "[7369]\ttrain-rmse:1.723943\n",
      "[7370]\ttrain-rmse:1.723927\n",
      "[7371]\ttrain-rmse:1.723907\n",
      "[7372]\ttrain-rmse:1.723882\n",
      "[7373]\ttrain-rmse:1.723860\n",
      "[7374]\ttrain-rmse:1.723837\n",
      "[7375]\ttrain-rmse:1.723822\n",
      "[7376]\ttrain-rmse:1.723807\n",
      "[7377]\ttrain-rmse:1.723786\n",
      "[7378]\ttrain-rmse:1.723769\n",
      "[7379]\ttrain-rmse:1.723753\n",
      "[7380]\ttrain-rmse:1.723724\n",
      "[7381]\ttrain-rmse:1.723709\n",
      "[7382]\ttrain-rmse:1.723684\n",
      "[7383]\ttrain-rmse:1.723668\n",
      "[7384]\ttrain-rmse:1.723651\n",
      "[7385]\ttrain-rmse:1.723627\n",
      "[7386]\ttrain-rmse:1.723613\n",
      "[7387]\ttrain-rmse:1.723595\n",
      "[7388]\ttrain-rmse:1.723577\n",
      "[7389]\ttrain-rmse:1.723564\n",
      "[7390]\ttrain-rmse:1.723551\n",
      "[7391]\ttrain-rmse:1.723529\n",
      "[7392]\ttrain-rmse:1.723505\n",
      "[7393]\ttrain-rmse:1.723486\n",
      "[7394]\ttrain-rmse:1.723471\n",
      "[7395]\ttrain-rmse:1.723457\n",
      "[7396]\ttrain-rmse:1.723441\n",
      "[7397]\ttrain-rmse:1.723419\n",
      "[7398]\ttrain-rmse:1.723397\n",
      "[7399]\ttrain-rmse:1.723380\n",
      "[7400]\ttrain-rmse:1.723369\n",
      "[7401]\ttrain-rmse:1.723350\n",
      "[7402]\ttrain-rmse:1.723336\n",
      "[7403]\ttrain-rmse:1.723316\n",
      "[7404]\ttrain-rmse:1.723306\n",
      "[7405]\ttrain-rmse:1.723289\n",
      "[7406]\ttrain-rmse:1.723263\n",
      "[7407]\ttrain-rmse:1.723241\n",
      "[7408]\ttrain-rmse:1.723228\n",
      "[7409]\ttrain-rmse:1.723210\n",
      "[7410]\ttrain-rmse:1.723194\n",
      "[7411]\ttrain-rmse:1.723170\n",
      "[7412]\ttrain-rmse:1.723156\n",
      "[7413]\ttrain-rmse:1.723133\n",
      "[7414]\ttrain-rmse:1.723117\n",
      "[7415]\ttrain-rmse:1.723101\n",
      "[7416]\ttrain-rmse:1.723080\n",
      "[7417]\ttrain-rmse:1.723055\n",
      "[7418]\ttrain-rmse:1.723042\n",
      "[7419]\ttrain-rmse:1.723027\n",
      "[7420]\ttrain-rmse:1.723008\n",
      "[7421]\ttrain-rmse:1.722993\n",
      "[7422]\ttrain-rmse:1.722981\n",
      "[7423]\ttrain-rmse:1.722962\n",
      "[7424]\ttrain-rmse:1.722941\n",
      "[7425]\ttrain-rmse:1.722925\n",
      "[7426]\ttrain-rmse:1.722905\n",
      "[7427]\ttrain-rmse:1.722891\n",
      "[7428]\ttrain-rmse:1.722878\n",
      "[7429]\ttrain-rmse:1.722860\n",
      "[7430]\ttrain-rmse:1.722841\n",
      "[7431]\ttrain-rmse:1.722823\n",
      "[7432]\ttrain-rmse:1.722810\n",
      "[7433]\ttrain-rmse:1.722792\n",
      "[7434]\ttrain-rmse:1.722776\n",
      "[7435]\ttrain-rmse:1.722756\n",
      "[7436]\ttrain-rmse:1.722737\n",
      "[7437]\ttrain-rmse:1.722716\n",
      "[7438]\ttrain-rmse:1.722700\n",
      "[7439]\ttrain-rmse:1.722686\n",
      "[7440]\ttrain-rmse:1.722673\n",
      "[7441]\ttrain-rmse:1.722654\n",
      "[7442]\ttrain-rmse:1.722632\n",
      "[7443]\ttrain-rmse:1.722603\n",
      "[7444]\ttrain-rmse:1.722579\n",
      "[7445]\ttrain-rmse:1.722561\n",
      "[7446]\ttrain-rmse:1.722544\n",
      "[7447]\ttrain-rmse:1.722529\n",
      "[7448]\ttrain-rmse:1.722515\n",
      "[7449]\ttrain-rmse:1.722502\n",
      "[7450]\ttrain-rmse:1.722484\n",
      "[7451]\ttrain-rmse:1.722467\n",
      "[7452]\ttrain-rmse:1.722444\n",
      "[7453]\ttrain-rmse:1.722428\n",
      "[7454]\ttrain-rmse:1.722412\n",
      "[7455]\ttrain-rmse:1.722388\n",
      "[7456]\ttrain-rmse:1.722371\n",
      "[7457]\ttrain-rmse:1.722354\n",
      "[7458]\ttrain-rmse:1.722332\n",
      "[7459]\ttrain-rmse:1.722316\n",
      "[7460]\ttrain-rmse:1.722297\n",
      "[7461]\ttrain-rmse:1.722279\n",
      "[7462]\ttrain-rmse:1.722257\n",
      "[7463]\ttrain-rmse:1.722243\n",
      "[7464]\ttrain-rmse:1.722224\n",
      "[7465]\ttrain-rmse:1.722201\n",
      "[7466]\ttrain-rmse:1.722180\n",
      "[7467]\ttrain-rmse:1.722163\n",
      "[7468]\ttrain-rmse:1.722140\n",
      "[7469]\ttrain-rmse:1.722123\n",
      "[7470]\ttrain-rmse:1.722100\n",
      "[7471]\ttrain-rmse:1.722086\n",
      "[7472]\ttrain-rmse:1.722070\n",
      "[7473]\ttrain-rmse:1.722049\n",
      "[7474]\ttrain-rmse:1.722035\n",
      "[7475]\ttrain-rmse:1.722017\n",
      "[7476]\ttrain-rmse:1.722002\n",
      "[7477]\ttrain-rmse:1.721979\n",
      "[7478]\ttrain-rmse:1.721959\n",
      "[7479]\ttrain-rmse:1.721939\n",
      "[7480]\ttrain-rmse:1.721922\n",
      "[7481]\ttrain-rmse:1.721897\n",
      "[7482]\ttrain-rmse:1.721879\n",
      "[7483]\ttrain-rmse:1.721856\n",
      "[7484]\ttrain-rmse:1.721841\n",
      "[7485]\ttrain-rmse:1.721823\n",
      "[7486]\ttrain-rmse:1.721803\n",
      "[7487]\ttrain-rmse:1.721788\n",
      "[7488]\ttrain-rmse:1.721764\n",
      "[7489]\ttrain-rmse:1.721749\n",
      "[7490]\ttrain-rmse:1.721731\n",
      "[7491]\ttrain-rmse:1.721722\n",
      "[7492]\ttrain-rmse:1.721704\n",
      "[7493]\ttrain-rmse:1.721682\n",
      "[7494]\ttrain-rmse:1.721668\n",
      "[7495]\ttrain-rmse:1.721652\n",
      "[7496]\ttrain-rmse:1.721632\n",
      "[7497]\ttrain-rmse:1.721614\n",
      "[7498]\ttrain-rmse:1.721599\n",
      "[7499]\ttrain-rmse:1.721575\n",
      "[7500]\ttrain-rmse:1.721564\n",
      "[7501]\ttrain-rmse:1.721545\n",
      "[7502]\ttrain-rmse:1.721534\n",
      "[7503]\ttrain-rmse:1.721513\n",
      "[7504]\ttrain-rmse:1.721494\n",
      "[7505]\ttrain-rmse:1.721480\n",
      "[7506]\ttrain-rmse:1.721466\n",
      "[7507]\ttrain-rmse:1.721451\n",
      "[7508]\ttrain-rmse:1.721429\n",
      "[7509]\ttrain-rmse:1.721407\n",
      "[7510]\ttrain-rmse:1.721393\n",
      "[7511]\ttrain-rmse:1.721367\n",
      "[7512]\ttrain-rmse:1.721354\n",
      "[7513]\ttrain-rmse:1.721331\n",
      "[7514]\ttrain-rmse:1.721315\n",
      "[7515]\ttrain-rmse:1.721299\n",
      "[7516]\ttrain-rmse:1.721288\n",
      "[7517]\ttrain-rmse:1.721274\n",
      "[7518]\ttrain-rmse:1.721256\n",
      "[7519]\ttrain-rmse:1.721237\n",
      "[7520]\ttrain-rmse:1.721220\n",
      "[7521]\ttrain-rmse:1.721200\n",
      "[7522]\ttrain-rmse:1.721181\n",
      "[7523]\ttrain-rmse:1.721156\n",
      "[7524]\ttrain-rmse:1.721139\n",
      "[7525]\ttrain-rmse:1.721124\n",
      "[7526]\ttrain-rmse:1.721110\n",
      "[7527]\ttrain-rmse:1.721091\n",
      "[7528]\ttrain-rmse:1.721079\n",
      "[7529]\ttrain-rmse:1.721065\n",
      "[7530]\ttrain-rmse:1.721045\n",
      "[7531]\ttrain-rmse:1.721029\n",
      "[7532]\ttrain-rmse:1.721011\n",
      "[7533]\ttrain-rmse:1.720996\n",
      "[7534]\ttrain-rmse:1.720973\n",
      "[7535]\ttrain-rmse:1.720960\n",
      "[7536]\ttrain-rmse:1.720944\n",
      "[7537]\ttrain-rmse:1.720928\n",
      "[7538]\ttrain-rmse:1.720907\n",
      "[7539]\ttrain-rmse:1.720892\n",
      "[7540]\ttrain-rmse:1.720870\n",
      "[7541]\ttrain-rmse:1.720856\n",
      "[7542]\ttrain-rmse:1.720834\n",
      "[7543]\ttrain-rmse:1.720822\n",
      "[7544]\ttrain-rmse:1.720799\n",
      "[7545]\ttrain-rmse:1.720782\n",
      "[7546]\ttrain-rmse:1.720757\n",
      "[7547]\ttrain-rmse:1.720734\n",
      "[7548]\ttrain-rmse:1.720717\n",
      "[7549]\ttrain-rmse:1.720697\n",
      "[7550]\ttrain-rmse:1.720683\n",
      "[7551]\ttrain-rmse:1.720670\n",
      "[7552]\ttrain-rmse:1.720657\n",
      "[7553]\ttrain-rmse:1.720638\n",
      "[7554]\ttrain-rmse:1.720620\n",
      "[7555]\ttrain-rmse:1.720605\n",
      "[7556]\ttrain-rmse:1.720585\n",
      "[7557]\ttrain-rmse:1.720569\n",
      "[7558]\ttrain-rmse:1.720553\n",
      "[7559]\ttrain-rmse:1.720531\n",
      "[7560]\ttrain-rmse:1.720513\n",
      "[7561]\ttrain-rmse:1.720490\n",
      "[7562]\ttrain-rmse:1.720475\n",
      "[7563]\ttrain-rmse:1.720462\n",
      "[7564]\ttrain-rmse:1.720439\n",
      "[7565]\ttrain-rmse:1.720423\n",
      "[7566]\ttrain-rmse:1.720408\n",
      "[7567]\ttrain-rmse:1.720388\n",
      "[7568]\ttrain-rmse:1.720366\n",
      "[7569]\ttrain-rmse:1.720352\n",
      "[7570]\ttrain-rmse:1.720335\n",
      "[7571]\ttrain-rmse:1.720321\n",
      "[7572]\ttrain-rmse:1.720305\n",
      "[7573]\ttrain-rmse:1.720284\n",
      "[7574]\ttrain-rmse:1.720266\n",
      "[7575]\ttrain-rmse:1.720244\n",
      "[7576]\ttrain-rmse:1.720222\n",
      "[7577]\ttrain-rmse:1.720202\n",
      "[7578]\ttrain-rmse:1.720187\n",
      "[7579]\ttrain-rmse:1.720171\n",
      "[7580]\ttrain-rmse:1.720157\n",
      "[7581]\ttrain-rmse:1.720137\n",
      "[7582]\ttrain-rmse:1.720118\n",
      "[7583]\ttrain-rmse:1.720103\n",
      "[7584]\ttrain-rmse:1.720076\n",
      "[7585]\ttrain-rmse:1.720054\n",
      "[7586]\ttrain-rmse:1.720041\n",
      "[7587]\ttrain-rmse:1.720026\n",
      "[7588]\ttrain-rmse:1.720010\n",
      "[7589]\ttrain-rmse:1.719998\n",
      "[7590]\ttrain-rmse:1.719977\n",
      "[7591]\ttrain-rmse:1.719965\n",
      "[7592]\ttrain-rmse:1.719945\n",
      "[7593]\ttrain-rmse:1.719927\n",
      "[7594]\ttrain-rmse:1.719900\n",
      "[7595]\ttrain-rmse:1.719878\n",
      "[7596]\ttrain-rmse:1.719862\n",
      "[7597]\ttrain-rmse:1.719842\n",
      "[7598]\ttrain-rmse:1.719825\n",
      "[7599]\ttrain-rmse:1.719810\n",
      "[7600]\ttrain-rmse:1.719796\n",
      "[7601]\ttrain-rmse:1.719781\n",
      "[7602]\ttrain-rmse:1.719764\n",
      "[7603]\ttrain-rmse:1.719745\n",
      "[7604]\ttrain-rmse:1.719727\n",
      "[7605]\ttrain-rmse:1.719707\n",
      "[7606]\ttrain-rmse:1.719683\n",
      "[7607]\ttrain-rmse:1.719661\n",
      "[7608]\ttrain-rmse:1.719646\n",
      "[7609]\ttrain-rmse:1.719620\n",
      "[7610]\ttrain-rmse:1.719601\n",
      "[7611]\ttrain-rmse:1.719587\n",
      "[7612]\ttrain-rmse:1.719567\n",
      "[7613]\ttrain-rmse:1.719551\n",
      "[7614]\ttrain-rmse:1.719537\n",
      "[7615]\ttrain-rmse:1.719520\n",
      "[7616]\ttrain-rmse:1.719505\n",
      "[7617]\ttrain-rmse:1.719483\n",
      "[7618]\ttrain-rmse:1.719469\n",
      "[7619]\ttrain-rmse:1.719453\n",
      "[7620]\ttrain-rmse:1.719435\n",
      "[7621]\ttrain-rmse:1.719408\n",
      "[7622]\ttrain-rmse:1.719387\n",
      "[7623]\ttrain-rmse:1.719374\n",
      "[7624]\ttrain-rmse:1.719354\n",
      "[7625]\ttrain-rmse:1.719332\n",
      "[7626]\ttrain-rmse:1.719316\n",
      "[7627]\ttrain-rmse:1.719301\n",
      "[7628]\ttrain-rmse:1.719283\n",
      "[7629]\ttrain-rmse:1.719268\n",
      "[7630]\ttrain-rmse:1.719256\n",
      "[7631]\ttrain-rmse:1.719238\n",
      "[7632]\ttrain-rmse:1.719219\n",
      "[7633]\ttrain-rmse:1.719199\n",
      "[7634]\ttrain-rmse:1.719175\n",
      "[7635]\ttrain-rmse:1.719163\n",
      "[7636]\ttrain-rmse:1.719146\n",
      "[7637]\ttrain-rmse:1.719126\n",
      "[7638]\ttrain-rmse:1.719100\n",
      "[7639]\ttrain-rmse:1.719082\n",
      "[7640]\ttrain-rmse:1.719069\n",
      "[7641]\ttrain-rmse:1.719054\n",
      "[7642]\ttrain-rmse:1.719032\n",
      "[7643]\ttrain-rmse:1.719018\n",
      "[7644]\ttrain-rmse:1.718998\n",
      "[7645]\ttrain-rmse:1.718979\n",
      "[7646]\ttrain-rmse:1.718959\n",
      "[7647]\ttrain-rmse:1.718938\n",
      "[7648]\ttrain-rmse:1.718919\n",
      "[7649]\ttrain-rmse:1.718905\n",
      "[7650]\ttrain-rmse:1.718888\n",
      "[7651]\ttrain-rmse:1.718868\n",
      "[7652]\ttrain-rmse:1.718849\n",
      "[7653]\ttrain-rmse:1.718837\n",
      "[7654]\ttrain-rmse:1.718826\n",
      "[7655]\ttrain-rmse:1.718804\n",
      "[7656]\ttrain-rmse:1.718782\n",
      "[7657]\ttrain-rmse:1.718765\n",
      "[7658]\ttrain-rmse:1.718750\n",
      "[7659]\ttrain-rmse:1.718730\n",
      "[7660]\ttrain-rmse:1.718709\n",
      "[7661]\ttrain-rmse:1.718690\n",
      "[7662]\ttrain-rmse:1.718668\n",
      "[7663]\ttrain-rmse:1.718659\n",
      "[7664]\ttrain-rmse:1.718640\n",
      "[7665]\ttrain-rmse:1.718625\n",
      "[7666]\ttrain-rmse:1.718610\n",
      "[7667]\ttrain-rmse:1.718595\n",
      "[7668]\ttrain-rmse:1.718581\n",
      "[7669]\ttrain-rmse:1.718564\n",
      "[7670]\ttrain-rmse:1.718552\n",
      "[7671]\ttrain-rmse:1.718536\n",
      "[7672]\ttrain-rmse:1.718521\n",
      "[7673]\ttrain-rmse:1.718505\n",
      "[7674]\ttrain-rmse:1.718492\n",
      "[7675]\ttrain-rmse:1.718475\n",
      "[7676]\ttrain-rmse:1.718461\n",
      "[7677]\ttrain-rmse:1.718446\n",
      "[7678]\ttrain-rmse:1.718427\n",
      "[7679]\ttrain-rmse:1.718413\n",
      "[7680]\ttrain-rmse:1.718399\n",
      "[7681]\ttrain-rmse:1.718386\n",
      "[7682]\ttrain-rmse:1.718368\n",
      "[7683]\ttrain-rmse:1.718356\n",
      "[7684]\ttrain-rmse:1.718328\n",
      "[7685]\ttrain-rmse:1.718313\n",
      "[7686]\ttrain-rmse:1.718298\n",
      "[7687]\ttrain-rmse:1.718287\n",
      "[7688]\ttrain-rmse:1.718272\n",
      "[7689]\ttrain-rmse:1.718253\n",
      "[7690]\ttrain-rmse:1.718241\n",
      "[7691]\ttrain-rmse:1.718228\n",
      "[7692]\ttrain-rmse:1.718207\n",
      "[7693]\ttrain-rmse:1.718190\n",
      "[7694]\ttrain-rmse:1.718167\n",
      "[7695]\ttrain-rmse:1.718151\n",
      "[7696]\ttrain-rmse:1.718133\n",
      "[7697]\ttrain-rmse:1.718119\n",
      "[7698]\ttrain-rmse:1.718099\n",
      "[7699]\ttrain-rmse:1.718084\n",
      "[7700]\ttrain-rmse:1.718071\n",
      "[7701]\ttrain-rmse:1.718055\n",
      "[7702]\ttrain-rmse:1.718045\n",
      "[7703]\ttrain-rmse:1.718030\n",
      "[7704]\ttrain-rmse:1.718007\n",
      "[7705]\ttrain-rmse:1.717988\n",
      "[7706]\ttrain-rmse:1.717974\n",
      "[7707]\ttrain-rmse:1.717961\n",
      "[7708]\ttrain-rmse:1.717950\n",
      "[7709]\ttrain-rmse:1.717930\n",
      "[7710]\ttrain-rmse:1.717914\n",
      "[7711]\ttrain-rmse:1.717893\n",
      "[7712]\ttrain-rmse:1.717879\n",
      "[7713]\ttrain-rmse:1.717859\n",
      "[7714]\ttrain-rmse:1.717838\n",
      "[7715]\ttrain-rmse:1.717819\n",
      "[7716]\ttrain-rmse:1.717800\n",
      "[7717]\ttrain-rmse:1.717786\n",
      "[7718]\ttrain-rmse:1.717771\n",
      "[7719]\ttrain-rmse:1.717754\n",
      "[7720]\ttrain-rmse:1.717739\n",
      "[7721]\ttrain-rmse:1.717723\n",
      "[7722]\ttrain-rmse:1.717699\n",
      "[7723]\ttrain-rmse:1.717687\n",
      "[7724]\ttrain-rmse:1.717672\n",
      "[7725]\ttrain-rmse:1.717653\n",
      "[7726]\ttrain-rmse:1.717630\n",
      "[7727]\ttrain-rmse:1.717604\n",
      "[7728]\ttrain-rmse:1.717589\n",
      "[7729]\ttrain-rmse:1.717573\n",
      "[7730]\ttrain-rmse:1.717554\n",
      "[7731]\ttrain-rmse:1.717547\n",
      "[7732]\ttrain-rmse:1.717528\n",
      "[7733]\ttrain-rmse:1.717511\n",
      "[7734]\ttrain-rmse:1.717499\n",
      "[7735]\ttrain-rmse:1.717484\n",
      "[7736]\ttrain-rmse:1.717466\n",
      "[7737]\ttrain-rmse:1.717452\n",
      "[7738]\ttrain-rmse:1.717435\n",
      "[7739]\ttrain-rmse:1.717421\n",
      "[7740]\ttrain-rmse:1.717404\n",
      "[7741]\ttrain-rmse:1.717394\n",
      "[7742]\ttrain-rmse:1.717372\n",
      "[7743]\ttrain-rmse:1.717363\n",
      "[7744]\ttrain-rmse:1.717353\n",
      "[7745]\ttrain-rmse:1.717335\n",
      "[7746]\ttrain-rmse:1.717319\n",
      "[7747]\ttrain-rmse:1.717302\n",
      "[7748]\ttrain-rmse:1.717286\n",
      "[7749]\ttrain-rmse:1.717274\n",
      "[7750]\ttrain-rmse:1.717259\n",
      "[7751]\ttrain-rmse:1.717246\n",
      "[7752]\ttrain-rmse:1.717230\n",
      "[7753]\ttrain-rmse:1.717208\n",
      "[7754]\ttrain-rmse:1.717190\n",
      "[7755]\ttrain-rmse:1.717173\n",
      "[7756]\ttrain-rmse:1.717155\n",
      "[7757]\ttrain-rmse:1.717135\n",
      "[7758]\ttrain-rmse:1.717123\n",
      "[7759]\ttrain-rmse:1.717105\n",
      "[7760]\ttrain-rmse:1.717083\n",
      "[7761]\ttrain-rmse:1.717069\n",
      "[7762]\ttrain-rmse:1.717054\n",
      "[7763]\ttrain-rmse:1.717033\n",
      "[7764]\ttrain-rmse:1.717021\n",
      "[7765]\ttrain-rmse:1.717006\n",
      "[7766]\ttrain-rmse:1.716990\n",
      "[7767]\ttrain-rmse:1.716977\n",
      "[7768]\ttrain-rmse:1.716962\n",
      "[7769]\ttrain-rmse:1.716946\n",
      "[7770]\ttrain-rmse:1.716929\n",
      "[7771]\ttrain-rmse:1.716910\n",
      "[7772]\ttrain-rmse:1.716896\n",
      "[7773]\ttrain-rmse:1.716881\n",
      "[7774]\ttrain-rmse:1.716861\n",
      "[7775]\ttrain-rmse:1.716841\n",
      "[7776]\ttrain-rmse:1.716824\n",
      "[7777]\ttrain-rmse:1.716795\n",
      "[7778]\ttrain-rmse:1.716783\n",
      "[7779]\ttrain-rmse:1.716765\n",
      "[7780]\ttrain-rmse:1.716743\n",
      "[7781]\ttrain-rmse:1.716729\n",
      "[7782]\ttrain-rmse:1.716710\n",
      "[7783]\ttrain-rmse:1.716691\n",
      "[7784]\ttrain-rmse:1.716676\n",
      "[7785]\ttrain-rmse:1.716664\n",
      "[7786]\ttrain-rmse:1.716646\n",
      "[7787]\ttrain-rmse:1.716634\n",
      "[7788]\ttrain-rmse:1.716619\n",
      "[7789]\ttrain-rmse:1.716603\n",
      "[7790]\ttrain-rmse:1.716586\n",
      "[7791]\ttrain-rmse:1.716571\n",
      "[7792]\ttrain-rmse:1.716558\n",
      "[7793]\ttrain-rmse:1.716542\n",
      "[7794]\ttrain-rmse:1.716527\n",
      "[7795]\ttrain-rmse:1.716512\n",
      "[7796]\ttrain-rmse:1.716491\n",
      "[7797]\ttrain-rmse:1.716481\n",
      "[7798]\ttrain-rmse:1.716465\n",
      "[7799]\ttrain-rmse:1.716451\n",
      "[7800]\ttrain-rmse:1.716440\n",
      "[7801]\ttrain-rmse:1.716424\n",
      "[7802]\ttrain-rmse:1.716409\n",
      "[7803]\ttrain-rmse:1.716399\n",
      "[7804]\ttrain-rmse:1.716386\n",
      "[7805]\ttrain-rmse:1.716367\n",
      "[7806]\ttrain-rmse:1.716345\n",
      "[7807]\ttrain-rmse:1.716327\n",
      "[7808]\ttrain-rmse:1.716311\n",
      "[7809]\ttrain-rmse:1.716293\n",
      "[7810]\ttrain-rmse:1.716282\n",
      "[7811]\ttrain-rmse:1.716267\n",
      "[7812]\ttrain-rmse:1.716253\n",
      "[7813]\ttrain-rmse:1.716236\n",
      "[7814]\ttrain-rmse:1.716225\n",
      "[7815]\ttrain-rmse:1.716210\n",
      "[7816]\ttrain-rmse:1.716194\n",
      "[7817]\ttrain-rmse:1.716182\n",
      "[7818]\ttrain-rmse:1.716158\n",
      "[7819]\ttrain-rmse:1.716142\n",
      "[7820]\ttrain-rmse:1.716125\n",
      "[7821]\ttrain-rmse:1.716112\n",
      "[7822]\ttrain-rmse:1.716094\n",
      "[7823]\ttrain-rmse:1.716076\n",
      "[7824]\ttrain-rmse:1.716057\n",
      "[7825]\ttrain-rmse:1.716045\n",
      "[7826]\ttrain-rmse:1.716026\n",
      "[7827]\ttrain-rmse:1.716014\n",
      "[7828]\ttrain-rmse:1.715998\n",
      "[7829]\ttrain-rmse:1.715984\n",
      "[7830]\ttrain-rmse:1.715967\n",
      "[7831]\ttrain-rmse:1.715951\n",
      "[7832]\ttrain-rmse:1.715929\n",
      "[7833]\ttrain-rmse:1.715909\n",
      "[7834]\ttrain-rmse:1.715897\n",
      "[7835]\ttrain-rmse:1.715882\n",
      "[7836]\ttrain-rmse:1.715868\n",
      "[7837]\ttrain-rmse:1.715852\n",
      "[7838]\ttrain-rmse:1.715838\n",
      "[7839]\ttrain-rmse:1.715818\n",
      "[7840]\ttrain-rmse:1.715800\n",
      "[7841]\ttrain-rmse:1.715792\n",
      "[7842]\ttrain-rmse:1.715773\n",
      "[7843]\ttrain-rmse:1.715759\n",
      "[7844]\ttrain-rmse:1.715743\n",
      "[7845]\ttrain-rmse:1.715727\n",
      "[7846]\ttrain-rmse:1.715710\n",
      "[7847]\ttrain-rmse:1.715700\n",
      "[7848]\ttrain-rmse:1.715678\n",
      "[7849]\ttrain-rmse:1.715658\n",
      "[7850]\ttrain-rmse:1.715641\n",
      "[7851]\ttrain-rmse:1.715623\n",
      "[7852]\ttrain-rmse:1.715615\n",
      "[7853]\ttrain-rmse:1.715600\n",
      "[7854]\ttrain-rmse:1.715584\n",
      "[7855]\ttrain-rmse:1.715573\n",
      "[7856]\ttrain-rmse:1.715558\n",
      "[7857]\ttrain-rmse:1.715536\n",
      "[7858]\ttrain-rmse:1.715527\n",
      "[7859]\ttrain-rmse:1.715513\n",
      "[7860]\ttrain-rmse:1.715500\n",
      "[7861]\ttrain-rmse:1.715485\n",
      "[7862]\ttrain-rmse:1.715472\n",
      "[7863]\ttrain-rmse:1.715454\n",
      "[7864]\ttrain-rmse:1.715439\n",
      "[7865]\ttrain-rmse:1.715419\n",
      "[7866]\ttrain-rmse:1.715404\n",
      "[7867]\ttrain-rmse:1.715381\n",
      "[7868]\ttrain-rmse:1.715366\n",
      "[7869]\ttrain-rmse:1.715346\n",
      "[7870]\ttrain-rmse:1.715335\n",
      "[7871]\ttrain-rmse:1.715322\n",
      "[7872]\ttrain-rmse:1.715311\n",
      "[7873]\ttrain-rmse:1.715290\n",
      "[7874]\ttrain-rmse:1.715273\n",
      "[7875]\ttrain-rmse:1.715261\n",
      "[7876]\ttrain-rmse:1.715243\n",
      "[7877]\ttrain-rmse:1.715225\n",
      "[7878]\ttrain-rmse:1.715211\n",
      "[7879]\ttrain-rmse:1.715198\n",
      "[7880]\ttrain-rmse:1.715181\n",
      "[7881]\ttrain-rmse:1.715165\n",
      "[7882]\ttrain-rmse:1.715150\n",
      "[7883]\ttrain-rmse:1.715130\n",
      "[7884]\ttrain-rmse:1.715112\n",
      "[7885]\ttrain-rmse:1.715097\n",
      "[7886]\ttrain-rmse:1.715080\n",
      "[7887]\ttrain-rmse:1.715067\n",
      "[7888]\ttrain-rmse:1.715053\n",
      "[7889]\ttrain-rmse:1.715037\n",
      "[7890]\ttrain-rmse:1.715022\n",
      "[7891]\ttrain-rmse:1.715005\n",
      "[7892]\ttrain-rmse:1.714992\n",
      "[7893]\ttrain-rmse:1.714973\n",
      "[7894]\ttrain-rmse:1.714961\n",
      "[7895]\ttrain-rmse:1.714938\n",
      "[7896]\ttrain-rmse:1.714915\n",
      "[7897]\ttrain-rmse:1.714896\n",
      "[7898]\ttrain-rmse:1.714884\n",
      "[7899]\ttrain-rmse:1.714873\n",
      "[7900]\ttrain-rmse:1.714857\n",
      "[7901]\ttrain-rmse:1.714843\n",
      "[7902]\ttrain-rmse:1.714826\n",
      "[7903]\ttrain-rmse:1.714813\n",
      "[7904]\ttrain-rmse:1.714803\n",
      "[7905]\ttrain-rmse:1.714788\n",
      "[7906]\ttrain-rmse:1.714772\n",
      "[7907]\ttrain-rmse:1.714761\n",
      "[7908]\ttrain-rmse:1.714750\n",
      "[7909]\ttrain-rmse:1.714738\n",
      "[7910]\ttrain-rmse:1.714726\n",
      "[7911]\ttrain-rmse:1.714711\n",
      "[7912]\ttrain-rmse:1.714690\n",
      "[7913]\ttrain-rmse:1.714673\n",
      "[7914]\ttrain-rmse:1.714657\n",
      "[7915]\ttrain-rmse:1.714645\n",
      "[7916]\ttrain-rmse:1.714632\n",
      "[7917]\ttrain-rmse:1.714615\n",
      "[7918]\ttrain-rmse:1.714598\n",
      "[7919]\ttrain-rmse:1.714587\n",
      "[7920]\ttrain-rmse:1.714575\n",
      "[7921]\ttrain-rmse:1.714555\n",
      "[7922]\ttrain-rmse:1.714540\n",
      "[7923]\ttrain-rmse:1.714526\n",
      "[7924]\ttrain-rmse:1.714513\n",
      "[7925]\ttrain-rmse:1.714491\n",
      "[7926]\ttrain-rmse:1.714481\n",
      "[7927]\ttrain-rmse:1.714467\n",
      "[7928]\ttrain-rmse:1.714453\n",
      "[7929]\ttrain-rmse:1.714430\n",
      "[7930]\ttrain-rmse:1.714414\n",
      "[7931]\ttrain-rmse:1.714401\n",
      "[7932]\ttrain-rmse:1.714386\n",
      "[7933]\ttrain-rmse:1.714369\n",
      "[7934]\ttrain-rmse:1.714355\n",
      "[7935]\ttrain-rmse:1.714336\n",
      "[7936]\ttrain-rmse:1.714313\n",
      "[7937]\ttrain-rmse:1.714293\n",
      "[7938]\ttrain-rmse:1.714280\n",
      "[7939]\ttrain-rmse:1.714262\n",
      "[7940]\ttrain-rmse:1.714250\n",
      "[7941]\ttrain-rmse:1.714229\n",
      "[7942]\ttrain-rmse:1.714216\n",
      "[7943]\ttrain-rmse:1.714203\n",
      "[7944]\ttrain-rmse:1.714187\n",
      "[7945]\ttrain-rmse:1.714171\n",
      "[7946]\ttrain-rmse:1.714154\n",
      "[7947]\ttrain-rmse:1.714139\n",
      "[7948]\ttrain-rmse:1.714129\n",
      "[7949]\ttrain-rmse:1.714108\n",
      "[7950]\ttrain-rmse:1.714094\n",
      "[7951]\ttrain-rmse:1.714077\n",
      "[7952]\ttrain-rmse:1.714065\n",
      "[7953]\ttrain-rmse:1.714048\n",
      "[7954]\ttrain-rmse:1.714033\n",
      "[7955]\ttrain-rmse:1.714016\n",
      "[7956]\ttrain-rmse:1.714005\n",
      "[7957]\ttrain-rmse:1.713991\n",
      "[7958]\ttrain-rmse:1.713971\n",
      "[7959]\ttrain-rmse:1.713958\n",
      "[7960]\ttrain-rmse:1.713945\n",
      "[7961]\ttrain-rmse:1.713925\n",
      "[7962]\ttrain-rmse:1.713909\n",
      "[7963]\ttrain-rmse:1.713895\n",
      "[7964]\ttrain-rmse:1.713868\n",
      "[7965]\ttrain-rmse:1.713856\n",
      "[7966]\ttrain-rmse:1.713840\n",
      "[7967]\ttrain-rmse:1.713828\n",
      "[7968]\ttrain-rmse:1.713807\n",
      "[7969]\ttrain-rmse:1.713790\n",
      "[7970]\ttrain-rmse:1.713776\n",
      "[7971]\ttrain-rmse:1.713763\n",
      "[7972]\ttrain-rmse:1.713747\n",
      "[7973]\ttrain-rmse:1.713736\n",
      "[7974]\ttrain-rmse:1.713721\n",
      "[7975]\ttrain-rmse:1.713701\n",
      "[7976]\ttrain-rmse:1.713684\n",
      "[7977]\ttrain-rmse:1.713671\n",
      "[7978]\ttrain-rmse:1.713655\n",
      "[7979]\ttrain-rmse:1.713645\n",
      "[7980]\ttrain-rmse:1.713634\n",
      "[7981]\ttrain-rmse:1.713619\n",
      "[7982]\ttrain-rmse:1.713601\n",
      "[7983]\ttrain-rmse:1.713585\n",
      "[7984]\ttrain-rmse:1.713558\n",
      "[7985]\ttrain-rmse:1.713546\n",
      "[7986]\ttrain-rmse:1.713531\n",
      "[7987]\ttrain-rmse:1.713521\n",
      "[7988]\ttrain-rmse:1.713495\n",
      "[7989]\ttrain-rmse:1.713485\n",
      "[7990]\ttrain-rmse:1.713472\n",
      "[7991]\ttrain-rmse:1.713454\n",
      "[7992]\ttrain-rmse:1.713437\n",
      "[7993]\ttrain-rmse:1.713423\n",
      "[7994]\ttrain-rmse:1.713411\n",
      "[7995]\ttrain-rmse:1.713399\n",
      "[7996]\ttrain-rmse:1.713377\n",
      "[7997]\ttrain-rmse:1.713362\n",
      "[7998]\ttrain-rmse:1.713346\n",
      "[7999]\ttrain-rmse:1.713324\n",
      "[8000]\ttrain-rmse:1.713307\n",
      "[8001]\ttrain-rmse:1.713284\n",
      "[8002]\ttrain-rmse:1.713265\n",
      "[8003]\ttrain-rmse:1.713247\n",
      "[8004]\ttrain-rmse:1.713231\n",
      "[8005]\ttrain-rmse:1.713216\n",
      "[8006]\ttrain-rmse:1.713204\n",
      "[8007]\ttrain-rmse:1.713188\n",
      "[8008]\ttrain-rmse:1.713179\n",
      "[8009]\ttrain-rmse:1.713164\n",
      "[8010]\ttrain-rmse:1.713148\n",
      "[8011]\ttrain-rmse:1.713126\n",
      "[8012]\ttrain-rmse:1.713111\n",
      "[8013]\ttrain-rmse:1.713092\n",
      "[8014]\ttrain-rmse:1.713078\n",
      "[8015]\ttrain-rmse:1.713061\n",
      "[8016]\ttrain-rmse:1.713049\n",
      "[8017]\ttrain-rmse:1.713034\n",
      "[8018]\ttrain-rmse:1.713020\n",
      "[8019]\ttrain-rmse:1.713000\n",
      "[8020]\ttrain-rmse:1.712980\n",
      "[8021]\ttrain-rmse:1.712958\n",
      "[8022]\ttrain-rmse:1.712945\n",
      "[8023]\ttrain-rmse:1.712933\n",
      "[8024]\ttrain-rmse:1.712913\n",
      "[8025]\ttrain-rmse:1.712898\n",
      "[8026]\ttrain-rmse:1.712882\n",
      "[8027]\ttrain-rmse:1.712868\n",
      "[8028]\ttrain-rmse:1.712856\n",
      "[8029]\ttrain-rmse:1.712841\n",
      "[8030]\ttrain-rmse:1.712823\n",
      "[8031]\ttrain-rmse:1.712810\n",
      "[8032]\ttrain-rmse:1.712800\n",
      "[8033]\ttrain-rmse:1.712787\n",
      "[8034]\ttrain-rmse:1.712772\n",
      "[8035]\ttrain-rmse:1.712759\n",
      "[8036]\ttrain-rmse:1.712744\n",
      "[8037]\ttrain-rmse:1.712728\n",
      "[8038]\ttrain-rmse:1.712714\n",
      "[8039]\ttrain-rmse:1.712704\n",
      "[8040]\ttrain-rmse:1.712688\n",
      "[8041]\ttrain-rmse:1.712675\n",
      "[8042]\ttrain-rmse:1.712651\n",
      "[8043]\ttrain-rmse:1.712638\n",
      "[8044]\ttrain-rmse:1.712623\n",
      "[8045]\ttrain-rmse:1.712608\n",
      "[8046]\ttrain-rmse:1.712587\n",
      "[8047]\ttrain-rmse:1.712566\n",
      "[8048]\ttrain-rmse:1.712548\n",
      "[8049]\ttrain-rmse:1.712530\n",
      "[8050]\ttrain-rmse:1.712513\n",
      "[8051]\ttrain-rmse:1.712498\n",
      "[8052]\ttrain-rmse:1.712485\n",
      "[8053]\ttrain-rmse:1.712466\n",
      "[8054]\ttrain-rmse:1.712445\n",
      "[8055]\ttrain-rmse:1.712435\n",
      "[8056]\ttrain-rmse:1.712421\n",
      "[8057]\ttrain-rmse:1.712402\n",
      "[8058]\ttrain-rmse:1.712381\n",
      "[8059]\ttrain-rmse:1.712365\n",
      "[8060]\ttrain-rmse:1.712353\n",
      "[8061]\ttrain-rmse:1.712342\n",
      "[8062]\ttrain-rmse:1.712332\n",
      "[8063]\ttrain-rmse:1.712315\n",
      "[8064]\ttrain-rmse:1.712302\n",
      "[8065]\ttrain-rmse:1.712286\n",
      "[8066]\ttrain-rmse:1.712269\n",
      "[8067]\ttrain-rmse:1.712252\n",
      "[8068]\ttrain-rmse:1.712237\n",
      "[8069]\ttrain-rmse:1.712221\n",
      "[8070]\ttrain-rmse:1.712210\n",
      "[8071]\ttrain-rmse:1.712196\n",
      "[8072]\ttrain-rmse:1.712181\n",
      "[8073]\ttrain-rmse:1.712152\n",
      "[8074]\ttrain-rmse:1.712138\n",
      "[8075]\ttrain-rmse:1.712129\n",
      "[8076]\ttrain-rmse:1.712114\n",
      "[8077]\ttrain-rmse:1.712102\n",
      "[8078]\ttrain-rmse:1.712093\n",
      "[8079]\ttrain-rmse:1.712071\n",
      "[8080]\ttrain-rmse:1.712049\n",
      "[8081]\ttrain-rmse:1.712034\n",
      "[8082]\ttrain-rmse:1.712021\n",
      "[8083]\ttrain-rmse:1.712003\n",
      "[8084]\ttrain-rmse:1.711981\n",
      "[8085]\ttrain-rmse:1.711969\n",
      "[8086]\ttrain-rmse:1.711955\n",
      "[8087]\ttrain-rmse:1.711941\n",
      "[8088]\ttrain-rmse:1.711925\n",
      "[8089]\ttrain-rmse:1.711912\n",
      "[8090]\ttrain-rmse:1.711900\n",
      "[8091]\ttrain-rmse:1.711885\n",
      "[8092]\ttrain-rmse:1.711869\n",
      "[8093]\ttrain-rmse:1.711850\n",
      "[8094]\ttrain-rmse:1.711833\n",
      "[8095]\ttrain-rmse:1.711816\n",
      "[8096]\ttrain-rmse:1.711799\n",
      "[8097]\ttrain-rmse:1.711785\n",
      "[8098]\ttrain-rmse:1.711774\n",
      "[8099]\ttrain-rmse:1.711764\n",
      "[8100]\ttrain-rmse:1.711748\n",
      "[8101]\ttrain-rmse:1.711737\n",
      "[8102]\ttrain-rmse:1.711724\n",
      "[8103]\ttrain-rmse:1.711710\n",
      "[8104]\ttrain-rmse:1.711695\n",
      "[8105]\ttrain-rmse:1.711677\n",
      "[8106]\ttrain-rmse:1.711658\n",
      "[8107]\ttrain-rmse:1.711646\n",
      "[8108]\ttrain-rmse:1.711630\n",
      "[8109]\ttrain-rmse:1.711617\n",
      "[8110]\ttrain-rmse:1.711602\n",
      "[8111]\ttrain-rmse:1.711587\n",
      "[8112]\ttrain-rmse:1.711575\n",
      "[8113]\ttrain-rmse:1.711559\n",
      "[8114]\ttrain-rmse:1.711546\n",
      "[8115]\ttrain-rmse:1.711528\n",
      "[8116]\ttrain-rmse:1.711514\n",
      "[8117]\ttrain-rmse:1.711497\n",
      "[8118]\ttrain-rmse:1.711477\n",
      "[8119]\ttrain-rmse:1.711456\n",
      "[8120]\ttrain-rmse:1.711441\n",
      "[8121]\ttrain-rmse:1.711423\n",
      "[8122]\ttrain-rmse:1.711408\n",
      "[8123]\ttrain-rmse:1.711396\n",
      "[8124]\ttrain-rmse:1.711385\n",
      "[8125]\ttrain-rmse:1.711374\n",
      "[8126]\ttrain-rmse:1.711363\n",
      "[8127]\ttrain-rmse:1.711351\n",
      "[8128]\ttrain-rmse:1.711339\n",
      "[8129]\ttrain-rmse:1.711326\n",
      "[8130]\ttrain-rmse:1.711314\n",
      "[8131]\ttrain-rmse:1.711304\n",
      "[8132]\ttrain-rmse:1.711289\n",
      "[8133]\ttrain-rmse:1.711274\n",
      "[8134]\ttrain-rmse:1.711254\n",
      "[8135]\ttrain-rmse:1.711236\n",
      "[8136]\ttrain-rmse:1.711217\n",
      "[8137]\ttrain-rmse:1.711193\n",
      "[8138]\ttrain-rmse:1.711180\n",
      "[8139]\ttrain-rmse:1.711167\n",
      "[8140]\ttrain-rmse:1.711153\n",
      "[8141]\ttrain-rmse:1.711137\n",
      "[8142]\ttrain-rmse:1.711118\n",
      "[8143]\ttrain-rmse:1.711108\n",
      "[8144]\ttrain-rmse:1.711088\n",
      "[8145]\ttrain-rmse:1.711075\n",
      "[8146]\ttrain-rmse:1.711051\n",
      "[8147]\ttrain-rmse:1.711036\n",
      "[8148]\ttrain-rmse:1.711023\n",
      "[8149]\ttrain-rmse:1.711012\n",
      "[8150]\ttrain-rmse:1.710999\n",
      "[8151]\ttrain-rmse:1.710988\n",
      "[8152]\ttrain-rmse:1.710972\n",
      "[8153]\ttrain-rmse:1.710959\n",
      "[8154]\ttrain-rmse:1.710945\n",
      "[8155]\ttrain-rmse:1.710936\n",
      "[8156]\ttrain-rmse:1.710919\n",
      "[8157]\ttrain-rmse:1.710906\n",
      "[8158]\ttrain-rmse:1.710887\n",
      "[8159]\ttrain-rmse:1.710878\n",
      "[8160]\ttrain-rmse:1.710868\n",
      "[8161]\ttrain-rmse:1.710854\n",
      "[8162]\ttrain-rmse:1.710838\n",
      "[8163]\ttrain-rmse:1.710828\n",
      "[8164]\ttrain-rmse:1.710815\n",
      "[8165]\ttrain-rmse:1.710804\n",
      "[8166]\ttrain-rmse:1.710791\n",
      "[8167]\ttrain-rmse:1.710776\n",
      "[8168]\ttrain-rmse:1.710761\n",
      "[8169]\ttrain-rmse:1.710747\n",
      "[8170]\ttrain-rmse:1.710726\n",
      "[8171]\ttrain-rmse:1.710713\n",
      "[8172]\ttrain-rmse:1.710701\n",
      "[8173]\ttrain-rmse:1.710690\n",
      "[8174]\ttrain-rmse:1.710677\n",
      "[8175]\ttrain-rmse:1.710659\n",
      "[8176]\ttrain-rmse:1.710641\n",
      "[8177]\ttrain-rmse:1.710628\n",
      "[8178]\ttrain-rmse:1.710613\n",
      "[8179]\ttrain-rmse:1.710600\n",
      "[8180]\ttrain-rmse:1.710583\n",
      "[8181]\ttrain-rmse:1.710574\n",
      "[8182]\ttrain-rmse:1.710564\n",
      "[8183]\ttrain-rmse:1.710545\n",
      "[8184]\ttrain-rmse:1.710530\n",
      "[8185]\ttrain-rmse:1.710513\n",
      "[8186]\ttrain-rmse:1.710496\n",
      "[8187]\ttrain-rmse:1.710487\n",
      "[8188]\ttrain-rmse:1.710472\n",
      "[8189]\ttrain-rmse:1.710455\n",
      "[8190]\ttrain-rmse:1.710437\n",
      "[8191]\ttrain-rmse:1.710426\n",
      "[8192]\ttrain-rmse:1.710417\n",
      "[8193]\ttrain-rmse:1.710394\n",
      "[8194]\ttrain-rmse:1.710383\n",
      "[8195]\ttrain-rmse:1.710367\n",
      "[8196]\ttrain-rmse:1.710351\n",
      "[8197]\ttrain-rmse:1.710335\n",
      "[8198]\ttrain-rmse:1.710321\n",
      "[8199]\ttrain-rmse:1.710302\n",
      "[8200]\ttrain-rmse:1.710289\n",
      "[8201]\ttrain-rmse:1.710274\n",
      "[8202]\ttrain-rmse:1.710258\n",
      "[8203]\ttrain-rmse:1.710242\n",
      "[8204]\ttrain-rmse:1.710229\n",
      "[8205]\ttrain-rmse:1.710210\n",
      "[8206]\ttrain-rmse:1.710194\n",
      "[8207]\ttrain-rmse:1.710178\n",
      "[8208]\ttrain-rmse:1.710165\n",
      "[8209]\ttrain-rmse:1.710148\n",
      "[8210]\ttrain-rmse:1.710132\n",
      "[8211]\ttrain-rmse:1.710119\n",
      "[8212]\ttrain-rmse:1.710104\n",
      "[8213]\ttrain-rmse:1.710090\n",
      "[8214]\ttrain-rmse:1.710076\n",
      "[8215]\ttrain-rmse:1.710063\n",
      "[8216]\ttrain-rmse:1.710052\n",
      "[8217]\ttrain-rmse:1.710035\n",
      "[8218]\ttrain-rmse:1.710018\n",
      "[8219]\ttrain-rmse:1.710003\n",
      "[8220]\ttrain-rmse:1.709982\n",
      "[8221]\ttrain-rmse:1.709967\n",
      "[8222]\ttrain-rmse:1.709952\n",
      "[8223]\ttrain-rmse:1.709938\n",
      "[8224]\ttrain-rmse:1.709925\n",
      "[8225]\ttrain-rmse:1.709914\n",
      "[8226]\ttrain-rmse:1.709896\n",
      "[8227]\ttrain-rmse:1.709886\n",
      "[8228]\ttrain-rmse:1.709870\n",
      "[8229]\ttrain-rmse:1.709860\n",
      "[8230]\ttrain-rmse:1.709844\n",
      "[8231]\ttrain-rmse:1.709832\n",
      "[8232]\ttrain-rmse:1.709822\n",
      "[8233]\ttrain-rmse:1.709809\n",
      "[8234]\ttrain-rmse:1.709797\n",
      "[8235]\ttrain-rmse:1.709786\n",
      "[8236]\ttrain-rmse:1.709767\n",
      "[8237]\ttrain-rmse:1.709749\n",
      "[8238]\ttrain-rmse:1.709736\n",
      "[8239]\ttrain-rmse:1.709722\n",
      "[8240]\ttrain-rmse:1.709712\n",
      "[8241]\ttrain-rmse:1.709695\n",
      "[8242]\ttrain-rmse:1.709684\n",
      "[8243]\ttrain-rmse:1.709671\n",
      "[8244]\ttrain-rmse:1.709658\n",
      "[8245]\ttrain-rmse:1.709638\n",
      "[8246]\ttrain-rmse:1.709623\n",
      "[8247]\ttrain-rmse:1.709613\n",
      "[8248]\ttrain-rmse:1.709602\n",
      "[8249]\ttrain-rmse:1.709587\n",
      "[8250]\ttrain-rmse:1.709570\n",
      "[8251]\ttrain-rmse:1.709556\n",
      "[8252]\ttrain-rmse:1.709547\n",
      "[8253]\ttrain-rmse:1.709530\n",
      "[8254]\ttrain-rmse:1.709522\n",
      "[8255]\ttrain-rmse:1.709515\n",
      "[8256]\ttrain-rmse:1.709493\n",
      "[8257]\ttrain-rmse:1.709482\n",
      "[8258]\ttrain-rmse:1.709466\n",
      "[8259]\ttrain-rmse:1.709454\n",
      "[8260]\ttrain-rmse:1.709443\n",
      "[8261]\ttrain-rmse:1.709430\n",
      "[8262]\ttrain-rmse:1.709419\n",
      "[8263]\ttrain-rmse:1.709401\n",
      "[8264]\ttrain-rmse:1.709383\n",
      "[8265]\ttrain-rmse:1.709373\n",
      "[8266]\ttrain-rmse:1.709351\n",
      "[8267]\ttrain-rmse:1.709337\n",
      "[8268]\ttrain-rmse:1.709328\n",
      "[8269]\ttrain-rmse:1.709312\n",
      "[8270]\ttrain-rmse:1.709299\n",
      "[8271]\ttrain-rmse:1.709285\n",
      "[8272]\ttrain-rmse:1.709271\n",
      "[8273]\ttrain-rmse:1.709257\n",
      "[8274]\ttrain-rmse:1.709241\n",
      "[8275]\ttrain-rmse:1.709226\n",
      "[8276]\ttrain-rmse:1.709211\n",
      "[8277]\ttrain-rmse:1.709196\n",
      "[8278]\ttrain-rmse:1.709182\n",
      "[8279]\ttrain-rmse:1.709167\n",
      "[8280]\ttrain-rmse:1.709150\n",
      "[8281]\ttrain-rmse:1.709134\n",
      "[8282]\ttrain-rmse:1.709118\n",
      "[8283]\ttrain-rmse:1.709098\n",
      "[8284]\ttrain-rmse:1.709085\n",
      "[8285]\ttrain-rmse:1.709075\n",
      "[8286]\ttrain-rmse:1.709062\n",
      "[8287]\ttrain-rmse:1.709051\n",
      "[8288]\ttrain-rmse:1.709035\n",
      "[8289]\ttrain-rmse:1.709026\n",
      "[8290]\ttrain-rmse:1.709011\n",
      "[8291]\ttrain-rmse:1.708998\n",
      "[8292]\ttrain-rmse:1.708985\n",
      "[8293]\ttrain-rmse:1.708971\n",
      "[8294]\ttrain-rmse:1.708956\n",
      "[8295]\ttrain-rmse:1.708942\n",
      "[8296]\ttrain-rmse:1.708925\n",
      "[8297]\ttrain-rmse:1.708910\n",
      "[8298]\ttrain-rmse:1.708893\n",
      "[8299]\ttrain-rmse:1.708878\n",
      "[8300]\ttrain-rmse:1.708862\n",
      "[8301]\ttrain-rmse:1.708850\n",
      "[8302]\ttrain-rmse:1.708828\n",
      "[8303]\ttrain-rmse:1.708805\n",
      "[8304]\ttrain-rmse:1.708788\n",
      "[8305]\ttrain-rmse:1.708772\n",
      "[8306]\ttrain-rmse:1.708755\n",
      "[8307]\ttrain-rmse:1.708742\n",
      "[8308]\ttrain-rmse:1.708728\n",
      "[8309]\ttrain-rmse:1.708718\n",
      "[8310]\ttrain-rmse:1.708707\n",
      "[8311]\ttrain-rmse:1.708693\n",
      "[8312]\ttrain-rmse:1.708679\n",
      "[8313]\ttrain-rmse:1.708667\n",
      "[8314]\ttrain-rmse:1.708642\n",
      "[8315]\ttrain-rmse:1.708628\n",
      "[8316]\ttrain-rmse:1.708614\n",
      "[8317]\ttrain-rmse:1.708594\n",
      "[8318]\ttrain-rmse:1.708579\n",
      "[8319]\ttrain-rmse:1.708565\n",
      "[8320]\ttrain-rmse:1.708556\n",
      "[8321]\ttrain-rmse:1.708535\n",
      "[8322]\ttrain-rmse:1.708521\n",
      "[8323]\ttrain-rmse:1.708509\n",
      "[8324]\ttrain-rmse:1.708494\n",
      "[8325]\ttrain-rmse:1.708479\n",
      "[8326]\ttrain-rmse:1.708467\n",
      "[8327]\ttrain-rmse:1.708453\n",
      "[8328]\ttrain-rmse:1.708433\n",
      "[8329]\ttrain-rmse:1.708422\n",
      "[8330]\ttrain-rmse:1.708403\n",
      "[8331]\ttrain-rmse:1.708389\n",
      "[8332]\ttrain-rmse:1.708370\n",
      "[8333]\ttrain-rmse:1.708350\n",
      "[8334]\ttrain-rmse:1.708337\n",
      "[8335]\ttrain-rmse:1.708326\n",
      "[8336]\ttrain-rmse:1.708307\n",
      "[8337]\ttrain-rmse:1.708291\n",
      "[8338]\ttrain-rmse:1.708277\n",
      "[8339]\ttrain-rmse:1.708273\n",
      "[8340]\ttrain-rmse:1.708256\n",
      "[8341]\ttrain-rmse:1.708246\n",
      "[8342]\ttrain-rmse:1.708226\n",
      "[8343]\ttrain-rmse:1.708215\n",
      "[8344]\ttrain-rmse:1.708201\n",
      "[8345]\ttrain-rmse:1.708190\n",
      "[8346]\ttrain-rmse:1.708180\n",
      "[8347]\ttrain-rmse:1.708160\n",
      "[8348]\ttrain-rmse:1.708150\n",
      "[8349]\ttrain-rmse:1.708132\n",
      "[8350]\ttrain-rmse:1.708116\n",
      "[8351]\ttrain-rmse:1.708097\n",
      "[8352]\ttrain-rmse:1.708081\n",
      "[8353]\ttrain-rmse:1.708065\n",
      "[8354]\ttrain-rmse:1.708043\n",
      "[8355]\ttrain-rmse:1.708026\n",
      "[8356]\ttrain-rmse:1.708012\n",
      "[8357]\ttrain-rmse:1.708001\n",
      "[8358]\ttrain-rmse:1.707994\n",
      "[8359]\ttrain-rmse:1.707974\n",
      "[8360]\ttrain-rmse:1.707960\n",
      "[8361]\ttrain-rmse:1.707939\n",
      "[8362]\ttrain-rmse:1.707924\n",
      "[8363]\ttrain-rmse:1.707906\n",
      "[8364]\ttrain-rmse:1.707890\n",
      "[8365]\ttrain-rmse:1.707868\n",
      "[8366]\ttrain-rmse:1.707844\n",
      "[8367]\ttrain-rmse:1.707833\n",
      "[8368]\ttrain-rmse:1.707817\n",
      "[8369]\ttrain-rmse:1.707807\n",
      "[8370]\ttrain-rmse:1.707796\n",
      "[8371]\ttrain-rmse:1.707780\n",
      "[8372]\ttrain-rmse:1.707772\n",
      "[8373]\ttrain-rmse:1.707765\n",
      "[8374]\ttrain-rmse:1.707751\n",
      "[8375]\ttrain-rmse:1.707743\n",
      "[8376]\ttrain-rmse:1.707727\n",
      "[8377]\ttrain-rmse:1.707712\n",
      "[8378]\ttrain-rmse:1.707699\n",
      "[8379]\ttrain-rmse:1.707681\n",
      "[8380]\ttrain-rmse:1.707669\n",
      "[8381]\ttrain-rmse:1.707649\n",
      "[8382]\ttrain-rmse:1.707638\n",
      "[8383]\ttrain-rmse:1.707620\n",
      "[8384]\ttrain-rmse:1.707609\n",
      "[8385]\ttrain-rmse:1.707584\n",
      "[8386]\ttrain-rmse:1.707565\n",
      "[8387]\ttrain-rmse:1.707554\n",
      "[8388]\ttrain-rmse:1.707541\n",
      "[8389]\ttrain-rmse:1.707533\n",
      "[8390]\ttrain-rmse:1.707517\n",
      "[8391]\ttrain-rmse:1.707504\n",
      "[8392]\ttrain-rmse:1.707493\n",
      "[8393]\ttrain-rmse:1.707474\n",
      "[8394]\ttrain-rmse:1.707458\n",
      "[8395]\ttrain-rmse:1.707448\n",
      "[8396]\ttrain-rmse:1.707431\n",
      "[8397]\ttrain-rmse:1.707419\n",
      "[8398]\ttrain-rmse:1.707408\n",
      "[8399]\ttrain-rmse:1.707393\n",
      "[8400]\ttrain-rmse:1.707380\n",
      "[8401]\ttrain-rmse:1.707362\n",
      "[8402]\ttrain-rmse:1.707350\n",
      "[8403]\ttrain-rmse:1.707338\n",
      "[8404]\ttrain-rmse:1.707329\n",
      "[8405]\ttrain-rmse:1.707310\n",
      "[8406]\ttrain-rmse:1.707298\n",
      "[8407]\ttrain-rmse:1.707284\n",
      "[8408]\ttrain-rmse:1.707270\n",
      "[8409]\ttrain-rmse:1.707259\n",
      "[8410]\ttrain-rmse:1.707239\n",
      "[8411]\ttrain-rmse:1.707224\n",
      "[8412]\ttrain-rmse:1.707211\n",
      "[8413]\ttrain-rmse:1.707201\n",
      "[8414]\ttrain-rmse:1.707189\n",
      "[8415]\ttrain-rmse:1.707174\n",
      "[8416]\ttrain-rmse:1.707163\n",
      "[8417]\ttrain-rmse:1.707142\n",
      "[8418]\ttrain-rmse:1.707125\n",
      "[8419]\ttrain-rmse:1.707113\n",
      "[8420]\ttrain-rmse:1.707101\n",
      "[8421]\ttrain-rmse:1.707078\n",
      "[8422]\ttrain-rmse:1.707062\n",
      "[8423]\ttrain-rmse:1.707046\n",
      "[8424]\ttrain-rmse:1.707031\n",
      "[8425]\ttrain-rmse:1.707018\n",
      "[8426]\ttrain-rmse:1.707008\n",
      "[8427]\ttrain-rmse:1.706994\n",
      "[8428]\ttrain-rmse:1.706986\n",
      "[8429]\ttrain-rmse:1.706974\n",
      "[8430]\ttrain-rmse:1.706956\n",
      "[8431]\ttrain-rmse:1.706946\n",
      "[8432]\ttrain-rmse:1.706933\n",
      "[8433]\ttrain-rmse:1.706914\n",
      "[8434]\ttrain-rmse:1.706900\n",
      "[8435]\ttrain-rmse:1.706878\n",
      "[8436]\ttrain-rmse:1.706866\n",
      "[8437]\ttrain-rmse:1.706856\n",
      "[8438]\ttrain-rmse:1.706841\n",
      "[8439]\ttrain-rmse:1.706834\n",
      "[8440]\ttrain-rmse:1.706819\n",
      "[8441]\ttrain-rmse:1.706801\n",
      "[8442]\ttrain-rmse:1.706789\n",
      "[8443]\ttrain-rmse:1.706775\n",
      "[8444]\ttrain-rmse:1.706756\n",
      "[8445]\ttrain-rmse:1.706748\n",
      "[8446]\ttrain-rmse:1.706731\n",
      "[8447]\ttrain-rmse:1.706710\n",
      "[8448]\ttrain-rmse:1.706697\n",
      "[8449]\ttrain-rmse:1.706688\n",
      "[8450]\ttrain-rmse:1.706681\n",
      "[8451]\ttrain-rmse:1.706667\n",
      "[8452]\ttrain-rmse:1.706653\n",
      "[8453]\ttrain-rmse:1.706643\n",
      "[8454]\ttrain-rmse:1.706632\n",
      "[8455]\ttrain-rmse:1.706617\n",
      "[8456]\ttrain-rmse:1.706596\n",
      "[8457]\ttrain-rmse:1.706579\n",
      "[8458]\ttrain-rmse:1.706550\n",
      "[8459]\ttrain-rmse:1.706542\n",
      "[8460]\ttrain-rmse:1.706528\n",
      "[8461]\ttrain-rmse:1.706517\n",
      "[8462]\ttrain-rmse:1.706498\n",
      "[8463]\ttrain-rmse:1.706483\n",
      "[8464]\ttrain-rmse:1.706465\n",
      "[8465]\ttrain-rmse:1.706452\n",
      "[8466]\ttrain-rmse:1.706440\n",
      "[8467]\ttrain-rmse:1.706431\n",
      "[8468]\ttrain-rmse:1.706421\n",
      "[8469]\ttrain-rmse:1.706405\n",
      "[8470]\ttrain-rmse:1.706388\n",
      "[8471]\ttrain-rmse:1.706378\n",
      "[8472]\ttrain-rmse:1.706362\n",
      "[8473]\ttrain-rmse:1.706350\n",
      "[8474]\ttrain-rmse:1.706339\n",
      "[8475]\ttrain-rmse:1.706324\n",
      "[8476]\ttrain-rmse:1.706311\n",
      "[8477]\ttrain-rmse:1.706300\n",
      "[8478]\ttrain-rmse:1.706294\n",
      "[8479]\ttrain-rmse:1.706288\n",
      "[8480]\ttrain-rmse:1.706271\n",
      "[8481]\ttrain-rmse:1.706262\n",
      "[8482]\ttrain-rmse:1.706248\n",
      "[8483]\ttrain-rmse:1.706228\n",
      "[8484]\ttrain-rmse:1.706221\n",
      "[8485]\ttrain-rmse:1.706207\n",
      "[8486]\ttrain-rmse:1.706193\n",
      "[8487]\ttrain-rmse:1.706174\n",
      "[8488]\ttrain-rmse:1.706165\n",
      "[8489]\ttrain-rmse:1.706153\n",
      "[8490]\ttrain-rmse:1.706142\n",
      "[8491]\ttrain-rmse:1.706134\n",
      "[8492]\ttrain-rmse:1.706119\n",
      "[8493]\ttrain-rmse:1.706107\n",
      "[8494]\ttrain-rmse:1.706099\n",
      "[8495]\ttrain-rmse:1.706081\n",
      "[8496]\ttrain-rmse:1.706067\n",
      "[8497]\ttrain-rmse:1.706054\n",
      "[8498]\ttrain-rmse:1.706044\n",
      "[8499]\ttrain-rmse:1.706027\n",
      "[8500]\ttrain-rmse:1.706007\n",
      "[8501]\ttrain-rmse:1.705996\n",
      "[8502]\ttrain-rmse:1.705984\n",
      "[8503]\ttrain-rmse:1.705972\n",
      "[8504]\ttrain-rmse:1.705959\n",
      "[8505]\ttrain-rmse:1.705944\n",
      "[8506]\ttrain-rmse:1.705921\n",
      "[8507]\ttrain-rmse:1.705910\n",
      "[8508]\ttrain-rmse:1.705895\n",
      "[8509]\ttrain-rmse:1.705886\n",
      "[8510]\ttrain-rmse:1.705871\n",
      "[8511]\ttrain-rmse:1.705857\n",
      "[8512]\ttrain-rmse:1.705846\n",
      "[8513]\ttrain-rmse:1.705833\n",
      "[8514]\ttrain-rmse:1.705822\n",
      "[8515]\ttrain-rmse:1.705805\n",
      "[8516]\ttrain-rmse:1.705793\n",
      "[8517]\ttrain-rmse:1.705778\n",
      "[8518]\ttrain-rmse:1.705760\n",
      "[8519]\ttrain-rmse:1.705745\n",
      "[8520]\ttrain-rmse:1.705727\n",
      "[8521]\ttrain-rmse:1.705710\n",
      "[8522]\ttrain-rmse:1.705692\n",
      "[8523]\ttrain-rmse:1.705676\n",
      "[8524]\ttrain-rmse:1.705663\n",
      "[8525]\ttrain-rmse:1.705648\n",
      "[8526]\ttrain-rmse:1.705630\n",
      "[8527]\ttrain-rmse:1.705618\n",
      "[8528]\ttrain-rmse:1.705611\n",
      "[8529]\ttrain-rmse:1.705590\n",
      "[8530]\ttrain-rmse:1.705577\n",
      "[8531]\ttrain-rmse:1.705564\n",
      "[8532]\ttrain-rmse:1.705551\n",
      "[8533]\ttrain-rmse:1.705541\n",
      "[8534]\ttrain-rmse:1.705530\n",
      "[8535]\ttrain-rmse:1.705517\n",
      "[8536]\ttrain-rmse:1.705503\n",
      "[8537]\ttrain-rmse:1.705496\n",
      "[8538]\ttrain-rmse:1.705490\n",
      "[8539]\ttrain-rmse:1.705473\n",
      "[8540]\ttrain-rmse:1.705457\n",
      "[8541]\ttrain-rmse:1.705445\n",
      "[8542]\ttrain-rmse:1.705429\n",
      "[8543]\ttrain-rmse:1.705408\n",
      "[8544]\ttrain-rmse:1.705395\n",
      "[8545]\ttrain-rmse:1.705372\n",
      "[8546]\ttrain-rmse:1.705362\n",
      "[8547]\ttrain-rmse:1.705346\n",
      "[8548]\ttrain-rmse:1.705336\n",
      "[8549]\ttrain-rmse:1.705329\n",
      "[8550]\ttrain-rmse:1.705314\n",
      "[8551]\ttrain-rmse:1.705305\n",
      "[8552]\ttrain-rmse:1.705292\n",
      "[8553]\ttrain-rmse:1.705274\n",
      "[8554]\ttrain-rmse:1.705264\n",
      "[8555]\ttrain-rmse:1.705253\n",
      "[8556]\ttrain-rmse:1.705245\n",
      "[8557]\ttrain-rmse:1.705231\n",
      "[8558]\ttrain-rmse:1.705219\n",
      "[8559]\ttrain-rmse:1.705212\n",
      "[8560]\ttrain-rmse:1.705204\n",
      "[8561]\ttrain-rmse:1.705187\n",
      "[8562]\ttrain-rmse:1.705177\n",
      "[8563]\ttrain-rmse:1.705162\n",
      "[8564]\ttrain-rmse:1.705145\n",
      "[8565]\ttrain-rmse:1.705137\n",
      "[8566]\ttrain-rmse:1.705124\n",
      "[8567]\ttrain-rmse:1.705114\n",
      "[8568]\ttrain-rmse:1.705103\n",
      "[8569]\ttrain-rmse:1.705092\n",
      "[8570]\ttrain-rmse:1.705076\n",
      "[8571]\ttrain-rmse:1.705058\n",
      "[8572]\ttrain-rmse:1.705045\n",
      "[8573]\ttrain-rmse:1.705031\n",
      "[8574]\ttrain-rmse:1.705019\n",
      "[8575]\ttrain-rmse:1.705009\n",
      "[8576]\ttrain-rmse:1.704993\n",
      "[8577]\ttrain-rmse:1.704981\n",
      "[8578]\ttrain-rmse:1.704972\n",
      "[8579]\ttrain-rmse:1.704958\n",
      "[8580]\ttrain-rmse:1.704940\n",
      "[8581]\ttrain-rmse:1.704931\n",
      "[8582]\ttrain-rmse:1.704915\n",
      "[8583]\ttrain-rmse:1.704904\n",
      "[8584]\ttrain-rmse:1.704891\n",
      "[8585]\ttrain-rmse:1.704876\n",
      "[8586]\ttrain-rmse:1.704861\n",
      "[8587]\ttrain-rmse:1.704844\n",
      "[8588]\ttrain-rmse:1.704826\n",
      "[8589]\ttrain-rmse:1.704813\n",
      "[8590]\ttrain-rmse:1.704798\n",
      "[8591]\ttrain-rmse:1.704786\n",
      "[8592]\ttrain-rmse:1.704773\n",
      "[8593]\ttrain-rmse:1.704765\n",
      "[8594]\ttrain-rmse:1.704751\n",
      "[8595]\ttrain-rmse:1.704744\n",
      "[8596]\ttrain-rmse:1.704731\n",
      "[8597]\ttrain-rmse:1.704714\n",
      "[8598]\ttrain-rmse:1.704698\n",
      "[8599]\ttrain-rmse:1.704685\n",
      "[8600]\ttrain-rmse:1.704664\n",
      "[8601]\ttrain-rmse:1.704655\n",
      "[8602]\ttrain-rmse:1.704645\n",
      "[8603]\ttrain-rmse:1.704628\n",
      "[8604]\ttrain-rmse:1.704616\n",
      "[8605]\ttrain-rmse:1.704609\n",
      "[8606]\ttrain-rmse:1.704598\n",
      "[8607]\ttrain-rmse:1.704589\n",
      "[8608]\ttrain-rmse:1.704576\n",
      "[8609]\ttrain-rmse:1.704555\n",
      "[8610]\ttrain-rmse:1.704542\n",
      "[8611]\ttrain-rmse:1.704526\n",
      "[8612]\ttrain-rmse:1.704515\n",
      "[8613]\ttrain-rmse:1.704499\n",
      "[8614]\ttrain-rmse:1.704485\n",
      "[8615]\ttrain-rmse:1.704470\n",
      "[8616]\ttrain-rmse:1.704455\n",
      "[8617]\ttrain-rmse:1.704443\n",
      "[8618]\ttrain-rmse:1.704430\n",
      "[8619]\ttrain-rmse:1.704413\n",
      "[8620]\ttrain-rmse:1.704391\n",
      "[8621]\ttrain-rmse:1.704376\n",
      "[8622]\ttrain-rmse:1.704365\n",
      "[8623]\ttrain-rmse:1.704352\n",
      "[8624]\ttrain-rmse:1.704337\n",
      "[8625]\ttrain-rmse:1.704322\n",
      "[8626]\ttrain-rmse:1.704306\n",
      "[8627]\ttrain-rmse:1.704300\n",
      "[8628]\ttrain-rmse:1.704288\n",
      "[8629]\ttrain-rmse:1.704278\n",
      "[8630]\ttrain-rmse:1.704263\n",
      "[8631]\ttrain-rmse:1.704252\n",
      "[8632]\ttrain-rmse:1.704237\n",
      "[8633]\ttrain-rmse:1.704223\n",
      "[8634]\ttrain-rmse:1.704211\n",
      "[8635]\ttrain-rmse:1.704198\n",
      "[8636]\ttrain-rmse:1.704191\n",
      "[8637]\ttrain-rmse:1.704177\n",
      "[8638]\ttrain-rmse:1.704168\n",
      "[8639]\ttrain-rmse:1.704153\n",
      "[8640]\ttrain-rmse:1.704137\n",
      "[8641]\ttrain-rmse:1.704126\n",
      "[8642]\ttrain-rmse:1.704118\n",
      "[8643]\ttrain-rmse:1.704107\n",
      "[8644]\ttrain-rmse:1.704095\n",
      "[8645]\ttrain-rmse:1.704084\n",
      "[8646]\ttrain-rmse:1.704071\n",
      "[8647]\ttrain-rmse:1.704055\n",
      "[8648]\ttrain-rmse:1.704043\n",
      "[8649]\ttrain-rmse:1.704029\n",
      "[8650]\ttrain-rmse:1.704017\n",
      "[8651]\ttrain-rmse:1.704000\n",
      "[8652]\ttrain-rmse:1.703986\n",
      "[8653]\ttrain-rmse:1.703974\n",
      "[8654]\ttrain-rmse:1.703962\n",
      "[8655]\ttrain-rmse:1.703947\n",
      "[8656]\ttrain-rmse:1.703935\n",
      "[8657]\ttrain-rmse:1.703926\n",
      "[8658]\ttrain-rmse:1.703916\n",
      "[8659]\ttrain-rmse:1.703897\n",
      "[8660]\ttrain-rmse:1.703882\n",
      "[8661]\ttrain-rmse:1.703868\n",
      "[8662]\ttrain-rmse:1.703851\n",
      "[8663]\ttrain-rmse:1.703834\n",
      "[8664]\ttrain-rmse:1.703817\n",
      "[8665]\ttrain-rmse:1.703801\n",
      "[8666]\ttrain-rmse:1.703788\n",
      "[8667]\ttrain-rmse:1.703774\n",
      "[8668]\ttrain-rmse:1.703767\n",
      "[8669]\ttrain-rmse:1.703752\n",
      "[8670]\ttrain-rmse:1.703737\n",
      "[8671]\ttrain-rmse:1.703722\n",
      "[8672]\ttrain-rmse:1.703709\n",
      "[8673]\ttrain-rmse:1.703698\n",
      "[8674]\ttrain-rmse:1.703686\n",
      "[8675]\ttrain-rmse:1.703678\n",
      "[8676]\ttrain-rmse:1.703665\n",
      "[8677]\ttrain-rmse:1.703653\n",
      "[8678]\ttrain-rmse:1.703636\n",
      "[8679]\ttrain-rmse:1.703622\n",
      "[8680]\ttrain-rmse:1.703609\n",
      "[8681]\ttrain-rmse:1.703600\n",
      "[8682]\ttrain-rmse:1.703589\n",
      "[8683]\ttrain-rmse:1.703576\n",
      "[8684]\ttrain-rmse:1.703567\n",
      "[8685]\ttrain-rmse:1.703555\n",
      "[8686]\ttrain-rmse:1.703539\n",
      "[8687]\ttrain-rmse:1.703523\n",
      "[8688]\ttrain-rmse:1.703511\n",
      "[8689]\ttrain-rmse:1.703497\n",
      "[8690]\ttrain-rmse:1.703482\n",
      "[8691]\ttrain-rmse:1.703470\n",
      "[8692]\ttrain-rmse:1.703457\n",
      "[8693]\ttrain-rmse:1.703444\n",
      "[8694]\ttrain-rmse:1.703436\n",
      "[8695]\ttrain-rmse:1.703419\n",
      "[8696]\ttrain-rmse:1.703402\n",
      "[8697]\ttrain-rmse:1.703385\n",
      "[8698]\ttrain-rmse:1.703369\n",
      "[8699]\ttrain-rmse:1.703352\n",
      "[8700]\ttrain-rmse:1.703334\n",
      "[8701]\ttrain-rmse:1.703323\n",
      "[8702]\ttrain-rmse:1.703312\n",
      "[8703]\ttrain-rmse:1.703301\n",
      "[8704]\ttrain-rmse:1.703287\n",
      "[8705]\ttrain-rmse:1.703274\n",
      "[8706]\ttrain-rmse:1.703260\n",
      "[8707]\ttrain-rmse:1.703252\n",
      "[8708]\ttrain-rmse:1.703239\n",
      "[8709]\ttrain-rmse:1.703222\n",
      "[8710]\ttrain-rmse:1.703204\n",
      "[8711]\ttrain-rmse:1.703186\n",
      "[8712]\ttrain-rmse:1.703176\n",
      "[8713]\ttrain-rmse:1.703166\n",
      "[8714]\ttrain-rmse:1.703154\n",
      "[8715]\ttrain-rmse:1.703142\n",
      "[8716]\ttrain-rmse:1.703127\n",
      "[8717]\ttrain-rmse:1.703112\n",
      "[8718]\ttrain-rmse:1.703099\n",
      "[8719]\ttrain-rmse:1.703086\n",
      "[8720]\ttrain-rmse:1.703074\n",
      "[8721]\ttrain-rmse:1.703056\n",
      "[8722]\ttrain-rmse:1.703041\n",
      "[8723]\ttrain-rmse:1.703024\n",
      "[8724]\ttrain-rmse:1.703009\n",
      "[8725]\ttrain-rmse:1.702984\n",
      "[8726]\ttrain-rmse:1.702966\n",
      "[8727]\ttrain-rmse:1.702956\n",
      "[8728]\ttrain-rmse:1.702942\n",
      "[8729]\ttrain-rmse:1.702929\n",
      "[8730]\ttrain-rmse:1.702917\n",
      "[8731]\ttrain-rmse:1.702906\n",
      "[8732]\ttrain-rmse:1.702893\n",
      "[8733]\ttrain-rmse:1.702879\n",
      "[8734]\ttrain-rmse:1.702866\n",
      "[8735]\ttrain-rmse:1.702854\n",
      "[8736]\ttrain-rmse:1.702842\n",
      "[8737]\ttrain-rmse:1.702833\n",
      "[8738]\ttrain-rmse:1.702819\n",
      "[8739]\ttrain-rmse:1.702809\n",
      "[8740]\ttrain-rmse:1.702796\n",
      "[8741]\ttrain-rmse:1.702778\n",
      "[8742]\ttrain-rmse:1.702760\n",
      "[8743]\ttrain-rmse:1.702748\n",
      "[8744]\ttrain-rmse:1.702732\n",
      "[8745]\ttrain-rmse:1.702720\n",
      "[8746]\ttrain-rmse:1.702709\n",
      "[8747]\ttrain-rmse:1.702700\n",
      "[8748]\ttrain-rmse:1.702688\n",
      "[8749]\ttrain-rmse:1.702676\n",
      "[8750]\ttrain-rmse:1.702663\n",
      "[8751]\ttrain-rmse:1.702639\n",
      "[8752]\ttrain-rmse:1.702626\n",
      "[8753]\ttrain-rmse:1.702615\n",
      "[8754]\ttrain-rmse:1.702600\n",
      "[8755]\ttrain-rmse:1.702575\n",
      "[8756]\ttrain-rmse:1.702566\n",
      "[8757]\ttrain-rmse:1.702552\n",
      "[8758]\ttrain-rmse:1.702535\n",
      "[8759]\ttrain-rmse:1.702524\n",
      "[8760]\ttrain-rmse:1.702513\n",
      "[8761]\ttrain-rmse:1.702500\n",
      "[8762]\ttrain-rmse:1.702490\n",
      "[8763]\ttrain-rmse:1.702475\n",
      "[8764]\ttrain-rmse:1.702466\n",
      "[8765]\ttrain-rmse:1.702457\n",
      "[8766]\ttrain-rmse:1.702440\n",
      "[8767]\ttrain-rmse:1.702431\n",
      "[8768]\ttrain-rmse:1.702416\n",
      "[8769]\ttrain-rmse:1.702404\n",
      "[8770]\ttrain-rmse:1.702387\n",
      "[8771]\ttrain-rmse:1.702378\n",
      "[8772]\ttrain-rmse:1.702366\n",
      "[8773]\ttrain-rmse:1.702351\n",
      "[8774]\ttrain-rmse:1.702339\n",
      "[8775]\ttrain-rmse:1.702328\n",
      "[8776]\ttrain-rmse:1.702318\n",
      "[8777]\ttrain-rmse:1.702305\n",
      "[8778]\ttrain-rmse:1.702291\n",
      "[8779]\ttrain-rmse:1.702273\n",
      "[8780]\ttrain-rmse:1.702263\n",
      "[8781]\ttrain-rmse:1.702250\n",
      "[8782]\ttrain-rmse:1.702234\n",
      "[8783]\ttrain-rmse:1.702225\n",
      "[8784]\ttrain-rmse:1.702205\n",
      "[8785]\ttrain-rmse:1.702187\n",
      "[8786]\ttrain-rmse:1.702171\n",
      "[8787]\ttrain-rmse:1.702161\n",
      "[8788]\ttrain-rmse:1.702153\n",
      "[8789]\ttrain-rmse:1.702139\n",
      "[8790]\ttrain-rmse:1.702122\n",
      "[8791]\ttrain-rmse:1.702111\n",
      "[8792]\ttrain-rmse:1.702096\n",
      "[8793]\ttrain-rmse:1.702086\n",
      "[8794]\ttrain-rmse:1.702077\n",
      "[8795]\ttrain-rmse:1.702068\n",
      "[8796]\ttrain-rmse:1.702057\n",
      "[8797]\ttrain-rmse:1.702049\n",
      "[8798]\ttrain-rmse:1.702030\n",
      "[8799]\ttrain-rmse:1.702015\n",
      "[8800]\ttrain-rmse:1.702008\n",
      "[8801]\ttrain-rmse:1.701995\n",
      "[8802]\ttrain-rmse:1.701980\n",
      "[8803]\ttrain-rmse:1.701967\n",
      "[8804]\ttrain-rmse:1.701958\n",
      "[8805]\ttrain-rmse:1.701952\n",
      "[8806]\ttrain-rmse:1.701937\n",
      "[8807]\ttrain-rmse:1.701920\n",
      "[8808]\ttrain-rmse:1.701910\n",
      "[8809]\ttrain-rmse:1.701898\n",
      "[8810]\ttrain-rmse:1.701889\n",
      "[8811]\ttrain-rmse:1.701876\n",
      "[8812]\ttrain-rmse:1.701861\n",
      "[8813]\ttrain-rmse:1.701849\n",
      "[8814]\ttrain-rmse:1.701838\n",
      "[8815]\ttrain-rmse:1.701829\n",
      "[8816]\ttrain-rmse:1.701820\n",
      "[8817]\ttrain-rmse:1.701808\n",
      "[8818]\ttrain-rmse:1.701798\n",
      "[8819]\ttrain-rmse:1.701783\n",
      "[8820]\ttrain-rmse:1.701775\n",
      "[8821]\ttrain-rmse:1.701761\n",
      "[8822]\ttrain-rmse:1.701747\n",
      "[8823]\ttrain-rmse:1.701736\n",
      "[8824]\ttrain-rmse:1.701724\n",
      "[8825]\ttrain-rmse:1.701712\n",
      "[8826]\ttrain-rmse:1.701701\n",
      "[8827]\ttrain-rmse:1.701684\n",
      "[8828]\ttrain-rmse:1.701670\n",
      "[8829]\ttrain-rmse:1.701662\n",
      "[8830]\ttrain-rmse:1.701642\n",
      "[8831]\ttrain-rmse:1.701631\n",
      "[8832]\ttrain-rmse:1.701616\n",
      "[8833]\ttrain-rmse:1.701603\n",
      "[8834]\ttrain-rmse:1.701591\n",
      "[8835]\ttrain-rmse:1.701576\n",
      "[8836]\ttrain-rmse:1.701560\n",
      "[8837]\ttrain-rmse:1.701543\n",
      "[8838]\ttrain-rmse:1.701527\n",
      "[8839]\ttrain-rmse:1.701516\n",
      "[8840]\ttrain-rmse:1.701503\n",
      "[8841]\ttrain-rmse:1.701493\n",
      "[8842]\ttrain-rmse:1.701483\n",
      "[8843]\ttrain-rmse:1.701466\n",
      "[8844]\ttrain-rmse:1.701455\n",
      "[8845]\ttrain-rmse:1.701443\n",
      "[8846]\ttrain-rmse:1.701429\n",
      "[8847]\ttrain-rmse:1.701408\n",
      "[8848]\ttrain-rmse:1.701390\n",
      "[8849]\ttrain-rmse:1.701378\n",
      "[8850]\ttrain-rmse:1.701370\n",
      "[8851]\ttrain-rmse:1.701350\n",
      "[8852]\ttrain-rmse:1.701339\n",
      "[8853]\ttrain-rmse:1.701328\n",
      "[8854]\ttrain-rmse:1.701312\n",
      "[8855]\ttrain-rmse:1.701304\n",
      "[8856]\ttrain-rmse:1.701290\n",
      "[8857]\ttrain-rmse:1.701270\n",
      "[8858]\ttrain-rmse:1.701255\n",
      "[8859]\ttrain-rmse:1.701245\n",
      "[8860]\ttrain-rmse:1.701233\n",
      "[8861]\ttrain-rmse:1.701220\n",
      "[8862]\ttrain-rmse:1.701210\n",
      "[8863]\ttrain-rmse:1.701201\n",
      "[8864]\ttrain-rmse:1.701187\n",
      "[8865]\ttrain-rmse:1.701169\n",
      "[8866]\ttrain-rmse:1.701154\n",
      "[8867]\ttrain-rmse:1.701147\n",
      "[8868]\ttrain-rmse:1.701132\n",
      "[8869]\ttrain-rmse:1.701122\n",
      "[8870]\ttrain-rmse:1.701113\n",
      "[8871]\ttrain-rmse:1.701094\n",
      "[8872]\ttrain-rmse:1.701085\n",
      "[8873]\ttrain-rmse:1.701074\n",
      "[8874]\ttrain-rmse:1.701060\n",
      "[8875]\ttrain-rmse:1.701049\n",
      "[8876]\ttrain-rmse:1.701037\n",
      "[8877]\ttrain-rmse:1.701029\n",
      "[8878]\ttrain-rmse:1.701013\n",
      "[8879]\ttrain-rmse:1.700999\n",
      "[8880]\ttrain-rmse:1.700986\n",
      "[8881]\ttrain-rmse:1.700976\n",
      "[8882]\ttrain-rmse:1.700963\n",
      "[8883]\ttrain-rmse:1.700952\n",
      "[8884]\ttrain-rmse:1.700938\n",
      "[8885]\ttrain-rmse:1.700920\n",
      "[8886]\ttrain-rmse:1.700909\n",
      "[8887]\ttrain-rmse:1.700894\n",
      "[8888]\ttrain-rmse:1.700883\n",
      "[8889]\ttrain-rmse:1.700874\n",
      "[8890]\ttrain-rmse:1.700866\n",
      "[8891]\ttrain-rmse:1.700850\n",
      "[8892]\ttrain-rmse:1.700838\n",
      "[8893]\ttrain-rmse:1.700825\n",
      "[8894]\ttrain-rmse:1.700815\n",
      "[8895]\ttrain-rmse:1.700798\n",
      "[8896]\ttrain-rmse:1.700783\n",
      "[8897]\ttrain-rmse:1.700769\n",
      "[8898]\ttrain-rmse:1.700750\n",
      "[8899]\ttrain-rmse:1.700738\n",
      "[8900]\ttrain-rmse:1.700720\n",
      "[8901]\ttrain-rmse:1.700714\n",
      "[8902]\ttrain-rmse:1.700700\n",
      "[8903]\ttrain-rmse:1.700688\n",
      "[8904]\ttrain-rmse:1.700675\n",
      "[8905]\ttrain-rmse:1.700661\n",
      "[8906]\ttrain-rmse:1.700643\n",
      "[8907]\ttrain-rmse:1.700630\n",
      "[8908]\ttrain-rmse:1.700620\n",
      "[8909]\ttrain-rmse:1.700604\n",
      "[8910]\ttrain-rmse:1.700595\n",
      "[8911]\ttrain-rmse:1.700584\n",
      "[8912]\ttrain-rmse:1.700575\n",
      "[8913]\ttrain-rmse:1.700567\n",
      "[8914]\ttrain-rmse:1.700550\n",
      "[8915]\ttrain-rmse:1.700540\n",
      "[8916]\ttrain-rmse:1.700534\n",
      "[8917]\ttrain-rmse:1.700524\n",
      "[8918]\ttrain-rmse:1.700505\n",
      "[8919]\ttrain-rmse:1.700491\n",
      "[8920]\ttrain-rmse:1.700486\n",
      "[8921]\ttrain-rmse:1.700475\n",
      "[8922]\ttrain-rmse:1.700464\n",
      "[8923]\ttrain-rmse:1.700443\n",
      "[8924]\ttrain-rmse:1.700428\n",
      "[8925]\ttrain-rmse:1.700417\n",
      "[8926]\ttrain-rmse:1.700408\n",
      "[8927]\ttrain-rmse:1.700395\n",
      "[8928]\ttrain-rmse:1.700386\n",
      "[8929]\ttrain-rmse:1.700374\n",
      "[8930]\ttrain-rmse:1.700364\n",
      "[8931]\ttrain-rmse:1.700353\n",
      "[8932]\ttrain-rmse:1.700335\n",
      "[8933]\ttrain-rmse:1.700320\n",
      "[8934]\ttrain-rmse:1.700310\n",
      "[8935]\ttrain-rmse:1.700297\n",
      "[8936]\ttrain-rmse:1.700283\n",
      "[8937]\ttrain-rmse:1.700276\n",
      "[8938]\ttrain-rmse:1.700258\n",
      "[8939]\ttrain-rmse:1.700249\n",
      "[8940]\ttrain-rmse:1.700235\n",
      "[8941]\ttrain-rmse:1.700218\n",
      "[8942]\ttrain-rmse:1.700210\n",
      "[8943]\ttrain-rmse:1.700197\n",
      "[8944]\ttrain-rmse:1.700187\n",
      "[8945]\ttrain-rmse:1.700181\n",
      "[8946]\ttrain-rmse:1.700169\n",
      "[8947]\ttrain-rmse:1.700154\n",
      "[8948]\ttrain-rmse:1.700144\n",
      "[8949]\ttrain-rmse:1.700128\n",
      "[8950]\ttrain-rmse:1.700119\n",
      "[8951]\ttrain-rmse:1.700108\n",
      "[8952]\ttrain-rmse:1.700089\n",
      "[8953]\ttrain-rmse:1.700077\n",
      "[8954]\ttrain-rmse:1.700064\n",
      "[8955]\ttrain-rmse:1.700054\n",
      "[8956]\ttrain-rmse:1.700046\n",
      "[8957]\ttrain-rmse:1.700032\n",
      "[8958]\ttrain-rmse:1.700020\n",
      "[8959]\ttrain-rmse:1.700008\n",
      "[8960]\ttrain-rmse:1.699998\n",
      "[8961]\ttrain-rmse:1.699989\n",
      "[8962]\ttrain-rmse:1.699982\n",
      "[8963]\ttrain-rmse:1.699968\n",
      "[8964]\ttrain-rmse:1.699950\n",
      "[8965]\ttrain-rmse:1.699931\n",
      "[8966]\ttrain-rmse:1.699918\n",
      "[8967]\ttrain-rmse:1.699902\n",
      "[8968]\ttrain-rmse:1.699884\n",
      "[8969]\ttrain-rmse:1.699869\n",
      "[8970]\ttrain-rmse:1.699860\n",
      "[8971]\ttrain-rmse:1.699848\n",
      "[8972]\ttrain-rmse:1.699836\n",
      "[8973]\ttrain-rmse:1.699830\n",
      "[8974]\ttrain-rmse:1.699813\n",
      "[8975]\ttrain-rmse:1.699798\n",
      "[8976]\ttrain-rmse:1.699785\n",
      "[8977]\ttrain-rmse:1.699768\n",
      "[8978]\ttrain-rmse:1.699756\n",
      "[8979]\ttrain-rmse:1.699740\n",
      "[8980]\ttrain-rmse:1.699723\n",
      "[8981]\ttrain-rmse:1.699714\n",
      "[8982]\ttrain-rmse:1.699705\n",
      "[8983]\ttrain-rmse:1.699690\n",
      "[8984]\ttrain-rmse:1.699681\n",
      "[8985]\ttrain-rmse:1.699665\n",
      "[8986]\ttrain-rmse:1.699654\n",
      "[8987]\ttrain-rmse:1.699641\n",
      "[8988]\ttrain-rmse:1.699633\n",
      "[8989]\ttrain-rmse:1.699619\n",
      "[8990]\ttrain-rmse:1.699604\n",
      "[8991]\ttrain-rmse:1.699589\n",
      "[8992]\ttrain-rmse:1.699580\n",
      "[8993]\ttrain-rmse:1.699566\n",
      "[8994]\ttrain-rmse:1.699553\n",
      "[8995]\ttrain-rmse:1.699540\n",
      "[8996]\ttrain-rmse:1.699527\n",
      "[8997]\ttrain-rmse:1.699514\n",
      "[8998]\ttrain-rmse:1.699507\n",
      "[8999]\ttrain-rmse:1.699489\n",
      "[9000]\ttrain-rmse:1.699481\n",
      "[9001]\ttrain-rmse:1.699467\n",
      "[9002]\ttrain-rmse:1.699460\n",
      "[9003]\ttrain-rmse:1.699444\n",
      "[9004]\ttrain-rmse:1.699433\n",
      "[9005]\ttrain-rmse:1.699420\n",
      "[9006]\ttrain-rmse:1.699410\n",
      "[9007]\ttrain-rmse:1.699396\n",
      "[9008]\ttrain-rmse:1.699386\n",
      "[9009]\ttrain-rmse:1.699374\n",
      "[9010]\ttrain-rmse:1.699363\n",
      "[9011]\ttrain-rmse:1.699351\n",
      "[9012]\ttrain-rmse:1.699339\n",
      "[9013]\ttrain-rmse:1.699329\n",
      "[9014]\ttrain-rmse:1.699318\n",
      "[9015]\ttrain-rmse:1.699303\n",
      "[9016]\ttrain-rmse:1.699289\n",
      "[9017]\ttrain-rmse:1.699275\n",
      "[9018]\ttrain-rmse:1.699261\n",
      "[9019]\ttrain-rmse:1.699252\n",
      "[9020]\ttrain-rmse:1.699237\n",
      "[9021]\ttrain-rmse:1.699228\n",
      "[9022]\ttrain-rmse:1.699215\n",
      "[9023]\ttrain-rmse:1.699201\n",
      "[9024]\ttrain-rmse:1.699187\n",
      "[9025]\ttrain-rmse:1.699171\n",
      "[9026]\ttrain-rmse:1.699160\n",
      "[9027]\ttrain-rmse:1.699145\n",
      "[9028]\ttrain-rmse:1.699138\n",
      "[9029]\ttrain-rmse:1.699124\n",
      "[9030]\ttrain-rmse:1.699115\n",
      "[9031]\ttrain-rmse:1.699101\n",
      "[9032]\ttrain-rmse:1.699088\n",
      "[9033]\ttrain-rmse:1.699077\n",
      "[9034]\ttrain-rmse:1.699062\n",
      "[9035]\ttrain-rmse:1.699054\n",
      "[9036]\ttrain-rmse:1.699039\n",
      "[9037]\ttrain-rmse:1.699032\n",
      "[9038]\ttrain-rmse:1.699013\n",
      "[9039]\ttrain-rmse:1.698999\n",
      "[9040]\ttrain-rmse:1.698987\n",
      "[9041]\ttrain-rmse:1.698974\n",
      "[9042]\ttrain-rmse:1.698960\n",
      "[9043]\ttrain-rmse:1.698949\n",
      "[9044]\ttrain-rmse:1.698935\n",
      "[9045]\ttrain-rmse:1.698915\n",
      "[9046]\ttrain-rmse:1.698902\n",
      "[9047]\ttrain-rmse:1.698893\n",
      "[9048]\ttrain-rmse:1.698882\n",
      "[9049]\ttrain-rmse:1.698865\n",
      "[9050]\ttrain-rmse:1.698850\n",
      "[9051]\ttrain-rmse:1.698841\n",
      "[9052]\ttrain-rmse:1.698830\n",
      "[9053]\ttrain-rmse:1.698820\n",
      "[9054]\ttrain-rmse:1.698806\n",
      "[9055]\ttrain-rmse:1.698795\n",
      "[9056]\ttrain-rmse:1.698788\n",
      "[9057]\ttrain-rmse:1.698767\n",
      "[9058]\ttrain-rmse:1.698750\n",
      "[9059]\ttrain-rmse:1.698743\n",
      "[9060]\ttrain-rmse:1.698729\n",
      "[9061]\ttrain-rmse:1.698714\n",
      "[9062]\ttrain-rmse:1.698700\n",
      "[9063]\ttrain-rmse:1.698686\n",
      "[9064]\ttrain-rmse:1.698676\n",
      "[9065]\ttrain-rmse:1.698659\n",
      "[9066]\ttrain-rmse:1.698652\n",
      "[9067]\ttrain-rmse:1.698632\n",
      "[9068]\ttrain-rmse:1.698625\n",
      "[9069]\ttrain-rmse:1.698608\n",
      "[9070]\ttrain-rmse:1.698596\n",
      "[9071]\ttrain-rmse:1.698583\n",
      "[9072]\ttrain-rmse:1.698567\n",
      "[9073]\ttrain-rmse:1.698552\n",
      "[9074]\ttrain-rmse:1.698537\n",
      "[9075]\ttrain-rmse:1.698524\n",
      "[9076]\ttrain-rmse:1.698510\n",
      "[9077]\ttrain-rmse:1.698497\n",
      "[9078]\ttrain-rmse:1.698478\n",
      "[9079]\ttrain-rmse:1.698465\n",
      "[9080]\ttrain-rmse:1.698454\n",
      "[9081]\ttrain-rmse:1.698431\n",
      "[9082]\ttrain-rmse:1.698411\n",
      "[9083]\ttrain-rmse:1.698403\n",
      "[9084]\ttrain-rmse:1.698392\n",
      "[9085]\ttrain-rmse:1.698384\n",
      "[9086]\ttrain-rmse:1.698371\n",
      "[9087]\ttrain-rmse:1.698364\n",
      "[9088]\ttrain-rmse:1.698347\n",
      "[9089]\ttrain-rmse:1.698338\n",
      "[9090]\ttrain-rmse:1.698330\n",
      "[9091]\ttrain-rmse:1.698313\n",
      "[9092]\ttrain-rmse:1.698302\n",
      "[9093]\ttrain-rmse:1.698289\n",
      "[9094]\ttrain-rmse:1.698269\n",
      "[9095]\ttrain-rmse:1.698261\n",
      "[9096]\ttrain-rmse:1.698249\n",
      "[9097]\ttrain-rmse:1.698238\n",
      "[9098]\ttrain-rmse:1.698227\n",
      "[9099]\ttrain-rmse:1.698207\n",
      "[9100]\ttrain-rmse:1.698199\n",
      "[9101]\ttrain-rmse:1.698189\n",
      "[9102]\ttrain-rmse:1.698176\n",
      "[9103]\ttrain-rmse:1.698167\n",
      "[9104]\ttrain-rmse:1.698149\n",
      "[9105]\ttrain-rmse:1.698141\n",
      "[9106]\ttrain-rmse:1.698126\n",
      "[9107]\ttrain-rmse:1.698111\n",
      "[9108]\ttrain-rmse:1.698098\n",
      "[9109]\ttrain-rmse:1.698091\n",
      "[9110]\ttrain-rmse:1.698083\n",
      "[9111]\ttrain-rmse:1.698073\n",
      "[9112]\ttrain-rmse:1.698056\n",
      "[9113]\ttrain-rmse:1.698041\n",
      "[9114]\ttrain-rmse:1.698032\n",
      "[9115]\ttrain-rmse:1.698024\n",
      "[9116]\ttrain-rmse:1.698016\n",
      "[9117]\ttrain-rmse:1.698009\n",
      "[9118]\ttrain-rmse:1.697992\n",
      "[9119]\ttrain-rmse:1.697972\n",
      "[9120]\ttrain-rmse:1.697960\n",
      "[9121]\ttrain-rmse:1.697947\n",
      "[9122]\ttrain-rmse:1.697931\n",
      "[9123]\ttrain-rmse:1.697915\n",
      "[9124]\ttrain-rmse:1.697903\n",
      "[9125]\ttrain-rmse:1.697895\n",
      "[9126]\ttrain-rmse:1.697881\n",
      "[9127]\ttrain-rmse:1.697870\n",
      "[9128]\ttrain-rmse:1.697857\n",
      "[9129]\ttrain-rmse:1.697845\n",
      "[9130]\ttrain-rmse:1.697836\n",
      "[9131]\ttrain-rmse:1.697826\n",
      "[9132]\ttrain-rmse:1.697816\n",
      "[9133]\ttrain-rmse:1.697809\n",
      "[9134]\ttrain-rmse:1.697796\n",
      "[9135]\ttrain-rmse:1.697781\n",
      "[9136]\ttrain-rmse:1.697765\n",
      "[9137]\ttrain-rmse:1.697749\n",
      "[9138]\ttrain-rmse:1.697733\n",
      "[9139]\ttrain-rmse:1.697723\n",
      "[9140]\ttrain-rmse:1.697710\n",
      "[9141]\ttrain-rmse:1.697699\n",
      "[9142]\ttrain-rmse:1.697692\n",
      "[9143]\ttrain-rmse:1.697684\n",
      "[9144]\ttrain-rmse:1.697675\n",
      "[9145]\ttrain-rmse:1.697662\n",
      "[9146]\ttrain-rmse:1.697651\n",
      "[9147]\ttrain-rmse:1.697636\n",
      "[9148]\ttrain-rmse:1.697627\n",
      "[9149]\ttrain-rmse:1.697613\n",
      "[9150]\ttrain-rmse:1.697598\n",
      "[9151]\ttrain-rmse:1.697586\n",
      "[9152]\ttrain-rmse:1.697576\n",
      "[9153]\ttrain-rmse:1.697567\n",
      "[9154]\ttrain-rmse:1.697551\n",
      "[9155]\ttrain-rmse:1.697547\n",
      "[9156]\ttrain-rmse:1.697535\n",
      "[9157]\ttrain-rmse:1.697524\n",
      "[9158]\ttrain-rmse:1.697512\n",
      "[9159]\ttrain-rmse:1.697493\n",
      "[9160]\ttrain-rmse:1.697481\n",
      "[9161]\ttrain-rmse:1.697473\n",
      "[9162]\ttrain-rmse:1.697463\n",
      "[9163]\ttrain-rmse:1.697449\n",
      "[9164]\ttrain-rmse:1.697442\n",
      "[9165]\ttrain-rmse:1.697434\n",
      "[9166]\ttrain-rmse:1.697417\n",
      "[9167]\ttrain-rmse:1.697403\n",
      "[9168]\ttrain-rmse:1.697388\n",
      "[9169]\ttrain-rmse:1.697376\n",
      "[9170]\ttrain-rmse:1.697362\n",
      "[9171]\ttrain-rmse:1.697352\n",
      "[9172]\ttrain-rmse:1.697343\n",
      "[9173]\ttrain-rmse:1.697328\n",
      "[9174]\ttrain-rmse:1.697320\n",
      "[9175]\ttrain-rmse:1.697304\n",
      "[9176]\ttrain-rmse:1.697294\n",
      "[9177]\ttrain-rmse:1.697280\n",
      "[9178]\ttrain-rmse:1.697273\n",
      "[9179]\ttrain-rmse:1.697266\n",
      "[9180]\ttrain-rmse:1.697252\n",
      "[9181]\ttrain-rmse:1.697237\n",
      "[9182]\ttrain-rmse:1.697230\n",
      "[9183]\ttrain-rmse:1.697223\n",
      "[9184]\ttrain-rmse:1.697208\n",
      "[9185]\ttrain-rmse:1.697193\n",
      "[9186]\ttrain-rmse:1.697175\n",
      "[9187]\ttrain-rmse:1.697162\n",
      "[9188]\ttrain-rmse:1.697148\n",
      "[9189]\ttrain-rmse:1.697138\n",
      "[9190]\ttrain-rmse:1.697123\n",
      "[9191]\ttrain-rmse:1.697106\n",
      "[9192]\ttrain-rmse:1.697097\n",
      "[9193]\ttrain-rmse:1.697078\n",
      "[9194]\ttrain-rmse:1.697065\n",
      "[9195]\ttrain-rmse:1.697050\n",
      "[9196]\ttrain-rmse:1.697035\n",
      "[9197]\ttrain-rmse:1.697021\n",
      "[9198]\ttrain-rmse:1.697005\n",
      "[9199]\ttrain-rmse:1.696993\n",
      "[9200]\ttrain-rmse:1.696976\n",
      "[9201]\ttrain-rmse:1.696965\n",
      "[9202]\ttrain-rmse:1.696960\n",
      "[9203]\ttrain-rmse:1.696950\n",
      "[9204]\ttrain-rmse:1.696940\n",
      "[9205]\ttrain-rmse:1.696933\n",
      "[9206]\ttrain-rmse:1.696920\n",
      "[9207]\ttrain-rmse:1.696902\n",
      "[9208]\ttrain-rmse:1.696893\n",
      "[9209]\ttrain-rmse:1.696884\n",
      "[9210]\ttrain-rmse:1.696875\n",
      "[9211]\ttrain-rmse:1.696860\n",
      "[9212]\ttrain-rmse:1.696855\n",
      "[9213]\ttrain-rmse:1.696842\n",
      "[9214]\ttrain-rmse:1.696828\n",
      "[9215]\ttrain-rmse:1.696816\n",
      "[9216]\ttrain-rmse:1.696798\n",
      "[9217]\ttrain-rmse:1.696784\n",
      "[9218]\ttrain-rmse:1.696772\n",
      "[9219]\ttrain-rmse:1.696755\n",
      "[9220]\ttrain-rmse:1.696742\n",
      "[9221]\ttrain-rmse:1.696734\n",
      "[9222]\ttrain-rmse:1.696720\n",
      "[9223]\ttrain-rmse:1.696697\n",
      "[9224]\ttrain-rmse:1.696685\n",
      "[9225]\ttrain-rmse:1.696672\n",
      "[9226]\ttrain-rmse:1.696661\n",
      "[9227]\ttrain-rmse:1.696651\n",
      "[9228]\ttrain-rmse:1.696641\n",
      "[9229]\ttrain-rmse:1.696631\n",
      "[9230]\ttrain-rmse:1.696623\n",
      "[9231]\ttrain-rmse:1.696608\n",
      "[9232]\ttrain-rmse:1.696593\n",
      "[9233]\ttrain-rmse:1.696583\n",
      "[9234]\ttrain-rmse:1.696573\n",
      "[9235]\ttrain-rmse:1.696558\n",
      "[9236]\ttrain-rmse:1.696543\n",
      "[9237]\ttrain-rmse:1.696532\n",
      "[9238]\ttrain-rmse:1.696521\n",
      "[9239]\ttrain-rmse:1.696509\n",
      "[9240]\ttrain-rmse:1.696499\n",
      "[9241]\ttrain-rmse:1.696486\n",
      "[9242]\ttrain-rmse:1.696479\n",
      "[9243]\ttrain-rmse:1.696468\n",
      "[9244]\ttrain-rmse:1.696462\n",
      "[9245]\ttrain-rmse:1.696450\n",
      "[9246]\ttrain-rmse:1.696440\n",
      "[9247]\ttrain-rmse:1.696431\n",
      "[9248]\ttrain-rmse:1.696415\n",
      "[9249]\ttrain-rmse:1.696403\n",
      "[9250]\ttrain-rmse:1.696393\n",
      "[9251]\ttrain-rmse:1.696381\n",
      "[9252]\ttrain-rmse:1.696368\n",
      "[9253]\ttrain-rmse:1.696357\n",
      "[9254]\ttrain-rmse:1.696339\n",
      "[9255]\ttrain-rmse:1.696329\n",
      "[9256]\ttrain-rmse:1.696312\n",
      "[9257]\ttrain-rmse:1.696303\n",
      "[9258]\ttrain-rmse:1.696292\n",
      "[9259]\ttrain-rmse:1.696280\n",
      "[9260]\ttrain-rmse:1.696267\n",
      "[9261]\ttrain-rmse:1.696250\n",
      "[9262]\ttrain-rmse:1.696237\n",
      "[9263]\ttrain-rmse:1.696229\n",
      "[9264]\ttrain-rmse:1.696222\n",
      "[9265]\ttrain-rmse:1.696214\n",
      "[9266]\ttrain-rmse:1.696196\n",
      "[9267]\ttrain-rmse:1.696189\n",
      "[9268]\ttrain-rmse:1.696176\n",
      "[9269]\ttrain-rmse:1.696163\n",
      "[9270]\ttrain-rmse:1.696151\n",
      "[9271]\ttrain-rmse:1.696136\n",
      "[9272]\ttrain-rmse:1.696122\n",
      "[9273]\ttrain-rmse:1.696116\n",
      "[9274]\ttrain-rmse:1.696105\n",
      "[9275]\ttrain-rmse:1.696094\n",
      "[9276]\ttrain-rmse:1.696082\n",
      "[9277]\ttrain-rmse:1.696072\n",
      "[9278]\ttrain-rmse:1.696059\n",
      "[9279]\ttrain-rmse:1.696050\n",
      "[9280]\ttrain-rmse:1.696036\n",
      "[9281]\ttrain-rmse:1.696018\n",
      "[9282]\ttrain-rmse:1.696007\n",
      "[9283]\ttrain-rmse:1.695993\n",
      "[9284]\ttrain-rmse:1.695981\n",
      "[9285]\ttrain-rmse:1.695967\n",
      "[9286]\ttrain-rmse:1.695954\n",
      "[9287]\ttrain-rmse:1.695947\n",
      "[9288]\ttrain-rmse:1.695939\n",
      "[9289]\ttrain-rmse:1.695928\n",
      "[9290]\ttrain-rmse:1.695917\n",
      "[9291]\ttrain-rmse:1.695908\n",
      "[9292]\ttrain-rmse:1.695894\n",
      "[9293]\ttrain-rmse:1.695883\n",
      "[9294]\ttrain-rmse:1.695866\n",
      "[9295]\ttrain-rmse:1.695853\n",
      "[9296]\ttrain-rmse:1.695842\n",
      "[9297]\ttrain-rmse:1.695830\n",
      "[9298]\ttrain-rmse:1.695814\n",
      "[9299]\ttrain-rmse:1.695806\n",
      "[9300]\ttrain-rmse:1.695788\n",
      "[9301]\ttrain-rmse:1.695778\n",
      "[9302]\ttrain-rmse:1.695770\n",
      "[9303]\ttrain-rmse:1.695754\n",
      "[9304]\ttrain-rmse:1.695739\n",
      "[9305]\ttrain-rmse:1.695725\n",
      "[9306]\ttrain-rmse:1.695715\n",
      "[9307]\ttrain-rmse:1.695699\n",
      "[9308]\ttrain-rmse:1.695681\n",
      "[9309]\ttrain-rmse:1.695669\n",
      "[9310]\ttrain-rmse:1.695660\n",
      "[9311]\ttrain-rmse:1.695649\n",
      "[9312]\ttrain-rmse:1.695632\n",
      "[9313]\ttrain-rmse:1.695624\n",
      "[9314]\ttrain-rmse:1.695607\n",
      "[9315]\ttrain-rmse:1.695598\n",
      "[9316]\ttrain-rmse:1.695592\n",
      "[9317]\ttrain-rmse:1.695581\n",
      "[9318]\ttrain-rmse:1.695578\n",
      "[9319]\ttrain-rmse:1.695571\n",
      "[9320]\ttrain-rmse:1.695558\n",
      "[9321]\ttrain-rmse:1.695544\n",
      "[9322]\ttrain-rmse:1.695526\n",
      "[9323]\ttrain-rmse:1.695517\n",
      "[9324]\ttrain-rmse:1.695504\n",
      "[9325]\ttrain-rmse:1.695490\n",
      "[9326]\ttrain-rmse:1.695476\n",
      "[9327]\ttrain-rmse:1.695467\n",
      "[9328]\ttrain-rmse:1.695455\n",
      "[9329]\ttrain-rmse:1.695443\n",
      "[9330]\ttrain-rmse:1.695430\n",
      "[9331]\ttrain-rmse:1.695412\n",
      "[9332]\ttrain-rmse:1.695405\n",
      "[9333]\ttrain-rmse:1.695394\n",
      "[9334]\ttrain-rmse:1.695382\n",
      "[9335]\ttrain-rmse:1.695369\n",
      "[9336]\ttrain-rmse:1.695360\n",
      "[9337]\ttrain-rmse:1.695343\n",
      "[9338]\ttrain-rmse:1.695331\n",
      "[9339]\ttrain-rmse:1.695320\n",
      "[9340]\ttrain-rmse:1.695312\n",
      "[9341]\ttrain-rmse:1.695302\n",
      "[9342]\ttrain-rmse:1.695291\n",
      "[9343]\ttrain-rmse:1.695276\n",
      "[9344]\ttrain-rmse:1.695263\n",
      "[9345]\ttrain-rmse:1.695254\n",
      "[9346]\ttrain-rmse:1.695242\n",
      "[9347]\ttrain-rmse:1.695230\n",
      "[9348]\ttrain-rmse:1.695221\n",
      "[9349]\ttrain-rmse:1.695214\n",
      "[9350]\ttrain-rmse:1.695197\n",
      "[9351]\ttrain-rmse:1.695183\n",
      "[9352]\ttrain-rmse:1.695170\n",
      "[9353]\ttrain-rmse:1.695156\n",
      "[9354]\ttrain-rmse:1.695148\n",
      "[9355]\ttrain-rmse:1.695134\n",
      "[9356]\ttrain-rmse:1.695123\n",
      "[9357]\ttrain-rmse:1.695108\n",
      "[9358]\ttrain-rmse:1.695094\n",
      "[9359]\ttrain-rmse:1.695078\n",
      "[9360]\ttrain-rmse:1.695074\n",
      "[9361]\ttrain-rmse:1.695063\n",
      "[9362]\ttrain-rmse:1.695051\n",
      "[9363]\ttrain-rmse:1.695036\n",
      "[9364]\ttrain-rmse:1.695019\n",
      "[9365]\ttrain-rmse:1.695008\n",
      "[9366]\ttrain-rmse:1.694995\n",
      "[9367]\ttrain-rmse:1.694984\n",
      "[9368]\ttrain-rmse:1.694972\n",
      "[9369]\ttrain-rmse:1.694956\n",
      "[9370]\ttrain-rmse:1.694940\n",
      "[9371]\ttrain-rmse:1.694927\n",
      "[9372]\ttrain-rmse:1.694914\n",
      "[9373]\ttrain-rmse:1.694900\n",
      "[9374]\ttrain-rmse:1.694885\n",
      "[9375]\ttrain-rmse:1.694870\n",
      "[9376]\ttrain-rmse:1.694863\n",
      "[9377]\ttrain-rmse:1.694856\n",
      "[9378]\ttrain-rmse:1.694840\n",
      "[9379]\ttrain-rmse:1.694826\n",
      "[9380]\ttrain-rmse:1.694813\n",
      "[9381]\ttrain-rmse:1.694797\n",
      "[9382]\ttrain-rmse:1.694786\n",
      "[9383]\ttrain-rmse:1.694772\n",
      "[9384]\ttrain-rmse:1.694763\n",
      "[9385]\ttrain-rmse:1.694748\n",
      "[9386]\ttrain-rmse:1.694735\n",
      "[9387]\ttrain-rmse:1.694724\n",
      "[9388]\ttrain-rmse:1.694714\n",
      "[9389]\ttrain-rmse:1.694697\n",
      "[9390]\ttrain-rmse:1.694685\n",
      "[9391]\ttrain-rmse:1.694676\n",
      "[9392]\ttrain-rmse:1.694670\n",
      "[9393]\ttrain-rmse:1.694657\n",
      "[9394]\ttrain-rmse:1.694641\n",
      "[9395]\ttrain-rmse:1.694629\n",
      "[9396]\ttrain-rmse:1.694616\n",
      "[9397]\ttrain-rmse:1.694598\n",
      "[9398]\ttrain-rmse:1.694588\n",
      "[9399]\ttrain-rmse:1.694576\n",
      "[9400]\ttrain-rmse:1.694563\n",
      "[9401]\ttrain-rmse:1.694553\n",
      "[9402]\ttrain-rmse:1.694543\n",
      "[9403]\ttrain-rmse:1.694525\n",
      "[9404]\ttrain-rmse:1.694520\n",
      "[9405]\ttrain-rmse:1.694507\n",
      "[9406]\ttrain-rmse:1.694495\n",
      "[9407]\ttrain-rmse:1.694487\n",
      "[9408]\ttrain-rmse:1.694477\n",
      "[9409]\ttrain-rmse:1.694463\n",
      "[9410]\ttrain-rmse:1.694452\n",
      "[9411]\ttrain-rmse:1.694441\n",
      "[9412]\ttrain-rmse:1.694432\n",
      "[9413]\ttrain-rmse:1.694417\n",
      "[9414]\ttrain-rmse:1.694412\n",
      "[9415]\ttrain-rmse:1.694403\n",
      "[9416]\ttrain-rmse:1.694390\n",
      "[9417]\ttrain-rmse:1.694381\n",
      "[9418]\ttrain-rmse:1.694370\n",
      "[9419]\ttrain-rmse:1.694357\n",
      "[9420]\ttrain-rmse:1.694349\n",
      "[9421]\ttrain-rmse:1.694343\n",
      "[9422]\ttrain-rmse:1.694329\n",
      "[9423]\ttrain-rmse:1.694319\n",
      "[9424]\ttrain-rmse:1.694308\n",
      "[9425]\ttrain-rmse:1.694298\n",
      "[9426]\ttrain-rmse:1.694283\n",
      "[9427]\ttrain-rmse:1.694276\n",
      "[9428]\ttrain-rmse:1.694264\n",
      "[9429]\ttrain-rmse:1.694254\n",
      "[9430]\ttrain-rmse:1.694240\n",
      "[9431]\ttrain-rmse:1.694228\n",
      "[9432]\ttrain-rmse:1.694220\n",
      "[9433]\ttrain-rmse:1.694211\n",
      "[9434]\ttrain-rmse:1.694196\n",
      "[9435]\ttrain-rmse:1.694184\n",
      "[9436]\ttrain-rmse:1.694177\n",
      "[9437]\ttrain-rmse:1.694160\n",
      "[9438]\ttrain-rmse:1.694150\n",
      "[9439]\ttrain-rmse:1.694140\n",
      "[9440]\ttrain-rmse:1.694126\n",
      "[9441]\ttrain-rmse:1.694114\n",
      "[9442]\ttrain-rmse:1.694103\n",
      "[9443]\ttrain-rmse:1.694096\n",
      "[9444]\ttrain-rmse:1.694085\n",
      "[9445]\ttrain-rmse:1.694074\n",
      "[9446]\ttrain-rmse:1.694066\n",
      "[9447]\ttrain-rmse:1.694049\n",
      "[9448]\ttrain-rmse:1.694034\n",
      "[9449]\ttrain-rmse:1.694029\n",
      "[9450]\ttrain-rmse:1.694013\n",
      "[9451]\ttrain-rmse:1.694001\n",
      "[9452]\ttrain-rmse:1.693989\n",
      "[9453]\ttrain-rmse:1.693980\n",
      "[9454]\ttrain-rmse:1.693968\n",
      "[9455]\ttrain-rmse:1.693959\n",
      "[9456]\ttrain-rmse:1.693946\n",
      "[9457]\ttrain-rmse:1.693933\n",
      "[9458]\ttrain-rmse:1.693925\n",
      "[9459]\ttrain-rmse:1.693913\n",
      "[9460]\ttrain-rmse:1.693902\n",
      "[9461]\ttrain-rmse:1.693889\n",
      "[9462]\ttrain-rmse:1.693879\n",
      "[9463]\ttrain-rmse:1.693870\n",
      "[9464]\ttrain-rmse:1.693855\n",
      "[9465]\ttrain-rmse:1.693847\n",
      "[9466]\ttrain-rmse:1.693834\n",
      "[9467]\ttrain-rmse:1.693823\n",
      "[9468]\ttrain-rmse:1.693813\n",
      "[9469]\ttrain-rmse:1.693801\n",
      "[9470]\ttrain-rmse:1.693790\n",
      "[9471]\ttrain-rmse:1.693777\n",
      "[9472]\ttrain-rmse:1.693767\n",
      "[9473]\ttrain-rmse:1.693758\n",
      "[9474]\ttrain-rmse:1.693741\n",
      "[9475]\ttrain-rmse:1.693731\n",
      "[9476]\ttrain-rmse:1.693720\n",
      "[9477]\ttrain-rmse:1.693711\n",
      "[9478]\ttrain-rmse:1.693697\n",
      "[9479]\ttrain-rmse:1.693680\n",
      "[9480]\ttrain-rmse:1.693663\n",
      "[9481]\ttrain-rmse:1.693653\n",
      "[9482]\ttrain-rmse:1.693647\n",
      "[9483]\ttrain-rmse:1.693635\n",
      "[9484]\ttrain-rmse:1.693621\n",
      "[9485]\ttrain-rmse:1.693613\n",
      "[9486]\ttrain-rmse:1.693602\n",
      "[9487]\ttrain-rmse:1.693588\n",
      "[9488]\ttrain-rmse:1.693576\n",
      "[9489]\ttrain-rmse:1.693571\n",
      "[9490]\ttrain-rmse:1.693560\n",
      "[9491]\ttrain-rmse:1.693551\n",
      "[9492]\ttrain-rmse:1.693544\n",
      "[9493]\ttrain-rmse:1.693529\n",
      "[9494]\ttrain-rmse:1.693513\n",
      "[9495]\ttrain-rmse:1.693501\n",
      "[9496]\ttrain-rmse:1.693493\n",
      "[9497]\ttrain-rmse:1.693480\n",
      "[9498]\ttrain-rmse:1.693466\n",
      "[9499]\ttrain-rmse:1.693460\n",
      "[9500]\ttrain-rmse:1.693448\n",
      "[9501]\ttrain-rmse:1.693435\n",
      "[9502]\ttrain-rmse:1.693423\n",
      "[9503]\ttrain-rmse:1.693409\n",
      "[9504]\ttrain-rmse:1.693398\n",
      "[9505]\ttrain-rmse:1.693380\n",
      "[9506]\ttrain-rmse:1.693374\n",
      "[9507]\ttrain-rmse:1.693366\n",
      "[9508]\ttrain-rmse:1.693358\n",
      "[9509]\ttrain-rmse:1.693349\n",
      "[9510]\ttrain-rmse:1.693335\n",
      "[9511]\ttrain-rmse:1.693321\n",
      "[9512]\ttrain-rmse:1.693310\n",
      "[9513]\ttrain-rmse:1.693302\n",
      "[9514]\ttrain-rmse:1.693297\n",
      "[9515]\ttrain-rmse:1.693282\n",
      "[9516]\ttrain-rmse:1.693276\n",
      "[9517]\ttrain-rmse:1.693270\n",
      "[9518]\ttrain-rmse:1.693257\n",
      "[9519]\ttrain-rmse:1.693250\n",
      "[9520]\ttrain-rmse:1.693244\n",
      "[9521]\ttrain-rmse:1.693227\n",
      "[9522]\ttrain-rmse:1.693212\n",
      "[9523]\ttrain-rmse:1.693196\n",
      "[9524]\ttrain-rmse:1.693182\n",
      "[9525]\ttrain-rmse:1.693169\n",
      "[9526]\ttrain-rmse:1.693158\n",
      "[9527]\ttrain-rmse:1.693149\n",
      "[9528]\ttrain-rmse:1.693141\n",
      "[9529]\ttrain-rmse:1.693130\n",
      "[9530]\ttrain-rmse:1.693123\n",
      "[9531]\ttrain-rmse:1.693111\n",
      "[9532]\ttrain-rmse:1.693096\n",
      "[9533]\ttrain-rmse:1.693081\n",
      "[9534]\ttrain-rmse:1.693070\n",
      "[9535]\ttrain-rmse:1.693062\n",
      "[9536]\ttrain-rmse:1.693047\n",
      "[9537]\ttrain-rmse:1.693039\n",
      "[9538]\ttrain-rmse:1.693021\n",
      "[9539]\ttrain-rmse:1.693002\n",
      "[9540]\ttrain-rmse:1.692991\n",
      "[9541]\ttrain-rmse:1.692980\n",
      "[9542]\ttrain-rmse:1.692968\n",
      "[9543]\ttrain-rmse:1.692953\n",
      "[9544]\ttrain-rmse:1.692945\n",
      "[9545]\ttrain-rmse:1.692932\n",
      "[9546]\ttrain-rmse:1.692917\n",
      "[9547]\ttrain-rmse:1.692904\n",
      "[9548]\ttrain-rmse:1.692898\n",
      "[9549]\ttrain-rmse:1.692887\n",
      "[9550]\ttrain-rmse:1.692879\n",
      "[9551]\ttrain-rmse:1.692865\n",
      "[9552]\ttrain-rmse:1.692856\n",
      "[9553]\ttrain-rmse:1.692844\n",
      "[9554]\ttrain-rmse:1.692831\n",
      "[9555]\ttrain-rmse:1.692819\n",
      "[9556]\ttrain-rmse:1.692814\n",
      "[9557]\ttrain-rmse:1.692801\n",
      "[9558]\ttrain-rmse:1.692788\n",
      "[9559]\ttrain-rmse:1.692780\n",
      "[9560]\ttrain-rmse:1.692769\n",
      "[9561]\ttrain-rmse:1.692759\n",
      "[9562]\ttrain-rmse:1.692751\n",
      "[9563]\ttrain-rmse:1.692738\n",
      "[9564]\ttrain-rmse:1.692727\n",
      "[9565]\ttrain-rmse:1.692711\n",
      "[9566]\ttrain-rmse:1.692701\n",
      "[9567]\ttrain-rmse:1.692690\n",
      "[9568]\ttrain-rmse:1.692678\n",
      "[9569]\ttrain-rmse:1.692667\n",
      "[9570]\ttrain-rmse:1.692654\n",
      "[9571]\ttrain-rmse:1.692644\n",
      "[9572]\ttrain-rmse:1.692633\n",
      "[9573]\ttrain-rmse:1.692619\n",
      "[9574]\ttrain-rmse:1.692613\n",
      "[9575]\ttrain-rmse:1.692604\n",
      "[9576]\ttrain-rmse:1.692589\n",
      "[9577]\ttrain-rmse:1.692581\n",
      "[9578]\ttrain-rmse:1.692570\n",
      "[9579]\ttrain-rmse:1.692560\n",
      "[9580]\ttrain-rmse:1.692540\n",
      "[9581]\ttrain-rmse:1.692529\n",
      "[9582]\ttrain-rmse:1.692518\n",
      "[9583]\ttrain-rmse:1.692508\n",
      "[9584]\ttrain-rmse:1.692503\n",
      "[9585]\ttrain-rmse:1.692491\n",
      "[9586]\ttrain-rmse:1.692482\n",
      "[9587]\ttrain-rmse:1.692474\n",
      "[9588]\ttrain-rmse:1.692462\n",
      "[9589]\ttrain-rmse:1.692456\n",
      "[9590]\ttrain-rmse:1.692448\n",
      "[9591]\ttrain-rmse:1.692432\n",
      "[9592]\ttrain-rmse:1.692425\n",
      "[9593]\ttrain-rmse:1.692414\n",
      "[9594]\ttrain-rmse:1.692402\n",
      "[9595]\ttrain-rmse:1.692388\n",
      "[9596]\ttrain-rmse:1.692367\n",
      "[9597]\ttrain-rmse:1.692354\n",
      "[9598]\ttrain-rmse:1.692342\n",
      "[9599]\ttrain-rmse:1.692335\n",
      "[9600]\ttrain-rmse:1.692321\n",
      "[9601]\ttrain-rmse:1.692310\n",
      "[9602]\ttrain-rmse:1.692301\n",
      "[9603]\ttrain-rmse:1.692290\n",
      "[9604]\ttrain-rmse:1.692283\n",
      "[9605]\ttrain-rmse:1.692266\n",
      "[9606]\ttrain-rmse:1.692255\n",
      "[9607]\ttrain-rmse:1.692243\n",
      "[9608]\ttrain-rmse:1.692235\n",
      "[9609]\ttrain-rmse:1.692220\n",
      "[9610]\ttrain-rmse:1.692210\n",
      "[9611]\ttrain-rmse:1.692200\n",
      "[9612]\ttrain-rmse:1.692192\n",
      "[9613]\ttrain-rmse:1.692183\n",
      "[9614]\ttrain-rmse:1.692172\n",
      "[9615]\ttrain-rmse:1.692162\n",
      "[9616]\ttrain-rmse:1.692147\n",
      "[9617]\ttrain-rmse:1.692132\n",
      "[9618]\ttrain-rmse:1.692126\n",
      "[9619]\ttrain-rmse:1.692112\n",
      "[9620]\ttrain-rmse:1.692098\n",
      "[9621]\ttrain-rmse:1.692084\n",
      "[9622]\ttrain-rmse:1.692069\n",
      "[9623]\ttrain-rmse:1.692055\n",
      "[9624]\ttrain-rmse:1.692044\n",
      "[9625]\ttrain-rmse:1.692031\n",
      "[9626]\ttrain-rmse:1.692019\n",
      "[9627]\ttrain-rmse:1.692007\n",
      "[9628]\ttrain-rmse:1.691992\n",
      "[9629]\ttrain-rmse:1.691981\n",
      "[9630]\ttrain-rmse:1.691967\n",
      "[9631]\ttrain-rmse:1.691958\n",
      "[9632]\ttrain-rmse:1.691947\n",
      "[9633]\ttrain-rmse:1.691935\n",
      "[9634]\ttrain-rmse:1.691925\n",
      "[9635]\ttrain-rmse:1.691915\n",
      "[9636]\ttrain-rmse:1.691901\n",
      "[9637]\ttrain-rmse:1.691891\n",
      "[9638]\ttrain-rmse:1.691886\n",
      "[9639]\ttrain-rmse:1.691868\n",
      "[9640]\ttrain-rmse:1.691852\n",
      "[9641]\ttrain-rmse:1.691836\n",
      "[9642]\ttrain-rmse:1.691826\n",
      "[9643]\ttrain-rmse:1.691812\n",
      "[9644]\ttrain-rmse:1.691805\n",
      "[9645]\ttrain-rmse:1.691794\n",
      "[9646]\ttrain-rmse:1.691787\n",
      "[9647]\ttrain-rmse:1.691772\n",
      "[9648]\ttrain-rmse:1.691763\n",
      "[9649]\ttrain-rmse:1.691757\n",
      "[9650]\ttrain-rmse:1.691743\n",
      "[9651]\ttrain-rmse:1.691739\n",
      "[9652]\ttrain-rmse:1.691727\n",
      "[9653]\ttrain-rmse:1.691710\n",
      "[9654]\ttrain-rmse:1.691701\n",
      "[9655]\ttrain-rmse:1.691691\n",
      "[9656]\ttrain-rmse:1.691683\n",
      "[9657]\ttrain-rmse:1.691670\n",
      "[9658]\ttrain-rmse:1.691655\n",
      "[9659]\ttrain-rmse:1.691647\n",
      "[9660]\ttrain-rmse:1.691631\n",
      "[9661]\ttrain-rmse:1.691617\n",
      "[9662]\ttrain-rmse:1.691603\n",
      "[9663]\ttrain-rmse:1.691594\n",
      "[9664]\ttrain-rmse:1.691583\n",
      "[9665]\ttrain-rmse:1.691575\n",
      "[9666]\ttrain-rmse:1.691561\n",
      "[9667]\ttrain-rmse:1.691546\n",
      "[9668]\ttrain-rmse:1.691531\n",
      "[9669]\ttrain-rmse:1.691516\n",
      "[9670]\ttrain-rmse:1.691513\n",
      "[9671]\ttrain-rmse:1.691503\n",
      "[9672]\ttrain-rmse:1.691489\n",
      "[9673]\ttrain-rmse:1.691470\n",
      "[9674]\ttrain-rmse:1.691462\n",
      "[9675]\ttrain-rmse:1.691459\n",
      "[9676]\ttrain-rmse:1.691450\n",
      "[9677]\ttrain-rmse:1.691443\n",
      "[9678]\ttrain-rmse:1.691434\n",
      "[9679]\ttrain-rmse:1.691419\n",
      "[9680]\ttrain-rmse:1.691410\n",
      "[9681]\ttrain-rmse:1.691398\n",
      "[9682]\ttrain-rmse:1.691384\n",
      "[9683]\ttrain-rmse:1.691368\n",
      "[9684]\ttrain-rmse:1.691350\n",
      "[9685]\ttrain-rmse:1.691334\n",
      "[9686]\ttrain-rmse:1.691326\n",
      "[9687]\ttrain-rmse:1.691318\n",
      "[9688]\ttrain-rmse:1.691310\n",
      "[9689]\ttrain-rmse:1.691299\n",
      "[9690]\ttrain-rmse:1.691282\n",
      "[9691]\ttrain-rmse:1.691270\n",
      "[9692]\ttrain-rmse:1.691259\n",
      "[9693]\ttrain-rmse:1.691249\n",
      "[9694]\ttrain-rmse:1.691234\n",
      "[9695]\ttrain-rmse:1.691227\n",
      "[9696]\ttrain-rmse:1.691218\n",
      "[9697]\ttrain-rmse:1.691206\n",
      "[9698]\ttrain-rmse:1.691192\n",
      "[9699]\ttrain-rmse:1.691180\n",
      "[9700]\ttrain-rmse:1.691168\n",
      "[9701]\ttrain-rmse:1.691157\n",
      "[9702]\ttrain-rmse:1.691142\n",
      "[9703]\ttrain-rmse:1.691123\n",
      "[9704]\ttrain-rmse:1.691115\n",
      "[9705]\ttrain-rmse:1.691101\n",
      "[9706]\ttrain-rmse:1.691089\n",
      "[9707]\ttrain-rmse:1.691069\n",
      "[9708]\ttrain-rmse:1.691056\n",
      "[9709]\ttrain-rmse:1.691043\n",
      "[9710]\ttrain-rmse:1.691033\n",
      "[9711]\ttrain-rmse:1.691020\n",
      "[9712]\ttrain-rmse:1.691014\n",
      "[9713]\ttrain-rmse:1.691001\n",
      "[9714]\ttrain-rmse:1.690994\n",
      "[9715]\ttrain-rmse:1.690983\n",
      "[9716]\ttrain-rmse:1.690970\n",
      "[9717]\ttrain-rmse:1.690963\n",
      "[9718]\ttrain-rmse:1.690956\n",
      "[9719]\ttrain-rmse:1.690943\n",
      "[9720]\ttrain-rmse:1.690930\n",
      "[9721]\ttrain-rmse:1.690919\n",
      "[9722]\ttrain-rmse:1.690912\n",
      "[9723]\ttrain-rmse:1.690900\n",
      "[9724]\ttrain-rmse:1.690893\n",
      "[9725]\ttrain-rmse:1.690886\n",
      "[9726]\ttrain-rmse:1.690871\n",
      "[9727]\ttrain-rmse:1.690861\n",
      "[9728]\ttrain-rmse:1.690850\n",
      "[9729]\ttrain-rmse:1.690838\n",
      "[9730]\ttrain-rmse:1.690831\n",
      "[9731]\ttrain-rmse:1.690824\n",
      "[9732]\ttrain-rmse:1.690812\n",
      "[9733]\ttrain-rmse:1.690805\n",
      "[9734]\ttrain-rmse:1.690790\n",
      "[9735]\ttrain-rmse:1.690778\n",
      "[9736]\ttrain-rmse:1.690766\n",
      "[9737]\ttrain-rmse:1.690758\n",
      "[9738]\ttrain-rmse:1.690744\n",
      "[9739]\ttrain-rmse:1.690729\n",
      "[9740]\ttrain-rmse:1.690721\n",
      "[9741]\ttrain-rmse:1.690710\n",
      "[9742]\ttrain-rmse:1.690689\n",
      "[9743]\ttrain-rmse:1.690681\n",
      "[9744]\ttrain-rmse:1.690666\n",
      "[9745]\ttrain-rmse:1.690656\n",
      "[9746]\ttrain-rmse:1.690645\n",
      "[9747]\ttrain-rmse:1.690628\n",
      "[9748]\ttrain-rmse:1.690621\n",
      "[9749]\ttrain-rmse:1.690613\n",
      "[9750]\ttrain-rmse:1.690598\n",
      "[9751]\ttrain-rmse:1.690587\n",
      "[9752]\ttrain-rmse:1.690582\n",
      "[9753]\ttrain-rmse:1.690572\n",
      "[9754]\ttrain-rmse:1.690555\n",
      "[9755]\ttrain-rmse:1.690545\n",
      "[9756]\ttrain-rmse:1.690531\n",
      "[9757]\ttrain-rmse:1.690518\n",
      "[9758]\ttrain-rmse:1.690506\n",
      "[9759]\ttrain-rmse:1.690489\n",
      "[9760]\ttrain-rmse:1.690475\n",
      "[9761]\ttrain-rmse:1.690468\n",
      "[9762]\ttrain-rmse:1.690461\n",
      "[9763]\ttrain-rmse:1.690452\n",
      "[9764]\ttrain-rmse:1.690439\n",
      "[9765]\ttrain-rmse:1.690426\n",
      "[9766]\ttrain-rmse:1.690420\n",
      "[9767]\ttrain-rmse:1.690409\n",
      "[9768]\ttrain-rmse:1.690402\n",
      "[9769]\ttrain-rmse:1.690388\n",
      "[9770]\ttrain-rmse:1.690379\n",
      "[9771]\ttrain-rmse:1.690370\n",
      "[9772]\ttrain-rmse:1.690362\n",
      "[9773]\ttrain-rmse:1.690353\n",
      "[9774]\ttrain-rmse:1.690343\n",
      "[9775]\ttrain-rmse:1.690331\n",
      "[9776]\ttrain-rmse:1.690322\n",
      "[9777]\ttrain-rmse:1.690309\n",
      "[9778]\ttrain-rmse:1.690297\n",
      "[9779]\ttrain-rmse:1.690282\n",
      "[9780]\ttrain-rmse:1.690274\n",
      "[9781]\ttrain-rmse:1.690263\n",
      "[9782]\ttrain-rmse:1.690253\n",
      "[9783]\ttrain-rmse:1.690241\n",
      "[9784]\ttrain-rmse:1.690229\n",
      "[9785]\ttrain-rmse:1.690222\n",
      "[9786]\ttrain-rmse:1.690211\n",
      "[9787]\ttrain-rmse:1.690202\n",
      "[9788]\ttrain-rmse:1.690194\n",
      "[9789]\ttrain-rmse:1.690183\n",
      "[9790]\ttrain-rmse:1.690169\n",
      "[9791]\ttrain-rmse:1.690156\n",
      "[9792]\ttrain-rmse:1.690141\n",
      "[9793]\ttrain-rmse:1.690131\n",
      "[9794]\ttrain-rmse:1.690114\n",
      "[9795]\ttrain-rmse:1.690101\n",
      "[9796]\ttrain-rmse:1.690088\n",
      "[9797]\ttrain-rmse:1.690080\n",
      "[9798]\ttrain-rmse:1.690067\n",
      "[9799]\ttrain-rmse:1.690052\n",
      "[9800]\ttrain-rmse:1.690036\n",
      "[9801]\ttrain-rmse:1.690020\n",
      "[9802]\ttrain-rmse:1.690013\n",
      "[9803]\ttrain-rmse:1.689998\n",
      "[9804]\ttrain-rmse:1.689982\n",
      "[9805]\ttrain-rmse:1.689970\n",
      "[9806]\ttrain-rmse:1.689955\n",
      "[9807]\ttrain-rmse:1.689945\n",
      "[9808]\ttrain-rmse:1.689934\n",
      "[9809]\ttrain-rmse:1.689921\n",
      "[9810]\ttrain-rmse:1.689905\n",
      "[9811]\ttrain-rmse:1.689889\n",
      "[9812]\ttrain-rmse:1.689882\n",
      "[9813]\ttrain-rmse:1.689875\n",
      "[9814]\ttrain-rmse:1.689869\n",
      "[9815]\ttrain-rmse:1.689864\n",
      "[9816]\ttrain-rmse:1.689849\n",
      "[9817]\ttrain-rmse:1.689844\n",
      "[9818]\ttrain-rmse:1.689830\n",
      "[9819]\ttrain-rmse:1.689816\n",
      "[9820]\ttrain-rmse:1.689805\n",
      "[9821]\ttrain-rmse:1.689793\n",
      "[9822]\ttrain-rmse:1.689781\n",
      "[9823]\ttrain-rmse:1.689773\n",
      "[9824]\ttrain-rmse:1.689759\n",
      "[9825]\ttrain-rmse:1.689749\n",
      "[9826]\ttrain-rmse:1.689739\n",
      "[9827]\ttrain-rmse:1.689726\n",
      "[9828]\ttrain-rmse:1.689716\n",
      "[9829]\ttrain-rmse:1.689706\n",
      "[9830]\ttrain-rmse:1.689696\n",
      "[9831]\ttrain-rmse:1.689688\n",
      "[9832]\ttrain-rmse:1.689675\n",
      "[9833]\ttrain-rmse:1.689662\n",
      "[9834]\ttrain-rmse:1.689648\n",
      "[9835]\ttrain-rmse:1.689636\n",
      "[9836]\ttrain-rmse:1.689619\n",
      "[9837]\ttrain-rmse:1.689610\n",
      "[9838]\ttrain-rmse:1.689599\n",
      "[9839]\ttrain-rmse:1.689588\n",
      "[9840]\ttrain-rmse:1.689581\n",
      "[9841]\ttrain-rmse:1.689570\n",
      "[9842]\ttrain-rmse:1.689560\n",
      "[9843]\ttrain-rmse:1.689549\n",
      "[9844]\ttrain-rmse:1.689535\n",
      "[9845]\ttrain-rmse:1.689527\n",
      "[9846]\ttrain-rmse:1.689521\n",
      "[9847]\ttrain-rmse:1.689513\n",
      "[9848]\ttrain-rmse:1.689506\n",
      "[9849]\ttrain-rmse:1.689498\n",
      "[9850]\ttrain-rmse:1.689490\n",
      "[9851]\ttrain-rmse:1.689477\n",
      "[9852]\ttrain-rmse:1.689469\n",
      "[9853]\ttrain-rmse:1.689453\n",
      "[9854]\ttrain-rmse:1.689446\n",
      "[9855]\ttrain-rmse:1.689438\n",
      "[9856]\ttrain-rmse:1.689427\n",
      "[9857]\ttrain-rmse:1.689417\n",
      "[9858]\ttrain-rmse:1.689409\n",
      "[9859]\ttrain-rmse:1.689396\n",
      "[9860]\ttrain-rmse:1.689384\n",
      "[9861]\ttrain-rmse:1.689371\n",
      "[9862]\ttrain-rmse:1.689363\n",
      "[9863]\ttrain-rmse:1.689352\n",
      "[9864]\ttrain-rmse:1.689337\n",
      "[9865]\ttrain-rmse:1.689321\n",
      "[9866]\ttrain-rmse:1.689314\n",
      "[9867]\ttrain-rmse:1.689302\n",
      "[9868]\ttrain-rmse:1.689296\n",
      "[9869]\ttrain-rmse:1.689279\n",
      "[9870]\ttrain-rmse:1.689266\n",
      "[9871]\ttrain-rmse:1.689252\n",
      "[9872]\ttrain-rmse:1.689246\n",
      "[9873]\ttrain-rmse:1.689236\n",
      "[9874]\ttrain-rmse:1.689231\n",
      "[9875]\ttrain-rmse:1.689221\n",
      "[9876]\ttrain-rmse:1.689202\n",
      "[9877]\ttrain-rmse:1.689191\n",
      "[9878]\ttrain-rmse:1.689178\n",
      "[9879]\ttrain-rmse:1.689170\n",
      "[9880]\ttrain-rmse:1.689166\n",
      "[9881]\ttrain-rmse:1.689157\n",
      "[9882]\ttrain-rmse:1.689147\n",
      "[9883]\ttrain-rmse:1.689139\n",
      "[9884]\ttrain-rmse:1.689130\n",
      "[9885]\ttrain-rmse:1.689116\n",
      "[9886]\ttrain-rmse:1.689107\n",
      "[9887]\ttrain-rmse:1.689099\n",
      "[9888]\ttrain-rmse:1.689088\n",
      "[9889]\ttrain-rmse:1.689081\n",
      "[9890]\ttrain-rmse:1.689067\n",
      "[9891]\ttrain-rmse:1.689054\n",
      "[9892]\ttrain-rmse:1.689037\n",
      "[9893]\ttrain-rmse:1.689026\n",
      "[9894]\ttrain-rmse:1.689018\n",
      "[9895]\ttrain-rmse:1.689006\n",
      "[9896]\ttrain-rmse:1.688995\n",
      "[9897]\ttrain-rmse:1.688984\n",
      "[9898]\ttrain-rmse:1.688969\n",
      "[9899]\ttrain-rmse:1.688962\n",
      "[9900]\ttrain-rmse:1.688951\n",
      "[9901]\ttrain-rmse:1.688940\n",
      "[9902]\ttrain-rmse:1.688927\n",
      "[9903]\ttrain-rmse:1.688911\n",
      "[9904]\ttrain-rmse:1.688902\n",
      "[9905]\ttrain-rmse:1.688884\n",
      "[9906]\ttrain-rmse:1.688877\n",
      "[9907]\ttrain-rmse:1.688871\n",
      "[9908]\ttrain-rmse:1.688863\n",
      "[9909]\ttrain-rmse:1.688850\n",
      "[9910]\ttrain-rmse:1.688838\n",
      "[9911]\ttrain-rmse:1.688829\n",
      "[9912]\ttrain-rmse:1.688822\n",
      "[9913]\ttrain-rmse:1.688811\n",
      "[9914]\ttrain-rmse:1.688799\n",
      "[9915]\ttrain-rmse:1.688785\n",
      "[9916]\ttrain-rmse:1.688774\n",
      "[9917]\ttrain-rmse:1.688767\n",
      "[9918]\ttrain-rmse:1.688751\n",
      "[9919]\ttrain-rmse:1.688737\n",
      "[9920]\ttrain-rmse:1.688726\n",
      "[9921]\ttrain-rmse:1.688719\n",
      "[9922]\ttrain-rmse:1.688712\n",
      "[9923]\ttrain-rmse:1.688704\n",
      "[9924]\ttrain-rmse:1.688697\n",
      "[9925]\ttrain-rmse:1.688682\n",
      "[9926]\ttrain-rmse:1.688673\n",
      "[9927]\ttrain-rmse:1.688663\n",
      "[9928]\ttrain-rmse:1.688650\n",
      "[9929]\ttrain-rmse:1.688639\n",
      "[9930]\ttrain-rmse:1.688631\n",
      "[9931]\ttrain-rmse:1.688623\n",
      "[9932]\ttrain-rmse:1.688618\n",
      "[9933]\ttrain-rmse:1.688606\n",
      "[9934]\ttrain-rmse:1.688594\n",
      "[9935]\ttrain-rmse:1.688581\n",
      "[9936]\ttrain-rmse:1.688575\n",
      "[9937]\ttrain-rmse:1.688569\n",
      "[9938]\ttrain-rmse:1.688560\n",
      "[9939]\ttrain-rmse:1.688548\n",
      "[9940]\ttrain-rmse:1.688535\n",
      "[9941]\ttrain-rmse:1.688529\n",
      "[9942]\ttrain-rmse:1.688524\n",
      "[9943]\ttrain-rmse:1.688516\n",
      "[9944]\ttrain-rmse:1.688508\n",
      "[9945]\ttrain-rmse:1.688496\n",
      "[9946]\ttrain-rmse:1.688490\n",
      "[9947]\ttrain-rmse:1.688480\n",
      "[9948]\ttrain-rmse:1.688464\n",
      "[9949]\ttrain-rmse:1.688451\n",
      "[9950]\ttrain-rmse:1.688446\n",
      "[9951]\ttrain-rmse:1.688433\n",
      "[9952]\ttrain-rmse:1.688423\n",
      "[9953]\ttrain-rmse:1.688412\n",
      "[9954]\ttrain-rmse:1.688396\n",
      "[9955]\ttrain-rmse:1.688381\n",
      "[9956]\ttrain-rmse:1.688363\n",
      "[9957]\ttrain-rmse:1.688354\n",
      "[9958]\ttrain-rmse:1.688342\n",
      "[9959]\ttrain-rmse:1.688326\n",
      "[9960]\ttrain-rmse:1.688314\n",
      "[9961]\ttrain-rmse:1.688304\n",
      "[9962]\ttrain-rmse:1.688298\n",
      "[9963]\ttrain-rmse:1.688289\n",
      "[9964]\ttrain-rmse:1.688277\n",
      "[9965]\ttrain-rmse:1.688268\n",
      "[9966]\ttrain-rmse:1.688259\n",
      "[9967]\ttrain-rmse:1.688250\n",
      "[9968]\ttrain-rmse:1.688239\n",
      "[9969]\ttrain-rmse:1.688229\n",
      "[9970]\ttrain-rmse:1.688221\n",
      "[9971]\ttrain-rmse:1.688207\n",
      "[9972]\ttrain-rmse:1.688196\n",
      "[9973]\ttrain-rmse:1.688190\n",
      "[9974]\ttrain-rmse:1.688182\n",
      "[9975]\ttrain-rmse:1.688168\n",
      "[9976]\ttrain-rmse:1.688156\n",
      "[9977]\ttrain-rmse:1.688144\n",
      "[9978]\ttrain-rmse:1.688136\n",
      "[9979]\ttrain-rmse:1.688121\n",
      "[9980]\ttrain-rmse:1.688103\n",
      "[9981]\ttrain-rmse:1.688092\n",
      "[9982]\ttrain-rmse:1.688082\n",
      "[9983]\ttrain-rmse:1.688073\n",
      "[9984]\ttrain-rmse:1.688067\n",
      "[9985]\ttrain-rmse:1.688054\n",
      "[9986]\ttrain-rmse:1.688044\n",
      "[9987]\ttrain-rmse:1.688034\n",
      "[9988]\ttrain-rmse:1.688021\n",
      "[9989]\ttrain-rmse:1.688012\n",
      "[9990]\ttrain-rmse:1.687995\n",
      "[9991]\ttrain-rmse:1.687983\n",
      "[9992]\ttrain-rmse:1.687971\n",
      "[9993]\ttrain-rmse:1.687964\n",
      "[9994]\ttrain-rmse:1.687947\n",
      "[9995]\ttrain-rmse:1.687935\n",
      "[9996]\ttrain-rmse:1.687924\n",
      "[9997]\ttrain-rmse:1.687910\n",
      "[9998]\ttrain-rmse:1.687897\n",
      "[9999]\ttrain-rmse:1.687889\n",
      "[10000]\ttrain-rmse:1.687873\n",
      "[10001]\ttrain-rmse:1.687862\n",
      "[10002]\ttrain-rmse:1.687853\n",
      "[10003]\ttrain-rmse:1.687845\n",
      "[10004]\ttrain-rmse:1.687830\n",
      "[10005]\ttrain-rmse:1.687824\n",
      "[10006]\ttrain-rmse:1.687811\n",
      "[10007]\ttrain-rmse:1.687797\n",
      "[10008]\ttrain-rmse:1.687785\n",
      "[10009]\ttrain-rmse:1.687777\n",
      "[10010]\ttrain-rmse:1.687767\n",
      "[10011]\ttrain-rmse:1.687753\n",
      "[10012]\ttrain-rmse:1.687743\n",
      "[10013]\ttrain-rmse:1.687735\n",
      "[10014]\ttrain-rmse:1.687723\n",
      "[10015]\ttrain-rmse:1.687710\n",
      "[10016]\ttrain-rmse:1.687700\n",
      "[10017]\ttrain-rmse:1.687691\n",
      "[10018]\ttrain-rmse:1.687677\n",
      "[10019]\ttrain-rmse:1.687668\n",
      "[10020]\ttrain-rmse:1.687660\n",
      "[10021]\ttrain-rmse:1.687647\n",
      "[10022]\ttrain-rmse:1.687634\n",
      "[10023]\ttrain-rmse:1.687621\n",
      "[10024]\ttrain-rmse:1.687611\n",
      "[10025]\ttrain-rmse:1.687599\n",
      "[10026]\ttrain-rmse:1.687591\n",
      "[10027]\ttrain-rmse:1.687577\n",
      "[10028]\ttrain-rmse:1.687567\n",
      "[10029]\ttrain-rmse:1.687561\n",
      "[10030]\ttrain-rmse:1.687556\n",
      "[10031]\ttrain-rmse:1.687543\n",
      "[10032]\ttrain-rmse:1.687533\n",
      "[10033]\ttrain-rmse:1.687522\n",
      "[10034]\ttrain-rmse:1.687509\n",
      "[10035]\ttrain-rmse:1.687495\n",
      "[10036]\ttrain-rmse:1.687478\n",
      "[10037]\ttrain-rmse:1.687465\n",
      "[10038]\ttrain-rmse:1.687460\n",
      "[10039]\ttrain-rmse:1.687445\n",
      "[10040]\ttrain-rmse:1.687438\n",
      "[10041]\ttrain-rmse:1.687432\n",
      "[10042]\ttrain-rmse:1.687418\n",
      "[10043]\ttrain-rmse:1.687408\n",
      "[10044]\ttrain-rmse:1.687396\n",
      "[10045]\ttrain-rmse:1.687387\n",
      "[10046]\ttrain-rmse:1.687374\n",
      "[10047]\ttrain-rmse:1.687367\n",
      "[10048]\ttrain-rmse:1.687357\n",
      "[10049]\ttrain-rmse:1.687344\n",
      "[10050]\ttrain-rmse:1.687334\n",
      "[10051]\ttrain-rmse:1.687327\n",
      "[10052]\ttrain-rmse:1.687312\n",
      "[10053]\ttrain-rmse:1.687310\n",
      "[10054]\ttrain-rmse:1.687300\n",
      "[10055]\ttrain-rmse:1.687290\n",
      "[10056]\ttrain-rmse:1.687282\n",
      "[10057]\ttrain-rmse:1.687274\n",
      "[10058]\ttrain-rmse:1.687261\n",
      "[10059]\ttrain-rmse:1.687255\n",
      "[10060]\ttrain-rmse:1.687241\n",
      "[10061]\ttrain-rmse:1.687228\n",
      "[10062]\ttrain-rmse:1.687221\n",
      "[10063]\ttrain-rmse:1.687209\n",
      "[10064]\ttrain-rmse:1.687202\n",
      "[10065]\ttrain-rmse:1.687191\n",
      "[10066]\ttrain-rmse:1.687177\n",
      "[10067]\ttrain-rmse:1.687164\n",
      "[10068]\ttrain-rmse:1.687148\n",
      "[10069]\ttrain-rmse:1.687136\n",
      "[10070]\ttrain-rmse:1.687129\n",
      "[10071]\ttrain-rmse:1.687122\n",
      "[10072]\ttrain-rmse:1.687110\n",
      "[10073]\ttrain-rmse:1.687103\n",
      "[10074]\ttrain-rmse:1.687096\n",
      "[10075]\ttrain-rmse:1.687083\n",
      "[10076]\ttrain-rmse:1.687072\n",
      "[10077]\ttrain-rmse:1.687064\n",
      "[10078]\ttrain-rmse:1.687053\n",
      "[10079]\ttrain-rmse:1.687045\n",
      "[10080]\ttrain-rmse:1.687036\n",
      "[10081]\ttrain-rmse:1.687030\n",
      "[10082]\ttrain-rmse:1.687017\n",
      "[10083]\ttrain-rmse:1.687010\n",
      "[10084]\ttrain-rmse:1.686995\n",
      "[10085]\ttrain-rmse:1.686986\n",
      "[10086]\ttrain-rmse:1.686976\n",
      "[10087]\ttrain-rmse:1.686963\n",
      "[10088]\ttrain-rmse:1.686947\n",
      "[10089]\ttrain-rmse:1.686938\n",
      "[10090]\ttrain-rmse:1.686927\n",
      "[10091]\ttrain-rmse:1.686917\n",
      "[10092]\ttrain-rmse:1.686910\n",
      "[10093]\ttrain-rmse:1.686900\n",
      "[10094]\ttrain-rmse:1.686891\n",
      "[10095]\ttrain-rmse:1.686872\n",
      "[10096]\ttrain-rmse:1.686861\n",
      "[10097]\ttrain-rmse:1.686851\n",
      "[10098]\ttrain-rmse:1.686840\n",
      "[10099]\ttrain-rmse:1.686831\n",
      "[10100]\ttrain-rmse:1.686813\n",
      "[10101]\ttrain-rmse:1.686801\n",
      "[10102]\ttrain-rmse:1.686792\n",
      "[10103]\ttrain-rmse:1.686779\n",
      "[10104]\ttrain-rmse:1.686767\n",
      "[10105]\ttrain-rmse:1.686750\n",
      "[10106]\ttrain-rmse:1.686740\n",
      "[10107]\ttrain-rmse:1.686731\n",
      "[10108]\ttrain-rmse:1.686718\n",
      "[10109]\ttrain-rmse:1.686706\n",
      "[10110]\ttrain-rmse:1.686696\n",
      "[10111]\ttrain-rmse:1.686688\n",
      "[10112]\ttrain-rmse:1.686681\n",
      "[10113]\ttrain-rmse:1.686672\n",
      "[10114]\ttrain-rmse:1.686661\n",
      "[10115]\ttrain-rmse:1.686653\n",
      "[10116]\ttrain-rmse:1.686648\n",
      "[10117]\ttrain-rmse:1.686630\n",
      "[10118]\ttrain-rmse:1.686618\n",
      "[10119]\ttrain-rmse:1.686604\n",
      "[10120]\ttrain-rmse:1.686593\n",
      "[10121]\ttrain-rmse:1.686585\n",
      "[10122]\ttrain-rmse:1.686571\n",
      "[10123]\ttrain-rmse:1.686556\n",
      "[10124]\ttrain-rmse:1.686549\n",
      "[10125]\ttrain-rmse:1.686536\n",
      "[10126]\ttrain-rmse:1.686527\n",
      "[10127]\ttrain-rmse:1.686513\n",
      "[10128]\ttrain-rmse:1.686501\n",
      "[10129]\ttrain-rmse:1.686495\n",
      "[10130]\ttrain-rmse:1.686486\n",
      "[10131]\ttrain-rmse:1.686475\n",
      "[10132]\ttrain-rmse:1.686466\n",
      "[10133]\ttrain-rmse:1.686453\n",
      "[10134]\ttrain-rmse:1.686447\n",
      "[10135]\ttrain-rmse:1.686433\n",
      "[10136]\ttrain-rmse:1.686421\n",
      "[10137]\ttrain-rmse:1.686403\n",
      "[10138]\ttrain-rmse:1.686381\n",
      "[10139]\ttrain-rmse:1.686373\n",
      "[10140]\ttrain-rmse:1.686368\n",
      "[10141]\ttrain-rmse:1.686350\n",
      "[10142]\ttrain-rmse:1.686339\n",
      "[10143]\ttrain-rmse:1.686333\n",
      "[10144]\ttrain-rmse:1.686323\n",
      "[10145]\ttrain-rmse:1.686318\n",
      "[10146]\ttrain-rmse:1.686303\n",
      "[10147]\ttrain-rmse:1.686295\n",
      "[10148]\ttrain-rmse:1.686284\n",
      "[10149]\ttrain-rmse:1.686269\n",
      "[10150]\ttrain-rmse:1.686258\n",
      "[10151]\ttrain-rmse:1.686251\n",
      "[10152]\ttrain-rmse:1.686235\n",
      "[10153]\ttrain-rmse:1.686225\n",
      "[10154]\ttrain-rmse:1.686217\n",
      "[10155]\ttrain-rmse:1.686206\n",
      "[10156]\ttrain-rmse:1.686193\n",
      "[10157]\ttrain-rmse:1.686183\n",
      "[10158]\ttrain-rmse:1.686175\n",
      "[10159]\ttrain-rmse:1.686162\n",
      "[10160]\ttrain-rmse:1.686150\n",
      "[10161]\ttrain-rmse:1.686135\n",
      "[10162]\ttrain-rmse:1.686120\n",
      "[10163]\ttrain-rmse:1.686107\n",
      "[10164]\ttrain-rmse:1.686096\n",
      "[10165]\ttrain-rmse:1.686083\n",
      "[10166]\ttrain-rmse:1.686073\n",
      "[10167]\ttrain-rmse:1.686065\n",
      "[10168]\ttrain-rmse:1.686056\n",
      "[10169]\ttrain-rmse:1.686048\n",
      "[10170]\ttrain-rmse:1.686038\n",
      "[10171]\ttrain-rmse:1.686028\n",
      "[10172]\ttrain-rmse:1.686022\n",
      "[10173]\ttrain-rmse:1.686018\n",
      "[10174]\ttrain-rmse:1.686003\n",
      "[10175]\ttrain-rmse:1.685989\n",
      "[10176]\ttrain-rmse:1.685980\n",
      "[10177]\ttrain-rmse:1.685965\n",
      "[10178]\ttrain-rmse:1.685954\n",
      "[10179]\ttrain-rmse:1.685940\n",
      "[10180]\ttrain-rmse:1.685932\n",
      "[10181]\ttrain-rmse:1.685922\n",
      "[10182]\ttrain-rmse:1.685909\n",
      "[10183]\ttrain-rmse:1.685898\n",
      "[10184]\ttrain-rmse:1.685892\n",
      "[10185]\ttrain-rmse:1.685882\n",
      "[10186]\ttrain-rmse:1.685875\n",
      "[10187]\ttrain-rmse:1.685864\n",
      "[10188]\ttrain-rmse:1.685853\n",
      "[10189]\ttrain-rmse:1.685842\n",
      "[10190]\ttrain-rmse:1.685832\n",
      "[10191]\ttrain-rmse:1.685819\n",
      "[10192]\ttrain-rmse:1.685815\n",
      "[10193]\ttrain-rmse:1.685802\n",
      "[10194]\ttrain-rmse:1.685788\n",
      "[10195]\ttrain-rmse:1.685774\n",
      "[10196]\ttrain-rmse:1.685759\n",
      "[10197]\ttrain-rmse:1.685747\n",
      "[10198]\ttrain-rmse:1.685737\n",
      "[10199]\ttrain-rmse:1.685731\n",
      "[10200]\ttrain-rmse:1.685718\n",
      "[10201]\ttrain-rmse:1.685706\n",
      "[10202]\ttrain-rmse:1.685695\n",
      "[10203]\ttrain-rmse:1.685685\n",
      "[10204]\ttrain-rmse:1.685672\n",
      "[10205]\ttrain-rmse:1.685663\n",
      "[10206]\ttrain-rmse:1.685647\n",
      "[10207]\ttrain-rmse:1.685638\n",
      "[10208]\ttrain-rmse:1.685633\n",
      "[10209]\ttrain-rmse:1.685621\n",
      "[10210]\ttrain-rmse:1.685611\n",
      "[10211]\ttrain-rmse:1.685600\n",
      "[10212]\ttrain-rmse:1.685589\n",
      "[10213]\ttrain-rmse:1.685583\n",
      "[10214]\ttrain-rmse:1.685574\n",
      "[10215]\ttrain-rmse:1.685566\n",
      "[10216]\ttrain-rmse:1.685557\n",
      "[10217]\ttrain-rmse:1.685545\n",
      "[10218]\ttrain-rmse:1.685535\n",
      "[10219]\ttrain-rmse:1.685527\n",
      "[10220]\ttrain-rmse:1.685516\n",
      "[10221]\ttrain-rmse:1.685503\n",
      "[10222]\ttrain-rmse:1.685493\n",
      "[10223]\ttrain-rmse:1.685480\n",
      "[10224]\ttrain-rmse:1.685469\n",
      "[10225]\ttrain-rmse:1.685461\n",
      "[10226]\ttrain-rmse:1.685455\n",
      "[10227]\ttrain-rmse:1.685443\n",
      "[10228]\ttrain-rmse:1.685431\n",
      "[10229]\ttrain-rmse:1.685419\n",
      "[10230]\ttrain-rmse:1.685411\n",
      "[10231]\ttrain-rmse:1.685398\n",
      "[10232]\ttrain-rmse:1.685389\n",
      "[10233]\ttrain-rmse:1.685374\n",
      "[10234]\ttrain-rmse:1.685360\n",
      "[10235]\ttrain-rmse:1.685351\n",
      "[10236]\ttrain-rmse:1.685338\n",
      "[10237]\ttrain-rmse:1.685326\n",
      "[10238]\ttrain-rmse:1.685312\n",
      "[10239]\ttrain-rmse:1.685301\n",
      "[10240]\ttrain-rmse:1.685288\n",
      "[10241]\ttrain-rmse:1.685276\n",
      "[10242]\ttrain-rmse:1.685270\n",
      "[10243]\ttrain-rmse:1.685260\n",
      "[10244]\ttrain-rmse:1.685250\n",
      "[10245]\ttrain-rmse:1.685238\n",
      "[10246]\ttrain-rmse:1.685222\n",
      "[10247]\ttrain-rmse:1.685210\n",
      "[10248]\ttrain-rmse:1.685197\n",
      "[10249]\ttrain-rmse:1.685192\n",
      "[10250]\ttrain-rmse:1.685186\n",
      "[10251]\ttrain-rmse:1.685174\n",
      "[10252]\ttrain-rmse:1.685160\n",
      "[10253]\ttrain-rmse:1.685148\n",
      "[10254]\ttrain-rmse:1.685144\n",
      "[10255]\ttrain-rmse:1.685132\n",
      "[10256]\ttrain-rmse:1.685128\n",
      "[10257]\ttrain-rmse:1.685113\n",
      "[10258]\ttrain-rmse:1.685108\n",
      "[10259]\ttrain-rmse:1.685100\n",
      "[10260]\ttrain-rmse:1.685086\n",
      "[10261]\ttrain-rmse:1.685075\n",
      "[10262]\ttrain-rmse:1.685063\n",
      "[10263]\ttrain-rmse:1.685053\n",
      "[10264]\ttrain-rmse:1.685042\n",
      "[10265]\ttrain-rmse:1.685032\n",
      "[10266]\ttrain-rmse:1.685022\n",
      "[10267]\ttrain-rmse:1.685016\n",
      "[10268]\ttrain-rmse:1.685010\n",
      "[10269]\ttrain-rmse:1.684996\n",
      "[10270]\ttrain-rmse:1.684990\n",
      "[10271]\ttrain-rmse:1.684976\n",
      "[10272]\ttrain-rmse:1.684969\n",
      "[10273]\ttrain-rmse:1.684955\n",
      "[10274]\ttrain-rmse:1.684945\n",
      "[10275]\ttrain-rmse:1.684932\n",
      "[10276]\ttrain-rmse:1.684924\n",
      "[10277]\ttrain-rmse:1.684912\n",
      "[10278]\ttrain-rmse:1.684893\n",
      "[10279]\ttrain-rmse:1.684883\n",
      "[10280]\ttrain-rmse:1.684873\n",
      "[10281]\ttrain-rmse:1.684864\n",
      "[10282]\ttrain-rmse:1.684855\n",
      "[10283]\ttrain-rmse:1.684843\n",
      "[10284]\ttrain-rmse:1.684830\n",
      "[10285]\ttrain-rmse:1.684824\n",
      "[10286]\ttrain-rmse:1.684808\n",
      "[10287]\ttrain-rmse:1.684797\n",
      "[10288]\ttrain-rmse:1.684783\n",
      "[10289]\ttrain-rmse:1.684776\n",
      "[10290]\ttrain-rmse:1.684764\n",
      "[10291]\ttrain-rmse:1.684755\n",
      "[10292]\ttrain-rmse:1.684752\n",
      "[10293]\ttrain-rmse:1.684740\n",
      "[10294]\ttrain-rmse:1.684727\n",
      "[10295]\ttrain-rmse:1.684717\n",
      "[10296]\ttrain-rmse:1.684708\n",
      "[10297]\ttrain-rmse:1.684703\n",
      "[10298]\ttrain-rmse:1.684695\n",
      "[10299]\ttrain-rmse:1.684682\n",
      "[10300]\ttrain-rmse:1.684674\n",
      "[10301]\ttrain-rmse:1.684661\n",
      "[10302]\ttrain-rmse:1.684654\n",
      "[10303]\ttrain-rmse:1.684647\n",
      "[10304]\ttrain-rmse:1.684635\n",
      "[10305]\ttrain-rmse:1.684622\n",
      "[10306]\ttrain-rmse:1.684610\n",
      "[10307]\ttrain-rmse:1.684603\n",
      "[10308]\ttrain-rmse:1.684586\n",
      "[10309]\ttrain-rmse:1.684577\n",
      "[10310]\ttrain-rmse:1.684564\n",
      "[10311]\ttrain-rmse:1.684557\n",
      "[10312]\ttrain-rmse:1.684550\n",
      "[10313]\ttrain-rmse:1.684542\n",
      "[10314]\ttrain-rmse:1.684531\n",
      "[10315]\ttrain-rmse:1.684521\n",
      "[10316]\ttrain-rmse:1.684512\n",
      "[10317]\ttrain-rmse:1.684500\n",
      "[10318]\ttrain-rmse:1.684490\n",
      "[10319]\ttrain-rmse:1.684472\n",
      "[10320]\ttrain-rmse:1.684464\n",
      "[10321]\ttrain-rmse:1.684458\n",
      "[10322]\ttrain-rmse:1.684444\n",
      "[10323]\ttrain-rmse:1.684433\n",
      "[10324]\ttrain-rmse:1.684420\n",
      "[10325]\ttrain-rmse:1.684412\n",
      "[10326]\ttrain-rmse:1.684401\n",
      "[10327]\ttrain-rmse:1.684388\n",
      "[10328]\ttrain-rmse:1.684377\n",
      "[10329]\ttrain-rmse:1.684363\n",
      "[10330]\ttrain-rmse:1.684347\n",
      "[10331]\ttrain-rmse:1.684337\n",
      "[10332]\ttrain-rmse:1.684326\n",
      "[10333]\ttrain-rmse:1.684317\n",
      "[10334]\ttrain-rmse:1.684314\n",
      "[10335]\ttrain-rmse:1.684297\n",
      "[10336]\ttrain-rmse:1.684286\n",
      "[10337]\ttrain-rmse:1.684272\n",
      "[10338]\ttrain-rmse:1.684261\n",
      "[10339]\ttrain-rmse:1.684250\n",
      "[10340]\ttrain-rmse:1.684237\n",
      "[10341]\ttrain-rmse:1.684226\n",
      "[10342]\ttrain-rmse:1.684217\n",
      "[10343]\ttrain-rmse:1.684209\n",
      "[10344]\ttrain-rmse:1.684198\n",
      "[10345]\ttrain-rmse:1.684183\n",
      "[10346]\ttrain-rmse:1.684175\n",
      "[10347]\ttrain-rmse:1.684165\n",
      "[10348]\ttrain-rmse:1.684151\n",
      "[10349]\ttrain-rmse:1.684144\n",
      "[10350]\ttrain-rmse:1.684136\n",
      "[10351]\ttrain-rmse:1.684131\n",
      "[10352]\ttrain-rmse:1.684124\n",
      "[10353]\ttrain-rmse:1.684108\n",
      "[10354]\ttrain-rmse:1.684105\n",
      "[10355]\ttrain-rmse:1.684093\n",
      "[10356]\ttrain-rmse:1.684083\n",
      "[10357]\ttrain-rmse:1.684071\n",
      "[10358]\ttrain-rmse:1.684062\n",
      "[10359]\ttrain-rmse:1.684049\n",
      "[10360]\ttrain-rmse:1.684040\n",
      "[10361]\ttrain-rmse:1.684029\n",
      "[10362]\ttrain-rmse:1.684013\n",
      "[10363]\ttrain-rmse:1.683999\n",
      "[10364]\ttrain-rmse:1.683990\n",
      "[10365]\ttrain-rmse:1.683969\n",
      "[10366]\ttrain-rmse:1.683955\n",
      "[10367]\ttrain-rmse:1.683947\n",
      "[10368]\ttrain-rmse:1.683941\n",
      "[10369]\ttrain-rmse:1.683934\n",
      "[10370]\ttrain-rmse:1.683926\n",
      "[10371]\ttrain-rmse:1.683914\n",
      "[10372]\ttrain-rmse:1.683911\n",
      "[10373]\ttrain-rmse:1.683903\n",
      "[10374]\ttrain-rmse:1.683889\n",
      "[10375]\ttrain-rmse:1.683875\n",
      "[10376]\ttrain-rmse:1.683864\n",
      "[10377]\ttrain-rmse:1.683850\n",
      "[10378]\ttrain-rmse:1.683843\n",
      "[10379]\ttrain-rmse:1.683829\n",
      "[10380]\ttrain-rmse:1.683816\n",
      "[10381]\ttrain-rmse:1.683807\n",
      "[10382]\ttrain-rmse:1.683795\n",
      "[10383]\ttrain-rmse:1.683786\n",
      "[10384]\ttrain-rmse:1.683774\n",
      "[10385]\ttrain-rmse:1.683766\n",
      "[10386]\ttrain-rmse:1.683749\n",
      "[10387]\ttrain-rmse:1.683730\n",
      "[10388]\ttrain-rmse:1.683716\n",
      "[10389]\ttrain-rmse:1.683710\n",
      "[10390]\ttrain-rmse:1.683702\n",
      "[10391]\ttrain-rmse:1.683694\n",
      "[10392]\ttrain-rmse:1.683683\n",
      "[10393]\ttrain-rmse:1.683674\n",
      "[10394]\ttrain-rmse:1.683666\n",
      "[10395]\ttrain-rmse:1.683654\n",
      "[10396]\ttrain-rmse:1.683646\n",
      "[10397]\ttrain-rmse:1.683628\n",
      "[10398]\ttrain-rmse:1.683617\n",
      "[10399]\ttrain-rmse:1.683606\n",
      "[10400]\ttrain-rmse:1.683596\n",
      "[10401]\ttrain-rmse:1.683588\n",
      "[10402]\ttrain-rmse:1.683577\n",
      "[10403]\ttrain-rmse:1.683564\n",
      "[10404]\ttrain-rmse:1.683558\n",
      "[10405]\ttrain-rmse:1.683547\n",
      "[10406]\ttrain-rmse:1.683543\n",
      "[10407]\ttrain-rmse:1.683531\n",
      "[10408]\ttrain-rmse:1.683521\n",
      "[10409]\ttrain-rmse:1.683509\n",
      "[10410]\ttrain-rmse:1.683500\n",
      "[10411]\ttrain-rmse:1.683489\n",
      "[10412]\ttrain-rmse:1.683475\n",
      "[10413]\ttrain-rmse:1.683458\n",
      "[10414]\ttrain-rmse:1.683446\n",
      "[10415]\ttrain-rmse:1.683441\n",
      "[10416]\ttrain-rmse:1.683437\n",
      "[10417]\ttrain-rmse:1.683427\n",
      "[10418]\ttrain-rmse:1.683416\n",
      "[10419]\ttrain-rmse:1.683408\n",
      "[10420]\ttrain-rmse:1.683399\n",
      "[10421]\ttrain-rmse:1.683388\n",
      "[10422]\ttrain-rmse:1.683376\n",
      "[10423]\ttrain-rmse:1.683366\n",
      "[10424]\ttrain-rmse:1.683356\n",
      "[10425]\ttrain-rmse:1.683343\n",
      "[10426]\ttrain-rmse:1.683337\n",
      "[10427]\ttrain-rmse:1.683331\n",
      "[10428]\ttrain-rmse:1.683323\n",
      "[10429]\ttrain-rmse:1.683310\n",
      "[10430]\ttrain-rmse:1.683304\n",
      "[10431]\ttrain-rmse:1.683294\n",
      "[10432]\ttrain-rmse:1.683286\n",
      "[10433]\ttrain-rmse:1.683273\n",
      "[10434]\ttrain-rmse:1.683267\n",
      "[10435]\ttrain-rmse:1.683251\n",
      "[10436]\ttrain-rmse:1.683242\n",
      "[10437]\ttrain-rmse:1.683229\n",
      "[10438]\ttrain-rmse:1.683215\n",
      "[10439]\ttrain-rmse:1.683205\n",
      "[10440]\ttrain-rmse:1.683190\n",
      "[10441]\ttrain-rmse:1.683179\n",
      "[10442]\ttrain-rmse:1.683165\n",
      "[10443]\ttrain-rmse:1.683156\n",
      "[10444]\ttrain-rmse:1.683147\n",
      "[10445]\ttrain-rmse:1.683139\n",
      "[10446]\ttrain-rmse:1.683125\n",
      "[10447]\ttrain-rmse:1.683114\n",
      "[10448]\ttrain-rmse:1.683108\n",
      "[10449]\ttrain-rmse:1.683095\n",
      "[10450]\ttrain-rmse:1.683082\n",
      "[10451]\ttrain-rmse:1.683063\n",
      "[10452]\ttrain-rmse:1.683052\n",
      "[10453]\ttrain-rmse:1.683039\n",
      "[10454]\ttrain-rmse:1.683030\n",
      "[10455]\ttrain-rmse:1.683025\n",
      "[10456]\ttrain-rmse:1.683016\n",
      "[10457]\ttrain-rmse:1.683009\n",
      "[10458]\ttrain-rmse:1.682999\n",
      "[10459]\ttrain-rmse:1.682988\n",
      "[10460]\ttrain-rmse:1.682977\n",
      "[10461]\ttrain-rmse:1.682965\n",
      "[10462]\ttrain-rmse:1.682955\n",
      "[10463]\ttrain-rmse:1.682941\n",
      "[10464]\ttrain-rmse:1.682934\n",
      "[10465]\ttrain-rmse:1.682922\n",
      "[10466]\ttrain-rmse:1.682914\n",
      "[10467]\ttrain-rmse:1.682900\n",
      "[10468]\ttrain-rmse:1.682892\n",
      "[10469]\ttrain-rmse:1.682880\n",
      "[10470]\ttrain-rmse:1.682867\n",
      "[10471]\ttrain-rmse:1.682856\n",
      "[10472]\ttrain-rmse:1.682851\n",
      "[10473]\ttrain-rmse:1.682844\n",
      "[10474]\ttrain-rmse:1.682838\n",
      "[10475]\ttrain-rmse:1.682824\n",
      "[10476]\ttrain-rmse:1.682811\n",
      "[10477]\ttrain-rmse:1.682799\n",
      "[10478]\ttrain-rmse:1.682782\n",
      "[10479]\ttrain-rmse:1.682769\n",
      "[10480]\ttrain-rmse:1.682762\n",
      "[10481]\ttrain-rmse:1.682749\n",
      "[10482]\ttrain-rmse:1.682741\n",
      "[10483]\ttrain-rmse:1.682735\n",
      "[10484]\ttrain-rmse:1.682725\n",
      "[10485]\ttrain-rmse:1.682718\n",
      "[10486]\ttrain-rmse:1.682705\n",
      "[10487]\ttrain-rmse:1.682690\n",
      "[10488]\ttrain-rmse:1.682680\n",
      "[10489]\ttrain-rmse:1.682670\n",
      "[10490]\ttrain-rmse:1.682663\n",
      "[10491]\ttrain-rmse:1.682650\n",
      "[10492]\ttrain-rmse:1.682635\n",
      "[10493]\ttrain-rmse:1.682629\n",
      "[10494]\ttrain-rmse:1.682620\n",
      "[10495]\ttrain-rmse:1.682605\n",
      "[10496]\ttrain-rmse:1.682596\n",
      "[10497]\ttrain-rmse:1.682585\n",
      "[10498]\ttrain-rmse:1.682577\n",
      "[10499]\ttrain-rmse:1.682565\n",
      "[10500]\ttrain-rmse:1.682557\n",
      "[10501]\ttrain-rmse:1.682552\n",
      "[10502]\ttrain-rmse:1.682540\n",
      "[10503]\ttrain-rmse:1.682536\n",
      "[10504]\ttrain-rmse:1.682525\n",
      "[10505]\ttrain-rmse:1.682516\n",
      "[10506]\ttrain-rmse:1.682504\n",
      "[10507]\ttrain-rmse:1.682495\n",
      "[10508]\ttrain-rmse:1.682489\n",
      "[10509]\ttrain-rmse:1.682481\n",
      "[10510]\ttrain-rmse:1.682472\n",
      "[10511]\ttrain-rmse:1.682460\n",
      "[10512]\ttrain-rmse:1.682453\n",
      "[10513]\ttrain-rmse:1.682442\n",
      "[10514]\ttrain-rmse:1.682429\n",
      "[10515]\ttrain-rmse:1.682418\n",
      "[10516]\ttrain-rmse:1.682407\n",
      "[10517]\ttrain-rmse:1.682399\n",
      "[10518]\ttrain-rmse:1.682387\n",
      "[10519]\ttrain-rmse:1.682380\n",
      "[10520]\ttrain-rmse:1.682372\n",
      "[10521]\ttrain-rmse:1.682364\n",
      "[10522]\ttrain-rmse:1.682357\n",
      "[10523]\ttrain-rmse:1.682336\n",
      "[10524]\ttrain-rmse:1.682325\n",
      "[10525]\ttrain-rmse:1.682307\n",
      "[10526]\ttrain-rmse:1.682296\n",
      "[10527]\ttrain-rmse:1.682286\n",
      "[10528]\ttrain-rmse:1.682274\n",
      "[10529]\ttrain-rmse:1.682261\n",
      "[10530]\ttrain-rmse:1.682246\n",
      "[10531]\ttrain-rmse:1.682237\n",
      "[10532]\ttrain-rmse:1.682231\n",
      "[10533]\ttrain-rmse:1.682226\n",
      "[10534]\ttrain-rmse:1.682218\n",
      "[10535]\ttrain-rmse:1.682211\n",
      "[10536]\ttrain-rmse:1.682200\n",
      "[10537]\ttrain-rmse:1.682192\n",
      "[10538]\ttrain-rmse:1.682184\n",
      "[10539]\ttrain-rmse:1.682178\n",
      "[10540]\ttrain-rmse:1.682169\n",
      "[10541]\ttrain-rmse:1.682157\n",
      "[10542]\ttrain-rmse:1.682143\n",
      "[10543]\ttrain-rmse:1.682132\n",
      "[10544]\ttrain-rmse:1.682120\n",
      "[10545]\ttrain-rmse:1.682108\n",
      "[10546]\ttrain-rmse:1.682103\n",
      "[10547]\ttrain-rmse:1.682089\n",
      "[10548]\ttrain-rmse:1.682083\n",
      "[10549]\ttrain-rmse:1.682079\n",
      "[10550]\ttrain-rmse:1.682072\n",
      "[10551]\ttrain-rmse:1.682054\n",
      "[10552]\ttrain-rmse:1.682044\n",
      "[10553]\ttrain-rmse:1.682033\n",
      "[10554]\ttrain-rmse:1.682024\n",
      "[10555]\ttrain-rmse:1.682021\n",
      "[10556]\ttrain-rmse:1.682013\n",
      "[10557]\ttrain-rmse:1.682001\n",
      "[10558]\ttrain-rmse:1.681989\n",
      "[10559]\ttrain-rmse:1.681982\n",
      "[10560]\ttrain-rmse:1.681972\n",
      "[10561]\ttrain-rmse:1.681969\n",
      "[10562]\ttrain-rmse:1.681958\n",
      "[10563]\ttrain-rmse:1.681947\n",
      "[10564]\ttrain-rmse:1.681939\n",
      "[10565]\ttrain-rmse:1.681927\n",
      "[10566]\ttrain-rmse:1.681916\n",
      "[10567]\ttrain-rmse:1.681904\n",
      "[10568]\ttrain-rmse:1.681890\n",
      "[10569]\ttrain-rmse:1.681881\n",
      "[10570]\ttrain-rmse:1.681876\n",
      "[10571]\ttrain-rmse:1.681869\n",
      "[10572]\ttrain-rmse:1.681866\n",
      "[10573]\ttrain-rmse:1.681850\n",
      "[10574]\ttrain-rmse:1.681845\n",
      "[10575]\ttrain-rmse:1.681837\n",
      "[10576]\ttrain-rmse:1.681825\n",
      "[10577]\ttrain-rmse:1.681816\n",
      "[10578]\ttrain-rmse:1.681810\n",
      "[10579]\ttrain-rmse:1.681802\n",
      "[10580]\ttrain-rmse:1.681790\n",
      "[10581]\ttrain-rmse:1.681779\n",
      "[10582]\ttrain-rmse:1.681773\n",
      "[10583]\ttrain-rmse:1.681758\n",
      "[10584]\ttrain-rmse:1.681751\n",
      "[10585]\ttrain-rmse:1.681738\n",
      "[10586]\ttrain-rmse:1.681719\n",
      "[10587]\ttrain-rmse:1.681710\n",
      "[10588]\ttrain-rmse:1.681698\n",
      "[10589]\ttrain-rmse:1.681687\n",
      "[10590]\ttrain-rmse:1.681673\n",
      "[10591]\ttrain-rmse:1.681668\n",
      "[10592]\ttrain-rmse:1.681652\n",
      "[10593]\ttrain-rmse:1.681645\n",
      "[10594]\ttrain-rmse:1.681638\n",
      "[10595]\ttrain-rmse:1.681627\n",
      "[10596]\ttrain-rmse:1.681619\n",
      "[10597]\ttrain-rmse:1.681609\n",
      "[10598]\ttrain-rmse:1.681604\n",
      "[10599]\ttrain-rmse:1.681591\n",
      "[10600]\ttrain-rmse:1.681580\n",
      "[10601]\ttrain-rmse:1.681571\n",
      "[10602]\ttrain-rmse:1.681554\n",
      "[10603]\ttrain-rmse:1.681539\n",
      "[10604]\ttrain-rmse:1.681530\n",
      "[10605]\ttrain-rmse:1.681516\n",
      "[10606]\ttrain-rmse:1.681499\n",
      "[10607]\ttrain-rmse:1.681492\n",
      "[10608]\ttrain-rmse:1.681481\n",
      "[10609]\ttrain-rmse:1.681472\n",
      "[10610]\ttrain-rmse:1.681458\n",
      "[10611]\ttrain-rmse:1.681450\n",
      "[10612]\ttrain-rmse:1.681442\n",
      "[10613]\ttrain-rmse:1.681431\n",
      "[10614]\ttrain-rmse:1.681421\n",
      "[10615]\ttrain-rmse:1.681412\n",
      "[10616]\ttrain-rmse:1.681405\n",
      "[10617]\ttrain-rmse:1.681388\n",
      "[10618]\ttrain-rmse:1.681382\n",
      "[10619]\ttrain-rmse:1.681377\n",
      "[10620]\ttrain-rmse:1.681368\n",
      "[10621]\ttrain-rmse:1.681356\n",
      "[10622]\ttrain-rmse:1.681351\n",
      "[10623]\ttrain-rmse:1.681336\n",
      "[10624]\ttrain-rmse:1.681329\n",
      "[10625]\ttrain-rmse:1.681318\n",
      "[10626]\ttrain-rmse:1.681315\n",
      "[10627]\ttrain-rmse:1.681304\n",
      "[10628]\ttrain-rmse:1.681292\n",
      "[10629]\ttrain-rmse:1.681288\n",
      "[10630]\ttrain-rmse:1.681274\n",
      "[10631]\ttrain-rmse:1.681264\n",
      "[10632]\ttrain-rmse:1.681250\n",
      "[10633]\ttrain-rmse:1.681240\n",
      "[10634]\ttrain-rmse:1.681235\n",
      "[10635]\ttrain-rmse:1.681218\n",
      "[10636]\ttrain-rmse:1.681207\n",
      "[10637]\ttrain-rmse:1.681199\n",
      "[10638]\ttrain-rmse:1.681190\n",
      "[10639]\ttrain-rmse:1.681175\n",
      "[10640]\ttrain-rmse:1.681168\n",
      "[10641]\ttrain-rmse:1.681154\n",
      "[10642]\ttrain-rmse:1.681142\n",
      "[10643]\ttrain-rmse:1.681130\n",
      "[10644]\ttrain-rmse:1.681125\n",
      "[10645]\ttrain-rmse:1.681113\n",
      "[10646]\ttrain-rmse:1.681094\n",
      "[10647]\ttrain-rmse:1.681081\n",
      "[10648]\ttrain-rmse:1.681074\n",
      "[10649]\ttrain-rmse:1.681062\n",
      "[10650]\ttrain-rmse:1.681052\n",
      "[10651]\ttrain-rmse:1.681032\n",
      "[10652]\ttrain-rmse:1.681018\n",
      "[10653]\ttrain-rmse:1.681008\n",
      "[10654]\ttrain-rmse:1.680995\n",
      "[10655]\ttrain-rmse:1.680985\n",
      "[10656]\ttrain-rmse:1.680974\n",
      "[10657]\ttrain-rmse:1.680969\n",
      "[10658]\ttrain-rmse:1.680960\n",
      "[10659]\ttrain-rmse:1.680950\n",
      "[10660]\ttrain-rmse:1.680945\n",
      "[10661]\ttrain-rmse:1.680936\n",
      "[10662]\ttrain-rmse:1.680931\n",
      "[10663]\ttrain-rmse:1.680921\n",
      "[10664]\ttrain-rmse:1.680913\n",
      "[10665]\ttrain-rmse:1.680904\n",
      "[10666]\ttrain-rmse:1.680895\n",
      "[10667]\ttrain-rmse:1.680881\n",
      "[10668]\ttrain-rmse:1.680873\n",
      "[10669]\ttrain-rmse:1.680861\n",
      "[10670]\ttrain-rmse:1.680851\n",
      "[10671]\ttrain-rmse:1.680845\n",
      "[10672]\ttrain-rmse:1.680835\n",
      "[10673]\ttrain-rmse:1.680822\n",
      "[10674]\ttrain-rmse:1.680812\n",
      "[10675]\ttrain-rmse:1.680806\n",
      "[10676]\ttrain-rmse:1.680799\n",
      "[10677]\ttrain-rmse:1.680786\n",
      "[10678]\ttrain-rmse:1.680779\n",
      "[10679]\ttrain-rmse:1.680774\n",
      "[10680]\ttrain-rmse:1.680766\n",
      "[10681]\ttrain-rmse:1.680753\n",
      "[10682]\ttrain-rmse:1.680739\n",
      "[10683]\ttrain-rmse:1.680723\n",
      "[10684]\ttrain-rmse:1.680707\n",
      "[10685]\ttrain-rmse:1.680700\n",
      "[10686]\ttrain-rmse:1.680684\n",
      "[10687]\ttrain-rmse:1.680676\n",
      "[10688]\ttrain-rmse:1.680663\n",
      "[10689]\ttrain-rmse:1.680654\n",
      "[10690]\ttrain-rmse:1.680644\n",
      "[10691]\ttrain-rmse:1.680628\n",
      "[10692]\ttrain-rmse:1.680615\n",
      "[10693]\ttrain-rmse:1.680600\n",
      "[10694]\ttrain-rmse:1.680591\n",
      "[10695]\ttrain-rmse:1.680584\n",
      "[10696]\ttrain-rmse:1.680573\n",
      "[10697]\ttrain-rmse:1.680559\n",
      "[10698]\ttrain-rmse:1.680554\n",
      "[10699]\ttrain-rmse:1.680547\n",
      "[10700]\ttrain-rmse:1.680534\n",
      "[10701]\ttrain-rmse:1.680524\n",
      "[10702]\ttrain-rmse:1.680512\n",
      "[10703]\ttrain-rmse:1.680508\n",
      "[10704]\ttrain-rmse:1.680494\n",
      "[10705]\ttrain-rmse:1.680482\n",
      "[10706]\ttrain-rmse:1.680469\n",
      "[10707]\ttrain-rmse:1.680454\n",
      "[10708]\ttrain-rmse:1.680441\n",
      "[10709]\ttrain-rmse:1.680433\n",
      "[10710]\ttrain-rmse:1.680418\n",
      "[10711]\ttrain-rmse:1.680409\n",
      "[10712]\ttrain-rmse:1.680398\n",
      "[10713]\ttrain-rmse:1.680388\n",
      "[10714]\ttrain-rmse:1.680380\n",
      "[10715]\ttrain-rmse:1.680375\n",
      "[10716]\ttrain-rmse:1.680367\n",
      "[10717]\ttrain-rmse:1.680358\n",
      "[10718]\ttrain-rmse:1.680345\n",
      "[10719]\ttrain-rmse:1.680336\n",
      "[10720]\ttrain-rmse:1.680330\n",
      "[10721]\ttrain-rmse:1.680325\n",
      "[10722]\ttrain-rmse:1.680315\n",
      "[10723]\ttrain-rmse:1.680308\n",
      "[10724]\ttrain-rmse:1.680300\n",
      "[10725]\ttrain-rmse:1.680290\n",
      "[10726]\ttrain-rmse:1.680279\n",
      "[10727]\ttrain-rmse:1.680264\n",
      "[10728]\ttrain-rmse:1.680253\n",
      "[10729]\ttrain-rmse:1.680246\n",
      "[10730]\ttrain-rmse:1.680240\n",
      "[10731]\ttrain-rmse:1.680229\n",
      "[10732]\ttrain-rmse:1.680219\n",
      "[10733]\ttrain-rmse:1.680207\n",
      "[10734]\ttrain-rmse:1.680200\n",
      "[10735]\ttrain-rmse:1.680189\n",
      "[10736]\ttrain-rmse:1.680178\n",
      "[10737]\ttrain-rmse:1.680162\n",
      "[10738]\ttrain-rmse:1.680151\n",
      "[10739]\ttrain-rmse:1.680143\n",
      "[10740]\ttrain-rmse:1.680134\n",
      "[10741]\ttrain-rmse:1.680119\n",
      "[10742]\ttrain-rmse:1.680115\n",
      "[10743]\ttrain-rmse:1.680107\n",
      "[10744]\ttrain-rmse:1.680101\n",
      "[10745]\ttrain-rmse:1.680092\n",
      "[10746]\ttrain-rmse:1.680083\n",
      "[10747]\ttrain-rmse:1.680070\n",
      "[10748]\ttrain-rmse:1.680062\n",
      "[10749]\ttrain-rmse:1.680054\n",
      "[10750]\ttrain-rmse:1.680043\n",
      "[10751]\ttrain-rmse:1.680035\n",
      "[10752]\ttrain-rmse:1.680019\n",
      "[10753]\ttrain-rmse:1.680010\n",
      "[10754]\ttrain-rmse:1.679993\n",
      "[10755]\ttrain-rmse:1.679981\n",
      "[10756]\ttrain-rmse:1.679976\n",
      "[10757]\ttrain-rmse:1.679966\n",
      "[10758]\ttrain-rmse:1.679956\n",
      "[10759]\ttrain-rmse:1.679937\n",
      "[10760]\ttrain-rmse:1.679923\n",
      "[10761]\ttrain-rmse:1.679916\n",
      "[10762]\ttrain-rmse:1.679909\n",
      "[10763]\ttrain-rmse:1.679899\n",
      "[10764]\ttrain-rmse:1.679888\n",
      "[10765]\ttrain-rmse:1.679879\n",
      "[10766]\ttrain-rmse:1.679866\n",
      "[10767]\ttrain-rmse:1.679859\n",
      "[10768]\ttrain-rmse:1.679849\n",
      "[10769]\ttrain-rmse:1.679834\n",
      "[10770]\ttrain-rmse:1.679829\n",
      "[10771]\ttrain-rmse:1.679815\n",
      "[10772]\ttrain-rmse:1.679807\n",
      "[10773]\ttrain-rmse:1.679804\n",
      "[10774]\ttrain-rmse:1.679795\n",
      "[10775]\ttrain-rmse:1.679791\n",
      "[10776]\ttrain-rmse:1.679783\n",
      "[10777]\ttrain-rmse:1.679771\n",
      "[10778]\ttrain-rmse:1.679764\n",
      "[10779]\ttrain-rmse:1.679751\n",
      "[10780]\ttrain-rmse:1.679741\n",
      "[10781]\ttrain-rmse:1.679728\n",
      "[10782]\ttrain-rmse:1.679713\n",
      "[10783]\ttrain-rmse:1.679702\n",
      "[10784]\ttrain-rmse:1.679694\n",
      "[10785]\ttrain-rmse:1.679681\n",
      "[10786]\ttrain-rmse:1.679670\n",
      "[10787]\ttrain-rmse:1.679659\n",
      "[10788]\ttrain-rmse:1.679650\n",
      "[10789]\ttrain-rmse:1.679643\n",
      "[10790]\ttrain-rmse:1.679635\n",
      "[10791]\ttrain-rmse:1.679622\n",
      "[10792]\ttrain-rmse:1.679615\n",
      "[10793]\ttrain-rmse:1.679607\n",
      "[10794]\ttrain-rmse:1.679598\n",
      "[10795]\ttrain-rmse:1.679585\n",
      "[10796]\ttrain-rmse:1.679569\n",
      "[10797]\ttrain-rmse:1.679560\n",
      "[10798]\ttrain-rmse:1.679549\n",
      "[10799]\ttrain-rmse:1.679542\n",
      "[10800]\ttrain-rmse:1.679533\n",
      "[10801]\ttrain-rmse:1.679523\n",
      "[10802]\ttrain-rmse:1.679513\n",
      "[10803]\ttrain-rmse:1.679506\n",
      "[10804]\ttrain-rmse:1.679496\n",
      "[10805]\ttrain-rmse:1.679485\n",
      "[10806]\ttrain-rmse:1.679476\n",
      "[10807]\ttrain-rmse:1.679463\n",
      "[10808]\ttrain-rmse:1.679449\n",
      "[10809]\ttrain-rmse:1.679441\n",
      "[10810]\ttrain-rmse:1.679427\n",
      "[10811]\ttrain-rmse:1.679414\n",
      "[10812]\ttrain-rmse:1.679403\n",
      "[10813]\ttrain-rmse:1.679387\n",
      "[10814]\ttrain-rmse:1.679377\n",
      "[10815]\ttrain-rmse:1.679366\n",
      "[10816]\ttrain-rmse:1.679357\n",
      "[10817]\ttrain-rmse:1.679346\n",
      "[10818]\ttrain-rmse:1.679338\n",
      "[10819]\ttrain-rmse:1.679329\n",
      "[10820]\ttrain-rmse:1.679320\n",
      "[10821]\ttrain-rmse:1.679312\n",
      "[10822]\ttrain-rmse:1.679296\n",
      "[10823]\ttrain-rmse:1.679285\n",
      "[10824]\ttrain-rmse:1.679278\n",
      "[10825]\ttrain-rmse:1.679263\n",
      "[10826]\ttrain-rmse:1.679250\n",
      "[10827]\ttrain-rmse:1.679241\n",
      "[10828]\ttrain-rmse:1.679229\n",
      "[10829]\ttrain-rmse:1.679222\n",
      "[10830]\ttrain-rmse:1.679216\n",
      "[10831]\ttrain-rmse:1.679212\n",
      "[10832]\ttrain-rmse:1.679199\n",
      "[10833]\ttrain-rmse:1.679186\n",
      "[10834]\ttrain-rmse:1.679176\n",
      "[10835]\ttrain-rmse:1.679166\n",
      "[10836]\ttrain-rmse:1.679153\n",
      "[10837]\ttrain-rmse:1.679142\n",
      "[10838]\ttrain-rmse:1.679135\n",
      "[10839]\ttrain-rmse:1.679124\n",
      "[10840]\ttrain-rmse:1.679106\n",
      "[10841]\ttrain-rmse:1.679095\n",
      "[10842]\ttrain-rmse:1.679088\n",
      "[10843]\ttrain-rmse:1.679078\n",
      "[10844]\ttrain-rmse:1.679063\n",
      "[10845]\ttrain-rmse:1.679056\n",
      "[10846]\ttrain-rmse:1.679043\n",
      "[10847]\ttrain-rmse:1.679038\n",
      "[10848]\ttrain-rmse:1.679029\n",
      "[10849]\ttrain-rmse:1.679018\n",
      "[10850]\ttrain-rmse:1.679010\n",
      "[10851]\ttrain-rmse:1.679002\n",
      "[10852]\ttrain-rmse:1.678991\n",
      "[10853]\ttrain-rmse:1.678984\n",
      "[10854]\ttrain-rmse:1.678969\n",
      "[10855]\ttrain-rmse:1.678960\n",
      "[10856]\ttrain-rmse:1.678952\n",
      "[10857]\ttrain-rmse:1.678947\n",
      "[10858]\ttrain-rmse:1.678936\n",
      "[10859]\ttrain-rmse:1.678925\n",
      "[10860]\ttrain-rmse:1.678918\n",
      "[10861]\ttrain-rmse:1.678908\n",
      "[10862]\ttrain-rmse:1.678900\n",
      "[10863]\ttrain-rmse:1.678891\n",
      "[10864]\ttrain-rmse:1.678880\n",
      "[10865]\ttrain-rmse:1.678875\n",
      "[10866]\ttrain-rmse:1.678868\n",
      "[10867]\ttrain-rmse:1.678858\n",
      "[10868]\ttrain-rmse:1.678848\n",
      "[10869]\ttrain-rmse:1.678838\n",
      "[10870]\ttrain-rmse:1.678827\n",
      "[10871]\ttrain-rmse:1.678815\n",
      "[10872]\ttrain-rmse:1.678805\n",
      "[10873]\ttrain-rmse:1.678788\n",
      "[10874]\ttrain-rmse:1.678777\n",
      "[10875]\ttrain-rmse:1.678767\n",
      "[10876]\ttrain-rmse:1.678760\n",
      "[10877]\ttrain-rmse:1.678755\n",
      "[10878]\ttrain-rmse:1.678741\n",
      "[10879]\ttrain-rmse:1.678732\n",
      "[10880]\ttrain-rmse:1.678715\n",
      "[10881]\ttrain-rmse:1.678709\n",
      "[10882]\ttrain-rmse:1.678698\n",
      "[10883]\ttrain-rmse:1.678688\n",
      "[10884]\ttrain-rmse:1.678674\n",
      "[10885]\ttrain-rmse:1.678664\n",
      "[10886]\ttrain-rmse:1.678653\n",
      "[10887]\ttrain-rmse:1.678645\n",
      "[10888]\ttrain-rmse:1.678638\n",
      "[10889]\ttrain-rmse:1.678628\n",
      "[10890]\ttrain-rmse:1.678622\n",
      "[10891]\ttrain-rmse:1.678616\n",
      "[10892]\ttrain-rmse:1.678609\n",
      "[10893]\ttrain-rmse:1.678599\n",
      "[10894]\ttrain-rmse:1.678588\n",
      "[10895]\ttrain-rmse:1.678579\n",
      "[10896]\ttrain-rmse:1.678569\n",
      "[10897]\ttrain-rmse:1.678561\n",
      "[10898]\ttrain-rmse:1.678547\n",
      "[10899]\ttrain-rmse:1.678536\n",
      "[10900]\ttrain-rmse:1.678522\n",
      "[10901]\ttrain-rmse:1.678517\n",
      "[10902]\ttrain-rmse:1.678501\n",
      "[10903]\ttrain-rmse:1.678494\n",
      "[10904]\ttrain-rmse:1.678488\n",
      "[10905]\ttrain-rmse:1.678477\n",
      "[10906]\ttrain-rmse:1.678473\n",
      "[10907]\ttrain-rmse:1.678464\n",
      "[10908]\ttrain-rmse:1.678459\n",
      "[10909]\ttrain-rmse:1.678447\n",
      "[10910]\ttrain-rmse:1.678437\n",
      "[10911]\ttrain-rmse:1.678425\n",
      "[10912]\ttrain-rmse:1.678417\n",
      "[10913]\ttrain-rmse:1.678409\n",
      "[10914]\ttrain-rmse:1.678396\n",
      "[10915]\ttrain-rmse:1.678387\n",
      "[10916]\ttrain-rmse:1.678376\n",
      "[10917]\ttrain-rmse:1.678368\n",
      "[10918]\ttrain-rmse:1.678355\n",
      "[10919]\ttrain-rmse:1.678345\n",
      "[10920]\ttrain-rmse:1.678342\n",
      "[10921]\ttrain-rmse:1.678336\n",
      "[10922]\ttrain-rmse:1.678323\n",
      "[10923]\ttrain-rmse:1.678304\n",
      "[10924]\ttrain-rmse:1.678294\n",
      "[10925]\ttrain-rmse:1.678289\n",
      "[10926]\ttrain-rmse:1.678271\n",
      "[10927]\ttrain-rmse:1.678254\n",
      "[10928]\ttrain-rmse:1.678249\n",
      "[10929]\ttrain-rmse:1.678240\n",
      "[10930]\ttrain-rmse:1.678232\n",
      "[10931]\ttrain-rmse:1.678224\n",
      "[10932]\ttrain-rmse:1.678215\n",
      "[10933]\ttrain-rmse:1.678209\n",
      "[10934]\ttrain-rmse:1.678197\n",
      "[10935]\ttrain-rmse:1.678190\n",
      "[10936]\ttrain-rmse:1.678178\n",
      "[10937]\ttrain-rmse:1.678167\n",
      "[10938]\ttrain-rmse:1.678154\n",
      "[10939]\ttrain-rmse:1.678147\n",
      "[10940]\ttrain-rmse:1.678131\n",
      "[10941]\ttrain-rmse:1.678120\n",
      "[10942]\ttrain-rmse:1.678110\n",
      "[10943]\ttrain-rmse:1.678101\n",
      "[10944]\ttrain-rmse:1.678094\n",
      "[10945]\ttrain-rmse:1.678084\n",
      "[10946]\ttrain-rmse:1.678076\n",
      "[10947]\ttrain-rmse:1.678067\n",
      "[10948]\ttrain-rmse:1.678057\n",
      "[10949]\ttrain-rmse:1.678041\n",
      "[10950]\ttrain-rmse:1.678031\n",
      "[10951]\ttrain-rmse:1.678023\n",
      "[10952]\ttrain-rmse:1.678012\n",
      "[10953]\ttrain-rmse:1.678003\n",
      "[10954]\ttrain-rmse:1.678001\n",
      "[10955]\ttrain-rmse:1.677989\n",
      "[10956]\ttrain-rmse:1.677977\n",
      "[10957]\ttrain-rmse:1.677974\n",
      "[10958]\ttrain-rmse:1.677961\n",
      "[10959]\ttrain-rmse:1.677948\n",
      "[10960]\ttrain-rmse:1.677938\n",
      "[10961]\ttrain-rmse:1.677931\n",
      "[10962]\ttrain-rmse:1.677915\n",
      "[10963]\ttrain-rmse:1.677907\n",
      "[10964]\ttrain-rmse:1.677900\n",
      "[10965]\ttrain-rmse:1.677893\n",
      "[10966]\ttrain-rmse:1.677879\n",
      "[10967]\ttrain-rmse:1.677866\n",
      "[10968]\ttrain-rmse:1.677860\n",
      "[10969]\ttrain-rmse:1.677848\n",
      "[10970]\ttrain-rmse:1.677846\n",
      "[10971]\ttrain-rmse:1.677835\n",
      "[10972]\ttrain-rmse:1.677822\n",
      "[10973]\ttrain-rmse:1.677816\n",
      "[10974]\ttrain-rmse:1.677807\n",
      "[10975]\ttrain-rmse:1.677797\n",
      "[10976]\ttrain-rmse:1.677785\n",
      "[10977]\ttrain-rmse:1.677774\n",
      "[10978]\ttrain-rmse:1.677756\n",
      "[10979]\ttrain-rmse:1.677739\n",
      "[10980]\ttrain-rmse:1.677732\n",
      "[10981]\ttrain-rmse:1.677723\n",
      "[10982]\ttrain-rmse:1.677717\n",
      "[10983]\ttrain-rmse:1.677707\n",
      "[10984]\ttrain-rmse:1.677694\n",
      "[10985]\ttrain-rmse:1.677681\n",
      "[10986]\ttrain-rmse:1.677670\n",
      "[10987]\ttrain-rmse:1.677660\n",
      "[10988]\ttrain-rmse:1.677650\n",
      "[10989]\ttrain-rmse:1.677640\n",
      "[10990]\ttrain-rmse:1.677628\n",
      "[10991]\ttrain-rmse:1.677617\n",
      "[10992]\ttrain-rmse:1.677609\n",
      "[10993]\ttrain-rmse:1.677602\n",
      "[10994]\ttrain-rmse:1.677599\n",
      "[10995]\ttrain-rmse:1.677591\n",
      "[10996]\ttrain-rmse:1.677576\n",
      "[10997]\ttrain-rmse:1.677571\n",
      "[10998]\ttrain-rmse:1.677562\n",
      "[10999]\ttrain-rmse:1.677556\n",
      "[11000]\ttrain-rmse:1.677549\n",
      "[11001]\ttrain-rmse:1.677539\n",
      "[11002]\ttrain-rmse:1.677520\n",
      "[11003]\ttrain-rmse:1.677510\n",
      "[11004]\ttrain-rmse:1.677498\n",
      "[11005]\ttrain-rmse:1.677479\n",
      "[11006]\ttrain-rmse:1.677471\n",
      "[11007]\ttrain-rmse:1.677457\n",
      "[11008]\ttrain-rmse:1.677452\n",
      "[11009]\ttrain-rmse:1.677442\n",
      "[11010]\ttrain-rmse:1.677435\n",
      "[11011]\ttrain-rmse:1.677427\n",
      "[11012]\ttrain-rmse:1.677415\n",
      "[11013]\ttrain-rmse:1.677406\n",
      "[11014]\ttrain-rmse:1.677393\n",
      "[11015]\ttrain-rmse:1.677381\n",
      "[11016]\ttrain-rmse:1.677373\n",
      "[11017]\ttrain-rmse:1.677365\n",
      "[11018]\ttrain-rmse:1.677353\n",
      "[11019]\ttrain-rmse:1.677341\n",
      "[11020]\ttrain-rmse:1.677327\n",
      "[11021]\ttrain-rmse:1.677316\n",
      "[11022]\ttrain-rmse:1.677306\n",
      "[11023]\ttrain-rmse:1.677294\n",
      "[11024]\ttrain-rmse:1.677288\n",
      "[11025]\ttrain-rmse:1.677279\n",
      "[11026]\ttrain-rmse:1.677259\n",
      "[11027]\ttrain-rmse:1.677248\n",
      "[11028]\ttrain-rmse:1.677232\n",
      "[11029]\ttrain-rmse:1.677217\n",
      "[11030]\ttrain-rmse:1.677209\n",
      "[11031]\ttrain-rmse:1.677199\n",
      "[11032]\ttrain-rmse:1.677192\n",
      "[11033]\ttrain-rmse:1.677183\n",
      "[11034]\ttrain-rmse:1.677168\n",
      "[11035]\ttrain-rmse:1.677158\n",
      "[11036]\ttrain-rmse:1.677150\n",
      "[11037]\ttrain-rmse:1.677140\n",
      "[11038]\ttrain-rmse:1.677133\n",
      "[11039]\ttrain-rmse:1.677121\n",
      "[11040]\ttrain-rmse:1.677104\n",
      "[11041]\ttrain-rmse:1.677093\n",
      "[11042]\ttrain-rmse:1.677080\n",
      "[11043]\ttrain-rmse:1.677069\n",
      "[11044]\ttrain-rmse:1.677055\n",
      "[11045]\ttrain-rmse:1.677042\n",
      "[11046]\ttrain-rmse:1.677036\n",
      "[11047]\ttrain-rmse:1.677021\n",
      "[11048]\ttrain-rmse:1.677006\n",
      "[11049]\ttrain-rmse:1.676992\n",
      "[11050]\ttrain-rmse:1.676976\n",
      "[11051]\ttrain-rmse:1.676969\n",
      "[11052]\ttrain-rmse:1.676954\n",
      "[11053]\ttrain-rmse:1.676944\n",
      "[11054]\ttrain-rmse:1.676939\n",
      "[11055]\ttrain-rmse:1.676927\n",
      "[11056]\ttrain-rmse:1.676918\n",
      "[11057]\ttrain-rmse:1.676909\n",
      "[11058]\ttrain-rmse:1.676896\n",
      "[11059]\ttrain-rmse:1.676887\n",
      "[11060]\ttrain-rmse:1.676878\n",
      "[11061]\ttrain-rmse:1.676871\n",
      "[11062]\ttrain-rmse:1.676857\n",
      "[11063]\ttrain-rmse:1.676845\n",
      "[11064]\ttrain-rmse:1.676832\n",
      "[11065]\ttrain-rmse:1.676821\n",
      "[11066]\ttrain-rmse:1.676813\n",
      "[11067]\ttrain-rmse:1.676798\n",
      "[11068]\ttrain-rmse:1.676790\n",
      "[11069]\ttrain-rmse:1.676784\n",
      "[11070]\ttrain-rmse:1.676774\n",
      "[11071]\ttrain-rmse:1.676768\n",
      "[11072]\ttrain-rmse:1.676764\n",
      "[11073]\ttrain-rmse:1.676759\n",
      "[11074]\ttrain-rmse:1.676746\n",
      "[11075]\ttrain-rmse:1.676740\n",
      "[11076]\ttrain-rmse:1.676728\n",
      "[11077]\ttrain-rmse:1.676718\n",
      "[11078]\ttrain-rmse:1.676711\n",
      "[11079]\ttrain-rmse:1.676701\n",
      "[11080]\ttrain-rmse:1.676693\n",
      "[11081]\ttrain-rmse:1.676682\n",
      "[11082]\ttrain-rmse:1.676674\n",
      "[11083]\ttrain-rmse:1.676665\n",
      "[11084]\ttrain-rmse:1.676654\n",
      "[11085]\ttrain-rmse:1.676649\n",
      "[11086]\ttrain-rmse:1.676639\n",
      "[11087]\ttrain-rmse:1.676629\n",
      "[11088]\ttrain-rmse:1.676617\n",
      "[11089]\ttrain-rmse:1.676602\n",
      "[11090]\ttrain-rmse:1.676591\n",
      "[11091]\ttrain-rmse:1.676581\n",
      "[11092]\ttrain-rmse:1.676570\n",
      "[11093]\ttrain-rmse:1.676557\n",
      "[11094]\ttrain-rmse:1.676546\n",
      "[11095]\ttrain-rmse:1.676539\n",
      "[11096]\ttrain-rmse:1.676533\n",
      "[11097]\ttrain-rmse:1.676525\n",
      "[11098]\ttrain-rmse:1.676517\n",
      "[11099]\ttrain-rmse:1.676506\n",
      "[11100]\ttrain-rmse:1.676495\n",
      "[11101]\ttrain-rmse:1.676484\n",
      "[11102]\ttrain-rmse:1.676474\n",
      "[11103]\ttrain-rmse:1.676464\n",
      "[11104]\ttrain-rmse:1.676455\n",
      "[11105]\ttrain-rmse:1.676446\n",
      "[11106]\ttrain-rmse:1.676438\n",
      "[11107]\ttrain-rmse:1.676428\n",
      "[11108]\ttrain-rmse:1.676416\n",
      "[11109]\ttrain-rmse:1.676404\n",
      "[11110]\ttrain-rmse:1.676398\n",
      "[11111]\ttrain-rmse:1.676386\n",
      "[11112]\ttrain-rmse:1.676381\n",
      "[11113]\ttrain-rmse:1.676366\n",
      "[11114]\ttrain-rmse:1.676360\n",
      "[11115]\ttrain-rmse:1.676351\n",
      "[11116]\ttrain-rmse:1.676338\n",
      "[11117]\ttrain-rmse:1.676331\n",
      "[11118]\ttrain-rmse:1.676323\n",
      "[11119]\ttrain-rmse:1.676311\n",
      "[11120]\ttrain-rmse:1.676301\n",
      "[11121]\ttrain-rmse:1.676293\n",
      "[11122]\ttrain-rmse:1.676285\n",
      "[11123]\ttrain-rmse:1.676273\n",
      "[11124]\ttrain-rmse:1.676260\n",
      "[11125]\ttrain-rmse:1.676245\n",
      "[11126]\ttrain-rmse:1.676233\n",
      "[11127]\ttrain-rmse:1.676222\n",
      "[11128]\ttrain-rmse:1.676214\n",
      "[11129]\ttrain-rmse:1.676205\n",
      "[11130]\ttrain-rmse:1.676194\n",
      "[11131]\ttrain-rmse:1.676184\n",
      "[11132]\ttrain-rmse:1.676166\n",
      "[11133]\ttrain-rmse:1.676156\n",
      "[11134]\ttrain-rmse:1.676149\n",
      "[11135]\ttrain-rmse:1.676140\n",
      "[11136]\ttrain-rmse:1.676131\n",
      "[11137]\ttrain-rmse:1.676115\n",
      "[11138]\ttrain-rmse:1.676106\n",
      "[11139]\ttrain-rmse:1.676100\n",
      "[11140]\ttrain-rmse:1.676092\n",
      "[11141]\ttrain-rmse:1.676086\n",
      "[11142]\ttrain-rmse:1.676078\n",
      "[11143]\ttrain-rmse:1.676071\n",
      "[11144]\ttrain-rmse:1.676065\n",
      "[11145]\ttrain-rmse:1.676055\n",
      "[11146]\ttrain-rmse:1.676042\n",
      "[11147]\ttrain-rmse:1.676035\n",
      "[11148]\ttrain-rmse:1.676020\n",
      "[11149]\ttrain-rmse:1.676012\n",
      "[11150]\ttrain-rmse:1.675998\n",
      "[11151]\ttrain-rmse:1.675990\n",
      "[11152]\ttrain-rmse:1.675980\n",
      "[11153]\ttrain-rmse:1.675971\n",
      "[11154]\ttrain-rmse:1.675957\n",
      "[11155]\ttrain-rmse:1.675947\n",
      "[11156]\ttrain-rmse:1.675940\n",
      "[11157]\ttrain-rmse:1.675932\n",
      "[11158]\ttrain-rmse:1.675927\n",
      "[11159]\ttrain-rmse:1.675917\n",
      "[11160]\ttrain-rmse:1.675908\n",
      "[11161]\ttrain-rmse:1.675902\n",
      "[11162]\ttrain-rmse:1.675887\n",
      "[11163]\ttrain-rmse:1.675876\n",
      "[11164]\ttrain-rmse:1.675871\n",
      "[11165]\ttrain-rmse:1.675866\n",
      "[11166]\ttrain-rmse:1.675856\n",
      "[11167]\ttrain-rmse:1.675848\n",
      "[11168]\ttrain-rmse:1.675840\n",
      "[11169]\ttrain-rmse:1.675827\n",
      "[11170]\ttrain-rmse:1.675816\n",
      "[11171]\ttrain-rmse:1.675807\n",
      "[11172]\ttrain-rmse:1.675799\n",
      "[11173]\ttrain-rmse:1.675793\n",
      "[11174]\ttrain-rmse:1.675783\n",
      "[11175]\ttrain-rmse:1.675772\n",
      "[11176]\ttrain-rmse:1.675758\n",
      "[11177]\ttrain-rmse:1.675753\n",
      "[11178]\ttrain-rmse:1.675746\n",
      "[11179]\ttrain-rmse:1.675730\n",
      "[11180]\ttrain-rmse:1.675720\n",
      "[11181]\ttrain-rmse:1.675705\n",
      "[11182]\ttrain-rmse:1.675696\n",
      "[11183]\ttrain-rmse:1.675686\n",
      "[11184]\ttrain-rmse:1.675677\n",
      "[11185]\ttrain-rmse:1.675666\n",
      "[11186]\ttrain-rmse:1.675660\n",
      "[11187]\ttrain-rmse:1.675654\n",
      "[11188]\ttrain-rmse:1.675645\n",
      "[11189]\ttrain-rmse:1.675636\n",
      "[11190]\ttrain-rmse:1.675631\n",
      "[11191]\ttrain-rmse:1.675625\n",
      "[11192]\ttrain-rmse:1.675612\n",
      "[11193]\ttrain-rmse:1.675604\n",
      "[11194]\ttrain-rmse:1.675591\n",
      "[11195]\ttrain-rmse:1.675574\n",
      "[11196]\ttrain-rmse:1.675560\n",
      "[11197]\ttrain-rmse:1.675549\n",
      "[11198]\ttrain-rmse:1.675538\n",
      "[11199]\ttrain-rmse:1.675524\n",
      "[11200]\ttrain-rmse:1.675514\n",
      "[11201]\ttrain-rmse:1.675502\n",
      "[11202]\ttrain-rmse:1.675492\n",
      "[11203]\ttrain-rmse:1.675482\n",
      "[11204]\ttrain-rmse:1.675470\n",
      "[11205]\ttrain-rmse:1.675460\n",
      "[11206]\ttrain-rmse:1.675453\n",
      "[11207]\ttrain-rmse:1.675441\n",
      "[11208]\ttrain-rmse:1.675428\n",
      "[11209]\ttrain-rmse:1.675419\n",
      "[11210]\ttrain-rmse:1.675410\n",
      "[11211]\ttrain-rmse:1.675395\n",
      "[11212]\ttrain-rmse:1.675385\n",
      "[11213]\ttrain-rmse:1.675372\n",
      "[11214]\ttrain-rmse:1.675361\n",
      "[11215]\ttrain-rmse:1.675344\n",
      "[11216]\ttrain-rmse:1.675332\n",
      "[11217]\ttrain-rmse:1.675324\n",
      "[11218]\ttrain-rmse:1.675318\n",
      "[11219]\ttrain-rmse:1.675311\n",
      "[11220]\ttrain-rmse:1.675303\n",
      "[11221]\ttrain-rmse:1.675295\n",
      "[11222]\ttrain-rmse:1.675285\n",
      "[11223]\ttrain-rmse:1.675271\n",
      "[11224]\ttrain-rmse:1.675266\n",
      "[11225]\ttrain-rmse:1.675254\n",
      "[11226]\ttrain-rmse:1.675240\n",
      "[11227]\ttrain-rmse:1.675231\n",
      "[11228]\ttrain-rmse:1.675225\n",
      "[11229]\ttrain-rmse:1.675217\n",
      "[11230]\ttrain-rmse:1.675212\n",
      "[11231]\ttrain-rmse:1.675202\n",
      "[11232]\ttrain-rmse:1.675194\n",
      "[11233]\ttrain-rmse:1.675190\n",
      "[11234]\ttrain-rmse:1.675177\n",
      "[11235]\ttrain-rmse:1.675173\n",
      "[11236]\ttrain-rmse:1.675155\n",
      "[11237]\ttrain-rmse:1.675142\n",
      "[11238]\ttrain-rmse:1.675132\n",
      "[11239]\ttrain-rmse:1.675114\n",
      "[11240]\ttrain-rmse:1.675100\n",
      "[11241]\ttrain-rmse:1.675088\n",
      "[11242]\ttrain-rmse:1.675082\n",
      "[11243]\ttrain-rmse:1.675071\n",
      "[11244]\ttrain-rmse:1.675064\n",
      "[11245]\ttrain-rmse:1.675057\n",
      "[11246]\ttrain-rmse:1.675049\n",
      "[11247]\ttrain-rmse:1.675039\n",
      "[11248]\ttrain-rmse:1.675028\n",
      "[11249]\ttrain-rmse:1.675019\n",
      "[11250]\ttrain-rmse:1.675015\n",
      "[11251]\ttrain-rmse:1.675005\n",
      "[11252]\ttrain-rmse:1.674992\n",
      "[11253]\ttrain-rmse:1.674976\n",
      "[11254]\ttrain-rmse:1.674968\n",
      "[11255]\ttrain-rmse:1.674963\n",
      "[11256]\ttrain-rmse:1.674952\n",
      "[11257]\ttrain-rmse:1.674942\n",
      "[11258]\ttrain-rmse:1.674938\n",
      "[11259]\ttrain-rmse:1.674932\n",
      "[11260]\ttrain-rmse:1.674923\n",
      "[11261]\ttrain-rmse:1.674915\n",
      "[11262]\ttrain-rmse:1.674904\n",
      "[11263]\ttrain-rmse:1.674888\n",
      "[11264]\ttrain-rmse:1.674878\n",
      "[11265]\ttrain-rmse:1.674870\n",
      "[11266]\ttrain-rmse:1.674861\n",
      "[11267]\ttrain-rmse:1.674839\n",
      "[11268]\ttrain-rmse:1.674826\n",
      "[11269]\ttrain-rmse:1.674816\n",
      "[11270]\ttrain-rmse:1.674811\n",
      "[11271]\ttrain-rmse:1.674804\n",
      "[11272]\ttrain-rmse:1.674796\n",
      "[11273]\ttrain-rmse:1.674783\n",
      "[11274]\ttrain-rmse:1.674767\n",
      "[11275]\ttrain-rmse:1.674755\n",
      "[11276]\ttrain-rmse:1.674739\n",
      "[11277]\ttrain-rmse:1.674729\n",
      "[11278]\ttrain-rmse:1.674718\n",
      "[11279]\ttrain-rmse:1.674711\n",
      "[11280]\ttrain-rmse:1.674703\n",
      "[11281]\ttrain-rmse:1.674695\n",
      "[11282]\ttrain-rmse:1.674684\n",
      "[11283]\ttrain-rmse:1.674674\n",
      "[11284]\ttrain-rmse:1.674664\n",
      "[11285]\ttrain-rmse:1.674653\n",
      "[11286]\ttrain-rmse:1.674645\n",
      "[11287]\ttrain-rmse:1.674636\n",
      "[11288]\ttrain-rmse:1.674627\n",
      "[11289]\ttrain-rmse:1.674613\n",
      "[11290]\ttrain-rmse:1.674601\n",
      "[11291]\ttrain-rmse:1.674593\n",
      "[11292]\ttrain-rmse:1.674589\n",
      "[11293]\ttrain-rmse:1.674582\n",
      "[11294]\ttrain-rmse:1.674570\n",
      "[11295]\ttrain-rmse:1.674556\n",
      "[11296]\ttrain-rmse:1.674545\n",
      "[11297]\ttrain-rmse:1.674537\n",
      "[11298]\ttrain-rmse:1.674523\n",
      "[11299]\ttrain-rmse:1.674515\n",
      "[11300]\ttrain-rmse:1.674503\n",
      "[11301]\ttrain-rmse:1.674494\n",
      "[11302]\ttrain-rmse:1.674481\n",
      "[11303]\ttrain-rmse:1.674467\n",
      "[11304]\ttrain-rmse:1.674453\n",
      "[11305]\ttrain-rmse:1.674443\n",
      "[11306]\ttrain-rmse:1.674433\n",
      "[11307]\ttrain-rmse:1.674421\n",
      "[11308]\ttrain-rmse:1.674412\n",
      "[11309]\ttrain-rmse:1.674404\n",
      "[11310]\ttrain-rmse:1.674396\n",
      "[11311]\ttrain-rmse:1.674390\n",
      "[11312]\ttrain-rmse:1.674384\n",
      "[11313]\ttrain-rmse:1.674372\n",
      "[11314]\ttrain-rmse:1.674360\n",
      "[11315]\ttrain-rmse:1.674353\n",
      "[11316]\ttrain-rmse:1.674345\n",
      "[11317]\ttrain-rmse:1.674336\n",
      "[11318]\ttrain-rmse:1.674328\n",
      "[11319]\ttrain-rmse:1.674318\n",
      "[11320]\ttrain-rmse:1.674307\n",
      "[11321]\ttrain-rmse:1.674303\n",
      "[11322]\ttrain-rmse:1.674300\n",
      "[11323]\ttrain-rmse:1.674294\n",
      "[11324]\ttrain-rmse:1.674282\n",
      "[11325]\ttrain-rmse:1.674275\n",
      "[11326]\ttrain-rmse:1.674265\n",
      "[11327]\ttrain-rmse:1.674255\n",
      "[11328]\ttrain-rmse:1.674245\n",
      "[11329]\ttrain-rmse:1.674241\n",
      "[11330]\ttrain-rmse:1.674231\n",
      "[11331]\ttrain-rmse:1.674213\n",
      "[11332]\ttrain-rmse:1.674204\n",
      "[11333]\ttrain-rmse:1.674199\n",
      "[11334]\ttrain-rmse:1.674189\n",
      "[11335]\ttrain-rmse:1.674185\n",
      "[11336]\ttrain-rmse:1.674177\n",
      "[11337]\ttrain-rmse:1.674167\n",
      "[11338]\ttrain-rmse:1.674157\n",
      "[11339]\ttrain-rmse:1.674154\n",
      "[11340]\ttrain-rmse:1.674146\n",
      "[11341]\ttrain-rmse:1.674137\n",
      "[11342]\ttrain-rmse:1.674132\n",
      "[11343]\ttrain-rmse:1.674123\n",
      "[11344]\ttrain-rmse:1.674117\n",
      "[11345]\ttrain-rmse:1.674106\n",
      "[11346]\ttrain-rmse:1.674096\n",
      "[11347]\ttrain-rmse:1.674084\n",
      "[11348]\ttrain-rmse:1.674078\n",
      "[11349]\ttrain-rmse:1.674063\n",
      "[11350]\ttrain-rmse:1.674048\n",
      "[11351]\ttrain-rmse:1.674039\n",
      "[11352]\ttrain-rmse:1.674033\n",
      "[11353]\ttrain-rmse:1.674022\n",
      "[11354]\ttrain-rmse:1.674010\n",
      "[11355]\ttrain-rmse:1.674004\n",
      "[11356]\ttrain-rmse:1.673994\n",
      "[11357]\ttrain-rmse:1.673988\n",
      "[11358]\ttrain-rmse:1.673978\n",
      "[11359]\ttrain-rmse:1.673970\n",
      "[11360]\ttrain-rmse:1.673956\n",
      "[11361]\ttrain-rmse:1.673942\n",
      "[11362]\ttrain-rmse:1.673935\n",
      "[11363]\ttrain-rmse:1.673928\n",
      "[11364]\ttrain-rmse:1.673913\n",
      "[11365]\ttrain-rmse:1.673898\n",
      "[11366]\ttrain-rmse:1.673887\n",
      "[11367]\ttrain-rmse:1.673874\n",
      "[11368]\ttrain-rmse:1.673859\n",
      "[11369]\ttrain-rmse:1.673851\n",
      "[11370]\ttrain-rmse:1.673844\n",
      "[11371]\ttrain-rmse:1.673837\n",
      "[11372]\ttrain-rmse:1.673824\n",
      "[11373]\ttrain-rmse:1.673811\n",
      "[11374]\ttrain-rmse:1.673806\n",
      "[11375]\ttrain-rmse:1.673797\n",
      "[11376]\ttrain-rmse:1.673786\n",
      "[11377]\ttrain-rmse:1.673775\n",
      "[11378]\ttrain-rmse:1.673769\n",
      "[11379]\ttrain-rmse:1.673760\n",
      "[11380]\ttrain-rmse:1.673747\n",
      "[11381]\ttrain-rmse:1.673739\n",
      "[11382]\ttrain-rmse:1.673728\n",
      "[11383]\ttrain-rmse:1.673723\n",
      "[11384]\ttrain-rmse:1.673706\n",
      "[11385]\ttrain-rmse:1.673699\n",
      "[11386]\ttrain-rmse:1.673691\n",
      "[11387]\ttrain-rmse:1.673682\n",
      "[11388]\ttrain-rmse:1.673671\n",
      "[11389]\ttrain-rmse:1.673660\n",
      "[11390]\ttrain-rmse:1.673651\n",
      "[11391]\ttrain-rmse:1.673645\n",
      "[11392]\ttrain-rmse:1.673638\n",
      "[11393]\ttrain-rmse:1.673628\n",
      "[11394]\ttrain-rmse:1.673620\n",
      "[11395]\ttrain-rmse:1.673609\n",
      "[11396]\ttrain-rmse:1.673596\n",
      "[11397]\ttrain-rmse:1.673587\n",
      "[11398]\ttrain-rmse:1.673579\n",
      "[11399]\ttrain-rmse:1.673570\n",
      "[11400]\ttrain-rmse:1.673563\n",
      "[11401]\ttrain-rmse:1.673552\n",
      "[11402]\ttrain-rmse:1.673538\n",
      "[11403]\ttrain-rmse:1.673532\n",
      "[11404]\ttrain-rmse:1.673525\n",
      "[11405]\ttrain-rmse:1.673514\n",
      "[11406]\ttrain-rmse:1.673500\n",
      "[11407]\ttrain-rmse:1.673491\n",
      "[11408]\ttrain-rmse:1.673483\n",
      "[11409]\ttrain-rmse:1.673479\n",
      "[11410]\ttrain-rmse:1.673476\n",
      "[11411]\ttrain-rmse:1.673467\n",
      "[11412]\ttrain-rmse:1.673455\n",
      "[11413]\ttrain-rmse:1.673452\n",
      "[11414]\ttrain-rmse:1.673443\n",
      "[11415]\ttrain-rmse:1.673436\n",
      "[11416]\ttrain-rmse:1.673428\n",
      "[11417]\ttrain-rmse:1.673413\n",
      "[11418]\ttrain-rmse:1.673399\n",
      "[11419]\ttrain-rmse:1.673386\n",
      "[11420]\ttrain-rmse:1.673372\n",
      "[11421]\ttrain-rmse:1.673361\n",
      "[11422]\ttrain-rmse:1.673356\n",
      "[11423]\ttrain-rmse:1.673342\n",
      "[11424]\ttrain-rmse:1.673329\n",
      "[11425]\ttrain-rmse:1.673323\n",
      "[11426]\ttrain-rmse:1.673317\n",
      "[11427]\ttrain-rmse:1.673309\n",
      "[11428]\ttrain-rmse:1.673300\n",
      "[11429]\ttrain-rmse:1.673287\n",
      "[11430]\ttrain-rmse:1.673277\n",
      "[11431]\ttrain-rmse:1.673268\n",
      "[11432]\ttrain-rmse:1.673258\n",
      "[11433]\ttrain-rmse:1.673243\n",
      "[11434]\ttrain-rmse:1.673235\n",
      "[11435]\ttrain-rmse:1.673226\n",
      "[11436]\ttrain-rmse:1.673217\n",
      "[11437]\ttrain-rmse:1.673204\n",
      "[11438]\ttrain-rmse:1.673199\n",
      "[11439]\ttrain-rmse:1.673186\n",
      "[11440]\ttrain-rmse:1.673173\n",
      "[11441]\ttrain-rmse:1.673163\n",
      "[11442]\ttrain-rmse:1.673153\n",
      "[11443]\ttrain-rmse:1.673147\n",
      "[11444]\ttrain-rmse:1.673133\n",
      "[11445]\ttrain-rmse:1.673124\n",
      "[11446]\ttrain-rmse:1.673119\n",
      "[11447]\ttrain-rmse:1.673104\n",
      "[11448]\ttrain-rmse:1.673098\n",
      "[11449]\ttrain-rmse:1.673090\n",
      "[11450]\ttrain-rmse:1.673078\n",
      "[11451]\ttrain-rmse:1.673070\n",
      "[11452]\ttrain-rmse:1.673061\n",
      "[11453]\ttrain-rmse:1.673051\n",
      "[11454]\ttrain-rmse:1.673048\n",
      "[11455]\ttrain-rmse:1.673035\n",
      "[11456]\ttrain-rmse:1.673026\n",
      "[11457]\ttrain-rmse:1.673015\n",
      "[11458]\ttrain-rmse:1.673007\n",
      "[11459]\ttrain-rmse:1.672995\n",
      "[11460]\ttrain-rmse:1.672980\n",
      "[11461]\ttrain-rmse:1.672971\n",
      "[11462]\ttrain-rmse:1.672963\n",
      "[11463]\ttrain-rmse:1.672949\n",
      "[11464]\ttrain-rmse:1.672941\n",
      "[11465]\ttrain-rmse:1.672930\n",
      "[11466]\ttrain-rmse:1.672920\n",
      "[11467]\ttrain-rmse:1.672913\n",
      "[11468]\ttrain-rmse:1.672904\n",
      "[11469]\ttrain-rmse:1.672894\n",
      "[11470]\ttrain-rmse:1.672881\n",
      "[11471]\ttrain-rmse:1.672874\n",
      "[11472]\ttrain-rmse:1.672866\n",
      "[11473]\ttrain-rmse:1.672855\n",
      "[11474]\ttrain-rmse:1.672849\n",
      "[11475]\ttrain-rmse:1.672838\n",
      "[11476]\ttrain-rmse:1.672822\n",
      "[11477]\ttrain-rmse:1.672809\n",
      "[11478]\ttrain-rmse:1.672798\n",
      "[11479]\ttrain-rmse:1.672791\n",
      "[11480]\ttrain-rmse:1.672785\n",
      "[11481]\ttrain-rmse:1.672772\n",
      "[11482]\ttrain-rmse:1.672762\n",
      "[11483]\ttrain-rmse:1.672753\n",
      "[11484]\ttrain-rmse:1.672742\n",
      "[11485]\ttrain-rmse:1.672735\n",
      "[11486]\ttrain-rmse:1.672724\n",
      "[11487]\ttrain-rmse:1.672712\n",
      "[11488]\ttrain-rmse:1.672699\n",
      "[11489]\ttrain-rmse:1.672692\n",
      "[11490]\ttrain-rmse:1.672685\n",
      "[11491]\ttrain-rmse:1.672671\n",
      "[11492]\ttrain-rmse:1.672661\n",
      "[11493]\ttrain-rmse:1.672655\n",
      "[11494]\ttrain-rmse:1.672645\n",
      "[11495]\ttrain-rmse:1.672635\n",
      "[11496]\ttrain-rmse:1.672623\n",
      "[11497]\ttrain-rmse:1.672608\n",
      "[11498]\ttrain-rmse:1.672591\n",
      "[11499]\ttrain-rmse:1.672582\n",
      "[11500]\ttrain-rmse:1.672572\n",
      "[11501]\ttrain-rmse:1.672555\n",
      "[11502]\ttrain-rmse:1.672542\n",
      "[11503]\ttrain-rmse:1.672535\n",
      "[11504]\ttrain-rmse:1.672530\n",
      "[11505]\ttrain-rmse:1.672523\n",
      "[11506]\ttrain-rmse:1.672516\n",
      "[11507]\ttrain-rmse:1.672502\n",
      "[11508]\ttrain-rmse:1.672489\n",
      "[11509]\ttrain-rmse:1.672482\n",
      "[11510]\ttrain-rmse:1.672472\n",
      "[11511]\ttrain-rmse:1.672461\n",
      "[11512]\ttrain-rmse:1.672455\n",
      "[11513]\ttrain-rmse:1.672448\n",
      "[11514]\ttrain-rmse:1.672441\n",
      "[11515]\ttrain-rmse:1.672426\n",
      "[11516]\ttrain-rmse:1.672418\n",
      "[11517]\ttrain-rmse:1.672409\n",
      "[11518]\ttrain-rmse:1.672402\n",
      "[11519]\ttrain-rmse:1.672387\n",
      "[11520]\ttrain-rmse:1.672376\n",
      "[11521]\ttrain-rmse:1.672369\n",
      "[11522]\ttrain-rmse:1.672360\n",
      "[11523]\ttrain-rmse:1.672351\n",
      "[11524]\ttrain-rmse:1.672341\n",
      "[11525]\ttrain-rmse:1.672334\n",
      "[11526]\ttrain-rmse:1.672325\n",
      "[11527]\ttrain-rmse:1.672315\n",
      "[11528]\ttrain-rmse:1.672304\n",
      "[11529]\ttrain-rmse:1.672298\n",
      "[11530]\ttrain-rmse:1.672288\n",
      "[11531]\ttrain-rmse:1.672279\n",
      "[11532]\ttrain-rmse:1.672268\n",
      "[11533]\ttrain-rmse:1.672261\n",
      "[11534]\ttrain-rmse:1.672252\n",
      "[11535]\ttrain-rmse:1.672245\n",
      "[11536]\ttrain-rmse:1.672234\n",
      "[11537]\ttrain-rmse:1.672221\n",
      "[11538]\ttrain-rmse:1.672210\n",
      "[11539]\ttrain-rmse:1.672203\n",
      "[11540]\ttrain-rmse:1.672192\n",
      "[11541]\ttrain-rmse:1.672183\n",
      "[11542]\ttrain-rmse:1.672174\n",
      "[11543]\ttrain-rmse:1.672167\n",
      "[11544]\ttrain-rmse:1.672156\n",
      "[11545]\ttrain-rmse:1.672145\n",
      "[11546]\ttrain-rmse:1.672134\n",
      "[11547]\ttrain-rmse:1.672128\n",
      "[11548]\ttrain-rmse:1.672122\n",
      "[11549]\ttrain-rmse:1.672108\n",
      "[11550]\ttrain-rmse:1.672098\n",
      "[11551]\ttrain-rmse:1.672094\n",
      "[11552]\ttrain-rmse:1.672084\n",
      "[11553]\ttrain-rmse:1.672076\n",
      "[11554]\ttrain-rmse:1.672067\n",
      "[11555]\ttrain-rmse:1.672064\n",
      "[11556]\ttrain-rmse:1.672059\n",
      "[11557]\ttrain-rmse:1.672050\n",
      "[11558]\ttrain-rmse:1.672044\n",
      "[11559]\ttrain-rmse:1.672032\n",
      "[11560]\ttrain-rmse:1.672020\n",
      "[11561]\ttrain-rmse:1.672012\n",
      "[11562]\ttrain-rmse:1.671999\n",
      "[11563]\ttrain-rmse:1.671986\n",
      "[11564]\ttrain-rmse:1.671979\n",
      "[11565]\ttrain-rmse:1.671965\n",
      "[11566]\ttrain-rmse:1.671959\n",
      "[11567]\ttrain-rmse:1.671952\n",
      "[11568]\ttrain-rmse:1.671944\n",
      "[11569]\ttrain-rmse:1.671931\n",
      "[11570]\ttrain-rmse:1.671918\n",
      "[11571]\ttrain-rmse:1.671909\n",
      "[11572]\ttrain-rmse:1.671901\n",
      "[11573]\ttrain-rmse:1.671889\n",
      "[11574]\ttrain-rmse:1.671878\n",
      "[11575]\ttrain-rmse:1.671867\n",
      "[11576]\ttrain-rmse:1.671855\n",
      "[11577]\ttrain-rmse:1.671851\n",
      "[11578]\ttrain-rmse:1.671839\n",
      "[11579]\ttrain-rmse:1.671829\n",
      "[11580]\ttrain-rmse:1.671815\n",
      "[11581]\ttrain-rmse:1.671802\n",
      "[11582]\ttrain-rmse:1.671791\n",
      "[11583]\ttrain-rmse:1.671780\n",
      "[11584]\ttrain-rmse:1.671772\n",
      "[11585]\ttrain-rmse:1.671763\n",
      "[11586]\ttrain-rmse:1.671754\n",
      "[11587]\ttrain-rmse:1.671740\n",
      "[11588]\ttrain-rmse:1.671728\n",
      "[11589]\ttrain-rmse:1.671718\n",
      "[11590]\ttrain-rmse:1.671706\n",
      "[11591]\ttrain-rmse:1.671695\n",
      "[11592]\ttrain-rmse:1.671682\n",
      "[11593]\ttrain-rmse:1.671672\n",
      "[11594]\ttrain-rmse:1.671665\n",
      "[11595]\ttrain-rmse:1.671658\n",
      "[11596]\ttrain-rmse:1.671654\n",
      "[11597]\ttrain-rmse:1.671645\n",
      "[11598]\ttrain-rmse:1.671634\n",
      "[11599]\ttrain-rmse:1.671625\n",
      "[11600]\ttrain-rmse:1.671613\n",
      "[11601]\ttrain-rmse:1.671607\n",
      "[11602]\ttrain-rmse:1.671598\n",
      "[11603]\ttrain-rmse:1.671589\n",
      "[11604]\ttrain-rmse:1.671579\n",
      "[11605]\ttrain-rmse:1.671576\n",
      "[11606]\ttrain-rmse:1.671568\n",
      "[11607]\ttrain-rmse:1.671557\n",
      "[11608]\ttrain-rmse:1.671544\n",
      "[11609]\ttrain-rmse:1.671535\n",
      "[11610]\ttrain-rmse:1.671525\n",
      "[11611]\ttrain-rmse:1.671516\n",
      "[11612]\ttrain-rmse:1.671505\n",
      "[11613]\ttrain-rmse:1.671494\n",
      "[11614]\ttrain-rmse:1.671480\n",
      "[11615]\ttrain-rmse:1.671472\n",
      "[11616]\ttrain-rmse:1.671466\n",
      "[11617]\ttrain-rmse:1.671457\n",
      "[11618]\ttrain-rmse:1.671446\n",
      "[11619]\ttrain-rmse:1.671437\n",
      "[11620]\ttrain-rmse:1.671431\n",
      "[11621]\ttrain-rmse:1.671422\n",
      "[11622]\ttrain-rmse:1.671413\n",
      "[11623]\ttrain-rmse:1.671404\n",
      "[11624]\ttrain-rmse:1.671395\n",
      "[11625]\ttrain-rmse:1.671381\n",
      "[11626]\ttrain-rmse:1.671366\n",
      "[11627]\ttrain-rmse:1.671353\n",
      "[11628]\ttrain-rmse:1.671345\n",
      "[11629]\ttrain-rmse:1.671335\n",
      "[11630]\ttrain-rmse:1.671326\n",
      "[11631]\ttrain-rmse:1.671319\n",
      "[11632]\ttrain-rmse:1.671313\n",
      "[11633]\ttrain-rmse:1.671306\n",
      "[11634]\ttrain-rmse:1.671297\n",
      "[11635]\ttrain-rmse:1.671285\n",
      "[11636]\ttrain-rmse:1.671278\n",
      "[11637]\ttrain-rmse:1.671271\n",
      "[11638]\ttrain-rmse:1.671261\n",
      "[11639]\ttrain-rmse:1.671246\n",
      "[11640]\ttrain-rmse:1.671235\n",
      "[11641]\ttrain-rmse:1.671223\n",
      "[11642]\ttrain-rmse:1.671218\n",
      "[11643]\ttrain-rmse:1.671209\n",
      "[11644]\ttrain-rmse:1.671201\n",
      "[11645]\ttrain-rmse:1.671190\n",
      "[11646]\ttrain-rmse:1.671176\n",
      "[11647]\ttrain-rmse:1.671165\n",
      "[11648]\ttrain-rmse:1.671152\n",
      "[11649]\ttrain-rmse:1.671141\n",
      "[11650]\ttrain-rmse:1.671131\n",
      "[11651]\ttrain-rmse:1.671120\n",
      "[11652]\ttrain-rmse:1.671108\n",
      "[11653]\ttrain-rmse:1.671100\n",
      "[11654]\ttrain-rmse:1.671086\n",
      "[11655]\ttrain-rmse:1.671078\n",
      "[11656]\ttrain-rmse:1.671068\n",
      "[11657]\ttrain-rmse:1.671059\n",
      "[11658]\ttrain-rmse:1.671052\n",
      "[11659]\ttrain-rmse:1.671043\n",
      "[11660]\ttrain-rmse:1.671029\n",
      "[11661]\ttrain-rmse:1.671013\n",
      "[11662]\ttrain-rmse:1.671007\n",
      "[11663]\ttrain-rmse:1.671000\n",
      "[11664]\ttrain-rmse:1.670991\n",
      "[11665]\ttrain-rmse:1.670977\n",
      "[11666]\ttrain-rmse:1.670969\n",
      "[11667]\ttrain-rmse:1.670955\n",
      "[11668]\ttrain-rmse:1.670944\n",
      "[11669]\ttrain-rmse:1.670933\n",
      "[11670]\ttrain-rmse:1.670931\n",
      "[11671]\ttrain-rmse:1.670923\n",
      "[11672]\ttrain-rmse:1.670915\n",
      "[11673]\ttrain-rmse:1.670904\n",
      "[11674]\ttrain-rmse:1.670898\n",
      "[11675]\ttrain-rmse:1.670890\n",
      "[11676]\ttrain-rmse:1.670877\n",
      "[11677]\ttrain-rmse:1.670863\n",
      "[11678]\ttrain-rmse:1.670849\n",
      "[11679]\ttrain-rmse:1.670845\n",
      "[11680]\ttrain-rmse:1.670836\n",
      "[11681]\ttrain-rmse:1.670821\n",
      "[11682]\ttrain-rmse:1.670814\n",
      "[11683]\ttrain-rmse:1.670808\n",
      "[11684]\ttrain-rmse:1.670800\n",
      "[11685]\ttrain-rmse:1.670784\n",
      "[11686]\ttrain-rmse:1.670777\n",
      "[11687]\ttrain-rmse:1.670767\n",
      "[11688]\ttrain-rmse:1.670760\n",
      "[11689]\ttrain-rmse:1.670749\n",
      "[11690]\ttrain-rmse:1.670738\n",
      "[11691]\ttrain-rmse:1.670731\n",
      "[11692]\ttrain-rmse:1.670719\n",
      "[11693]\ttrain-rmse:1.670711\n",
      "[11694]\ttrain-rmse:1.670705\n",
      "[11695]\ttrain-rmse:1.670689\n",
      "[11696]\ttrain-rmse:1.670683\n",
      "[11697]\ttrain-rmse:1.670676\n",
      "[11698]\ttrain-rmse:1.670664\n",
      "[11699]\ttrain-rmse:1.670655\n",
      "[11700]\ttrain-rmse:1.670646\n",
      "[11701]\ttrain-rmse:1.670633\n",
      "[11702]\ttrain-rmse:1.670627\n",
      "[11703]\ttrain-rmse:1.670617\n",
      "[11704]\ttrain-rmse:1.670610\n",
      "[11705]\ttrain-rmse:1.670594\n",
      "[11706]\ttrain-rmse:1.670585\n",
      "[11707]\ttrain-rmse:1.670579\n",
      "[11708]\ttrain-rmse:1.670569\n",
      "[11709]\ttrain-rmse:1.670565\n",
      "[11710]\ttrain-rmse:1.670558\n",
      "[11711]\ttrain-rmse:1.670549\n",
      "[11712]\ttrain-rmse:1.670540\n",
      "[11713]\ttrain-rmse:1.670525\n",
      "[11714]\ttrain-rmse:1.670518\n",
      "[11715]\ttrain-rmse:1.670509\n",
      "[11716]\ttrain-rmse:1.670496\n",
      "[11717]\ttrain-rmse:1.670483\n",
      "[11718]\ttrain-rmse:1.670475\n",
      "[11719]\ttrain-rmse:1.670468\n",
      "[11720]\ttrain-rmse:1.670463\n",
      "[11721]\ttrain-rmse:1.670450\n",
      "[11722]\ttrain-rmse:1.670440\n",
      "[11723]\ttrain-rmse:1.670429\n",
      "[11724]\ttrain-rmse:1.670419\n",
      "[11725]\ttrain-rmse:1.670411\n",
      "[11726]\ttrain-rmse:1.670404\n",
      "[11727]\ttrain-rmse:1.670395\n",
      "[11728]\ttrain-rmse:1.670385\n",
      "[11729]\ttrain-rmse:1.670368\n",
      "[11730]\ttrain-rmse:1.670355\n",
      "[11731]\ttrain-rmse:1.670346\n",
      "[11732]\ttrain-rmse:1.670333\n",
      "[11733]\ttrain-rmse:1.670328\n",
      "[11734]\ttrain-rmse:1.670312\n",
      "[11735]\ttrain-rmse:1.670308\n",
      "[11736]\ttrain-rmse:1.670299\n",
      "[11737]\ttrain-rmse:1.670288\n",
      "[11738]\ttrain-rmse:1.670281\n",
      "[11739]\ttrain-rmse:1.670273\n",
      "[11740]\ttrain-rmse:1.670269\n",
      "[11741]\ttrain-rmse:1.670256\n",
      "[11742]\ttrain-rmse:1.670251\n",
      "[11743]\ttrain-rmse:1.670239\n",
      "[11744]\ttrain-rmse:1.670230\n",
      "[11745]\ttrain-rmse:1.670221\n",
      "[11746]\ttrain-rmse:1.670213\n",
      "[11747]\ttrain-rmse:1.670207\n",
      "[11748]\ttrain-rmse:1.670196\n",
      "[11749]\ttrain-rmse:1.670187\n",
      "[11750]\ttrain-rmse:1.670182\n",
      "[11751]\ttrain-rmse:1.670171\n",
      "[11752]\ttrain-rmse:1.670163\n",
      "[11753]\ttrain-rmse:1.670160\n",
      "[11754]\ttrain-rmse:1.670148\n",
      "[11755]\ttrain-rmse:1.670131\n",
      "[11756]\ttrain-rmse:1.670125\n",
      "[11757]\ttrain-rmse:1.670120\n",
      "[11758]\ttrain-rmse:1.670114\n",
      "[11759]\ttrain-rmse:1.670099\n",
      "[11760]\ttrain-rmse:1.670088\n",
      "[11761]\ttrain-rmse:1.670074\n",
      "[11762]\ttrain-rmse:1.670062\n",
      "[11763]\ttrain-rmse:1.670055\n",
      "[11764]\ttrain-rmse:1.670050\n",
      "[11765]\ttrain-rmse:1.670042\n",
      "[11766]\ttrain-rmse:1.670029\n",
      "[11767]\ttrain-rmse:1.670021\n",
      "[11768]\ttrain-rmse:1.670014\n",
      "[11769]\ttrain-rmse:1.670004\n",
      "[11770]\ttrain-rmse:1.669998\n",
      "[11771]\ttrain-rmse:1.669984\n",
      "[11772]\ttrain-rmse:1.669975\n",
      "[11773]\ttrain-rmse:1.669971\n",
      "[11774]\ttrain-rmse:1.669963\n",
      "[11775]\ttrain-rmse:1.669958\n",
      "[11776]\ttrain-rmse:1.669947\n",
      "[11777]\ttrain-rmse:1.669941\n",
      "[11778]\ttrain-rmse:1.669931\n",
      "[11779]\ttrain-rmse:1.669917\n",
      "[11780]\ttrain-rmse:1.669909\n",
      "[11781]\ttrain-rmse:1.669902\n",
      "[11782]\ttrain-rmse:1.669893\n",
      "[11783]\ttrain-rmse:1.669882\n",
      "[11784]\ttrain-rmse:1.669872\n",
      "[11785]\ttrain-rmse:1.669864\n",
      "[11786]\ttrain-rmse:1.669846\n",
      "[11787]\ttrain-rmse:1.669837\n",
      "[11788]\ttrain-rmse:1.669825\n",
      "[11789]\ttrain-rmse:1.669815\n",
      "[11790]\ttrain-rmse:1.669806\n",
      "[11791]\ttrain-rmse:1.669799\n",
      "[11792]\ttrain-rmse:1.669789\n",
      "[11793]\ttrain-rmse:1.669780\n",
      "[11794]\ttrain-rmse:1.669765\n",
      "[11795]\ttrain-rmse:1.669754\n",
      "[11796]\ttrain-rmse:1.669748\n",
      "[11797]\ttrain-rmse:1.669730\n",
      "[11798]\ttrain-rmse:1.669714\n",
      "[11799]\ttrain-rmse:1.669708\n",
      "[11800]\ttrain-rmse:1.669703\n",
      "[11801]\ttrain-rmse:1.669696\n",
      "[11802]\ttrain-rmse:1.669683\n",
      "[11803]\ttrain-rmse:1.669673\n",
      "[11804]\ttrain-rmse:1.669661\n",
      "[11805]\ttrain-rmse:1.669649\n",
      "[11806]\ttrain-rmse:1.669641\n",
      "[11807]\ttrain-rmse:1.669630\n",
      "[11808]\ttrain-rmse:1.669619\n",
      "[11809]\ttrain-rmse:1.669613\n",
      "[11810]\ttrain-rmse:1.669599\n",
      "[11811]\ttrain-rmse:1.669588\n",
      "[11812]\ttrain-rmse:1.669572\n",
      "[11813]\ttrain-rmse:1.669567\n",
      "[11814]\ttrain-rmse:1.669556\n",
      "[11815]\ttrain-rmse:1.669549\n",
      "[11816]\ttrain-rmse:1.669539\n",
      "[11817]\ttrain-rmse:1.669526\n",
      "[11818]\ttrain-rmse:1.669517\n",
      "[11819]\ttrain-rmse:1.669510\n",
      "[11820]\ttrain-rmse:1.669494\n",
      "[11821]\ttrain-rmse:1.669487\n",
      "[11822]\ttrain-rmse:1.669479\n",
      "[11823]\ttrain-rmse:1.669473\n",
      "[11824]\ttrain-rmse:1.669462\n",
      "[11825]\ttrain-rmse:1.669457\n",
      "[11826]\ttrain-rmse:1.669452\n",
      "[11827]\ttrain-rmse:1.669443\n",
      "[11828]\ttrain-rmse:1.669429\n",
      "[11829]\ttrain-rmse:1.669418\n",
      "[11830]\ttrain-rmse:1.669414\n",
      "[11831]\ttrain-rmse:1.669403\n",
      "[11832]\ttrain-rmse:1.669394\n",
      "[11833]\ttrain-rmse:1.669383\n",
      "[11834]\ttrain-rmse:1.669374\n",
      "[11835]\ttrain-rmse:1.669370\n",
      "[11836]\ttrain-rmse:1.669360\n",
      "[11837]\ttrain-rmse:1.669350\n",
      "[11838]\ttrain-rmse:1.669340\n",
      "[11839]\ttrain-rmse:1.669332\n",
      "[11840]\ttrain-rmse:1.669324\n",
      "[11841]\ttrain-rmse:1.669314\n",
      "[11842]\ttrain-rmse:1.669309\n",
      "[11843]\ttrain-rmse:1.669300\n",
      "[11844]\ttrain-rmse:1.669291\n",
      "[11845]\ttrain-rmse:1.669281\n",
      "[11846]\ttrain-rmse:1.669265\n",
      "[11847]\ttrain-rmse:1.669257\n",
      "[11848]\ttrain-rmse:1.669247\n",
      "[11849]\ttrain-rmse:1.669239\n",
      "[11850]\ttrain-rmse:1.669218\n",
      "[11851]\ttrain-rmse:1.669214\n",
      "[11852]\ttrain-rmse:1.669199\n",
      "[11853]\ttrain-rmse:1.669193\n",
      "[11854]\ttrain-rmse:1.669179\n",
      "[11855]\ttrain-rmse:1.669170\n",
      "[11856]\ttrain-rmse:1.669157\n",
      "[11857]\ttrain-rmse:1.669150\n",
      "[11858]\ttrain-rmse:1.669142\n",
      "[11859]\ttrain-rmse:1.669137\n",
      "[11860]\ttrain-rmse:1.669127\n",
      "[11861]\ttrain-rmse:1.669117\n",
      "[11862]\ttrain-rmse:1.669113\n",
      "[11863]\ttrain-rmse:1.669102\n",
      "[11864]\ttrain-rmse:1.669094\n",
      "[11865]\ttrain-rmse:1.669087\n",
      "[11866]\ttrain-rmse:1.669080\n",
      "[11867]\ttrain-rmse:1.669072\n",
      "[11868]\ttrain-rmse:1.669065\n",
      "[11869]\ttrain-rmse:1.669056\n",
      "[11870]\ttrain-rmse:1.669039\n",
      "[11871]\ttrain-rmse:1.669029\n",
      "[11872]\ttrain-rmse:1.669020\n",
      "[11873]\ttrain-rmse:1.669012\n",
      "[11874]\ttrain-rmse:1.669001\n",
      "[11875]\ttrain-rmse:1.668985\n",
      "[11876]\ttrain-rmse:1.668976\n",
      "[11877]\ttrain-rmse:1.668969\n",
      "[11878]\ttrain-rmse:1.668958\n",
      "[11879]\ttrain-rmse:1.668943\n",
      "[11880]\ttrain-rmse:1.668929\n",
      "[11881]\ttrain-rmse:1.668923\n",
      "[11882]\ttrain-rmse:1.668910\n",
      "[11883]\ttrain-rmse:1.668896\n",
      "[11884]\ttrain-rmse:1.668884\n",
      "[11885]\ttrain-rmse:1.668873\n",
      "[11886]\ttrain-rmse:1.668865\n",
      "[11887]\ttrain-rmse:1.668850\n",
      "[11888]\ttrain-rmse:1.668846\n",
      "[11889]\ttrain-rmse:1.668838\n",
      "[11890]\ttrain-rmse:1.668827\n",
      "[11891]\ttrain-rmse:1.668818\n",
      "[11892]\ttrain-rmse:1.668808\n",
      "[11893]\ttrain-rmse:1.668802\n",
      "[11894]\ttrain-rmse:1.668797\n",
      "[11895]\ttrain-rmse:1.668790\n",
      "[11896]\ttrain-rmse:1.668780\n",
      "[11897]\ttrain-rmse:1.668777\n",
      "[11898]\ttrain-rmse:1.668764\n",
      "[11899]\ttrain-rmse:1.668757\n",
      "[11900]\ttrain-rmse:1.668752\n",
      "[11901]\ttrain-rmse:1.668745\n",
      "[11902]\ttrain-rmse:1.668731\n",
      "[11903]\ttrain-rmse:1.668726\n",
      "[11904]\ttrain-rmse:1.668713\n",
      "[11905]\ttrain-rmse:1.668710\n",
      "[11906]\ttrain-rmse:1.668698\n",
      "[11907]\ttrain-rmse:1.668689\n",
      "[11908]\ttrain-rmse:1.668676\n",
      "[11909]\ttrain-rmse:1.668666\n",
      "[11910]\ttrain-rmse:1.668657\n",
      "[11911]\ttrain-rmse:1.668649\n",
      "[11912]\ttrain-rmse:1.668642\n",
      "[11913]\ttrain-rmse:1.668631\n",
      "[11914]\ttrain-rmse:1.668623\n",
      "[11915]\ttrain-rmse:1.668610\n",
      "[11916]\ttrain-rmse:1.668599\n",
      "[11917]\ttrain-rmse:1.668590\n",
      "[11918]\ttrain-rmse:1.668579\n",
      "[11919]\ttrain-rmse:1.668565\n",
      "[11920]\ttrain-rmse:1.668555\n",
      "[11921]\ttrain-rmse:1.668553\n",
      "[11922]\ttrain-rmse:1.668548\n",
      "[11923]\ttrain-rmse:1.668533\n",
      "[11924]\ttrain-rmse:1.668520\n",
      "[11925]\ttrain-rmse:1.668513\n",
      "[11926]\ttrain-rmse:1.668503\n",
      "[11927]\ttrain-rmse:1.668496\n",
      "[11928]\ttrain-rmse:1.668484\n",
      "[11929]\ttrain-rmse:1.668474\n",
      "[11930]\ttrain-rmse:1.668463\n",
      "[11931]\ttrain-rmse:1.668451\n",
      "[11932]\ttrain-rmse:1.668439\n",
      "[11933]\ttrain-rmse:1.668430\n",
      "[11934]\ttrain-rmse:1.668422\n",
      "[11935]\ttrain-rmse:1.668411\n",
      "[11936]\ttrain-rmse:1.668403\n",
      "[11937]\ttrain-rmse:1.668395\n",
      "[11938]\ttrain-rmse:1.668383\n",
      "[11939]\ttrain-rmse:1.668367\n",
      "[11940]\ttrain-rmse:1.668361\n",
      "[11941]\ttrain-rmse:1.668347\n",
      "[11942]\ttrain-rmse:1.668336\n",
      "[11943]\ttrain-rmse:1.668325\n",
      "[11944]\ttrain-rmse:1.668314\n",
      "[11945]\ttrain-rmse:1.668308\n",
      "[11946]\ttrain-rmse:1.668300\n",
      "[11947]\ttrain-rmse:1.668290\n",
      "[11948]\ttrain-rmse:1.668278\n",
      "[11949]\ttrain-rmse:1.668269\n",
      "[11950]\ttrain-rmse:1.668267\n",
      "[11951]\ttrain-rmse:1.668260\n",
      "[11952]\ttrain-rmse:1.668257\n",
      "[11953]\ttrain-rmse:1.668242\n",
      "[11954]\ttrain-rmse:1.668229\n",
      "[11955]\ttrain-rmse:1.668224\n",
      "[11956]\ttrain-rmse:1.668220\n",
      "[11957]\ttrain-rmse:1.668212\n",
      "[11958]\ttrain-rmse:1.668202\n",
      "[11959]\ttrain-rmse:1.668191\n",
      "[11960]\ttrain-rmse:1.668183\n",
      "[11961]\ttrain-rmse:1.668165\n",
      "[11962]\ttrain-rmse:1.668159\n",
      "[11963]\ttrain-rmse:1.668147\n",
      "[11964]\ttrain-rmse:1.668141\n",
      "[11965]\ttrain-rmse:1.668138\n",
      "[11966]\ttrain-rmse:1.668120\n",
      "[11967]\ttrain-rmse:1.668106\n",
      "[11968]\ttrain-rmse:1.668099\n",
      "[11969]\ttrain-rmse:1.668086\n",
      "[11970]\ttrain-rmse:1.668073\n",
      "[11971]\ttrain-rmse:1.668060\n",
      "[11972]\ttrain-rmse:1.668048\n",
      "[11973]\ttrain-rmse:1.668037\n",
      "[11974]\ttrain-rmse:1.668024\n",
      "[11975]\ttrain-rmse:1.668019\n",
      "[11976]\ttrain-rmse:1.668014\n",
      "[11977]\ttrain-rmse:1.668007\n",
      "[11978]\ttrain-rmse:1.667993\n",
      "[11979]\ttrain-rmse:1.667980\n",
      "[11980]\ttrain-rmse:1.667966\n",
      "[11981]\ttrain-rmse:1.667960\n",
      "[11982]\ttrain-rmse:1.667943\n",
      "[11983]\ttrain-rmse:1.667931\n",
      "[11984]\ttrain-rmse:1.667924\n",
      "[11985]\ttrain-rmse:1.667914\n",
      "[11986]\ttrain-rmse:1.667903\n",
      "[11987]\ttrain-rmse:1.667890\n",
      "[11988]\ttrain-rmse:1.667878\n",
      "[11989]\ttrain-rmse:1.667868\n",
      "[11990]\ttrain-rmse:1.667860\n",
      "[11991]\ttrain-rmse:1.667852\n",
      "[11992]\ttrain-rmse:1.667837\n",
      "[11993]\ttrain-rmse:1.667823\n",
      "[11994]\ttrain-rmse:1.667813\n",
      "[11995]\ttrain-rmse:1.667801\n",
      "[11996]\ttrain-rmse:1.667793\n",
      "[11997]\ttrain-rmse:1.667785\n",
      "[11998]\ttrain-rmse:1.667771\n",
      "[11999]\ttrain-rmse:1.667765\n",
      "[12000]\ttrain-rmse:1.667757\n",
      "[12001]\ttrain-rmse:1.667747\n",
      "[12002]\ttrain-rmse:1.667733\n",
      "[12003]\ttrain-rmse:1.667723\n",
      "[12004]\ttrain-rmse:1.667710\n",
      "[12005]\ttrain-rmse:1.667707\n",
      "[12006]\ttrain-rmse:1.667700\n",
      "[12007]\ttrain-rmse:1.667687\n",
      "[12008]\ttrain-rmse:1.667675\n",
      "[12009]\ttrain-rmse:1.667667\n",
      "[12010]\ttrain-rmse:1.667660\n",
      "[12011]\ttrain-rmse:1.667655\n",
      "[12012]\ttrain-rmse:1.667642\n",
      "[12013]\ttrain-rmse:1.667631\n",
      "[12014]\ttrain-rmse:1.667618\n",
      "[12015]\ttrain-rmse:1.667615\n",
      "[12016]\ttrain-rmse:1.667607\n",
      "[12017]\ttrain-rmse:1.667597\n",
      "[12018]\ttrain-rmse:1.667586\n",
      "[12019]\ttrain-rmse:1.667579\n",
      "[12020]\ttrain-rmse:1.667570\n",
      "[12021]\ttrain-rmse:1.667564\n",
      "[12022]\ttrain-rmse:1.667557\n",
      "[12023]\ttrain-rmse:1.667546\n",
      "[12024]\ttrain-rmse:1.667531\n",
      "[12025]\ttrain-rmse:1.667518\n",
      "[12026]\ttrain-rmse:1.667511\n",
      "[12027]\ttrain-rmse:1.667498\n",
      "[12028]\ttrain-rmse:1.667483\n",
      "[12029]\ttrain-rmse:1.667474\n",
      "[12030]\ttrain-rmse:1.667468\n",
      "[12031]\ttrain-rmse:1.667457\n",
      "[12032]\ttrain-rmse:1.667448\n",
      "[12033]\ttrain-rmse:1.667435\n",
      "[12034]\ttrain-rmse:1.667425\n",
      "[12035]\ttrain-rmse:1.667418\n",
      "[12036]\ttrain-rmse:1.667406\n",
      "[12037]\ttrain-rmse:1.667395\n",
      "[12038]\ttrain-rmse:1.667381\n",
      "[12039]\ttrain-rmse:1.667372\n",
      "[12040]\ttrain-rmse:1.667360\n",
      "[12041]\ttrain-rmse:1.667349\n",
      "[12042]\ttrain-rmse:1.667344\n",
      "[12043]\ttrain-rmse:1.667328\n",
      "[12044]\ttrain-rmse:1.667317\n",
      "[12045]\ttrain-rmse:1.667309\n",
      "[12046]\ttrain-rmse:1.667302\n",
      "[12047]\ttrain-rmse:1.667292\n",
      "[12048]\ttrain-rmse:1.667276\n",
      "[12049]\ttrain-rmse:1.667269\n",
      "[12050]\ttrain-rmse:1.667260\n",
      "[12051]\ttrain-rmse:1.667253\n",
      "[12052]\ttrain-rmse:1.667247\n",
      "[12053]\ttrain-rmse:1.667238\n",
      "[12054]\ttrain-rmse:1.667228\n",
      "[12055]\ttrain-rmse:1.667221\n",
      "[12056]\ttrain-rmse:1.667212\n",
      "[12057]\ttrain-rmse:1.667203\n",
      "[12058]\ttrain-rmse:1.667195\n",
      "[12059]\ttrain-rmse:1.667186\n",
      "[12060]\ttrain-rmse:1.667178\n",
      "[12061]\ttrain-rmse:1.667174\n",
      "[12062]\ttrain-rmse:1.667163\n",
      "[12063]\ttrain-rmse:1.667150\n",
      "[12064]\ttrain-rmse:1.667143\n",
      "[12065]\ttrain-rmse:1.667136\n",
      "[12066]\ttrain-rmse:1.667127\n",
      "[12067]\ttrain-rmse:1.667119\n",
      "[12068]\ttrain-rmse:1.667108\n",
      "[12069]\ttrain-rmse:1.667098\n",
      "[12070]\ttrain-rmse:1.667085\n",
      "[12071]\ttrain-rmse:1.667071\n",
      "[12072]\ttrain-rmse:1.667065\n",
      "[12073]\ttrain-rmse:1.667053\n",
      "[12074]\ttrain-rmse:1.667042\n",
      "[12075]\ttrain-rmse:1.667027\n",
      "[12076]\ttrain-rmse:1.667020\n",
      "[12077]\ttrain-rmse:1.667011\n",
      "[12078]\ttrain-rmse:1.667001\n",
      "[12079]\ttrain-rmse:1.666991\n",
      "[12080]\ttrain-rmse:1.666986\n",
      "[12081]\ttrain-rmse:1.666978\n",
      "[12082]\ttrain-rmse:1.666975\n",
      "[12083]\ttrain-rmse:1.666965\n",
      "[12084]\ttrain-rmse:1.666954\n",
      "[12085]\ttrain-rmse:1.666951\n",
      "[12086]\ttrain-rmse:1.666943\n",
      "[12087]\ttrain-rmse:1.666930\n",
      "[12088]\ttrain-rmse:1.666917\n",
      "[12089]\ttrain-rmse:1.666908\n",
      "[12090]\ttrain-rmse:1.666903\n",
      "[12091]\ttrain-rmse:1.666897\n",
      "[12092]\ttrain-rmse:1.666883\n",
      "[12093]\ttrain-rmse:1.666874\n",
      "[12094]\ttrain-rmse:1.666863\n",
      "[12095]\ttrain-rmse:1.666847\n",
      "[12096]\ttrain-rmse:1.666844\n",
      "[12097]\ttrain-rmse:1.666840\n",
      "[12098]\ttrain-rmse:1.666830\n",
      "[12099]\ttrain-rmse:1.666816\n",
      "[12100]\ttrain-rmse:1.666808\n",
      "[12101]\ttrain-rmse:1.666796\n",
      "[12102]\ttrain-rmse:1.666789\n",
      "[12103]\ttrain-rmse:1.666778\n",
      "[12104]\ttrain-rmse:1.666771\n",
      "[12105]\ttrain-rmse:1.666765\n",
      "[12106]\ttrain-rmse:1.666758\n",
      "[12107]\ttrain-rmse:1.666744\n",
      "[12108]\ttrain-rmse:1.666734\n",
      "[12109]\ttrain-rmse:1.666726\n",
      "[12110]\ttrain-rmse:1.666715\n",
      "[12111]\ttrain-rmse:1.666711\n",
      "[12112]\ttrain-rmse:1.666701\n",
      "[12113]\ttrain-rmse:1.666690\n",
      "[12114]\ttrain-rmse:1.666680\n",
      "[12115]\ttrain-rmse:1.666672\n",
      "[12116]\ttrain-rmse:1.666662\n",
      "[12117]\ttrain-rmse:1.666653\n",
      "[12118]\ttrain-rmse:1.666641\n",
      "[12119]\ttrain-rmse:1.666632\n",
      "[12120]\ttrain-rmse:1.666626\n",
      "[12121]\ttrain-rmse:1.666618\n",
      "[12122]\ttrain-rmse:1.666610\n",
      "[12123]\ttrain-rmse:1.666599\n",
      "[12124]\ttrain-rmse:1.666590\n",
      "[12125]\ttrain-rmse:1.666582\n",
      "[12126]\ttrain-rmse:1.666572\n",
      "[12127]\ttrain-rmse:1.666563\n",
      "[12128]\ttrain-rmse:1.666556\n",
      "[12129]\ttrain-rmse:1.666548\n",
      "[12130]\ttrain-rmse:1.666541\n",
      "[12131]\ttrain-rmse:1.666533\n",
      "[12132]\ttrain-rmse:1.666522\n",
      "[12133]\ttrain-rmse:1.666516\n",
      "[12134]\ttrain-rmse:1.666510\n",
      "[12135]\ttrain-rmse:1.666502\n",
      "[12136]\ttrain-rmse:1.666497\n",
      "[12137]\ttrain-rmse:1.666484\n",
      "[12138]\ttrain-rmse:1.666471\n",
      "[12139]\ttrain-rmse:1.666465\n",
      "[12140]\ttrain-rmse:1.666456\n",
      "[12141]\ttrain-rmse:1.666451\n",
      "[12142]\ttrain-rmse:1.666443\n",
      "[12143]\ttrain-rmse:1.666435\n",
      "[12144]\ttrain-rmse:1.666428\n",
      "[12145]\ttrain-rmse:1.666420\n",
      "[12146]\ttrain-rmse:1.666412\n",
      "[12147]\ttrain-rmse:1.666407\n",
      "[12148]\ttrain-rmse:1.666398\n",
      "[12149]\ttrain-rmse:1.666385\n",
      "[12150]\ttrain-rmse:1.666373\n",
      "[12151]\ttrain-rmse:1.666363\n",
      "[12152]\ttrain-rmse:1.666357\n",
      "[12153]\ttrain-rmse:1.666350\n",
      "[12154]\ttrain-rmse:1.666340\n",
      "[12155]\ttrain-rmse:1.666330\n",
      "[12156]\ttrain-rmse:1.666322\n",
      "[12157]\ttrain-rmse:1.666315\n",
      "[12158]\ttrain-rmse:1.666309\n",
      "[12159]\ttrain-rmse:1.666303\n",
      "[12160]\ttrain-rmse:1.666294\n",
      "[12161]\ttrain-rmse:1.666279\n",
      "[12162]\ttrain-rmse:1.666271\n",
      "[12163]\ttrain-rmse:1.666265\n",
      "[12164]\ttrain-rmse:1.666254\n",
      "[12165]\ttrain-rmse:1.666239\n",
      "[12166]\ttrain-rmse:1.666231\n",
      "[12167]\ttrain-rmse:1.666222\n",
      "[12168]\ttrain-rmse:1.666211\n",
      "[12169]\ttrain-rmse:1.666200\n",
      "[12170]\ttrain-rmse:1.666192\n",
      "[12171]\ttrain-rmse:1.666183\n",
      "[12172]\ttrain-rmse:1.666172\n",
      "[12173]\ttrain-rmse:1.666162\n",
      "[12174]\ttrain-rmse:1.666152\n",
      "[12175]\ttrain-rmse:1.666137\n",
      "[12176]\ttrain-rmse:1.666130\n",
      "[12177]\ttrain-rmse:1.666124\n",
      "[12178]\ttrain-rmse:1.666116\n",
      "[12179]\ttrain-rmse:1.666110\n",
      "[12180]\ttrain-rmse:1.666098\n",
      "[12181]\ttrain-rmse:1.666088\n",
      "[12182]\ttrain-rmse:1.666079\n",
      "[12183]\ttrain-rmse:1.666073\n",
      "[12184]\ttrain-rmse:1.666062\n",
      "[12185]\ttrain-rmse:1.666056\n",
      "[12186]\ttrain-rmse:1.666049\n",
      "[12187]\ttrain-rmse:1.666041\n",
      "[12188]\ttrain-rmse:1.666035\n",
      "[12189]\ttrain-rmse:1.666023\n",
      "[12190]\ttrain-rmse:1.666012\n",
      "[12191]\ttrain-rmse:1.666001\n",
      "[12192]\ttrain-rmse:1.665989\n",
      "[12193]\ttrain-rmse:1.665980\n",
      "[12194]\ttrain-rmse:1.665974\n",
      "[12195]\ttrain-rmse:1.665966\n",
      "[12196]\ttrain-rmse:1.665954\n",
      "[12197]\ttrain-rmse:1.665947\n",
      "[12198]\ttrain-rmse:1.665940\n",
      "[12199]\ttrain-rmse:1.665927\n",
      "[12200]\ttrain-rmse:1.665914\n",
      "[12201]\ttrain-rmse:1.665904\n",
      "[12202]\ttrain-rmse:1.665895\n",
      "[12203]\ttrain-rmse:1.665891\n",
      "[12204]\ttrain-rmse:1.665883\n",
      "[12205]\ttrain-rmse:1.665869\n",
      "[12206]\ttrain-rmse:1.665861\n",
      "[12207]\ttrain-rmse:1.665852\n",
      "[12208]\ttrain-rmse:1.665845\n",
      "[12209]\ttrain-rmse:1.665838\n",
      "[12210]\ttrain-rmse:1.665830\n",
      "[12211]\ttrain-rmse:1.665822\n",
      "[12212]\ttrain-rmse:1.665816\n",
      "[12213]\ttrain-rmse:1.665814\n",
      "[12214]\ttrain-rmse:1.665811\n",
      "[12215]\ttrain-rmse:1.665806\n",
      "[12216]\ttrain-rmse:1.665802\n",
      "[12217]\ttrain-rmse:1.665795\n",
      "[12218]\ttrain-rmse:1.665783\n",
      "[12219]\ttrain-rmse:1.665778\n",
      "[12220]\ttrain-rmse:1.665767\n",
      "[12221]\ttrain-rmse:1.665758\n",
      "[12222]\ttrain-rmse:1.665746\n",
      "[12223]\ttrain-rmse:1.665740\n",
      "[12224]\ttrain-rmse:1.665730\n",
      "[12225]\ttrain-rmse:1.665726\n",
      "[12226]\ttrain-rmse:1.665714\n",
      "[12227]\ttrain-rmse:1.665706\n",
      "[12228]\ttrain-rmse:1.665698\n",
      "[12229]\ttrain-rmse:1.665688\n",
      "[12230]\ttrain-rmse:1.665678\n",
      "[12231]\ttrain-rmse:1.665669\n",
      "[12232]\ttrain-rmse:1.665657\n",
      "[12233]\ttrain-rmse:1.665648\n",
      "[12234]\ttrain-rmse:1.665637\n",
      "[12235]\ttrain-rmse:1.665630\n",
      "[12236]\ttrain-rmse:1.665623\n",
      "[12237]\ttrain-rmse:1.665615\n",
      "[12238]\ttrain-rmse:1.665602\n",
      "[12239]\ttrain-rmse:1.665592\n",
      "[12240]\ttrain-rmse:1.665587\n",
      "[12241]\ttrain-rmse:1.665577\n",
      "[12242]\ttrain-rmse:1.665572\n",
      "[12243]\ttrain-rmse:1.665562\n",
      "[12244]\ttrain-rmse:1.665549\n",
      "[12245]\ttrain-rmse:1.665539\n",
      "[12246]\ttrain-rmse:1.665521\n",
      "[12247]\ttrain-rmse:1.665509\n",
      "[12248]\ttrain-rmse:1.665496\n",
      "[12249]\ttrain-rmse:1.665490\n",
      "[12250]\ttrain-rmse:1.665482\n",
      "[12251]\ttrain-rmse:1.665475\n",
      "[12252]\ttrain-rmse:1.665464\n",
      "[12253]\ttrain-rmse:1.665451\n",
      "[12254]\ttrain-rmse:1.665446\n",
      "[12255]\ttrain-rmse:1.665437\n",
      "[12256]\ttrain-rmse:1.665429\n",
      "[12257]\ttrain-rmse:1.665421\n",
      "[12258]\ttrain-rmse:1.665411\n",
      "[12259]\ttrain-rmse:1.665405\n",
      "[12260]\ttrain-rmse:1.665397\n",
      "[12261]\ttrain-rmse:1.665383\n",
      "[12262]\ttrain-rmse:1.665374\n",
      "[12263]\ttrain-rmse:1.665362\n",
      "[12264]\ttrain-rmse:1.665356\n",
      "[12265]\ttrain-rmse:1.665350\n",
      "[12266]\ttrain-rmse:1.665343\n",
      "[12267]\ttrain-rmse:1.665337\n",
      "[12268]\ttrain-rmse:1.665327\n",
      "[12269]\ttrain-rmse:1.665321\n",
      "[12270]\ttrain-rmse:1.665313\n",
      "[12271]\ttrain-rmse:1.665307\n",
      "[12272]\ttrain-rmse:1.665300\n",
      "[12273]\ttrain-rmse:1.665289\n",
      "[12274]\ttrain-rmse:1.665279\n",
      "[12275]\ttrain-rmse:1.665270\n",
      "[12276]\ttrain-rmse:1.665261\n",
      "[12277]\ttrain-rmse:1.665251\n",
      "[12278]\ttrain-rmse:1.665240\n",
      "[12279]\ttrain-rmse:1.665234\n",
      "[12280]\ttrain-rmse:1.665226\n",
      "[12281]\ttrain-rmse:1.665214\n",
      "[12282]\ttrain-rmse:1.665208\n",
      "[12283]\ttrain-rmse:1.665196\n",
      "[12284]\ttrain-rmse:1.665189\n",
      "[12285]\ttrain-rmse:1.665183\n",
      "[12286]\ttrain-rmse:1.665173\n",
      "[12287]\ttrain-rmse:1.665170\n",
      "[12288]\ttrain-rmse:1.665161\n",
      "[12289]\ttrain-rmse:1.665146\n",
      "[12290]\ttrain-rmse:1.665136\n",
      "[12291]\ttrain-rmse:1.665123\n",
      "[12292]\ttrain-rmse:1.665109\n",
      "[12293]\ttrain-rmse:1.665104\n",
      "[12294]\ttrain-rmse:1.665090\n",
      "[12295]\ttrain-rmse:1.665080\n",
      "[12296]\ttrain-rmse:1.665075\n",
      "[12297]\ttrain-rmse:1.665066\n",
      "[12298]\ttrain-rmse:1.665058\n",
      "[12299]\ttrain-rmse:1.665054\n",
      "[12300]\ttrain-rmse:1.665045\n",
      "[12301]\ttrain-rmse:1.665037\n",
      "[12302]\ttrain-rmse:1.665026\n",
      "[12303]\ttrain-rmse:1.665013\n",
      "[12304]\ttrain-rmse:1.665006\n",
      "[12305]\ttrain-rmse:1.665003\n",
      "[12306]\ttrain-rmse:1.664996\n",
      "[12307]\ttrain-rmse:1.664980\n",
      "[12308]\ttrain-rmse:1.664972\n",
      "[12309]\ttrain-rmse:1.664962\n",
      "[12310]\ttrain-rmse:1.664956\n",
      "[12311]\ttrain-rmse:1.664944\n",
      "[12312]\ttrain-rmse:1.664932\n",
      "[12313]\ttrain-rmse:1.664925\n",
      "[12314]\ttrain-rmse:1.664920\n",
      "[12315]\ttrain-rmse:1.664913\n",
      "[12316]\ttrain-rmse:1.664905\n",
      "[12317]\ttrain-rmse:1.664897\n",
      "[12318]\ttrain-rmse:1.664884\n",
      "[12319]\ttrain-rmse:1.664875\n",
      "[12320]\ttrain-rmse:1.664860\n",
      "[12321]\ttrain-rmse:1.664852\n",
      "[12322]\ttrain-rmse:1.664843\n",
      "[12323]\ttrain-rmse:1.664830\n",
      "[12324]\ttrain-rmse:1.664816\n",
      "[12325]\ttrain-rmse:1.664809\n",
      "[12326]\ttrain-rmse:1.664797\n",
      "[12327]\ttrain-rmse:1.664788\n",
      "[12328]\ttrain-rmse:1.664777\n",
      "[12329]\ttrain-rmse:1.664766\n",
      "[12330]\ttrain-rmse:1.664758\n",
      "[12331]\ttrain-rmse:1.664752\n",
      "[12332]\ttrain-rmse:1.664746\n",
      "[12333]\ttrain-rmse:1.664736\n",
      "[12334]\ttrain-rmse:1.664723\n",
      "[12335]\ttrain-rmse:1.664718\n",
      "[12336]\ttrain-rmse:1.664711\n",
      "[12337]\ttrain-rmse:1.664709\n",
      "[12338]\ttrain-rmse:1.664696\n",
      "[12339]\ttrain-rmse:1.664685\n",
      "[12340]\ttrain-rmse:1.664677\n",
      "[12341]\ttrain-rmse:1.664672\n",
      "[12342]\ttrain-rmse:1.664660\n",
      "[12343]\ttrain-rmse:1.664651\n",
      "[12344]\ttrain-rmse:1.664642\n",
      "[12345]\ttrain-rmse:1.664635\n",
      "[12346]\ttrain-rmse:1.664623\n",
      "[12347]\ttrain-rmse:1.664616\n",
      "[12348]\ttrain-rmse:1.664609\n",
      "[12349]\ttrain-rmse:1.664599\n",
      "[12350]\ttrain-rmse:1.664589\n",
      "[12351]\ttrain-rmse:1.664579\n",
      "[12352]\ttrain-rmse:1.664568\n",
      "[12353]\ttrain-rmse:1.664559\n",
      "[12354]\ttrain-rmse:1.664551\n",
      "[12355]\ttrain-rmse:1.664540\n",
      "[12356]\ttrain-rmse:1.664525\n",
      "[12357]\ttrain-rmse:1.664519\n",
      "[12358]\ttrain-rmse:1.664511\n",
      "[12359]\ttrain-rmse:1.664503\n",
      "[12360]\ttrain-rmse:1.664492\n",
      "[12361]\ttrain-rmse:1.664483\n",
      "[12362]\ttrain-rmse:1.664475\n",
      "[12363]\ttrain-rmse:1.664468\n",
      "[12364]\ttrain-rmse:1.664460\n",
      "[12365]\ttrain-rmse:1.664456\n",
      "[12366]\ttrain-rmse:1.664454\n",
      "[12367]\ttrain-rmse:1.664448\n",
      "[12368]\ttrain-rmse:1.664436\n",
      "[12369]\ttrain-rmse:1.664424\n",
      "[12370]\ttrain-rmse:1.664414\n",
      "[12371]\ttrain-rmse:1.664410\n",
      "[12372]\ttrain-rmse:1.664396\n",
      "[12373]\ttrain-rmse:1.664388\n",
      "[12374]\ttrain-rmse:1.664380\n",
      "[12375]\ttrain-rmse:1.664373\n",
      "[12376]\ttrain-rmse:1.664365\n",
      "[12377]\ttrain-rmse:1.664354\n",
      "[12378]\ttrain-rmse:1.664347\n",
      "[12379]\ttrain-rmse:1.664336\n",
      "[12380]\ttrain-rmse:1.664328\n",
      "[12381]\ttrain-rmse:1.664319\n",
      "[12382]\ttrain-rmse:1.664316\n",
      "[12383]\ttrain-rmse:1.664309\n",
      "[12384]\ttrain-rmse:1.664302\n",
      "[12385]\ttrain-rmse:1.664287\n",
      "[12386]\ttrain-rmse:1.664277\n",
      "[12387]\ttrain-rmse:1.664270\n",
      "[12388]\ttrain-rmse:1.664259\n",
      "[12389]\ttrain-rmse:1.664251\n",
      "[12390]\ttrain-rmse:1.664242\n",
      "[12391]\ttrain-rmse:1.664231\n",
      "[12392]\ttrain-rmse:1.664222\n",
      "[12393]\ttrain-rmse:1.664216\n",
      "[12394]\ttrain-rmse:1.664204\n",
      "[12395]\ttrain-rmse:1.664196\n",
      "[12396]\ttrain-rmse:1.664184\n",
      "[12397]\ttrain-rmse:1.664177\n",
      "[12398]\ttrain-rmse:1.664171\n",
      "[12399]\ttrain-rmse:1.664155\n",
      "[12400]\ttrain-rmse:1.664151\n",
      "[12401]\ttrain-rmse:1.664143\n",
      "[12402]\ttrain-rmse:1.664131\n",
      "[12403]\ttrain-rmse:1.664118\n",
      "[12404]\ttrain-rmse:1.664108\n",
      "[12405]\ttrain-rmse:1.664102\n",
      "[12406]\ttrain-rmse:1.664093\n",
      "[12407]\ttrain-rmse:1.664088\n",
      "[12408]\ttrain-rmse:1.664075\n",
      "[12409]\ttrain-rmse:1.664064\n",
      "[12410]\ttrain-rmse:1.664053\n",
      "[12411]\ttrain-rmse:1.664041\n",
      "[12412]\ttrain-rmse:1.664034\n",
      "[12413]\ttrain-rmse:1.664022\n",
      "[12414]\ttrain-rmse:1.664012\n",
      "[12415]\ttrain-rmse:1.664007\n",
      "[12416]\ttrain-rmse:1.664002\n",
      "[12417]\ttrain-rmse:1.663996\n",
      "[12418]\ttrain-rmse:1.663987\n",
      "[12419]\ttrain-rmse:1.663975\n",
      "[12420]\ttrain-rmse:1.663969\n",
      "[12421]\ttrain-rmse:1.663959\n",
      "[12422]\ttrain-rmse:1.663952\n",
      "[12423]\ttrain-rmse:1.663945\n",
      "[12424]\ttrain-rmse:1.663934\n",
      "[12425]\ttrain-rmse:1.663924\n",
      "[12426]\ttrain-rmse:1.663916\n",
      "[12427]\ttrain-rmse:1.663909\n",
      "[12428]\ttrain-rmse:1.663897\n",
      "[12429]\ttrain-rmse:1.663887\n",
      "[12430]\ttrain-rmse:1.663883\n",
      "[12431]\ttrain-rmse:1.663868\n",
      "[12432]\ttrain-rmse:1.663860\n",
      "[12433]\ttrain-rmse:1.663850\n",
      "[12434]\ttrain-rmse:1.663842\n",
      "[12435]\ttrain-rmse:1.663834\n",
      "[12436]\ttrain-rmse:1.663823\n",
      "[12437]\ttrain-rmse:1.663810\n",
      "[12438]\ttrain-rmse:1.663807\n",
      "[12439]\ttrain-rmse:1.663798\n",
      "[12440]\ttrain-rmse:1.663788\n",
      "[12441]\ttrain-rmse:1.663782\n",
      "[12442]\ttrain-rmse:1.663771\n",
      "[12443]\ttrain-rmse:1.663762\n",
      "[12444]\ttrain-rmse:1.663754\n",
      "[12445]\ttrain-rmse:1.663745\n",
      "[12446]\ttrain-rmse:1.663726\n",
      "[12447]\ttrain-rmse:1.663720\n",
      "[12448]\ttrain-rmse:1.663714\n",
      "[12449]\ttrain-rmse:1.663706\n",
      "[12450]\ttrain-rmse:1.663697\n",
      "[12451]\ttrain-rmse:1.663692\n",
      "[12452]\ttrain-rmse:1.663682\n",
      "[12453]\ttrain-rmse:1.663672\n",
      "[12454]\ttrain-rmse:1.663662\n",
      "[12455]\ttrain-rmse:1.663655\n",
      "[12456]\ttrain-rmse:1.663647\n",
      "[12457]\ttrain-rmse:1.663639\n",
      "[12458]\ttrain-rmse:1.663629\n",
      "[12459]\ttrain-rmse:1.663626\n",
      "[12460]\ttrain-rmse:1.663616\n",
      "[12461]\ttrain-rmse:1.663612\n",
      "[12462]\ttrain-rmse:1.663605\n",
      "[12463]\ttrain-rmse:1.663601\n",
      "[12464]\ttrain-rmse:1.663586\n",
      "[12465]\ttrain-rmse:1.663578\n",
      "[12466]\ttrain-rmse:1.663568\n",
      "[12467]\ttrain-rmse:1.663554\n",
      "[12468]\ttrain-rmse:1.663540\n",
      "[12469]\ttrain-rmse:1.663533\n",
      "[12470]\ttrain-rmse:1.663529\n",
      "[12471]\ttrain-rmse:1.663517\n",
      "[12472]\ttrain-rmse:1.663504\n",
      "[12473]\ttrain-rmse:1.663493\n",
      "[12474]\ttrain-rmse:1.663483\n",
      "[12475]\ttrain-rmse:1.663479\n",
      "[12476]\ttrain-rmse:1.663472\n",
      "[12477]\ttrain-rmse:1.663461\n",
      "[12478]\ttrain-rmse:1.663449\n",
      "[12479]\ttrain-rmse:1.663433\n",
      "[12480]\ttrain-rmse:1.663428\n",
      "[12481]\ttrain-rmse:1.663420\n",
      "[12482]\ttrain-rmse:1.663411\n",
      "[12483]\ttrain-rmse:1.663406\n",
      "[12484]\ttrain-rmse:1.663396\n",
      "[12485]\ttrain-rmse:1.663389\n",
      "[12486]\ttrain-rmse:1.663381\n",
      "[12487]\ttrain-rmse:1.663371\n",
      "[12488]\ttrain-rmse:1.663365\n",
      "[12489]\ttrain-rmse:1.663349\n",
      "[12490]\ttrain-rmse:1.663337\n",
      "[12491]\ttrain-rmse:1.663324\n",
      "[12492]\ttrain-rmse:1.663314\n",
      "[12493]\ttrain-rmse:1.663310\n",
      "[12494]\ttrain-rmse:1.663298\n",
      "[12495]\ttrain-rmse:1.663291\n",
      "[12496]\ttrain-rmse:1.663285\n",
      "[12497]\ttrain-rmse:1.663281\n",
      "[12498]\ttrain-rmse:1.663274\n",
      "[12499]\ttrain-rmse:1.663270\n",
      "[12500]\ttrain-rmse:1.663261\n",
      "[12501]\ttrain-rmse:1.663250\n",
      "[12502]\ttrain-rmse:1.663233\n",
      "[12503]\ttrain-rmse:1.663220\n",
      "[12504]\ttrain-rmse:1.663215\n",
      "[12505]\ttrain-rmse:1.663206\n",
      "[12506]\ttrain-rmse:1.663198\n",
      "[12507]\ttrain-rmse:1.663188\n",
      "[12508]\ttrain-rmse:1.663179\n",
      "[12509]\ttrain-rmse:1.663162\n",
      "[12510]\ttrain-rmse:1.663159\n",
      "[12511]\ttrain-rmse:1.663154\n",
      "[12512]\ttrain-rmse:1.663144\n",
      "[12513]\ttrain-rmse:1.663135\n",
      "[12514]\ttrain-rmse:1.663131\n",
      "[12515]\ttrain-rmse:1.663126\n",
      "[12516]\ttrain-rmse:1.663116\n",
      "[12517]\ttrain-rmse:1.663109\n",
      "[12518]\ttrain-rmse:1.663100\n",
      "[12519]\ttrain-rmse:1.663093\n",
      "[12520]\ttrain-rmse:1.663085\n",
      "[12521]\ttrain-rmse:1.663075\n",
      "[12522]\ttrain-rmse:1.663062\n",
      "[12523]\ttrain-rmse:1.663050\n",
      "[12524]\ttrain-rmse:1.663034\n",
      "[12525]\ttrain-rmse:1.663029\n",
      "[12526]\ttrain-rmse:1.663015\n",
      "[12527]\ttrain-rmse:1.663008\n",
      "[12528]\ttrain-rmse:1.662992\n",
      "[12529]\ttrain-rmse:1.662986\n",
      "[12530]\ttrain-rmse:1.662976\n",
      "[12531]\ttrain-rmse:1.662963\n",
      "[12532]\ttrain-rmse:1.662957\n",
      "[12533]\ttrain-rmse:1.662946\n",
      "[12534]\ttrain-rmse:1.662935\n",
      "[12535]\ttrain-rmse:1.662925\n",
      "[12536]\ttrain-rmse:1.662918\n",
      "[12537]\ttrain-rmse:1.662906\n",
      "[12538]\ttrain-rmse:1.662900\n",
      "[12539]\ttrain-rmse:1.662886\n",
      "[12540]\ttrain-rmse:1.662875\n",
      "[12541]\ttrain-rmse:1.662867\n",
      "[12542]\ttrain-rmse:1.662859\n",
      "[12543]\ttrain-rmse:1.662845\n",
      "[12544]\ttrain-rmse:1.662836\n",
      "[12545]\ttrain-rmse:1.662826\n",
      "[12546]\ttrain-rmse:1.662815\n",
      "[12547]\ttrain-rmse:1.662800\n",
      "[12548]\ttrain-rmse:1.662786\n",
      "[12549]\ttrain-rmse:1.662776\n",
      "[12550]\ttrain-rmse:1.662763\n",
      "[12551]\ttrain-rmse:1.662758\n",
      "[12552]\ttrain-rmse:1.662750\n",
      "[12553]\ttrain-rmse:1.662742\n",
      "[12554]\ttrain-rmse:1.662736\n",
      "[12555]\ttrain-rmse:1.662723\n",
      "[12556]\ttrain-rmse:1.662719\n",
      "[12557]\ttrain-rmse:1.662709\n",
      "[12558]\ttrain-rmse:1.662703\n",
      "[12559]\ttrain-rmse:1.662700\n",
      "[12560]\ttrain-rmse:1.662694\n",
      "[12561]\ttrain-rmse:1.662685\n",
      "[12562]\ttrain-rmse:1.662677\n",
      "[12563]\ttrain-rmse:1.662661\n",
      "[12564]\ttrain-rmse:1.662652\n",
      "[12565]\ttrain-rmse:1.662641\n",
      "[12566]\ttrain-rmse:1.662635\n",
      "[12567]\ttrain-rmse:1.662622\n",
      "[12568]\ttrain-rmse:1.662614\n",
      "[12569]\ttrain-rmse:1.662605\n",
      "[12570]\ttrain-rmse:1.662594\n",
      "[12571]\ttrain-rmse:1.662584\n",
      "[12572]\ttrain-rmse:1.662575\n",
      "[12573]\ttrain-rmse:1.662568\n",
      "[12574]\ttrain-rmse:1.662563\n",
      "[12575]\ttrain-rmse:1.662557\n",
      "[12576]\ttrain-rmse:1.662549\n",
      "[12577]\ttrain-rmse:1.662541\n",
      "[12578]\ttrain-rmse:1.662535\n",
      "[12579]\ttrain-rmse:1.662530\n",
      "[12580]\ttrain-rmse:1.662526\n",
      "[12581]\ttrain-rmse:1.662521\n",
      "[12582]\ttrain-rmse:1.662509\n",
      "[12583]\ttrain-rmse:1.662502\n",
      "[12584]\ttrain-rmse:1.662498\n",
      "[12585]\ttrain-rmse:1.662489\n",
      "[12586]\ttrain-rmse:1.662481\n",
      "[12587]\ttrain-rmse:1.662475\n",
      "[12588]\ttrain-rmse:1.662464\n",
      "[12589]\ttrain-rmse:1.662454\n",
      "[12590]\ttrain-rmse:1.662445\n",
      "[12591]\ttrain-rmse:1.662435\n",
      "[12592]\ttrain-rmse:1.662426\n",
      "[12593]\ttrain-rmse:1.662421\n",
      "[12594]\ttrain-rmse:1.662415\n",
      "[12595]\ttrain-rmse:1.662405\n",
      "[12596]\ttrain-rmse:1.662398\n",
      "[12597]\ttrain-rmse:1.662388\n",
      "[12598]\ttrain-rmse:1.662380\n",
      "[12599]\ttrain-rmse:1.662367\n",
      "[12600]\ttrain-rmse:1.662359\n",
      "[12601]\ttrain-rmse:1.662353\n",
      "[12602]\ttrain-rmse:1.662341\n",
      "[12603]\ttrain-rmse:1.662327\n",
      "[12604]\ttrain-rmse:1.662308\n",
      "[12605]\ttrain-rmse:1.662302\n",
      "[12606]\ttrain-rmse:1.662294\n",
      "[12607]\ttrain-rmse:1.662285\n",
      "[12608]\ttrain-rmse:1.662281\n",
      "[12609]\ttrain-rmse:1.662266\n",
      "[12610]\ttrain-rmse:1.662259\n",
      "[12611]\ttrain-rmse:1.662256\n",
      "[12612]\ttrain-rmse:1.662250\n",
      "[12613]\ttrain-rmse:1.662238\n",
      "[12614]\ttrain-rmse:1.662225\n",
      "[12615]\ttrain-rmse:1.662215\n",
      "[12616]\ttrain-rmse:1.662208\n",
      "[12617]\ttrain-rmse:1.662202\n",
      "[12618]\ttrain-rmse:1.662195\n",
      "[12619]\ttrain-rmse:1.662184\n",
      "[12620]\ttrain-rmse:1.662175\n",
      "[12621]\ttrain-rmse:1.662170\n",
      "[12622]\ttrain-rmse:1.662163\n",
      "[12623]\ttrain-rmse:1.662148\n",
      "[12624]\ttrain-rmse:1.662142\n",
      "[12625]\ttrain-rmse:1.662136\n",
      "[12626]\ttrain-rmse:1.662125\n",
      "[12627]\ttrain-rmse:1.662119\n",
      "[12628]\ttrain-rmse:1.662115\n",
      "[12629]\ttrain-rmse:1.662108\n",
      "[12630]\ttrain-rmse:1.662101\n",
      "[12631]\ttrain-rmse:1.662093\n",
      "[12632]\ttrain-rmse:1.662089\n",
      "[12633]\ttrain-rmse:1.662080\n",
      "[12634]\ttrain-rmse:1.662075\n",
      "[12635]\ttrain-rmse:1.662070\n",
      "[12636]\ttrain-rmse:1.662061\n",
      "[12637]\ttrain-rmse:1.662057\n",
      "[12638]\ttrain-rmse:1.662046\n",
      "[12639]\ttrain-rmse:1.662033\n",
      "[12640]\ttrain-rmse:1.662024\n",
      "[12641]\ttrain-rmse:1.662010\n",
      "[12642]\ttrain-rmse:1.661994\n",
      "[12643]\ttrain-rmse:1.661992\n",
      "[12644]\ttrain-rmse:1.661986\n",
      "[12645]\ttrain-rmse:1.661982\n",
      "[12646]\ttrain-rmse:1.661973\n",
      "[12647]\ttrain-rmse:1.661961\n",
      "[12648]\ttrain-rmse:1.661950\n",
      "[12649]\ttrain-rmse:1.661938\n",
      "[12650]\ttrain-rmse:1.661928\n",
      "[12651]\ttrain-rmse:1.661921\n",
      "[12652]\ttrain-rmse:1.661910\n",
      "[12653]\ttrain-rmse:1.661898\n",
      "[12654]\ttrain-rmse:1.661889\n",
      "[12655]\ttrain-rmse:1.661876\n",
      "[12656]\ttrain-rmse:1.661867\n",
      "[12657]\ttrain-rmse:1.661862\n",
      "[12658]\ttrain-rmse:1.661853\n",
      "[12659]\ttrain-rmse:1.661843\n",
      "[12660]\ttrain-rmse:1.661836\n",
      "[12661]\ttrain-rmse:1.661825\n",
      "[12662]\ttrain-rmse:1.661814\n",
      "[12663]\ttrain-rmse:1.661806\n",
      "[12664]\ttrain-rmse:1.661795\n",
      "[12665]\ttrain-rmse:1.661791\n",
      "[12666]\ttrain-rmse:1.661782\n",
      "[12667]\ttrain-rmse:1.661772\n",
      "[12668]\ttrain-rmse:1.661762\n",
      "[12669]\ttrain-rmse:1.661752\n",
      "[12670]\ttrain-rmse:1.661745\n",
      "[12671]\ttrain-rmse:1.661736\n",
      "[12672]\ttrain-rmse:1.661725\n",
      "[12673]\ttrain-rmse:1.661719\n",
      "[12674]\ttrain-rmse:1.661714\n",
      "[12675]\ttrain-rmse:1.661709\n",
      "[12676]\ttrain-rmse:1.661695\n",
      "[12677]\ttrain-rmse:1.661683\n",
      "[12678]\ttrain-rmse:1.661672\n",
      "[12679]\ttrain-rmse:1.661666\n",
      "[12680]\ttrain-rmse:1.661656\n",
      "[12681]\ttrain-rmse:1.661648\n",
      "[12682]\ttrain-rmse:1.661639\n",
      "[12683]\ttrain-rmse:1.661634\n",
      "[12684]\ttrain-rmse:1.661623\n",
      "[12685]\ttrain-rmse:1.661610\n",
      "[12686]\ttrain-rmse:1.661600\n",
      "[12687]\ttrain-rmse:1.661590\n",
      "[12688]\ttrain-rmse:1.661587\n",
      "[12689]\ttrain-rmse:1.661581\n",
      "[12690]\ttrain-rmse:1.661570\n",
      "[12691]\ttrain-rmse:1.661558\n",
      "[12692]\ttrain-rmse:1.661544\n",
      "[12693]\ttrain-rmse:1.661531\n",
      "[12694]\ttrain-rmse:1.661527\n",
      "[12695]\ttrain-rmse:1.661521\n",
      "[12696]\ttrain-rmse:1.661507\n",
      "[12697]\ttrain-rmse:1.661498\n",
      "[12698]\ttrain-rmse:1.661490\n",
      "[12699]\ttrain-rmse:1.661480\n",
      "[12700]\ttrain-rmse:1.661471\n",
      "[12701]\ttrain-rmse:1.661463\n",
      "[12702]\ttrain-rmse:1.661453\n",
      "[12703]\ttrain-rmse:1.661446\n",
      "[12704]\ttrain-rmse:1.661441\n",
      "[12705]\ttrain-rmse:1.661430\n",
      "[12706]\ttrain-rmse:1.661425\n",
      "[12707]\ttrain-rmse:1.661417\n",
      "[12708]\ttrain-rmse:1.661404\n",
      "[12709]\ttrain-rmse:1.661391\n",
      "[12710]\ttrain-rmse:1.661378\n",
      "[12711]\ttrain-rmse:1.661366\n",
      "[12712]\ttrain-rmse:1.661361\n",
      "[12713]\ttrain-rmse:1.661350\n",
      "[12714]\ttrain-rmse:1.661338\n",
      "[12715]\ttrain-rmse:1.661328\n",
      "[12716]\ttrain-rmse:1.661319\n",
      "[12717]\ttrain-rmse:1.661308\n",
      "[12718]\ttrain-rmse:1.661301\n",
      "[12719]\ttrain-rmse:1.661290\n",
      "[12720]\ttrain-rmse:1.661279\n",
      "[12721]\ttrain-rmse:1.661273\n",
      "[12722]\ttrain-rmse:1.661263\n",
      "[12723]\ttrain-rmse:1.661253\n",
      "[12724]\ttrain-rmse:1.661247\n",
      "[12725]\ttrain-rmse:1.661237\n",
      "[12726]\ttrain-rmse:1.661221\n",
      "[12727]\ttrain-rmse:1.661211\n",
      "[12728]\ttrain-rmse:1.661206\n",
      "[12729]\ttrain-rmse:1.661202\n",
      "[12730]\ttrain-rmse:1.661190\n",
      "[12731]\ttrain-rmse:1.661186\n",
      "[12732]\ttrain-rmse:1.661176\n",
      "[12733]\ttrain-rmse:1.661169\n",
      "[12734]\ttrain-rmse:1.661168\n",
      "[12735]\ttrain-rmse:1.661158\n",
      "[12736]\ttrain-rmse:1.661149\n",
      "[12737]\ttrain-rmse:1.661140\n",
      "[12738]\ttrain-rmse:1.661133\n",
      "[12739]\ttrain-rmse:1.661125\n",
      "[12740]\ttrain-rmse:1.661118\n",
      "[12741]\ttrain-rmse:1.661111\n",
      "[12742]\ttrain-rmse:1.661102\n",
      "[12743]\ttrain-rmse:1.661097\n",
      "[12744]\ttrain-rmse:1.661087\n",
      "[12745]\ttrain-rmse:1.661084\n",
      "[12746]\ttrain-rmse:1.661072\n",
      "[12747]\ttrain-rmse:1.661059\n",
      "[12748]\ttrain-rmse:1.661051\n",
      "[12749]\ttrain-rmse:1.661047\n",
      "[12750]\ttrain-rmse:1.661041\n",
      "[12751]\ttrain-rmse:1.661028\n",
      "[12752]\ttrain-rmse:1.661014\n",
      "[12753]\ttrain-rmse:1.661004\n",
      "[12754]\ttrain-rmse:1.660994\n",
      "[12755]\ttrain-rmse:1.660985\n",
      "[12756]\ttrain-rmse:1.660978\n",
      "[12757]\ttrain-rmse:1.660967\n",
      "[12758]\ttrain-rmse:1.660963\n",
      "[12759]\ttrain-rmse:1.660958\n",
      "[12760]\ttrain-rmse:1.660940\n",
      "[12761]\ttrain-rmse:1.660930\n",
      "[12762]\ttrain-rmse:1.660918\n",
      "[12763]\ttrain-rmse:1.660908\n",
      "[12764]\ttrain-rmse:1.660899\n",
      "[12765]\ttrain-rmse:1.660886\n",
      "[12766]\ttrain-rmse:1.660878\n",
      "[12767]\ttrain-rmse:1.660872\n",
      "[12768]\ttrain-rmse:1.660861\n",
      "[12769]\ttrain-rmse:1.660854\n",
      "[12770]\ttrain-rmse:1.660843\n",
      "[12771]\ttrain-rmse:1.660835\n",
      "[12772]\ttrain-rmse:1.660827\n",
      "[12773]\ttrain-rmse:1.660820\n",
      "[12774]\ttrain-rmse:1.660817\n",
      "[12775]\ttrain-rmse:1.660808\n",
      "[12776]\ttrain-rmse:1.660792\n",
      "[12777]\ttrain-rmse:1.660786\n",
      "[12778]\ttrain-rmse:1.660780\n",
      "[12779]\ttrain-rmse:1.660764\n",
      "[12780]\ttrain-rmse:1.660754\n",
      "[12781]\ttrain-rmse:1.660748\n",
      "[12782]\ttrain-rmse:1.660739\n",
      "[12783]\ttrain-rmse:1.660729\n",
      "[12784]\ttrain-rmse:1.660721\n",
      "[12785]\ttrain-rmse:1.660707\n",
      "[12786]\ttrain-rmse:1.660702\n",
      "[12787]\ttrain-rmse:1.660694\n",
      "[12788]\ttrain-rmse:1.660682\n",
      "[12789]\ttrain-rmse:1.660674\n",
      "[12790]\ttrain-rmse:1.660663\n",
      "[12791]\ttrain-rmse:1.660652\n",
      "[12792]\ttrain-rmse:1.660645\n",
      "[12793]\ttrain-rmse:1.660637\n",
      "[12794]\ttrain-rmse:1.660631\n",
      "[12795]\ttrain-rmse:1.660622\n",
      "[12796]\ttrain-rmse:1.660609\n",
      "[12797]\ttrain-rmse:1.660594\n",
      "[12798]\ttrain-rmse:1.660582\n",
      "[12799]\ttrain-rmse:1.660577\n",
      "[12800]\ttrain-rmse:1.660570\n",
      "[12801]\ttrain-rmse:1.660562\n",
      "[12802]\ttrain-rmse:1.660552\n",
      "[12803]\ttrain-rmse:1.660543\n",
      "[12804]\ttrain-rmse:1.660530\n",
      "[12805]\ttrain-rmse:1.660523\n",
      "[12806]\ttrain-rmse:1.660513\n",
      "[12807]\ttrain-rmse:1.660502\n",
      "[12808]\ttrain-rmse:1.660496\n",
      "[12809]\ttrain-rmse:1.660489\n",
      "[12810]\ttrain-rmse:1.660487\n",
      "[12811]\ttrain-rmse:1.660480\n",
      "[12812]\ttrain-rmse:1.660471\n",
      "[12813]\ttrain-rmse:1.660461\n",
      "[12814]\ttrain-rmse:1.660452\n",
      "[12815]\ttrain-rmse:1.660440\n",
      "[12816]\ttrain-rmse:1.660431\n",
      "[12817]\ttrain-rmse:1.660422\n",
      "[12818]\ttrain-rmse:1.660414\n",
      "[12819]\ttrain-rmse:1.660406\n",
      "[12820]\ttrain-rmse:1.660397\n",
      "[12821]\ttrain-rmse:1.660390\n",
      "[12822]\ttrain-rmse:1.660380\n",
      "[12823]\ttrain-rmse:1.660372\n",
      "[12824]\ttrain-rmse:1.660361\n",
      "[12825]\ttrain-rmse:1.660354\n",
      "[12826]\ttrain-rmse:1.660345\n",
      "[12827]\ttrain-rmse:1.660338\n",
      "[12828]\ttrain-rmse:1.660324\n",
      "[12829]\ttrain-rmse:1.660315\n",
      "[12830]\ttrain-rmse:1.660303\n",
      "[12831]\ttrain-rmse:1.660300\n",
      "[12832]\ttrain-rmse:1.660285\n",
      "[12833]\ttrain-rmse:1.660277\n",
      "[12834]\ttrain-rmse:1.660268\n",
      "[12835]\ttrain-rmse:1.660259\n",
      "[12836]\ttrain-rmse:1.660245\n",
      "[12837]\ttrain-rmse:1.660237\n",
      "[12838]\ttrain-rmse:1.660227\n",
      "[12839]\ttrain-rmse:1.660215\n",
      "[12840]\ttrain-rmse:1.660207\n",
      "[12841]\ttrain-rmse:1.660204\n",
      "[12842]\ttrain-rmse:1.660191\n",
      "[12843]\ttrain-rmse:1.660178\n",
      "[12844]\ttrain-rmse:1.660171\n",
      "[12845]\ttrain-rmse:1.660160\n",
      "[12846]\ttrain-rmse:1.660152\n",
      "[12847]\ttrain-rmse:1.660131\n",
      "[12848]\ttrain-rmse:1.660118\n",
      "[12849]\ttrain-rmse:1.660108\n",
      "[12850]\ttrain-rmse:1.660100\n",
      "[12851]\ttrain-rmse:1.660093\n",
      "[12852]\ttrain-rmse:1.660082\n",
      "[12853]\ttrain-rmse:1.660073\n",
      "[12854]\ttrain-rmse:1.660066\n",
      "[12855]\ttrain-rmse:1.660059\n",
      "[12856]\ttrain-rmse:1.660047\n",
      "[12857]\ttrain-rmse:1.660038\n",
      "[12858]\ttrain-rmse:1.660029\n",
      "[12859]\ttrain-rmse:1.660020\n",
      "[12860]\ttrain-rmse:1.660017\n",
      "[12861]\ttrain-rmse:1.660004\n",
      "[12862]\ttrain-rmse:1.659992\n",
      "[12863]\ttrain-rmse:1.659984\n",
      "[12864]\ttrain-rmse:1.659974\n",
      "[12865]\ttrain-rmse:1.659967\n",
      "[12866]\ttrain-rmse:1.659961\n",
      "[12867]\ttrain-rmse:1.659956\n",
      "[12868]\ttrain-rmse:1.659945\n",
      "[12869]\ttrain-rmse:1.659939\n",
      "[12870]\ttrain-rmse:1.659934\n",
      "[12871]\ttrain-rmse:1.659924\n",
      "[12872]\ttrain-rmse:1.659915\n",
      "[12873]\ttrain-rmse:1.659907\n",
      "[12874]\ttrain-rmse:1.659891\n",
      "[12875]\ttrain-rmse:1.659882\n",
      "[12876]\ttrain-rmse:1.659870\n",
      "[12877]\ttrain-rmse:1.659860\n",
      "[12878]\ttrain-rmse:1.659847\n",
      "[12879]\ttrain-rmse:1.659840\n",
      "[12880]\ttrain-rmse:1.659826\n",
      "[12881]\ttrain-rmse:1.659816\n",
      "[12882]\ttrain-rmse:1.659808\n",
      "[12883]\ttrain-rmse:1.659799\n",
      "[12884]\ttrain-rmse:1.659793\n",
      "[12885]\ttrain-rmse:1.659780\n",
      "[12886]\ttrain-rmse:1.659768\n",
      "[12887]\ttrain-rmse:1.659760\n",
      "[12888]\ttrain-rmse:1.659747\n",
      "[12889]\ttrain-rmse:1.659729\n",
      "[12890]\ttrain-rmse:1.659717\n",
      "[12891]\ttrain-rmse:1.659710\n",
      "[12892]\ttrain-rmse:1.659701\n",
      "[12893]\ttrain-rmse:1.659694\n",
      "[12894]\ttrain-rmse:1.659690\n",
      "[12895]\ttrain-rmse:1.659679\n",
      "[12896]\ttrain-rmse:1.659670\n",
      "[12897]\ttrain-rmse:1.659660\n",
      "[12898]\ttrain-rmse:1.659655\n",
      "[12899]\ttrain-rmse:1.659648\n",
      "[12900]\ttrain-rmse:1.659641\n",
      "[12901]\ttrain-rmse:1.659631\n",
      "[12902]\ttrain-rmse:1.659625\n",
      "[12903]\ttrain-rmse:1.659618\n",
      "[12904]\ttrain-rmse:1.659613\n",
      "[12905]\ttrain-rmse:1.659601\n",
      "[12906]\ttrain-rmse:1.659591\n",
      "[12907]\ttrain-rmse:1.659584\n",
      "[12908]\ttrain-rmse:1.659577\n",
      "[12909]\ttrain-rmse:1.659562\n",
      "[12910]\ttrain-rmse:1.659555\n",
      "[12911]\ttrain-rmse:1.659551\n",
      "[12912]\ttrain-rmse:1.659537\n",
      "[12913]\ttrain-rmse:1.659534\n",
      "[12914]\ttrain-rmse:1.659527\n",
      "[12915]\ttrain-rmse:1.659510\n",
      "[12916]\ttrain-rmse:1.659501\n",
      "[12917]\ttrain-rmse:1.659494\n",
      "[12918]\ttrain-rmse:1.659481\n",
      "[12919]\ttrain-rmse:1.659477\n",
      "[12920]\ttrain-rmse:1.659466\n",
      "[12921]\ttrain-rmse:1.659458\n",
      "[12922]\ttrain-rmse:1.659449\n",
      "[12923]\ttrain-rmse:1.659441\n",
      "[12924]\ttrain-rmse:1.659433\n",
      "[12925]\ttrain-rmse:1.659421\n",
      "[12926]\ttrain-rmse:1.659417\n",
      "[12927]\ttrain-rmse:1.659413\n",
      "[12928]\ttrain-rmse:1.659405\n",
      "[12929]\ttrain-rmse:1.659399\n",
      "[12930]\ttrain-rmse:1.659385\n",
      "[12931]\ttrain-rmse:1.659379\n",
      "[12932]\ttrain-rmse:1.659369\n",
      "[12933]\ttrain-rmse:1.659358\n",
      "[12934]\ttrain-rmse:1.659345\n",
      "[12935]\ttrain-rmse:1.659336\n",
      "[12936]\ttrain-rmse:1.659328\n",
      "[12937]\ttrain-rmse:1.659320\n",
      "[12938]\ttrain-rmse:1.659314\n",
      "[12939]\ttrain-rmse:1.659305\n",
      "[12940]\ttrain-rmse:1.659295\n",
      "[12941]\ttrain-rmse:1.659283\n",
      "[12942]\ttrain-rmse:1.659274\n",
      "[12943]\ttrain-rmse:1.659267\n",
      "[12944]\ttrain-rmse:1.659250\n",
      "[12945]\ttrain-rmse:1.659243\n",
      "[12946]\ttrain-rmse:1.659235\n",
      "[12947]\ttrain-rmse:1.659227\n",
      "[12948]\ttrain-rmse:1.659218\n",
      "[12949]\ttrain-rmse:1.659214\n",
      "[12950]\ttrain-rmse:1.659204\n",
      "[12951]\ttrain-rmse:1.659192\n",
      "[12952]\ttrain-rmse:1.659179\n",
      "[12953]\ttrain-rmse:1.659176\n",
      "[12954]\ttrain-rmse:1.659166\n",
      "[12955]\ttrain-rmse:1.659156\n",
      "[12956]\ttrain-rmse:1.659149\n",
      "[12957]\ttrain-rmse:1.659146\n",
      "[12958]\ttrain-rmse:1.659138\n",
      "[12959]\ttrain-rmse:1.659130\n",
      "[12960]\ttrain-rmse:1.659124\n",
      "[12961]\ttrain-rmse:1.659113\n",
      "[12962]\ttrain-rmse:1.659105\n",
      "[12963]\ttrain-rmse:1.659097\n",
      "[12964]\ttrain-rmse:1.659084\n",
      "[12965]\ttrain-rmse:1.659077\n",
      "[12966]\ttrain-rmse:1.659065\n",
      "[12967]\ttrain-rmse:1.659054\n",
      "[12968]\ttrain-rmse:1.659046\n",
      "[12969]\ttrain-rmse:1.659037\n",
      "[12970]\ttrain-rmse:1.659032\n",
      "[12971]\ttrain-rmse:1.659026\n",
      "[12972]\ttrain-rmse:1.659014\n",
      "[12973]\ttrain-rmse:1.659001\n",
      "[12974]\ttrain-rmse:1.658988\n",
      "[12975]\ttrain-rmse:1.658973\n",
      "[12976]\ttrain-rmse:1.658969\n",
      "[12977]\ttrain-rmse:1.658964\n",
      "[12978]\ttrain-rmse:1.658952\n",
      "[12979]\ttrain-rmse:1.658945\n",
      "[12980]\ttrain-rmse:1.658932\n",
      "[12981]\ttrain-rmse:1.658921\n",
      "[12982]\ttrain-rmse:1.658904\n",
      "[12983]\ttrain-rmse:1.658896\n",
      "[12984]\ttrain-rmse:1.658880\n",
      "[12985]\ttrain-rmse:1.658872\n",
      "[12986]\ttrain-rmse:1.658866\n",
      "[12987]\ttrain-rmse:1.658854\n",
      "[12988]\ttrain-rmse:1.658847\n",
      "[12989]\ttrain-rmse:1.658840\n",
      "[12990]\ttrain-rmse:1.658829\n",
      "[12991]\ttrain-rmse:1.658821\n",
      "[12992]\ttrain-rmse:1.658814\n",
      "[12993]\ttrain-rmse:1.658810\n",
      "[12994]\ttrain-rmse:1.658804\n",
      "[12995]\ttrain-rmse:1.658790\n",
      "[12996]\ttrain-rmse:1.658779\n",
      "[12997]\ttrain-rmse:1.658773\n",
      "[12998]\ttrain-rmse:1.658758\n",
      "[12999]\ttrain-rmse:1.658748\n",
      "[13000]\ttrain-rmse:1.658735\n",
      "[13001]\ttrain-rmse:1.658724\n",
      "[13002]\ttrain-rmse:1.658723\n",
      "[13003]\ttrain-rmse:1.658713\n",
      "[13004]\ttrain-rmse:1.658699\n",
      "[13005]\ttrain-rmse:1.658693\n",
      "[13006]\ttrain-rmse:1.658688\n",
      "[13007]\ttrain-rmse:1.658682\n",
      "[13008]\ttrain-rmse:1.658675\n",
      "[13009]\ttrain-rmse:1.658663\n",
      "[13010]\ttrain-rmse:1.658654\n",
      "[13011]\ttrain-rmse:1.658642\n",
      "[13012]\ttrain-rmse:1.658633\n",
      "[13013]\ttrain-rmse:1.658625\n",
      "[13014]\ttrain-rmse:1.658616\n",
      "[13015]\ttrain-rmse:1.658608\n",
      "[13016]\ttrain-rmse:1.658598\n",
      "[13017]\ttrain-rmse:1.658591\n",
      "[13018]\ttrain-rmse:1.658586\n",
      "[13019]\ttrain-rmse:1.658579\n",
      "[13020]\ttrain-rmse:1.658571\n",
      "[13021]\ttrain-rmse:1.658561\n",
      "[13022]\ttrain-rmse:1.658556\n",
      "[13023]\ttrain-rmse:1.658549\n",
      "[13024]\ttrain-rmse:1.658539\n",
      "[13025]\ttrain-rmse:1.658527\n",
      "[13026]\ttrain-rmse:1.658523\n",
      "[13027]\ttrain-rmse:1.658517\n",
      "[13028]\ttrain-rmse:1.658508\n",
      "[13029]\ttrain-rmse:1.658498\n",
      "[13030]\ttrain-rmse:1.658490\n",
      "[13031]\ttrain-rmse:1.658484\n",
      "[13032]\ttrain-rmse:1.658475\n",
      "[13033]\ttrain-rmse:1.658467\n",
      "[13034]\ttrain-rmse:1.658454\n",
      "[13035]\ttrain-rmse:1.658450\n",
      "[13036]\ttrain-rmse:1.658443\n",
      "[13037]\ttrain-rmse:1.658432\n",
      "[13038]\ttrain-rmse:1.658421\n",
      "[13039]\ttrain-rmse:1.658413\n",
      "[13040]\ttrain-rmse:1.658404\n",
      "[13041]\ttrain-rmse:1.658389\n",
      "[13042]\ttrain-rmse:1.658380\n",
      "[13043]\ttrain-rmse:1.658366\n",
      "[13044]\ttrain-rmse:1.658361\n",
      "[13045]\ttrain-rmse:1.658349\n",
      "[13046]\ttrain-rmse:1.658337\n",
      "[13047]\ttrain-rmse:1.658328\n",
      "[13048]\ttrain-rmse:1.658321\n",
      "[13049]\ttrain-rmse:1.658316\n",
      "[13050]\ttrain-rmse:1.658306\n",
      "[13051]\ttrain-rmse:1.658299\n",
      "[13052]\ttrain-rmse:1.658292\n",
      "[13053]\ttrain-rmse:1.658287\n",
      "[13054]\ttrain-rmse:1.658279\n",
      "[13055]\ttrain-rmse:1.658272\n",
      "[13056]\ttrain-rmse:1.658261\n",
      "[13057]\ttrain-rmse:1.658252\n",
      "[13058]\ttrain-rmse:1.658246\n",
      "[13059]\ttrain-rmse:1.658239\n",
      "[13060]\ttrain-rmse:1.658226\n",
      "[13061]\ttrain-rmse:1.658222\n",
      "[13062]\ttrain-rmse:1.658218\n",
      "[13063]\ttrain-rmse:1.658211\n",
      "[13064]\ttrain-rmse:1.658200\n",
      "[13065]\ttrain-rmse:1.658197\n",
      "[13066]\ttrain-rmse:1.658190\n",
      "[13067]\ttrain-rmse:1.658180\n",
      "[13068]\ttrain-rmse:1.658171\n",
      "[13069]\ttrain-rmse:1.658159\n",
      "[13070]\ttrain-rmse:1.658147\n",
      "[13071]\ttrain-rmse:1.658139\n",
      "[13072]\ttrain-rmse:1.658129\n",
      "[13073]\ttrain-rmse:1.658124\n",
      "[13074]\ttrain-rmse:1.658119\n",
      "[13075]\ttrain-rmse:1.658112\n",
      "[13076]\ttrain-rmse:1.658100\n",
      "[13077]\ttrain-rmse:1.658093\n",
      "[13078]\ttrain-rmse:1.658081\n",
      "[13079]\ttrain-rmse:1.658076\n",
      "[13080]\ttrain-rmse:1.658071\n",
      "[13081]\ttrain-rmse:1.658065\n",
      "[13082]\ttrain-rmse:1.658055\n",
      "[13083]\ttrain-rmse:1.658040\n",
      "[13084]\ttrain-rmse:1.658032\n",
      "[13085]\ttrain-rmse:1.658021\n",
      "[13086]\ttrain-rmse:1.658016\n",
      "[13087]\ttrain-rmse:1.658008\n",
      "[13088]\ttrain-rmse:1.658001\n",
      "[13089]\ttrain-rmse:1.657993\n",
      "[13090]\ttrain-rmse:1.657985\n",
      "[13091]\ttrain-rmse:1.657980\n",
      "[13092]\ttrain-rmse:1.657971\n",
      "[13093]\ttrain-rmse:1.657965\n",
      "[13094]\ttrain-rmse:1.657955\n",
      "[13095]\ttrain-rmse:1.657947\n",
      "[13096]\ttrain-rmse:1.657938\n",
      "[13097]\ttrain-rmse:1.657927\n",
      "[13098]\ttrain-rmse:1.657920\n",
      "[13099]\ttrain-rmse:1.657912\n",
      "[13100]\ttrain-rmse:1.657900\n",
      "[13101]\ttrain-rmse:1.657889\n",
      "[13102]\ttrain-rmse:1.657878\n",
      "[13103]\ttrain-rmse:1.657867\n",
      "[13104]\ttrain-rmse:1.657862\n",
      "[13105]\ttrain-rmse:1.657856\n",
      "[13106]\ttrain-rmse:1.657843\n",
      "[13107]\ttrain-rmse:1.657829\n",
      "[13108]\ttrain-rmse:1.657818\n",
      "[13109]\ttrain-rmse:1.657812\n",
      "[13110]\ttrain-rmse:1.657799\n",
      "[13111]\ttrain-rmse:1.657795\n",
      "[13112]\ttrain-rmse:1.657787\n",
      "[13113]\ttrain-rmse:1.657779\n",
      "[13114]\ttrain-rmse:1.657766\n",
      "[13115]\ttrain-rmse:1.657757\n",
      "[13116]\ttrain-rmse:1.657747\n",
      "[13117]\ttrain-rmse:1.657741\n",
      "[13118]\ttrain-rmse:1.657732\n",
      "[13119]\ttrain-rmse:1.657720\n",
      "[13120]\ttrain-rmse:1.657707\n",
      "[13121]\ttrain-rmse:1.657703\n",
      "[13122]\ttrain-rmse:1.657692\n",
      "[13123]\ttrain-rmse:1.657685\n",
      "[13124]\ttrain-rmse:1.657678\n",
      "[13125]\ttrain-rmse:1.657663\n",
      "[13126]\ttrain-rmse:1.657655\n",
      "[13127]\ttrain-rmse:1.657636\n",
      "[13128]\ttrain-rmse:1.657628\n",
      "[13129]\ttrain-rmse:1.657620\n",
      "[13130]\ttrain-rmse:1.657614\n",
      "[13131]\ttrain-rmse:1.657607\n",
      "[13132]\ttrain-rmse:1.657598\n",
      "[13133]\ttrain-rmse:1.657593\n",
      "[13134]\ttrain-rmse:1.657584\n",
      "[13135]\ttrain-rmse:1.657576\n",
      "[13136]\ttrain-rmse:1.657567\n",
      "[13137]\ttrain-rmse:1.657562\n",
      "[13138]\ttrain-rmse:1.657555\n",
      "[13139]\ttrain-rmse:1.657549\n",
      "[13140]\ttrain-rmse:1.657537\n",
      "[13141]\ttrain-rmse:1.657530\n",
      "[13142]\ttrain-rmse:1.657522\n",
      "[13143]\ttrain-rmse:1.657520\n",
      "[13144]\ttrain-rmse:1.657512\n",
      "[13145]\ttrain-rmse:1.657499\n",
      "[13146]\ttrain-rmse:1.657490\n",
      "[13147]\ttrain-rmse:1.657482\n",
      "[13148]\ttrain-rmse:1.657466\n",
      "[13149]\ttrain-rmse:1.657457\n",
      "[13150]\ttrain-rmse:1.657444\n",
      "[13151]\ttrain-rmse:1.657438\n",
      "[13152]\ttrain-rmse:1.657423\n",
      "[13153]\ttrain-rmse:1.657411\n",
      "[13154]\ttrain-rmse:1.657407\n",
      "[13155]\ttrain-rmse:1.657401\n",
      "[13156]\ttrain-rmse:1.657392\n",
      "[13157]\ttrain-rmse:1.657385\n",
      "[13158]\ttrain-rmse:1.657372\n",
      "[13159]\ttrain-rmse:1.657358\n",
      "[13160]\ttrain-rmse:1.657350\n",
      "[13161]\ttrain-rmse:1.657343\n",
      "[13162]\ttrain-rmse:1.657341\n",
      "[13163]\ttrain-rmse:1.657333\n",
      "[13164]\ttrain-rmse:1.657326\n",
      "[13165]\ttrain-rmse:1.657314\n",
      "[13166]\ttrain-rmse:1.657306\n",
      "[13167]\ttrain-rmse:1.657298\n",
      "[13168]\ttrain-rmse:1.657288\n",
      "[13169]\ttrain-rmse:1.657282\n",
      "[13170]\ttrain-rmse:1.657270\n",
      "[13171]\ttrain-rmse:1.657255\n",
      "[13172]\ttrain-rmse:1.657248\n",
      "[13173]\ttrain-rmse:1.657233\n",
      "[13174]\ttrain-rmse:1.657227\n",
      "[13175]\ttrain-rmse:1.657216\n",
      "[13176]\ttrain-rmse:1.657209\n",
      "[13177]\ttrain-rmse:1.657199\n",
      "[13178]\ttrain-rmse:1.657192\n",
      "[13179]\ttrain-rmse:1.657185\n",
      "[13180]\ttrain-rmse:1.657181\n",
      "[13181]\ttrain-rmse:1.657169\n",
      "[13182]\ttrain-rmse:1.657160\n",
      "[13183]\ttrain-rmse:1.657154\n",
      "[13184]\ttrain-rmse:1.657139\n",
      "[13185]\ttrain-rmse:1.657132\n",
      "[13186]\ttrain-rmse:1.657123\n",
      "[13187]\ttrain-rmse:1.657108\n",
      "[13188]\ttrain-rmse:1.657102\n",
      "[13189]\ttrain-rmse:1.657094\n",
      "[13190]\ttrain-rmse:1.657090\n",
      "[13191]\ttrain-rmse:1.657076\n",
      "[13192]\ttrain-rmse:1.657073\n",
      "[13193]\ttrain-rmse:1.657065\n",
      "[13194]\ttrain-rmse:1.657053\n",
      "[13195]\ttrain-rmse:1.657042\n",
      "[13196]\ttrain-rmse:1.657033\n",
      "[13197]\ttrain-rmse:1.657029\n",
      "[13198]\ttrain-rmse:1.657018\n",
      "[13199]\ttrain-rmse:1.657008\n",
      "[13200]\ttrain-rmse:1.656999\n",
      "[13201]\ttrain-rmse:1.656994\n",
      "[13202]\ttrain-rmse:1.656985\n",
      "[13203]\ttrain-rmse:1.656978\n",
      "[13204]\ttrain-rmse:1.656972\n",
      "[13205]\ttrain-rmse:1.656963\n",
      "[13206]\ttrain-rmse:1.656952\n",
      "[13207]\ttrain-rmse:1.656943\n",
      "[13208]\ttrain-rmse:1.656934\n",
      "[13209]\ttrain-rmse:1.656921\n",
      "[13210]\ttrain-rmse:1.656917\n",
      "[13211]\ttrain-rmse:1.656903\n",
      "[13212]\ttrain-rmse:1.656893\n",
      "[13213]\ttrain-rmse:1.656883\n",
      "[13214]\ttrain-rmse:1.656873\n",
      "[13215]\ttrain-rmse:1.656866\n",
      "[13216]\ttrain-rmse:1.656857\n",
      "[13217]\ttrain-rmse:1.656847\n",
      "[13218]\ttrain-rmse:1.656839\n",
      "[13219]\ttrain-rmse:1.656826\n",
      "[13220]\ttrain-rmse:1.656814\n",
      "[13221]\ttrain-rmse:1.656800\n",
      "[13222]\ttrain-rmse:1.656795\n",
      "[13223]\ttrain-rmse:1.656780\n",
      "[13224]\ttrain-rmse:1.656770\n",
      "[13225]\ttrain-rmse:1.656766\n",
      "[13226]\ttrain-rmse:1.656752\n",
      "[13227]\ttrain-rmse:1.656744\n",
      "[13228]\ttrain-rmse:1.656742\n",
      "[13229]\ttrain-rmse:1.656730\n",
      "[13230]\ttrain-rmse:1.656721\n",
      "[13231]\ttrain-rmse:1.656717\n",
      "[13232]\ttrain-rmse:1.656709\n",
      "[13233]\ttrain-rmse:1.656700\n",
      "[13234]\ttrain-rmse:1.656690\n",
      "[13235]\ttrain-rmse:1.656678\n",
      "[13236]\ttrain-rmse:1.656669\n",
      "[13237]\ttrain-rmse:1.656659\n",
      "[13238]\ttrain-rmse:1.656649\n",
      "[13239]\ttrain-rmse:1.656641\n",
      "[13240]\ttrain-rmse:1.656635\n",
      "[13241]\ttrain-rmse:1.656626\n",
      "[13242]\ttrain-rmse:1.656621\n",
      "[13243]\ttrain-rmse:1.656618\n",
      "[13244]\ttrain-rmse:1.656606\n",
      "[13245]\ttrain-rmse:1.656598\n",
      "[13246]\ttrain-rmse:1.656591\n",
      "[13247]\ttrain-rmse:1.656588\n",
      "[13248]\ttrain-rmse:1.656581\n",
      "[13249]\ttrain-rmse:1.656572\n",
      "[13250]\ttrain-rmse:1.656561\n",
      "[13251]\ttrain-rmse:1.656550\n",
      "[13252]\ttrain-rmse:1.656543\n",
      "[13253]\ttrain-rmse:1.656526\n",
      "[13254]\ttrain-rmse:1.656516\n",
      "[13255]\ttrain-rmse:1.656502\n",
      "[13256]\ttrain-rmse:1.656492\n",
      "[13257]\ttrain-rmse:1.656485\n",
      "[13258]\ttrain-rmse:1.656476\n",
      "[13259]\ttrain-rmse:1.656471\n",
      "[13260]\ttrain-rmse:1.656462\n",
      "[13261]\ttrain-rmse:1.656451\n",
      "[13262]\ttrain-rmse:1.656447\n",
      "[13263]\ttrain-rmse:1.656440\n",
      "[13264]\ttrain-rmse:1.656426\n",
      "[13265]\ttrain-rmse:1.656416\n",
      "[13266]\ttrain-rmse:1.656406\n",
      "[13267]\ttrain-rmse:1.656402\n",
      "[13268]\ttrain-rmse:1.656395\n",
      "[13269]\ttrain-rmse:1.656385\n",
      "[13270]\ttrain-rmse:1.656377\n",
      "[13271]\ttrain-rmse:1.656371\n",
      "[13272]\ttrain-rmse:1.656361\n",
      "[13273]\ttrain-rmse:1.656352\n",
      "[13274]\ttrain-rmse:1.656345\n",
      "[13275]\ttrain-rmse:1.656338\n",
      "[13276]\ttrain-rmse:1.656330\n",
      "[13277]\ttrain-rmse:1.656321\n",
      "[13278]\ttrain-rmse:1.656311\n",
      "[13279]\ttrain-rmse:1.656305\n",
      "[13280]\ttrain-rmse:1.656294\n",
      "[13281]\ttrain-rmse:1.656283\n",
      "[13282]\ttrain-rmse:1.656272\n",
      "[13283]\ttrain-rmse:1.656262\n",
      "[13284]\ttrain-rmse:1.656260\n",
      "[13285]\ttrain-rmse:1.656256\n",
      "[13286]\ttrain-rmse:1.656245\n",
      "[13287]\ttrain-rmse:1.656242\n",
      "[13288]\ttrain-rmse:1.656235\n",
      "[13289]\ttrain-rmse:1.656230\n",
      "[13290]\ttrain-rmse:1.656219\n",
      "[13291]\ttrain-rmse:1.656207\n",
      "[13292]\ttrain-rmse:1.656195\n",
      "[13293]\ttrain-rmse:1.656190\n",
      "[13294]\ttrain-rmse:1.656180\n",
      "[13295]\ttrain-rmse:1.656171\n",
      "[13296]\ttrain-rmse:1.656160\n",
      "[13297]\ttrain-rmse:1.656153\n",
      "[13298]\ttrain-rmse:1.656147\n",
      "[13299]\ttrain-rmse:1.656139\n",
      "[13300]\ttrain-rmse:1.656136\n",
      "[13301]\ttrain-rmse:1.656126\n",
      "[13302]\ttrain-rmse:1.656119\n",
      "[13303]\ttrain-rmse:1.656108\n",
      "[13304]\ttrain-rmse:1.656097\n",
      "[13305]\ttrain-rmse:1.656085\n",
      "[13306]\ttrain-rmse:1.656080\n",
      "[13307]\ttrain-rmse:1.656067\n",
      "[13308]\ttrain-rmse:1.656059\n",
      "[13309]\ttrain-rmse:1.656049\n",
      "[13310]\ttrain-rmse:1.656041\n",
      "[13311]\ttrain-rmse:1.656035\n",
      "[13312]\ttrain-rmse:1.656027\n",
      "[13313]\ttrain-rmse:1.656017\n",
      "[13314]\ttrain-rmse:1.656010\n",
      "[13315]\ttrain-rmse:1.656005\n",
      "[13316]\ttrain-rmse:1.655996\n",
      "[13317]\ttrain-rmse:1.655982\n",
      "[13318]\ttrain-rmse:1.655975\n",
      "[13319]\ttrain-rmse:1.655966\n",
      "[13320]\ttrain-rmse:1.655956\n",
      "[13321]\ttrain-rmse:1.655941\n",
      "[13322]\ttrain-rmse:1.655931\n",
      "[13323]\ttrain-rmse:1.655918\n",
      "[13324]\ttrain-rmse:1.655910\n",
      "[13325]\ttrain-rmse:1.655905\n",
      "[13326]\ttrain-rmse:1.655896\n",
      "[13327]\ttrain-rmse:1.655887\n",
      "[13328]\ttrain-rmse:1.655877\n",
      "[13329]\ttrain-rmse:1.655865\n",
      "[13330]\ttrain-rmse:1.655857\n",
      "[13331]\ttrain-rmse:1.655848\n",
      "[13332]\ttrain-rmse:1.655844\n",
      "[13333]\ttrain-rmse:1.655833\n",
      "[13334]\ttrain-rmse:1.655826\n",
      "[13335]\ttrain-rmse:1.655819\n",
      "[13336]\ttrain-rmse:1.655808\n",
      "[13337]\ttrain-rmse:1.655805\n",
      "[13338]\ttrain-rmse:1.655795\n",
      "[13339]\ttrain-rmse:1.655784\n",
      "[13340]\ttrain-rmse:1.655775\n",
      "[13341]\ttrain-rmse:1.655768\n",
      "[13342]\ttrain-rmse:1.655762\n",
      "[13343]\ttrain-rmse:1.655755\n",
      "[13344]\ttrain-rmse:1.655738\n",
      "[13345]\ttrain-rmse:1.655735\n",
      "[13346]\ttrain-rmse:1.655727\n",
      "[13347]\ttrain-rmse:1.655715\n",
      "[13348]\ttrain-rmse:1.655705\n",
      "[13349]\ttrain-rmse:1.655697\n",
      "[13350]\ttrain-rmse:1.655689\n",
      "[13351]\ttrain-rmse:1.655679\n",
      "[13352]\ttrain-rmse:1.655670\n",
      "[13353]\ttrain-rmse:1.655665\n",
      "[13354]\ttrain-rmse:1.655656\n",
      "[13355]\ttrain-rmse:1.655646\n",
      "[13356]\ttrain-rmse:1.655636\n",
      "[13357]\ttrain-rmse:1.655629\n",
      "[13358]\ttrain-rmse:1.655622\n",
      "[13359]\ttrain-rmse:1.655616\n",
      "[13360]\ttrain-rmse:1.655611\n",
      "[13361]\ttrain-rmse:1.655608\n",
      "[13362]\ttrain-rmse:1.655600\n",
      "[13363]\ttrain-rmse:1.655592\n",
      "[13364]\ttrain-rmse:1.655580\n",
      "[13365]\ttrain-rmse:1.655573\n",
      "[13366]\ttrain-rmse:1.655565\n",
      "[13367]\ttrain-rmse:1.655559\n",
      "[13368]\ttrain-rmse:1.655550\n",
      "[13369]\ttrain-rmse:1.655538\n",
      "[13370]\ttrain-rmse:1.655529\n",
      "[13371]\ttrain-rmse:1.655524\n",
      "[13372]\ttrain-rmse:1.655514\n",
      "[13373]\ttrain-rmse:1.655505\n",
      "[13374]\ttrain-rmse:1.655493\n",
      "[13375]\ttrain-rmse:1.655483\n",
      "[13376]\ttrain-rmse:1.655476\n",
      "[13377]\ttrain-rmse:1.655466\n",
      "[13378]\ttrain-rmse:1.655457\n",
      "[13379]\ttrain-rmse:1.655452\n",
      "[13380]\ttrain-rmse:1.655440\n",
      "[13381]\ttrain-rmse:1.655430\n",
      "[13382]\ttrain-rmse:1.655426\n",
      "[13383]\ttrain-rmse:1.655419\n",
      "[13384]\ttrain-rmse:1.655410\n",
      "[13385]\ttrain-rmse:1.655399\n",
      "[13386]\ttrain-rmse:1.655396\n",
      "[13387]\ttrain-rmse:1.655385\n",
      "[13388]\ttrain-rmse:1.655378\n",
      "[13389]\ttrain-rmse:1.655372\n",
      "[13390]\ttrain-rmse:1.655361\n",
      "[13391]\ttrain-rmse:1.655351\n",
      "[13392]\ttrain-rmse:1.655343\n",
      "[13393]\ttrain-rmse:1.655336\n",
      "[13394]\ttrain-rmse:1.655330\n",
      "[13395]\ttrain-rmse:1.655317\n",
      "[13396]\ttrain-rmse:1.655303\n",
      "[13397]\ttrain-rmse:1.655293\n",
      "[13398]\ttrain-rmse:1.655284\n",
      "[13399]\ttrain-rmse:1.655275\n",
      "[13400]\ttrain-rmse:1.655267\n",
      "[13401]\ttrain-rmse:1.655264\n",
      "[13402]\ttrain-rmse:1.655254\n",
      "[13403]\ttrain-rmse:1.655242\n",
      "[13404]\ttrain-rmse:1.655233\n",
      "[13405]\ttrain-rmse:1.655228\n",
      "[13406]\ttrain-rmse:1.655222\n",
      "[13407]\ttrain-rmse:1.655216\n",
      "[13408]\ttrain-rmse:1.655211\n",
      "[13409]\ttrain-rmse:1.655203\n",
      "[13410]\ttrain-rmse:1.655198\n",
      "[13411]\ttrain-rmse:1.655190\n",
      "[13412]\ttrain-rmse:1.655185\n",
      "[13413]\ttrain-rmse:1.655181\n",
      "[13414]\ttrain-rmse:1.655168\n",
      "[13415]\ttrain-rmse:1.655162\n",
      "[13416]\ttrain-rmse:1.655157\n",
      "[13417]\ttrain-rmse:1.655149\n",
      "[13418]\ttrain-rmse:1.655143\n",
      "[13419]\ttrain-rmse:1.655140\n",
      "[13420]\ttrain-rmse:1.655132\n",
      "[13421]\ttrain-rmse:1.655123\n",
      "[13422]\ttrain-rmse:1.655110\n",
      "[13423]\ttrain-rmse:1.655102\n",
      "[13424]\ttrain-rmse:1.655096\n",
      "[13425]\ttrain-rmse:1.655090\n",
      "[13426]\ttrain-rmse:1.655082\n",
      "[13427]\ttrain-rmse:1.655075\n",
      "[13428]\ttrain-rmse:1.655063\n",
      "[13429]\ttrain-rmse:1.655057\n",
      "[13430]\ttrain-rmse:1.655048\n",
      "[13431]\ttrain-rmse:1.655040\n",
      "[13432]\ttrain-rmse:1.655033\n",
      "[13433]\ttrain-rmse:1.655022\n",
      "[13434]\ttrain-rmse:1.655011\n",
      "[13435]\ttrain-rmse:1.655000\n",
      "[13436]\ttrain-rmse:1.654992\n",
      "[13437]\ttrain-rmse:1.654982\n",
      "[13438]\ttrain-rmse:1.654978\n",
      "[13439]\ttrain-rmse:1.654969\n",
      "[13440]\ttrain-rmse:1.654960\n",
      "[13441]\ttrain-rmse:1.654950\n",
      "[13442]\ttrain-rmse:1.654943\n",
      "[13443]\ttrain-rmse:1.654928\n",
      "[13444]\ttrain-rmse:1.654923\n",
      "[13445]\ttrain-rmse:1.654913\n",
      "[13446]\ttrain-rmse:1.654907\n",
      "[13447]\ttrain-rmse:1.654897\n",
      "[13448]\ttrain-rmse:1.654884\n",
      "[13449]\ttrain-rmse:1.654876\n",
      "[13450]\ttrain-rmse:1.654867\n",
      "[13451]\ttrain-rmse:1.654863\n",
      "[13452]\ttrain-rmse:1.654855\n",
      "[13453]\ttrain-rmse:1.654848\n",
      "[13454]\ttrain-rmse:1.654838\n",
      "[13455]\ttrain-rmse:1.654828\n",
      "[13456]\ttrain-rmse:1.654816\n",
      "[13457]\ttrain-rmse:1.654805\n",
      "[13458]\ttrain-rmse:1.654791\n",
      "[13459]\ttrain-rmse:1.654783\n",
      "[13460]\ttrain-rmse:1.654775\n",
      "[13461]\ttrain-rmse:1.654770\n",
      "[13462]\ttrain-rmse:1.654757\n",
      "[13463]\ttrain-rmse:1.654744\n",
      "[13464]\ttrain-rmse:1.654739\n",
      "[13465]\ttrain-rmse:1.654734\n",
      "[13466]\ttrain-rmse:1.654726\n",
      "[13467]\ttrain-rmse:1.654719\n",
      "[13468]\ttrain-rmse:1.654708\n",
      "[13469]\ttrain-rmse:1.654695\n",
      "[13470]\ttrain-rmse:1.654682\n",
      "[13471]\ttrain-rmse:1.654675\n",
      "[13472]\ttrain-rmse:1.654664\n",
      "[13473]\ttrain-rmse:1.654655\n",
      "[13474]\ttrain-rmse:1.654641\n",
      "[13475]\ttrain-rmse:1.654629\n",
      "[13476]\ttrain-rmse:1.654623\n",
      "[13477]\ttrain-rmse:1.654614\n",
      "[13478]\ttrain-rmse:1.654612\n",
      "[13479]\ttrain-rmse:1.654605\n",
      "[13480]\ttrain-rmse:1.654596\n",
      "[13481]\ttrain-rmse:1.654583\n",
      "[13482]\ttrain-rmse:1.654575\n",
      "[13483]\ttrain-rmse:1.654568\n",
      "[13484]\ttrain-rmse:1.654553\n",
      "[13485]\ttrain-rmse:1.654544\n",
      "[13486]\ttrain-rmse:1.654534\n",
      "[13487]\ttrain-rmse:1.654526\n",
      "[13488]\ttrain-rmse:1.654514\n",
      "[13489]\ttrain-rmse:1.654508\n",
      "[13490]\ttrain-rmse:1.654501\n",
      "[13491]\ttrain-rmse:1.654494\n",
      "[13492]\ttrain-rmse:1.654484\n",
      "[13493]\ttrain-rmse:1.654477\n",
      "[13494]\ttrain-rmse:1.654471\n",
      "[13495]\ttrain-rmse:1.654459\n",
      "[13496]\ttrain-rmse:1.654455\n",
      "[13497]\ttrain-rmse:1.654443\n",
      "[13498]\ttrain-rmse:1.654431\n",
      "[13499]\ttrain-rmse:1.654427\n",
      "[13500]\ttrain-rmse:1.654419\n",
      "[13501]\ttrain-rmse:1.654411\n",
      "[13502]\ttrain-rmse:1.654405\n",
      "[13503]\ttrain-rmse:1.654399\n",
      "[13504]\ttrain-rmse:1.654397\n",
      "[13505]\ttrain-rmse:1.654384\n",
      "[13506]\ttrain-rmse:1.654378\n",
      "[13507]\ttrain-rmse:1.654370\n",
      "[13508]\ttrain-rmse:1.654359\n",
      "[13509]\ttrain-rmse:1.654352\n",
      "[13510]\ttrain-rmse:1.654347\n",
      "[13511]\ttrain-rmse:1.654335\n",
      "[13512]\ttrain-rmse:1.654327\n",
      "[13513]\ttrain-rmse:1.654317\n",
      "[13514]\ttrain-rmse:1.654308\n",
      "[13515]\ttrain-rmse:1.654301\n",
      "[13516]\ttrain-rmse:1.654293\n",
      "[13517]\ttrain-rmse:1.654287\n",
      "[13518]\ttrain-rmse:1.654281\n",
      "[13519]\ttrain-rmse:1.654272\n",
      "[13520]\ttrain-rmse:1.654264\n",
      "[13521]\ttrain-rmse:1.654251\n",
      "[13522]\ttrain-rmse:1.654247\n",
      "[13523]\ttrain-rmse:1.654239\n",
      "[13524]\ttrain-rmse:1.654231\n",
      "[13525]\ttrain-rmse:1.654224\n",
      "[13526]\ttrain-rmse:1.654218\n",
      "[13527]\ttrain-rmse:1.654211\n",
      "[13528]\ttrain-rmse:1.654207\n",
      "[13529]\ttrain-rmse:1.654200\n",
      "[13530]\ttrain-rmse:1.654187\n",
      "[13531]\ttrain-rmse:1.654180\n",
      "[13532]\ttrain-rmse:1.654174\n",
      "[13533]\ttrain-rmse:1.654168\n",
      "[13534]\ttrain-rmse:1.654159\n",
      "[13535]\ttrain-rmse:1.654152\n",
      "[13536]\ttrain-rmse:1.654140\n",
      "[13537]\ttrain-rmse:1.654133\n",
      "[13538]\ttrain-rmse:1.654123\n",
      "[13539]\ttrain-rmse:1.654112\n",
      "[13540]\ttrain-rmse:1.654102\n",
      "[13541]\ttrain-rmse:1.654097\n",
      "[13542]\ttrain-rmse:1.654087\n",
      "[13543]\ttrain-rmse:1.654082\n",
      "[13544]\ttrain-rmse:1.654075\n",
      "[13545]\ttrain-rmse:1.654066\n",
      "[13546]\ttrain-rmse:1.654061\n",
      "[13547]\ttrain-rmse:1.654052\n",
      "[13548]\ttrain-rmse:1.654048\n",
      "[13549]\ttrain-rmse:1.654043\n",
      "[13550]\ttrain-rmse:1.654032\n",
      "[13551]\ttrain-rmse:1.654020\n",
      "[13552]\ttrain-rmse:1.654012\n",
      "[13553]\ttrain-rmse:1.654002\n",
      "[13554]\ttrain-rmse:1.653996\n",
      "[13555]\ttrain-rmse:1.653990\n",
      "[13556]\ttrain-rmse:1.653984\n",
      "[13557]\ttrain-rmse:1.653979\n",
      "[13558]\ttrain-rmse:1.653971\n",
      "[13559]\ttrain-rmse:1.653966\n",
      "[13560]\ttrain-rmse:1.653954\n",
      "[13561]\ttrain-rmse:1.653939\n",
      "[13562]\ttrain-rmse:1.653933\n",
      "[13563]\ttrain-rmse:1.653929\n",
      "[13564]\ttrain-rmse:1.653918\n",
      "[13565]\ttrain-rmse:1.653912\n",
      "[13566]\ttrain-rmse:1.653900\n",
      "[13567]\ttrain-rmse:1.653894\n",
      "[13568]\ttrain-rmse:1.653886\n",
      "[13569]\ttrain-rmse:1.653879\n",
      "[13570]\ttrain-rmse:1.653871\n",
      "[13571]\ttrain-rmse:1.653865\n",
      "[13572]\ttrain-rmse:1.653853\n",
      "[13573]\ttrain-rmse:1.653843\n",
      "[13574]\ttrain-rmse:1.653839\n",
      "[13575]\ttrain-rmse:1.653831\n",
      "[13576]\ttrain-rmse:1.653822\n",
      "[13577]\ttrain-rmse:1.653813\n",
      "[13578]\ttrain-rmse:1.653802\n",
      "[13579]\ttrain-rmse:1.653792\n",
      "[13580]\ttrain-rmse:1.653786\n",
      "[13581]\ttrain-rmse:1.653774\n",
      "[13582]\ttrain-rmse:1.653767\n",
      "[13583]\ttrain-rmse:1.653756\n",
      "[13584]\ttrain-rmse:1.653745\n",
      "[13585]\ttrain-rmse:1.653736\n",
      "[13586]\ttrain-rmse:1.653726\n",
      "[13587]\ttrain-rmse:1.653716\n",
      "[13588]\ttrain-rmse:1.653705\n",
      "[13589]\ttrain-rmse:1.653695\n",
      "[13590]\ttrain-rmse:1.653685\n",
      "[13591]\ttrain-rmse:1.653672\n",
      "[13592]\ttrain-rmse:1.653666\n",
      "[13593]\ttrain-rmse:1.653660\n",
      "[13594]\ttrain-rmse:1.653656\n",
      "[13595]\ttrain-rmse:1.653650\n",
      "[13596]\ttrain-rmse:1.653644\n",
      "[13597]\ttrain-rmse:1.653633\n",
      "[13598]\ttrain-rmse:1.653619\n",
      "[13599]\ttrain-rmse:1.653613\n",
      "[13600]\ttrain-rmse:1.653605\n",
      "[13601]\ttrain-rmse:1.653598\n",
      "[13602]\ttrain-rmse:1.653587\n",
      "[13603]\ttrain-rmse:1.653580\n",
      "[13604]\ttrain-rmse:1.653569\n",
      "[13605]\ttrain-rmse:1.653560\n",
      "[13606]\ttrain-rmse:1.653553\n",
      "[13607]\ttrain-rmse:1.653548\n",
      "[13608]\ttrain-rmse:1.653542\n",
      "[13609]\ttrain-rmse:1.653537\n",
      "[13610]\ttrain-rmse:1.653523\n",
      "[13611]\ttrain-rmse:1.653518\n",
      "[13612]\ttrain-rmse:1.653505\n",
      "[13613]\ttrain-rmse:1.653494\n",
      "[13614]\ttrain-rmse:1.653486\n",
      "[13615]\ttrain-rmse:1.653477\n",
      "[13616]\ttrain-rmse:1.653471\n",
      "[13617]\ttrain-rmse:1.653465\n",
      "[13618]\ttrain-rmse:1.653455\n",
      "[13619]\ttrain-rmse:1.653444\n",
      "[13620]\ttrain-rmse:1.653437\n",
      "[13621]\ttrain-rmse:1.653430\n",
      "[13622]\ttrain-rmse:1.653424\n",
      "[13623]\ttrain-rmse:1.653413\n",
      "[13624]\ttrain-rmse:1.653401\n",
      "[13625]\ttrain-rmse:1.653386\n",
      "[13626]\ttrain-rmse:1.653376\n",
      "[13627]\ttrain-rmse:1.653360\n",
      "[13628]\ttrain-rmse:1.653353\n",
      "[13629]\ttrain-rmse:1.653345\n",
      "[13630]\ttrain-rmse:1.653337\n",
      "[13631]\ttrain-rmse:1.653324\n",
      "[13632]\ttrain-rmse:1.653311\n",
      "[13633]\ttrain-rmse:1.653302\n",
      "[13634]\ttrain-rmse:1.653291\n",
      "[13635]\ttrain-rmse:1.653286\n",
      "[13636]\ttrain-rmse:1.653276\n",
      "[13637]\ttrain-rmse:1.653271\n",
      "[13638]\ttrain-rmse:1.653264\n",
      "[13639]\ttrain-rmse:1.653252\n",
      "[13640]\ttrain-rmse:1.653242\n",
      "[13641]\ttrain-rmse:1.653232\n",
      "[13642]\ttrain-rmse:1.653227\n",
      "[13643]\ttrain-rmse:1.653213\n",
      "[13644]\ttrain-rmse:1.653209\n",
      "[13645]\ttrain-rmse:1.653205\n",
      "[13646]\ttrain-rmse:1.653195\n",
      "[13647]\ttrain-rmse:1.653187\n",
      "[13648]\ttrain-rmse:1.653177\n",
      "[13649]\ttrain-rmse:1.653170\n",
      "[13650]\ttrain-rmse:1.653160\n",
      "[13651]\ttrain-rmse:1.653147\n",
      "[13652]\ttrain-rmse:1.653141\n",
      "[13653]\ttrain-rmse:1.653135\n",
      "[13654]\ttrain-rmse:1.653130\n",
      "[13655]\ttrain-rmse:1.653122\n",
      "[13656]\ttrain-rmse:1.653110\n",
      "[13657]\ttrain-rmse:1.653107\n",
      "[13658]\ttrain-rmse:1.653099\n",
      "[13659]\ttrain-rmse:1.653090\n",
      "[13660]\ttrain-rmse:1.653080\n",
      "[13661]\ttrain-rmse:1.653073\n",
      "[13662]\ttrain-rmse:1.653055\n",
      "[13663]\ttrain-rmse:1.653046\n",
      "[13664]\ttrain-rmse:1.653037\n",
      "[13665]\ttrain-rmse:1.653027\n",
      "[13666]\ttrain-rmse:1.653021\n",
      "[13667]\ttrain-rmse:1.653012\n",
      "[13668]\ttrain-rmse:1.653007\n",
      "[13669]\ttrain-rmse:1.652998\n",
      "[13670]\ttrain-rmse:1.652987\n",
      "[13671]\ttrain-rmse:1.652981\n",
      "[13672]\ttrain-rmse:1.652974\n",
      "[13673]\ttrain-rmse:1.652961\n",
      "[13674]\ttrain-rmse:1.652949\n",
      "[13675]\ttrain-rmse:1.652946\n",
      "[13676]\ttrain-rmse:1.652936\n",
      "[13677]\ttrain-rmse:1.652929\n",
      "[13678]\ttrain-rmse:1.652921\n",
      "[13679]\ttrain-rmse:1.652916\n",
      "[13680]\ttrain-rmse:1.652905\n",
      "[13681]\ttrain-rmse:1.652893\n",
      "[13682]\ttrain-rmse:1.652887\n",
      "[13683]\ttrain-rmse:1.652876\n",
      "[13684]\ttrain-rmse:1.652870\n",
      "[13685]\ttrain-rmse:1.652859\n",
      "[13686]\ttrain-rmse:1.652851\n",
      "[13687]\ttrain-rmse:1.652841\n",
      "[13688]\ttrain-rmse:1.652832\n",
      "[13689]\ttrain-rmse:1.652828\n",
      "[13690]\ttrain-rmse:1.652817\n",
      "[13691]\ttrain-rmse:1.652806\n",
      "[13692]\ttrain-rmse:1.652800\n",
      "[13693]\ttrain-rmse:1.652793\n",
      "[13694]\ttrain-rmse:1.652785\n",
      "[13695]\ttrain-rmse:1.652774\n",
      "[13696]\ttrain-rmse:1.652762\n",
      "[13697]\ttrain-rmse:1.652753\n",
      "[13698]\ttrain-rmse:1.652744\n",
      "[13699]\ttrain-rmse:1.652738\n",
      "[13700]\ttrain-rmse:1.652732\n",
      "[13701]\ttrain-rmse:1.652729\n",
      "[13702]\ttrain-rmse:1.652723\n",
      "[13703]\ttrain-rmse:1.652710\n",
      "[13704]\ttrain-rmse:1.652700\n",
      "[13705]\ttrain-rmse:1.652689\n",
      "[13706]\ttrain-rmse:1.652684\n",
      "[13707]\ttrain-rmse:1.652673\n",
      "[13708]\ttrain-rmse:1.652664\n",
      "[13709]\ttrain-rmse:1.652649\n",
      "[13710]\ttrain-rmse:1.652645\n",
      "[13711]\ttrain-rmse:1.652633\n",
      "[13712]\ttrain-rmse:1.652627\n",
      "[13713]\ttrain-rmse:1.652620\n",
      "[13714]\ttrain-rmse:1.652607\n",
      "[13715]\ttrain-rmse:1.652599\n",
      "[13716]\ttrain-rmse:1.652594\n",
      "[13717]\ttrain-rmse:1.652587\n",
      "[13718]\ttrain-rmse:1.652577\n",
      "[13719]\ttrain-rmse:1.652565\n",
      "[13720]\ttrain-rmse:1.652555\n",
      "[13721]\ttrain-rmse:1.652544\n",
      "[13722]\ttrain-rmse:1.652535\n",
      "[13723]\ttrain-rmse:1.652530\n",
      "[13724]\ttrain-rmse:1.652523\n",
      "[13725]\ttrain-rmse:1.652515\n",
      "[13726]\ttrain-rmse:1.652508\n",
      "[13727]\ttrain-rmse:1.652495\n",
      "[13728]\ttrain-rmse:1.652482\n",
      "[13729]\ttrain-rmse:1.652475\n",
      "[13730]\ttrain-rmse:1.652464\n",
      "[13731]\ttrain-rmse:1.652458\n",
      "[13732]\ttrain-rmse:1.652452\n",
      "[13733]\ttrain-rmse:1.652442\n",
      "[13734]\ttrain-rmse:1.652432\n",
      "[13735]\ttrain-rmse:1.652423\n",
      "[13736]\ttrain-rmse:1.652414\n",
      "[13737]\ttrain-rmse:1.652398\n",
      "[13738]\ttrain-rmse:1.652395\n",
      "[13739]\ttrain-rmse:1.652389\n",
      "[13740]\ttrain-rmse:1.652381\n",
      "[13741]\ttrain-rmse:1.652374\n",
      "[13742]\ttrain-rmse:1.652367\n",
      "[13743]\ttrain-rmse:1.652357\n",
      "[13744]\ttrain-rmse:1.652347\n",
      "[13745]\ttrain-rmse:1.652340\n",
      "[13746]\ttrain-rmse:1.652332\n",
      "[13747]\ttrain-rmse:1.652323\n",
      "[13748]\ttrain-rmse:1.652314\n",
      "[13749]\ttrain-rmse:1.652305\n",
      "[13750]\ttrain-rmse:1.652298\n",
      "[13751]\ttrain-rmse:1.652285\n",
      "[13752]\ttrain-rmse:1.652278\n",
      "[13753]\ttrain-rmse:1.652269\n",
      "[13754]\ttrain-rmse:1.652261\n",
      "[13755]\ttrain-rmse:1.652253\n",
      "[13756]\ttrain-rmse:1.652242\n",
      "[13757]\ttrain-rmse:1.652234\n",
      "[13758]\ttrain-rmse:1.652223\n",
      "[13759]\ttrain-rmse:1.652212\n",
      "[13760]\ttrain-rmse:1.652208\n",
      "[13761]\ttrain-rmse:1.652199\n",
      "[13762]\ttrain-rmse:1.652188\n",
      "[13763]\ttrain-rmse:1.652175\n",
      "[13764]\ttrain-rmse:1.652173\n",
      "[13765]\ttrain-rmse:1.652163\n",
      "[13766]\ttrain-rmse:1.652155\n",
      "[13767]\ttrain-rmse:1.652150\n",
      "[13768]\ttrain-rmse:1.652142\n",
      "[13769]\ttrain-rmse:1.652135\n",
      "[13770]\ttrain-rmse:1.652122\n",
      "[13771]\ttrain-rmse:1.652117\n",
      "[13772]\ttrain-rmse:1.652109\n",
      "[13773]\ttrain-rmse:1.652092\n",
      "[13774]\ttrain-rmse:1.652084\n",
      "[13775]\ttrain-rmse:1.652076\n",
      "[13776]\ttrain-rmse:1.652071\n",
      "[13777]\ttrain-rmse:1.652061\n",
      "[13778]\ttrain-rmse:1.652055\n",
      "[13779]\ttrain-rmse:1.652045\n",
      "[13780]\ttrain-rmse:1.652034\n",
      "[13781]\ttrain-rmse:1.652021\n",
      "[13782]\ttrain-rmse:1.652016\n",
      "[13783]\ttrain-rmse:1.652007\n",
      "[13784]\ttrain-rmse:1.652003\n",
      "[13785]\ttrain-rmse:1.651991\n",
      "[13786]\ttrain-rmse:1.651983\n",
      "[13787]\ttrain-rmse:1.651971\n",
      "[13788]\ttrain-rmse:1.651965\n",
      "[13789]\ttrain-rmse:1.651957\n",
      "[13790]\ttrain-rmse:1.651950\n",
      "[13791]\ttrain-rmse:1.651942\n",
      "[13792]\ttrain-rmse:1.651936\n",
      "[13793]\ttrain-rmse:1.651933\n",
      "[13794]\ttrain-rmse:1.651925\n",
      "[13795]\ttrain-rmse:1.651914\n",
      "[13796]\ttrain-rmse:1.651906\n",
      "[13797]\ttrain-rmse:1.651897\n",
      "[13798]\ttrain-rmse:1.651889\n",
      "[13799]\ttrain-rmse:1.651885\n",
      "[13800]\ttrain-rmse:1.651873\n",
      "[13801]\ttrain-rmse:1.651867\n",
      "[13802]\ttrain-rmse:1.651857\n",
      "[13803]\ttrain-rmse:1.651852\n",
      "[13804]\ttrain-rmse:1.651839\n",
      "[13805]\ttrain-rmse:1.651826\n",
      "[13806]\ttrain-rmse:1.651813\n",
      "[13807]\ttrain-rmse:1.651807\n",
      "[13808]\ttrain-rmse:1.651798\n",
      "[13809]\ttrain-rmse:1.651792\n",
      "[13810]\ttrain-rmse:1.651783\n",
      "[13811]\ttrain-rmse:1.651773\n",
      "[13812]\ttrain-rmse:1.651765\n",
      "[13813]\ttrain-rmse:1.651757\n",
      "[13814]\ttrain-rmse:1.651748\n",
      "[13815]\ttrain-rmse:1.651744\n",
      "[13816]\ttrain-rmse:1.651740\n",
      "[13817]\ttrain-rmse:1.651724\n",
      "[13818]\ttrain-rmse:1.651719\n",
      "[13819]\ttrain-rmse:1.651714\n",
      "[13820]\ttrain-rmse:1.651709\n",
      "[13821]\ttrain-rmse:1.651701\n",
      "[13822]\ttrain-rmse:1.651692\n",
      "[13823]\ttrain-rmse:1.651682\n",
      "[13824]\ttrain-rmse:1.651676\n",
      "[13825]\ttrain-rmse:1.651673\n",
      "[13826]\ttrain-rmse:1.651662\n",
      "[13827]\ttrain-rmse:1.651654\n",
      "[13828]\ttrain-rmse:1.651643\n",
      "[13829]\ttrain-rmse:1.651631\n",
      "[13830]\ttrain-rmse:1.651620\n",
      "[13831]\ttrain-rmse:1.651612\n",
      "[13832]\ttrain-rmse:1.651604\n",
      "[13833]\ttrain-rmse:1.651596\n",
      "[13834]\ttrain-rmse:1.651588\n",
      "[13835]\ttrain-rmse:1.651576\n",
      "[13836]\ttrain-rmse:1.651567\n",
      "[13837]\ttrain-rmse:1.651556\n",
      "[13838]\ttrain-rmse:1.651543\n",
      "[13839]\ttrain-rmse:1.651537\n",
      "[13840]\ttrain-rmse:1.651527\n",
      "[13841]\ttrain-rmse:1.651513\n",
      "[13842]\ttrain-rmse:1.651502\n",
      "[13843]\ttrain-rmse:1.651491\n",
      "[13844]\ttrain-rmse:1.651481\n",
      "[13845]\ttrain-rmse:1.651473\n",
      "[13846]\ttrain-rmse:1.651467\n",
      "[13847]\ttrain-rmse:1.651459\n",
      "[13848]\ttrain-rmse:1.651454\n",
      "[13849]\ttrain-rmse:1.651445\n",
      "[13850]\ttrain-rmse:1.651437\n",
      "[13851]\ttrain-rmse:1.651427\n",
      "[13852]\ttrain-rmse:1.651420\n",
      "[13853]\ttrain-rmse:1.651408\n",
      "[13854]\ttrain-rmse:1.651399\n",
      "[13855]\ttrain-rmse:1.651389\n",
      "[13856]\ttrain-rmse:1.651379\n",
      "[13857]\ttrain-rmse:1.651369\n",
      "[13858]\ttrain-rmse:1.651360\n",
      "[13859]\ttrain-rmse:1.651348\n",
      "[13860]\ttrain-rmse:1.651339\n",
      "[13861]\ttrain-rmse:1.651329\n",
      "[13862]\ttrain-rmse:1.651320\n",
      "[13863]\ttrain-rmse:1.651310\n",
      "[13864]\ttrain-rmse:1.651304\n",
      "[13865]\ttrain-rmse:1.651296\n",
      "[13866]\ttrain-rmse:1.651290\n",
      "[13867]\ttrain-rmse:1.651275\n",
      "[13868]\ttrain-rmse:1.651265\n",
      "[13869]\ttrain-rmse:1.651258\n",
      "[13870]\ttrain-rmse:1.651253\n",
      "[13871]\ttrain-rmse:1.651247\n",
      "[13872]\ttrain-rmse:1.651239\n",
      "[13873]\ttrain-rmse:1.651234\n",
      "[13874]\ttrain-rmse:1.651224\n",
      "[13875]\ttrain-rmse:1.651216\n",
      "[13876]\ttrain-rmse:1.651210\n",
      "[13877]\ttrain-rmse:1.651205\n",
      "[13878]\ttrain-rmse:1.651192\n",
      "[13879]\ttrain-rmse:1.651185\n",
      "[13880]\ttrain-rmse:1.651177\n",
      "[13881]\ttrain-rmse:1.651172\n",
      "[13882]\ttrain-rmse:1.651161\n",
      "[13883]\ttrain-rmse:1.651150\n",
      "[13884]\ttrain-rmse:1.651143\n",
      "[13885]\ttrain-rmse:1.651131\n",
      "[13886]\ttrain-rmse:1.651123\n",
      "[13887]\ttrain-rmse:1.651116\n",
      "[13888]\ttrain-rmse:1.651101\n",
      "[13889]\ttrain-rmse:1.651090\n",
      "[13890]\ttrain-rmse:1.651075\n",
      "[13891]\ttrain-rmse:1.651069\n",
      "[13892]\ttrain-rmse:1.651065\n",
      "[13893]\ttrain-rmse:1.651061\n",
      "[13894]\ttrain-rmse:1.651052\n",
      "[13895]\ttrain-rmse:1.651044\n",
      "[13896]\ttrain-rmse:1.651027\n",
      "[13897]\ttrain-rmse:1.651023\n",
      "[13898]\ttrain-rmse:1.651015\n",
      "[13899]\ttrain-rmse:1.651006\n",
      "[13900]\ttrain-rmse:1.651000\n",
      "[13901]\ttrain-rmse:1.650990\n",
      "[13902]\ttrain-rmse:1.650979\n",
      "[13903]\ttrain-rmse:1.650973\n",
      "[13904]\ttrain-rmse:1.650966\n",
      "[13905]\ttrain-rmse:1.650956\n",
      "[13906]\ttrain-rmse:1.650947\n",
      "[13907]\ttrain-rmse:1.650938\n",
      "[13908]\ttrain-rmse:1.650926\n",
      "[13909]\ttrain-rmse:1.650924\n",
      "[13910]\ttrain-rmse:1.650919\n",
      "[13911]\ttrain-rmse:1.650908\n",
      "[13912]\ttrain-rmse:1.650900\n",
      "[13913]\ttrain-rmse:1.650894\n",
      "[13914]\ttrain-rmse:1.650883\n",
      "[13915]\ttrain-rmse:1.650876\n",
      "[13916]\ttrain-rmse:1.650870\n",
      "[13917]\ttrain-rmse:1.650864\n",
      "[13918]\ttrain-rmse:1.650854\n",
      "[13919]\ttrain-rmse:1.650845\n",
      "[13920]\ttrain-rmse:1.650833\n",
      "[13921]\ttrain-rmse:1.650822\n",
      "[13922]\ttrain-rmse:1.650816\n",
      "[13923]\ttrain-rmse:1.650813\n",
      "[13924]\ttrain-rmse:1.650805\n",
      "[13925]\ttrain-rmse:1.650800\n",
      "[13926]\ttrain-rmse:1.650789\n",
      "[13927]\ttrain-rmse:1.650785\n",
      "[13928]\ttrain-rmse:1.650767\n",
      "[13929]\ttrain-rmse:1.650757\n",
      "[13930]\ttrain-rmse:1.650748\n",
      "[13931]\ttrain-rmse:1.650742\n",
      "[13932]\ttrain-rmse:1.650734\n",
      "[13933]\ttrain-rmse:1.650723\n",
      "[13934]\ttrain-rmse:1.650713\n",
      "[13935]\ttrain-rmse:1.650709\n",
      "[13936]\ttrain-rmse:1.650702\n",
      "[13937]\ttrain-rmse:1.650696\n",
      "[13938]\ttrain-rmse:1.650689\n",
      "[13939]\ttrain-rmse:1.650683\n",
      "[13940]\ttrain-rmse:1.650675\n",
      "[13941]\ttrain-rmse:1.650662\n",
      "[13942]\ttrain-rmse:1.650653\n",
      "[13943]\ttrain-rmse:1.650649\n",
      "[13944]\ttrain-rmse:1.650639\n",
      "[13945]\ttrain-rmse:1.650635\n",
      "[13946]\ttrain-rmse:1.650623\n",
      "[13947]\ttrain-rmse:1.650616\n",
      "[13948]\ttrain-rmse:1.650604\n",
      "[13949]\ttrain-rmse:1.650593\n",
      "[13950]\ttrain-rmse:1.650583\n",
      "[13951]\ttrain-rmse:1.650575\n",
      "[13952]\ttrain-rmse:1.650566\n",
      "[13953]\ttrain-rmse:1.650560\n",
      "[13954]\ttrain-rmse:1.650554\n",
      "[13955]\ttrain-rmse:1.650545\n",
      "[13956]\ttrain-rmse:1.650538\n",
      "[13957]\ttrain-rmse:1.650523\n",
      "[13958]\ttrain-rmse:1.650511\n",
      "[13959]\ttrain-rmse:1.650506\n",
      "[13960]\ttrain-rmse:1.650498\n",
      "[13961]\ttrain-rmse:1.650491\n",
      "[13962]\ttrain-rmse:1.650484\n",
      "[13963]\ttrain-rmse:1.650473\n",
      "[13964]\ttrain-rmse:1.650464\n",
      "[13965]\ttrain-rmse:1.650459\n",
      "[13966]\ttrain-rmse:1.650446\n",
      "[13967]\ttrain-rmse:1.650437\n",
      "[13968]\ttrain-rmse:1.650430\n",
      "[13969]\ttrain-rmse:1.650422\n",
      "[13970]\ttrain-rmse:1.650410\n",
      "[13971]\ttrain-rmse:1.650402\n",
      "[13972]\ttrain-rmse:1.650390\n",
      "[13973]\ttrain-rmse:1.650383\n",
      "[13974]\ttrain-rmse:1.650379\n",
      "[13975]\ttrain-rmse:1.650367\n",
      "[13976]\ttrain-rmse:1.650359\n",
      "[13977]\ttrain-rmse:1.650351\n",
      "[13978]\ttrain-rmse:1.650347\n",
      "[13979]\ttrain-rmse:1.650341\n",
      "[13980]\ttrain-rmse:1.650331\n",
      "[13981]\ttrain-rmse:1.650321\n",
      "[13982]\ttrain-rmse:1.650313\n",
      "[13983]\ttrain-rmse:1.650303\n",
      "[13984]\ttrain-rmse:1.650294\n",
      "[13985]\ttrain-rmse:1.650289\n",
      "[13986]\ttrain-rmse:1.650280\n",
      "[13987]\ttrain-rmse:1.650265\n",
      "[13988]\ttrain-rmse:1.650253\n",
      "[13989]\ttrain-rmse:1.650246\n",
      "[13990]\ttrain-rmse:1.650237\n",
      "[13991]\ttrain-rmse:1.650228\n",
      "[13992]\ttrain-rmse:1.650226\n",
      "[13993]\ttrain-rmse:1.650214\n",
      "[13994]\ttrain-rmse:1.650200\n",
      "[13995]\ttrain-rmse:1.650195\n",
      "[13996]\ttrain-rmse:1.650187\n",
      "[13997]\ttrain-rmse:1.650176\n",
      "[13998]\ttrain-rmse:1.650164\n",
      "[13999]\ttrain-rmse:1.650155\n",
      "[14000]\ttrain-rmse:1.650148\n",
      "[14001]\ttrain-rmse:1.650137\n",
      "[14002]\ttrain-rmse:1.650129\n",
      "[14003]\ttrain-rmse:1.650120\n",
      "[14004]\ttrain-rmse:1.650112\n",
      "[14005]\ttrain-rmse:1.650103\n",
      "[14006]\ttrain-rmse:1.650099\n",
      "[14007]\ttrain-rmse:1.650089\n",
      "[14008]\ttrain-rmse:1.650082\n",
      "[14009]\ttrain-rmse:1.650077\n",
      "[14010]\ttrain-rmse:1.650069\n",
      "[14011]\ttrain-rmse:1.650059\n",
      "[14012]\ttrain-rmse:1.650054\n",
      "[14013]\ttrain-rmse:1.650044\n",
      "[14014]\ttrain-rmse:1.650041\n",
      "[14015]\ttrain-rmse:1.650033\n",
      "[14016]\ttrain-rmse:1.650026\n",
      "[14017]\ttrain-rmse:1.650018\n",
      "[14018]\ttrain-rmse:1.650010\n",
      "[14019]\ttrain-rmse:1.649997\n",
      "[14020]\ttrain-rmse:1.649985\n",
      "[14021]\ttrain-rmse:1.649983\n",
      "[14022]\ttrain-rmse:1.649977\n",
      "[14023]\ttrain-rmse:1.649973\n",
      "[14024]\ttrain-rmse:1.649968\n",
      "[14025]\ttrain-rmse:1.649965\n",
      "[14026]\ttrain-rmse:1.649954\n",
      "[14027]\ttrain-rmse:1.649947\n",
      "[14028]\ttrain-rmse:1.649939\n",
      "[14029]\ttrain-rmse:1.649929\n",
      "[14030]\ttrain-rmse:1.649920\n",
      "[14031]\ttrain-rmse:1.649909\n",
      "[14032]\ttrain-rmse:1.649897\n",
      "[14033]\ttrain-rmse:1.649888\n",
      "[14034]\ttrain-rmse:1.649877\n",
      "[14035]\ttrain-rmse:1.649865\n",
      "[14036]\ttrain-rmse:1.649862\n",
      "[14037]\ttrain-rmse:1.649857\n",
      "[14038]\ttrain-rmse:1.649854\n",
      "[14039]\ttrain-rmse:1.649838\n",
      "[14040]\ttrain-rmse:1.649830\n",
      "[14041]\ttrain-rmse:1.649818\n",
      "[14042]\ttrain-rmse:1.649812\n",
      "[14043]\ttrain-rmse:1.649808\n",
      "[14044]\ttrain-rmse:1.649798\n",
      "[14045]\ttrain-rmse:1.649789\n",
      "[14046]\ttrain-rmse:1.649786\n",
      "[14047]\ttrain-rmse:1.649779\n",
      "[14048]\ttrain-rmse:1.649768\n",
      "[14049]\ttrain-rmse:1.649758\n",
      "[14050]\ttrain-rmse:1.649748\n",
      "[14051]\ttrain-rmse:1.649740\n",
      "[14052]\ttrain-rmse:1.649729\n",
      "[14053]\ttrain-rmse:1.649713\n",
      "[14054]\ttrain-rmse:1.649700\n",
      "[14055]\ttrain-rmse:1.649690\n",
      "[14056]\ttrain-rmse:1.649678\n",
      "[14057]\ttrain-rmse:1.649665\n",
      "[14058]\ttrain-rmse:1.649654\n",
      "[14059]\ttrain-rmse:1.649650\n",
      "[14060]\ttrain-rmse:1.649636\n",
      "[14061]\ttrain-rmse:1.649630\n",
      "[14062]\ttrain-rmse:1.649619\n",
      "[14063]\ttrain-rmse:1.649607\n",
      "[14064]\ttrain-rmse:1.649597\n",
      "[14065]\ttrain-rmse:1.649583\n",
      "[14066]\ttrain-rmse:1.649578\n",
      "[14067]\ttrain-rmse:1.649573\n",
      "[14068]\ttrain-rmse:1.649565\n",
      "[14069]\ttrain-rmse:1.649553\n",
      "[14070]\ttrain-rmse:1.649546\n",
      "[14071]\ttrain-rmse:1.649535\n",
      "[14072]\ttrain-rmse:1.649529\n",
      "[14073]\ttrain-rmse:1.649523\n",
      "[14074]\ttrain-rmse:1.649519\n",
      "[14075]\ttrain-rmse:1.649506\n",
      "[14076]\ttrain-rmse:1.649498\n",
      "[14077]\ttrain-rmse:1.649490\n",
      "[14078]\ttrain-rmse:1.649483\n",
      "[14079]\ttrain-rmse:1.649475\n",
      "[14080]\ttrain-rmse:1.649466\n",
      "[14081]\ttrain-rmse:1.649458\n",
      "[14082]\ttrain-rmse:1.649449\n",
      "[14083]\ttrain-rmse:1.649438\n",
      "[14084]\ttrain-rmse:1.649428\n",
      "[14085]\ttrain-rmse:1.649419\n",
      "[14086]\ttrain-rmse:1.649413\n",
      "[14087]\ttrain-rmse:1.649408\n",
      "[14088]\ttrain-rmse:1.649404\n",
      "[14089]\ttrain-rmse:1.649400\n",
      "[14090]\ttrain-rmse:1.649392\n",
      "[14091]\ttrain-rmse:1.649384\n",
      "[14092]\ttrain-rmse:1.649378\n",
      "[14093]\ttrain-rmse:1.649365\n",
      "[14094]\ttrain-rmse:1.649357\n",
      "[14095]\ttrain-rmse:1.649346\n",
      "[14096]\ttrain-rmse:1.649336\n",
      "[14097]\ttrain-rmse:1.649326\n",
      "[14098]\ttrain-rmse:1.649317\n",
      "[14099]\ttrain-rmse:1.649310\n",
      "[14100]\ttrain-rmse:1.649305\n",
      "[14101]\ttrain-rmse:1.649297\n",
      "[14102]\ttrain-rmse:1.649287\n",
      "[14103]\ttrain-rmse:1.649277\n",
      "[14104]\ttrain-rmse:1.649275\n",
      "[14105]\ttrain-rmse:1.649266\n",
      "[14106]\ttrain-rmse:1.649258\n",
      "[14107]\ttrain-rmse:1.649248\n",
      "[14108]\ttrain-rmse:1.649241\n",
      "[14109]\ttrain-rmse:1.649236\n",
      "[14110]\ttrain-rmse:1.649228\n",
      "[14111]\ttrain-rmse:1.649220\n",
      "[14112]\ttrain-rmse:1.649211\n",
      "[14113]\ttrain-rmse:1.649205\n",
      "[14114]\ttrain-rmse:1.649195\n",
      "[14115]\ttrain-rmse:1.649187\n",
      "[14116]\ttrain-rmse:1.649180\n",
      "[14117]\ttrain-rmse:1.649170\n",
      "[14118]\ttrain-rmse:1.649162\n",
      "[14119]\ttrain-rmse:1.649152\n",
      "[14120]\ttrain-rmse:1.649142\n",
      "[14121]\ttrain-rmse:1.649133\n",
      "[14122]\ttrain-rmse:1.649127\n",
      "[14123]\ttrain-rmse:1.649120\n",
      "[14124]\ttrain-rmse:1.649113\n",
      "[14125]\ttrain-rmse:1.649108\n",
      "[14126]\ttrain-rmse:1.649104\n",
      "[14127]\ttrain-rmse:1.649096\n",
      "[14128]\ttrain-rmse:1.649089\n",
      "[14129]\ttrain-rmse:1.649083\n",
      "[14130]\ttrain-rmse:1.649077\n",
      "[14131]\ttrain-rmse:1.649073\n",
      "[14132]\ttrain-rmse:1.649061\n",
      "[14133]\ttrain-rmse:1.649056\n",
      "[14134]\ttrain-rmse:1.649049\n",
      "[14135]\ttrain-rmse:1.649035\n",
      "[14136]\ttrain-rmse:1.649028\n",
      "[14137]\ttrain-rmse:1.649020\n",
      "[14138]\ttrain-rmse:1.649007\n",
      "[14139]\ttrain-rmse:1.648993\n",
      "[14140]\ttrain-rmse:1.648983\n",
      "[14141]\ttrain-rmse:1.648975\n",
      "[14142]\ttrain-rmse:1.648969\n",
      "[14143]\ttrain-rmse:1.648962\n",
      "[14144]\ttrain-rmse:1.648954\n",
      "[14145]\ttrain-rmse:1.648943\n",
      "[14146]\ttrain-rmse:1.648936\n",
      "[14147]\ttrain-rmse:1.648930\n",
      "[14148]\ttrain-rmse:1.648922\n",
      "[14149]\ttrain-rmse:1.648913\n",
      "[14150]\ttrain-rmse:1.648898\n",
      "[14151]\ttrain-rmse:1.648893\n",
      "[14152]\ttrain-rmse:1.648880\n",
      "[14153]\ttrain-rmse:1.648872\n",
      "[14154]\ttrain-rmse:1.648868\n",
      "[14155]\ttrain-rmse:1.648862\n",
      "[14156]\ttrain-rmse:1.648855\n",
      "[14157]\ttrain-rmse:1.648851\n",
      "[14158]\ttrain-rmse:1.648841\n",
      "[14159]\ttrain-rmse:1.648833\n",
      "[14160]\ttrain-rmse:1.648823\n",
      "[14161]\ttrain-rmse:1.648816\n",
      "[14162]\ttrain-rmse:1.648804\n",
      "[14163]\ttrain-rmse:1.648796\n",
      "[14164]\ttrain-rmse:1.648786\n",
      "[14165]\ttrain-rmse:1.648776\n",
      "[14166]\ttrain-rmse:1.648772\n",
      "[14167]\ttrain-rmse:1.648764\n",
      "[14168]\ttrain-rmse:1.648754\n",
      "[14169]\ttrain-rmse:1.648747\n",
      "[14170]\ttrain-rmse:1.648740\n",
      "[14171]\ttrain-rmse:1.648731\n",
      "[14172]\ttrain-rmse:1.648723\n",
      "[14173]\ttrain-rmse:1.648716\n",
      "[14174]\ttrain-rmse:1.648705\n",
      "[14175]\ttrain-rmse:1.648697\n",
      "[14176]\ttrain-rmse:1.648688\n",
      "[14177]\ttrain-rmse:1.648677\n",
      "[14178]\ttrain-rmse:1.648670\n",
      "[14179]\ttrain-rmse:1.648661\n",
      "[14180]\ttrain-rmse:1.648653\n",
      "[14181]\ttrain-rmse:1.648643\n",
      "[14182]\ttrain-rmse:1.648634\n",
      "[14183]\ttrain-rmse:1.648625\n",
      "[14184]\ttrain-rmse:1.648621\n",
      "[14185]\ttrain-rmse:1.648609\n",
      "[14186]\ttrain-rmse:1.648603\n",
      "[14187]\ttrain-rmse:1.648593\n",
      "[14188]\ttrain-rmse:1.648582\n",
      "[14189]\ttrain-rmse:1.648576\n",
      "[14190]\ttrain-rmse:1.648566\n",
      "[14191]\ttrain-rmse:1.648549\n",
      "[14192]\ttrain-rmse:1.648543\n",
      "[14193]\ttrain-rmse:1.648537\n",
      "[14194]\ttrain-rmse:1.648529\n",
      "[14195]\ttrain-rmse:1.648518\n",
      "[14196]\ttrain-rmse:1.648512\n",
      "[14197]\ttrain-rmse:1.648502\n",
      "[14198]\ttrain-rmse:1.648498\n",
      "[14199]\ttrain-rmse:1.648494\n",
      "[14200]\ttrain-rmse:1.648485\n",
      "[14201]\ttrain-rmse:1.648476\n",
      "[14202]\ttrain-rmse:1.648468\n",
      "[14203]\ttrain-rmse:1.648461\n",
      "[14204]\ttrain-rmse:1.648453\n",
      "[14205]\ttrain-rmse:1.648447\n",
      "[14206]\ttrain-rmse:1.648441\n",
      "[14207]\ttrain-rmse:1.648431\n",
      "[14208]\ttrain-rmse:1.648420\n",
      "[14209]\ttrain-rmse:1.648415\n",
      "[14210]\ttrain-rmse:1.648408\n",
      "[14211]\ttrain-rmse:1.648402\n",
      "[14212]\ttrain-rmse:1.648397\n",
      "[14213]\ttrain-rmse:1.648386\n",
      "[14214]\ttrain-rmse:1.648377\n",
      "[14215]\ttrain-rmse:1.648372\n",
      "[14216]\ttrain-rmse:1.648366\n",
      "[14217]\ttrain-rmse:1.648352\n",
      "[14218]\ttrain-rmse:1.648345\n",
      "[14219]\ttrain-rmse:1.648343\n",
      "[14220]\ttrain-rmse:1.648335\n",
      "[14221]\ttrain-rmse:1.648324\n",
      "[14222]\ttrain-rmse:1.648317\n",
      "[14223]\ttrain-rmse:1.648305\n",
      "[14224]\ttrain-rmse:1.648298\n",
      "[14225]\ttrain-rmse:1.648292\n",
      "[14226]\ttrain-rmse:1.648277\n",
      "[14227]\ttrain-rmse:1.648267\n",
      "[14228]\ttrain-rmse:1.648258\n",
      "[14229]\ttrain-rmse:1.648252\n",
      "[14230]\ttrain-rmse:1.648240\n",
      "[14231]\ttrain-rmse:1.648234\n",
      "[14232]\ttrain-rmse:1.648227\n",
      "[14233]\ttrain-rmse:1.648215\n",
      "[14234]\ttrain-rmse:1.648207\n",
      "[14235]\ttrain-rmse:1.648201\n",
      "[14236]\ttrain-rmse:1.648195\n",
      "[14237]\ttrain-rmse:1.648183\n",
      "[14238]\ttrain-rmse:1.648175\n",
      "[14239]\ttrain-rmse:1.648168\n",
      "[14240]\ttrain-rmse:1.648162\n",
      "[14241]\ttrain-rmse:1.648157\n",
      "[14242]\ttrain-rmse:1.648154\n",
      "[14243]\ttrain-rmse:1.648147\n",
      "[14244]\ttrain-rmse:1.648142\n",
      "[14245]\ttrain-rmse:1.648131\n",
      "[14246]\ttrain-rmse:1.648123\n",
      "[14247]\ttrain-rmse:1.648118\n",
      "[14248]\ttrain-rmse:1.648111\n",
      "[14249]\ttrain-rmse:1.648101\n",
      "[14250]\ttrain-rmse:1.648096\n",
      "[14251]\ttrain-rmse:1.648084\n",
      "[14252]\ttrain-rmse:1.648076\n",
      "[14253]\ttrain-rmse:1.648070\n",
      "[14254]\ttrain-rmse:1.648057\n",
      "[14255]\ttrain-rmse:1.648052\n",
      "[14256]\ttrain-rmse:1.648047\n",
      "[14257]\ttrain-rmse:1.648044\n",
      "[14258]\ttrain-rmse:1.648038\n",
      "[14259]\ttrain-rmse:1.648029\n",
      "[14260]\ttrain-rmse:1.648016\n",
      "[14261]\ttrain-rmse:1.648003\n",
      "[14262]\ttrain-rmse:1.647995\n",
      "[14263]\ttrain-rmse:1.647984\n",
      "[14264]\ttrain-rmse:1.647982\n",
      "[14265]\ttrain-rmse:1.647977\n",
      "[14266]\ttrain-rmse:1.647969\n",
      "[14267]\ttrain-rmse:1.647958\n",
      "[14268]\ttrain-rmse:1.647954\n",
      "[14269]\ttrain-rmse:1.647947\n",
      "[14270]\ttrain-rmse:1.647942\n",
      "[14271]\ttrain-rmse:1.647936\n",
      "[14272]\ttrain-rmse:1.647922\n",
      "[14273]\ttrain-rmse:1.647910\n",
      "[14274]\ttrain-rmse:1.647903\n",
      "[14275]\ttrain-rmse:1.647894\n",
      "[14276]\ttrain-rmse:1.647891\n",
      "[14277]\ttrain-rmse:1.647885\n",
      "[14278]\ttrain-rmse:1.647876\n",
      "[14279]\ttrain-rmse:1.647870\n",
      "[14280]\ttrain-rmse:1.647862\n",
      "[14281]\ttrain-rmse:1.647855\n",
      "[14282]\ttrain-rmse:1.647846\n",
      "[14283]\ttrain-rmse:1.647838\n",
      "[14284]\ttrain-rmse:1.647830\n",
      "[14285]\ttrain-rmse:1.647825\n",
      "[14286]\ttrain-rmse:1.647814\n",
      "[14287]\ttrain-rmse:1.647797\n",
      "[14288]\ttrain-rmse:1.647786\n",
      "[14289]\ttrain-rmse:1.647777\n",
      "[14290]\ttrain-rmse:1.647770\n",
      "[14291]\ttrain-rmse:1.647763\n",
      "[14292]\ttrain-rmse:1.647751\n",
      "[14293]\ttrain-rmse:1.647744\n",
      "[14294]\ttrain-rmse:1.647737\n",
      "[14295]\ttrain-rmse:1.647728\n",
      "[14296]\ttrain-rmse:1.647724\n",
      "[14297]\ttrain-rmse:1.647716\n",
      "[14298]\ttrain-rmse:1.647712\n",
      "[14299]\ttrain-rmse:1.647708\n",
      "[14300]\ttrain-rmse:1.647702\n",
      "[14301]\ttrain-rmse:1.647693\n",
      "[14302]\ttrain-rmse:1.647686\n",
      "[14303]\ttrain-rmse:1.647681\n",
      "[14304]\ttrain-rmse:1.647670\n",
      "[14305]\ttrain-rmse:1.647660\n",
      "[14306]\ttrain-rmse:1.647653\n",
      "[14307]\ttrain-rmse:1.647645\n",
      "[14308]\ttrain-rmse:1.647637\n",
      "[14309]\ttrain-rmse:1.647629\n",
      "[14310]\ttrain-rmse:1.647623\n",
      "[14311]\ttrain-rmse:1.647610\n",
      "[14312]\ttrain-rmse:1.647604\n",
      "[14313]\ttrain-rmse:1.647595\n",
      "[14314]\ttrain-rmse:1.647586\n",
      "[14315]\ttrain-rmse:1.647571\n",
      "[14316]\ttrain-rmse:1.647558\n",
      "[14317]\ttrain-rmse:1.647547\n",
      "[14318]\ttrain-rmse:1.647534\n",
      "[14319]\ttrain-rmse:1.647525\n",
      "[14320]\ttrain-rmse:1.647514\n",
      "[14321]\ttrain-rmse:1.647509\n",
      "[14322]\ttrain-rmse:1.647504\n",
      "[14323]\ttrain-rmse:1.647492\n",
      "[14324]\ttrain-rmse:1.647483\n",
      "[14325]\ttrain-rmse:1.647473\n",
      "[14326]\ttrain-rmse:1.647467\n",
      "[14327]\ttrain-rmse:1.647464\n",
      "[14328]\ttrain-rmse:1.647456\n",
      "[14329]\ttrain-rmse:1.647445\n",
      "[14330]\ttrain-rmse:1.647439\n",
      "[14331]\ttrain-rmse:1.647432\n",
      "[14332]\ttrain-rmse:1.647423\n",
      "[14333]\ttrain-rmse:1.647415\n",
      "[14334]\ttrain-rmse:1.647413\n",
      "[14335]\ttrain-rmse:1.647405\n",
      "[14336]\ttrain-rmse:1.647395\n",
      "[14337]\ttrain-rmse:1.647388\n",
      "[14338]\ttrain-rmse:1.647379\n",
      "[14339]\ttrain-rmse:1.647377\n",
      "[14340]\ttrain-rmse:1.647367\n",
      "[14341]\ttrain-rmse:1.647355\n",
      "[14342]\ttrain-rmse:1.647353\n",
      "[14343]\ttrain-rmse:1.647344\n",
      "[14344]\ttrain-rmse:1.647338\n",
      "[14345]\ttrain-rmse:1.647323\n",
      "[14346]\ttrain-rmse:1.647311\n",
      "[14347]\ttrain-rmse:1.647303\n",
      "[14348]\ttrain-rmse:1.647290\n",
      "[14349]\ttrain-rmse:1.647284\n",
      "[14350]\ttrain-rmse:1.647279\n",
      "[14351]\ttrain-rmse:1.647270\n",
      "[14352]\ttrain-rmse:1.647262\n",
      "[14353]\ttrain-rmse:1.647249\n",
      "[14354]\ttrain-rmse:1.647243\n",
      "[14355]\ttrain-rmse:1.647236\n",
      "[14356]\ttrain-rmse:1.647232\n",
      "[14357]\ttrain-rmse:1.647222\n",
      "[14358]\ttrain-rmse:1.647211\n",
      "[14359]\ttrain-rmse:1.647202\n",
      "[14360]\ttrain-rmse:1.647194\n",
      "[14361]\ttrain-rmse:1.647187\n",
      "[14362]\ttrain-rmse:1.647182\n",
      "[14363]\ttrain-rmse:1.647172\n",
      "[14364]\ttrain-rmse:1.647166\n",
      "[14365]\ttrain-rmse:1.647160\n",
      "[14366]\ttrain-rmse:1.647150\n",
      "[14367]\ttrain-rmse:1.647139\n",
      "[14368]\ttrain-rmse:1.647133\n",
      "[14369]\ttrain-rmse:1.647126\n",
      "[14370]\ttrain-rmse:1.647117\n",
      "[14371]\ttrain-rmse:1.647104\n",
      "[14372]\ttrain-rmse:1.647103\n",
      "[14373]\ttrain-rmse:1.647098\n",
      "[14374]\ttrain-rmse:1.647088\n",
      "[14375]\ttrain-rmse:1.647079\n",
      "[14376]\ttrain-rmse:1.647065\n",
      "[14377]\ttrain-rmse:1.647059\n",
      "[14378]\ttrain-rmse:1.647048\n",
      "[14379]\ttrain-rmse:1.647043\n",
      "[14380]\ttrain-rmse:1.647033\n",
      "[14381]\ttrain-rmse:1.647027\n",
      "[14382]\ttrain-rmse:1.647018\n",
      "[14383]\ttrain-rmse:1.647009\n",
      "[14384]\ttrain-rmse:1.647001\n",
      "[14385]\ttrain-rmse:1.646994\n",
      "[14386]\ttrain-rmse:1.646986\n",
      "[14387]\ttrain-rmse:1.646981\n",
      "[14388]\ttrain-rmse:1.646976\n",
      "[14389]\ttrain-rmse:1.646968\n",
      "[14390]\ttrain-rmse:1.646961\n",
      "[14391]\ttrain-rmse:1.646947\n",
      "[14392]\ttrain-rmse:1.646935\n",
      "[14393]\ttrain-rmse:1.646925\n",
      "[14394]\ttrain-rmse:1.646912\n",
      "[14395]\ttrain-rmse:1.646905\n",
      "[14396]\ttrain-rmse:1.646897\n",
      "[14397]\ttrain-rmse:1.646887\n",
      "[14398]\ttrain-rmse:1.646878\n",
      "[14399]\ttrain-rmse:1.646868\n",
      "[14400]\ttrain-rmse:1.646857\n",
      "[14401]\ttrain-rmse:1.646847\n",
      "[14402]\ttrain-rmse:1.646842\n",
      "[14403]\ttrain-rmse:1.646836\n",
      "[14404]\ttrain-rmse:1.646820\n",
      "[14405]\ttrain-rmse:1.646811\n",
      "[14406]\ttrain-rmse:1.646804\n",
      "[14407]\ttrain-rmse:1.646795\n",
      "[14408]\ttrain-rmse:1.646791\n",
      "[14409]\ttrain-rmse:1.646782\n",
      "[14410]\ttrain-rmse:1.646774\n",
      "[14411]\ttrain-rmse:1.646764\n",
      "[14412]\ttrain-rmse:1.646758\n",
      "[14413]\ttrain-rmse:1.646754\n",
      "[14414]\ttrain-rmse:1.646748\n",
      "[14415]\ttrain-rmse:1.646740\n",
      "[14416]\ttrain-rmse:1.646736\n",
      "[14417]\ttrain-rmse:1.646726\n",
      "[14418]\ttrain-rmse:1.646718\n",
      "[14419]\ttrain-rmse:1.646709\n",
      "[14420]\ttrain-rmse:1.646703\n",
      "[14421]\ttrain-rmse:1.646696\n",
      "[14422]\ttrain-rmse:1.646685\n",
      "[14423]\ttrain-rmse:1.646674\n",
      "[14424]\ttrain-rmse:1.646665\n",
      "[14425]\ttrain-rmse:1.646659\n",
      "[14426]\ttrain-rmse:1.646654\n",
      "[14427]\ttrain-rmse:1.646647\n",
      "[14428]\ttrain-rmse:1.646639\n",
      "[14429]\ttrain-rmse:1.646633\n",
      "[14430]\ttrain-rmse:1.646628\n",
      "[14431]\ttrain-rmse:1.646622\n",
      "[14432]\ttrain-rmse:1.646612\n",
      "[14433]\ttrain-rmse:1.646603\n",
      "[14434]\ttrain-rmse:1.646597\n",
      "[14435]\ttrain-rmse:1.646587\n",
      "[14436]\ttrain-rmse:1.646581\n",
      "[14437]\ttrain-rmse:1.646572\n",
      "[14438]\ttrain-rmse:1.646557\n",
      "[14439]\ttrain-rmse:1.646552\n",
      "[14440]\ttrain-rmse:1.646544\n",
      "[14441]\ttrain-rmse:1.646536\n",
      "[14442]\ttrain-rmse:1.646521\n",
      "[14443]\ttrain-rmse:1.646511\n",
      "[14444]\ttrain-rmse:1.646498\n",
      "[14445]\ttrain-rmse:1.646487\n",
      "[14446]\ttrain-rmse:1.646479\n",
      "[14447]\ttrain-rmse:1.646468\n",
      "[14448]\ttrain-rmse:1.646460\n",
      "[14449]\ttrain-rmse:1.646452\n",
      "[14450]\ttrain-rmse:1.646445\n",
      "[14451]\ttrain-rmse:1.646438\n",
      "[14452]\ttrain-rmse:1.646429\n",
      "[14453]\ttrain-rmse:1.646422\n",
      "[14454]\ttrain-rmse:1.646414\n",
      "[14455]\ttrain-rmse:1.646407\n",
      "[14456]\ttrain-rmse:1.646397\n",
      "[14457]\ttrain-rmse:1.646389\n",
      "[14458]\ttrain-rmse:1.646378\n",
      "[14459]\ttrain-rmse:1.646370\n",
      "[14460]\ttrain-rmse:1.646363\n",
      "[14461]\ttrain-rmse:1.646359\n",
      "[14462]\ttrain-rmse:1.646354\n",
      "[14463]\ttrain-rmse:1.646341\n",
      "[14464]\ttrain-rmse:1.646334\n",
      "[14465]\ttrain-rmse:1.646328\n",
      "[14466]\ttrain-rmse:1.646321\n",
      "[14467]\ttrain-rmse:1.646315\n",
      "[14468]\ttrain-rmse:1.646308\n",
      "[14469]\ttrain-rmse:1.646300\n",
      "[14470]\ttrain-rmse:1.646289\n",
      "[14471]\ttrain-rmse:1.646278\n",
      "[14472]\ttrain-rmse:1.646268\n",
      "[14473]\ttrain-rmse:1.646262\n",
      "[14474]\ttrain-rmse:1.646255\n",
      "[14475]\ttrain-rmse:1.646244\n",
      "[14476]\ttrain-rmse:1.646230\n",
      "[14477]\ttrain-rmse:1.646225\n",
      "[14478]\ttrain-rmse:1.646212\n",
      "[14479]\ttrain-rmse:1.646205\n",
      "[14480]\ttrain-rmse:1.646199\n",
      "[14481]\ttrain-rmse:1.646190\n",
      "[14482]\ttrain-rmse:1.646184\n",
      "[14483]\ttrain-rmse:1.646170\n",
      "[14484]\ttrain-rmse:1.646161\n",
      "[14485]\ttrain-rmse:1.646153\n",
      "[14486]\ttrain-rmse:1.646148\n",
      "[14487]\ttrain-rmse:1.646143\n",
      "[14488]\ttrain-rmse:1.646132\n",
      "[14489]\ttrain-rmse:1.646129\n",
      "[14490]\ttrain-rmse:1.646114\n",
      "[14491]\ttrain-rmse:1.646107\n",
      "[14492]\ttrain-rmse:1.646104\n",
      "[14493]\ttrain-rmse:1.646093\n",
      "[14494]\ttrain-rmse:1.646085\n",
      "[14495]\ttrain-rmse:1.646074\n",
      "[14496]\ttrain-rmse:1.646063\n",
      "[14497]\ttrain-rmse:1.646055\n",
      "[14498]\ttrain-rmse:1.646048\n",
      "[14499]\ttrain-rmse:1.646039\n",
      "[14500]\ttrain-rmse:1.646035\n",
      "[14501]\ttrain-rmse:1.646026\n",
      "[14502]\ttrain-rmse:1.646017\n",
      "[14503]\ttrain-rmse:1.646010\n",
      "[14504]\ttrain-rmse:1.646002\n",
      "[14505]\ttrain-rmse:1.645995\n",
      "[14506]\ttrain-rmse:1.645987\n",
      "[14507]\ttrain-rmse:1.645984\n",
      "[14508]\ttrain-rmse:1.645976\n",
      "[14509]\ttrain-rmse:1.645971\n",
      "[14510]\ttrain-rmse:1.645961\n",
      "[14511]\ttrain-rmse:1.645958\n",
      "[14512]\ttrain-rmse:1.645951\n",
      "[14513]\ttrain-rmse:1.645938\n",
      "[14514]\ttrain-rmse:1.645931\n",
      "[14515]\ttrain-rmse:1.645921\n",
      "[14516]\ttrain-rmse:1.645916\n",
      "[14517]\ttrain-rmse:1.645908\n",
      "[14518]\ttrain-rmse:1.645900\n",
      "[14519]\ttrain-rmse:1.645890\n",
      "[14520]\ttrain-rmse:1.645879\n",
      "[14521]\ttrain-rmse:1.645870\n",
      "[14522]\ttrain-rmse:1.645866\n",
      "[14523]\ttrain-rmse:1.645858\n",
      "[14524]\ttrain-rmse:1.645846\n",
      "[14525]\ttrain-rmse:1.645840\n",
      "[14526]\ttrain-rmse:1.645830\n",
      "[14527]\ttrain-rmse:1.645819\n",
      "[14528]\ttrain-rmse:1.645810\n",
      "[14529]\ttrain-rmse:1.645805\n",
      "[14530]\ttrain-rmse:1.645793\n",
      "[14531]\ttrain-rmse:1.645783\n",
      "[14532]\ttrain-rmse:1.645779\n",
      "[14533]\ttrain-rmse:1.645770\n",
      "[14534]\ttrain-rmse:1.645762\n",
      "[14535]\ttrain-rmse:1.645758\n",
      "[14536]\ttrain-rmse:1.645750\n",
      "[14537]\ttrain-rmse:1.645739\n",
      "[14538]\ttrain-rmse:1.645731\n",
      "[14539]\ttrain-rmse:1.645725\n",
      "[14540]\ttrain-rmse:1.645710\n",
      "[14541]\ttrain-rmse:1.645706\n",
      "[14542]\ttrain-rmse:1.645700\n",
      "[14543]\ttrain-rmse:1.645692\n",
      "[14544]\ttrain-rmse:1.645684\n",
      "[14545]\ttrain-rmse:1.645672\n",
      "[14546]\ttrain-rmse:1.645666\n",
      "[14547]\ttrain-rmse:1.645658\n",
      "[14548]\ttrain-rmse:1.645651\n",
      "[14549]\ttrain-rmse:1.645647\n",
      "[14550]\ttrain-rmse:1.645637\n",
      "[14551]\ttrain-rmse:1.645627\n",
      "[14552]\ttrain-rmse:1.645618\n",
      "[14553]\ttrain-rmse:1.645615\n",
      "[14554]\ttrain-rmse:1.645611\n",
      "[14555]\ttrain-rmse:1.645602\n",
      "[14556]\ttrain-rmse:1.645594\n",
      "[14557]\ttrain-rmse:1.645587\n",
      "[14558]\ttrain-rmse:1.645578\n",
      "[14559]\ttrain-rmse:1.645570\n",
      "[14560]\ttrain-rmse:1.645561\n",
      "[14561]\ttrain-rmse:1.645553\n",
      "[14562]\ttrain-rmse:1.645542\n",
      "[14563]\ttrain-rmse:1.645538\n",
      "[14564]\ttrain-rmse:1.645528\n",
      "[14565]\ttrain-rmse:1.645518\n",
      "[14566]\ttrain-rmse:1.645509\n",
      "[14567]\ttrain-rmse:1.645499\n",
      "[14568]\ttrain-rmse:1.645492\n",
      "[14569]\ttrain-rmse:1.645489\n",
      "[14570]\ttrain-rmse:1.645486\n",
      "[14571]\ttrain-rmse:1.645477\n",
      "[14572]\ttrain-rmse:1.645469\n",
      "[14573]\ttrain-rmse:1.645459\n",
      "[14574]\ttrain-rmse:1.645452\n",
      "[14575]\ttrain-rmse:1.645448\n",
      "[14576]\ttrain-rmse:1.645443\n",
      "[14577]\ttrain-rmse:1.645431\n",
      "[14578]\ttrain-rmse:1.645418\n",
      "[14579]\ttrain-rmse:1.645407\n",
      "[14580]\ttrain-rmse:1.645398\n",
      "[14581]\ttrain-rmse:1.645383\n",
      "[14582]\ttrain-rmse:1.645376\n",
      "[14583]\ttrain-rmse:1.645368\n",
      "[14584]\ttrain-rmse:1.645362\n",
      "[14585]\ttrain-rmse:1.645354\n",
      "[14586]\ttrain-rmse:1.645344\n",
      "[14587]\ttrain-rmse:1.645340\n",
      "[14588]\ttrain-rmse:1.645331\n",
      "[14589]\ttrain-rmse:1.645322\n",
      "[14590]\ttrain-rmse:1.645315\n",
      "[14591]\ttrain-rmse:1.645302\n",
      "[14592]\ttrain-rmse:1.645295\n",
      "[14593]\ttrain-rmse:1.645286\n",
      "[14594]\ttrain-rmse:1.645278\n",
      "[14595]\ttrain-rmse:1.645271\n",
      "[14596]\ttrain-rmse:1.645264\n",
      "[14597]\ttrain-rmse:1.645259\n",
      "[14598]\ttrain-rmse:1.645245\n",
      "[14599]\ttrain-rmse:1.645236\n",
      "[14600]\ttrain-rmse:1.645227\n",
      "[14601]\ttrain-rmse:1.645218\n",
      "[14602]\ttrain-rmse:1.645211\n",
      "[14603]\ttrain-rmse:1.645199\n",
      "[14604]\ttrain-rmse:1.645195\n",
      "[14605]\ttrain-rmse:1.645185\n",
      "[14606]\ttrain-rmse:1.645181\n",
      "[14607]\ttrain-rmse:1.645171\n",
      "[14608]\ttrain-rmse:1.645158\n",
      "[14609]\ttrain-rmse:1.645148\n",
      "[14610]\ttrain-rmse:1.645142\n",
      "[14611]\ttrain-rmse:1.645137\n",
      "[14612]\ttrain-rmse:1.645129\n",
      "[14613]\ttrain-rmse:1.645117\n",
      "[14614]\ttrain-rmse:1.645114\n",
      "[14615]\ttrain-rmse:1.645109\n",
      "[14616]\ttrain-rmse:1.645101\n",
      "[14617]\ttrain-rmse:1.645093\n",
      "[14618]\ttrain-rmse:1.645083\n",
      "[14619]\ttrain-rmse:1.645077\n",
      "[14620]\ttrain-rmse:1.645069\n",
      "[14621]\ttrain-rmse:1.645061\n",
      "[14622]\ttrain-rmse:1.645046\n",
      "[14623]\ttrain-rmse:1.645043\n",
      "[14624]\ttrain-rmse:1.645038\n",
      "[14625]\ttrain-rmse:1.645028\n",
      "[14626]\ttrain-rmse:1.645023\n",
      "[14627]\ttrain-rmse:1.645007\n",
      "[14628]\ttrain-rmse:1.644994\n",
      "[14629]\ttrain-rmse:1.644986\n",
      "[14630]\ttrain-rmse:1.644976\n",
      "[14631]\ttrain-rmse:1.644968\n",
      "[14632]\ttrain-rmse:1.644956\n",
      "[14633]\ttrain-rmse:1.644950\n",
      "[14634]\ttrain-rmse:1.644942\n",
      "[14635]\ttrain-rmse:1.644936\n",
      "[14636]\ttrain-rmse:1.644931\n",
      "[14637]\ttrain-rmse:1.644924\n",
      "[14638]\ttrain-rmse:1.644917\n",
      "[14639]\ttrain-rmse:1.644907\n",
      "[14640]\ttrain-rmse:1.644899\n",
      "[14641]\ttrain-rmse:1.644892\n",
      "[14642]\ttrain-rmse:1.644881\n",
      "[14643]\ttrain-rmse:1.644871\n",
      "[14644]\ttrain-rmse:1.644867\n",
      "[14645]\ttrain-rmse:1.644863\n",
      "[14646]\ttrain-rmse:1.644854\n",
      "[14647]\ttrain-rmse:1.644847\n",
      "[14648]\ttrain-rmse:1.644838\n",
      "[14649]\ttrain-rmse:1.644829\n",
      "[14650]\ttrain-rmse:1.644820\n",
      "[14651]\ttrain-rmse:1.644809\n",
      "[14652]\ttrain-rmse:1.644798\n",
      "[14653]\ttrain-rmse:1.644794\n",
      "[14654]\ttrain-rmse:1.644785\n",
      "[14655]\ttrain-rmse:1.644780\n",
      "[14656]\ttrain-rmse:1.644770\n",
      "[14657]\ttrain-rmse:1.644763\n",
      "[14658]\ttrain-rmse:1.644758\n",
      "[14659]\ttrain-rmse:1.644753\n",
      "[14660]\ttrain-rmse:1.644747\n",
      "[14661]\ttrain-rmse:1.644746\n",
      "[14662]\ttrain-rmse:1.644741\n",
      "[14663]\ttrain-rmse:1.644734\n",
      "[14664]\ttrain-rmse:1.644732\n",
      "[14665]\ttrain-rmse:1.644722\n",
      "[14666]\ttrain-rmse:1.644715\n",
      "[14667]\ttrain-rmse:1.644702\n",
      "[14668]\ttrain-rmse:1.644696\n",
      "[14669]\ttrain-rmse:1.644694\n",
      "[14670]\ttrain-rmse:1.644682\n",
      "[14671]\ttrain-rmse:1.644677\n",
      "[14672]\ttrain-rmse:1.644667\n",
      "[14673]\ttrain-rmse:1.644661\n",
      "[14674]\ttrain-rmse:1.644652\n",
      "[14675]\ttrain-rmse:1.644646\n",
      "[14676]\ttrain-rmse:1.644633\n",
      "[14677]\ttrain-rmse:1.644628\n",
      "[14678]\ttrain-rmse:1.644620\n",
      "[14679]\ttrain-rmse:1.644613\n",
      "[14680]\ttrain-rmse:1.644606\n",
      "[14681]\ttrain-rmse:1.644602\n",
      "[14682]\ttrain-rmse:1.644596\n",
      "[14683]\ttrain-rmse:1.644590\n",
      "[14684]\ttrain-rmse:1.644582\n",
      "[14685]\ttrain-rmse:1.644577\n",
      "[14686]\ttrain-rmse:1.644568\n",
      "[14687]\ttrain-rmse:1.644558\n",
      "[14688]\ttrain-rmse:1.644546\n",
      "[14689]\ttrain-rmse:1.644535\n",
      "[14690]\ttrain-rmse:1.644530\n",
      "[14691]\ttrain-rmse:1.644527\n",
      "[14692]\ttrain-rmse:1.644520\n",
      "[14693]\ttrain-rmse:1.644512\n",
      "[14694]\ttrain-rmse:1.644500\n",
      "[14695]\ttrain-rmse:1.644488\n",
      "[14696]\ttrain-rmse:1.644483\n",
      "[14697]\ttrain-rmse:1.644473\n",
      "[14698]\ttrain-rmse:1.644467\n",
      "[14699]\ttrain-rmse:1.644458\n",
      "[14700]\ttrain-rmse:1.644449\n",
      "[14701]\ttrain-rmse:1.644445\n",
      "[14702]\ttrain-rmse:1.644431\n",
      "[14703]\ttrain-rmse:1.644423\n",
      "[14704]\ttrain-rmse:1.644416\n",
      "[14705]\ttrain-rmse:1.644405\n",
      "[14706]\ttrain-rmse:1.644402\n",
      "[14707]\ttrain-rmse:1.644396\n",
      "[14708]\ttrain-rmse:1.644387\n",
      "[14709]\ttrain-rmse:1.644380\n",
      "[14710]\ttrain-rmse:1.644377\n",
      "[14711]\ttrain-rmse:1.644372\n",
      "[14712]\ttrain-rmse:1.644361\n",
      "[14713]\ttrain-rmse:1.644357\n",
      "[14714]\ttrain-rmse:1.644351\n",
      "[14715]\ttrain-rmse:1.644343\n",
      "[14716]\ttrain-rmse:1.644333\n",
      "[14717]\ttrain-rmse:1.644327\n",
      "[14718]\ttrain-rmse:1.644320\n",
      "[14719]\ttrain-rmse:1.644311\n",
      "[14720]\ttrain-rmse:1.644303\n",
      "[14721]\ttrain-rmse:1.644301\n",
      "[14722]\ttrain-rmse:1.644297\n",
      "[14723]\ttrain-rmse:1.644283\n",
      "[14724]\ttrain-rmse:1.644276\n",
      "[14725]\ttrain-rmse:1.644265\n",
      "[14726]\ttrain-rmse:1.644258\n",
      "[14727]\ttrain-rmse:1.644250\n",
      "[14728]\ttrain-rmse:1.644244\n",
      "[14729]\ttrain-rmse:1.644238\n",
      "[14730]\ttrain-rmse:1.644231\n",
      "[14731]\ttrain-rmse:1.644223\n",
      "[14732]\ttrain-rmse:1.644215\n",
      "[14733]\ttrain-rmse:1.644198\n",
      "[14734]\ttrain-rmse:1.644190\n",
      "[14735]\ttrain-rmse:1.644183\n",
      "[14736]\ttrain-rmse:1.644174\n",
      "[14737]\ttrain-rmse:1.644165\n",
      "[14738]\ttrain-rmse:1.644159\n",
      "[14739]\ttrain-rmse:1.644151\n",
      "[14740]\ttrain-rmse:1.644142\n",
      "[14741]\ttrain-rmse:1.644133\n",
      "[14742]\ttrain-rmse:1.644122\n",
      "[14743]\ttrain-rmse:1.644112\n",
      "[14744]\ttrain-rmse:1.644107\n",
      "[14745]\ttrain-rmse:1.644096\n",
      "[14746]\ttrain-rmse:1.644088\n",
      "[14747]\ttrain-rmse:1.644080\n",
      "[14748]\ttrain-rmse:1.644068\n",
      "[14749]\ttrain-rmse:1.644055\n",
      "[14750]\ttrain-rmse:1.644051\n",
      "[14751]\ttrain-rmse:1.644041\n",
      "[14752]\ttrain-rmse:1.644039\n",
      "[14753]\ttrain-rmse:1.644033\n",
      "[14754]\ttrain-rmse:1.644021\n",
      "[14755]\ttrain-rmse:1.644011\n",
      "[14756]\ttrain-rmse:1.643999\n",
      "[14757]\ttrain-rmse:1.643990\n",
      "[14758]\ttrain-rmse:1.643984\n",
      "[14759]\ttrain-rmse:1.643976\n",
      "[14760]\ttrain-rmse:1.643972\n",
      "[14761]\ttrain-rmse:1.643968\n",
      "[14762]\ttrain-rmse:1.643956\n",
      "[14763]\ttrain-rmse:1.643951\n",
      "[14764]\ttrain-rmse:1.643947\n",
      "[14765]\ttrain-rmse:1.643930\n",
      "[14766]\ttrain-rmse:1.643923\n",
      "[14767]\ttrain-rmse:1.643916\n",
      "[14768]\ttrain-rmse:1.643910\n",
      "[14769]\ttrain-rmse:1.643898\n",
      "[14770]\ttrain-rmse:1.643891\n",
      "[14771]\ttrain-rmse:1.643885\n",
      "[14772]\ttrain-rmse:1.643876\n",
      "[14773]\ttrain-rmse:1.643866\n",
      "[14774]\ttrain-rmse:1.643854\n",
      "[14775]\ttrain-rmse:1.643842\n",
      "[14776]\ttrain-rmse:1.643836\n",
      "[14777]\ttrain-rmse:1.643829\n",
      "[14778]\ttrain-rmse:1.643818\n",
      "[14779]\ttrain-rmse:1.643812\n",
      "[14780]\ttrain-rmse:1.643806\n",
      "[14781]\ttrain-rmse:1.643800\n",
      "[14782]\ttrain-rmse:1.643794\n",
      "[14783]\ttrain-rmse:1.643790\n",
      "[14784]\ttrain-rmse:1.643781\n",
      "[14785]\ttrain-rmse:1.643772\n",
      "[14786]\ttrain-rmse:1.643759\n",
      "[14787]\ttrain-rmse:1.643754\n",
      "[14788]\ttrain-rmse:1.643748\n",
      "[14789]\ttrain-rmse:1.643740\n",
      "[14790]\ttrain-rmse:1.643733\n",
      "[14791]\ttrain-rmse:1.643724\n",
      "[14792]\ttrain-rmse:1.643717\n",
      "[14793]\ttrain-rmse:1.643714\n",
      "[14794]\ttrain-rmse:1.643709\n",
      "[14795]\ttrain-rmse:1.643705\n",
      "[14796]\ttrain-rmse:1.643696\n",
      "[14797]\ttrain-rmse:1.643694\n",
      "[14798]\ttrain-rmse:1.643687\n",
      "[14799]\ttrain-rmse:1.643673\n",
      "[14800]\ttrain-rmse:1.643669\n",
      "[14801]\ttrain-rmse:1.643658\n",
      "[14802]\ttrain-rmse:1.643649\n",
      "[14803]\ttrain-rmse:1.643640\n",
      "[14804]\ttrain-rmse:1.643629\n",
      "[14805]\ttrain-rmse:1.643625\n",
      "[14806]\ttrain-rmse:1.643612\n",
      "[14807]\ttrain-rmse:1.643603\n",
      "[14808]\ttrain-rmse:1.643592\n",
      "[14809]\ttrain-rmse:1.643583\n",
      "[14810]\ttrain-rmse:1.643577\n",
      "[14811]\ttrain-rmse:1.643567\n",
      "[14812]\ttrain-rmse:1.643562\n",
      "[14813]\ttrain-rmse:1.643549\n",
      "[14814]\ttrain-rmse:1.643538\n",
      "[14815]\ttrain-rmse:1.643535\n",
      "[14816]\ttrain-rmse:1.643523\n",
      "[14817]\ttrain-rmse:1.643513\n",
      "[14818]\ttrain-rmse:1.643505\n",
      "[14819]\ttrain-rmse:1.643490\n",
      "[14820]\ttrain-rmse:1.643479\n",
      "[14821]\ttrain-rmse:1.643471\n",
      "[14822]\ttrain-rmse:1.643463\n",
      "[14823]\ttrain-rmse:1.643450\n",
      "[14824]\ttrain-rmse:1.643439\n",
      "[14825]\ttrain-rmse:1.643432\n",
      "[14826]\ttrain-rmse:1.643425\n",
      "[14827]\ttrain-rmse:1.643417\n",
      "[14828]\ttrain-rmse:1.643407\n",
      "[14829]\ttrain-rmse:1.643392\n",
      "[14830]\ttrain-rmse:1.643385\n",
      "[14831]\ttrain-rmse:1.643380\n",
      "[14832]\ttrain-rmse:1.643377\n",
      "[14833]\ttrain-rmse:1.643364\n",
      "[14834]\ttrain-rmse:1.643357\n",
      "[14835]\ttrain-rmse:1.643355\n",
      "[14836]\ttrain-rmse:1.643352\n",
      "[14837]\ttrain-rmse:1.643347\n",
      "[14838]\ttrain-rmse:1.643339\n",
      "[14839]\ttrain-rmse:1.643325\n",
      "[14840]\ttrain-rmse:1.643314\n",
      "[14841]\ttrain-rmse:1.643301\n",
      "[14842]\ttrain-rmse:1.643298\n",
      "[14843]\ttrain-rmse:1.643293\n",
      "[14844]\ttrain-rmse:1.643288\n",
      "[14845]\ttrain-rmse:1.643285\n",
      "[14846]\ttrain-rmse:1.643280\n",
      "[14847]\ttrain-rmse:1.643270\n",
      "[14848]\ttrain-rmse:1.643263\n",
      "[14849]\ttrain-rmse:1.643253\n",
      "[14850]\ttrain-rmse:1.643243\n",
      "[14851]\ttrain-rmse:1.643233\n",
      "[14852]\ttrain-rmse:1.643227\n",
      "[14853]\ttrain-rmse:1.643221\n",
      "[14854]\ttrain-rmse:1.643213\n",
      "[14855]\ttrain-rmse:1.643205\n",
      "[14856]\ttrain-rmse:1.643193\n",
      "[14857]\ttrain-rmse:1.643185\n",
      "[14858]\ttrain-rmse:1.643177\n",
      "[14859]\ttrain-rmse:1.643168\n",
      "[14860]\ttrain-rmse:1.643163\n",
      "[14861]\ttrain-rmse:1.643157\n",
      "[14862]\ttrain-rmse:1.643147\n",
      "[14863]\ttrain-rmse:1.643139\n",
      "[14864]\ttrain-rmse:1.643134\n",
      "[14865]\ttrain-rmse:1.643126\n",
      "[14866]\ttrain-rmse:1.643117\n",
      "[14867]\ttrain-rmse:1.643110\n",
      "[14868]\ttrain-rmse:1.643105\n",
      "[14869]\ttrain-rmse:1.643097\n",
      "[14870]\ttrain-rmse:1.643092\n",
      "[14871]\ttrain-rmse:1.643086\n",
      "[14872]\ttrain-rmse:1.643076\n",
      "[14873]\ttrain-rmse:1.643062\n",
      "[14874]\ttrain-rmse:1.643055\n",
      "[14875]\ttrain-rmse:1.643049\n",
      "[14876]\ttrain-rmse:1.643038\n",
      "[14877]\ttrain-rmse:1.643030\n",
      "[14878]\ttrain-rmse:1.643018\n",
      "[14879]\ttrain-rmse:1.643009\n",
      "[14880]\ttrain-rmse:1.643000\n",
      "[14881]\ttrain-rmse:1.642992\n",
      "[14882]\ttrain-rmse:1.642985\n",
      "[14883]\ttrain-rmse:1.642977\n",
      "[14884]\ttrain-rmse:1.642969\n",
      "[14885]\ttrain-rmse:1.642964\n",
      "[14886]\ttrain-rmse:1.642955\n",
      "[14887]\ttrain-rmse:1.642944\n",
      "[14888]\ttrain-rmse:1.642933\n",
      "[14889]\ttrain-rmse:1.642926\n",
      "[14890]\ttrain-rmse:1.642916\n",
      "[14891]\ttrain-rmse:1.642905\n",
      "[14892]\ttrain-rmse:1.642899\n",
      "[14893]\ttrain-rmse:1.642889\n",
      "[14894]\ttrain-rmse:1.642884\n",
      "[14895]\ttrain-rmse:1.642877\n",
      "[14896]\ttrain-rmse:1.642870\n",
      "[14897]\ttrain-rmse:1.642860\n",
      "[14898]\ttrain-rmse:1.642850\n",
      "[14899]\ttrain-rmse:1.642840\n",
      "[14900]\ttrain-rmse:1.642832\n",
      "[14901]\ttrain-rmse:1.642825\n",
      "[14902]\ttrain-rmse:1.642815\n",
      "[14903]\ttrain-rmse:1.642808\n",
      "[14904]\ttrain-rmse:1.642797\n",
      "[14905]\ttrain-rmse:1.642788\n",
      "[14906]\ttrain-rmse:1.642779\n",
      "[14907]\ttrain-rmse:1.642773\n",
      "[14908]\ttrain-rmse:1.642765\n",
      "[14909]\ttrain-rmse:1.642756\n",
      "[14910]\ttrain-rmse:1.642749\n",
      "[14911]\ttrain-rmse:1.642740\n",
      "[14912]\ttrain-rmse:1.642734\n",
      "[14913]\ttrain-rmse:1.642716\n",
      "[14914]\ttrain-rmse:1.642707\n",
      "[14915]\ttrain-rmse:1.642696\n",
      "[14916]\ttrain-rmse:1.642685\n",
      "[14917]\ttrain-rmse:1.642677\n",
      "[14918]\ttrain-rmse:1.642672\n",
      "[14919]\ttrain-rmse:1.642665\n",
      "[14920]\ttrain-rmse:1.642658\n",
      "[14921]\ttrain-rmse:1.642649\n",
      "[14922]\ttrain-rmse:1.642638\n",
      "[14923]\ttrain-rmse:1.642630\n",
      "[14924]\ttrain-rmse:1.642618\n",
      "[14925]\ttrain-rmse:1.642611\n",
      "[14926]\ttrain-rmse:1.642603\n",
      "[14927]\ttrain-rmse:1.642593\n",
      "[14928]\ttrain-rmse:1.642582\n",
      "[14929]\ttrain-rmse:1.642569\n",
      "[14930]\ttrain-rmse:1.642561\n",
      "[14931]\ttrain-rmse:1.642558\n",
      "[14932]\ttrain-rmse:1.642552\n",
      "[14933]\ttrain-rmse:1.642542\n",
      "[14934]\ttrain-rmse:1.642538\n",
      "[14935]\ttrain-rmse:1.642532\n",
      "[14936]\ttrain-rmse:1.642524\n",
      "[14937]\ttrain-rmse:1.642510\n",
      "[14938]\ttrain-rmse:1.642506\n",
      "[14939]\ttrain-rmse:1.642495\n",
      "[14940]\ttrain-rmse:1.642491\n",
      "[14941]\ttrain-rmse:1.642490\n",
      "[14942]\ttrain-rmse:1.642486\n",
      "[14943]\ttrain-rmse:1.642479\n",
      "[14944]\ttrain-rmse:1.642471\n",
      "[14945]\ttrain-rmse:1.642460\n",
      "[14946]\ttrain-rmse:1.642447\n",
      "[14947]\ttrain-rmse:1.642438\n",
      "[14948]\ttrain-rmse:1.642431\n",
      "[14949]\ttrain-rmse:1.642424\n",
      "[14950]\ttrain-rmse:1.642416\n",
      "[14951]\ttrain-rmse:1.642407\n",
      "[14952]\ttrain-rmse:1.642398\n",
      "[14953]\ttrain-rmse:1.642391\n",
      "[14954]\ttrain-rmse:1.642383\n",
      "[14955]\ttrain-rmse:1.642376\n",
      "[14956]\ttrain-rmse:1.642367\n",
      "[14957]\ttrain-rmse:1.642360\n",
      "[14958]\ttrain-rmse:1.642356\n",
      "[14959]\ttrain-rmse:1.642346\n",
      "[14960]\ttrain-rmse:1.642344\n",
      "[14961]\ttrain-rmse:1.642337\n",
      "[14962]\ttrain-rmse:1.642326\n",
      "[14963]\ttrain-rmse:1.642323\n",
      "[14964]\ttrain-rmse:1.642310\n",
      "[14965]\ttrain-rmse:1.642306\n",
      "[14966]\ttrain-rmse:1.642298\n",
      "[14967]\ttrain-rmse:1.642291\n",
      "[14968]\ttrain-rmse:1.642277\n",
      "[14969]\ttrain-rmse:1.642266\n",
      "[14970]\ttrain-rmse:1.642262\n",
      "[14971]\ttrain-rmse:1.642253\n",
      "[14972]\ttrain-rmse:1.642243\n",
      "[14973]\ttrain-rmse:1.642239\n",
      "[14974]\ttrain-rmse:1.642233\n",
      "[14975]\ttrain-rmse:1.642224\n",
      "[14976]\ttrain-rmse:1.642214\n",
      "[14977]\ttrain-rmse:1.642202\n",
      "[14978]\ttrain-rmse:1.642197\n",
      "[14979]\ttrain-rmse:1.642190\n",
      "[14980]\ttrain-rmse:1.642181\n",
      "[14981]\ttrain-rmse:1.642175\n",
      "[14982]\ttrain-rmse:1.642167\n",
      "[14983]\ttrain-rmse:1.642157\n",
      "[14984]\ttrain-rmse:1.642154\n",
      "[14985]\ttrain-rmse:1.642145\n",
      "[14986]\ttrain-rmse:1.642137\n",
      "[14987]\ttrain-rmse:1.642126\n",
      "[14988]\ttrain-rmse:1.642122\n",
      "[14989]\ttrain-rmse:1.642116\n",
      "[14990]\ttrain-rmse:1.642108\n",
      "[14991]\ttrain-rmse:1.642105\n",
      "[14992]\ttrain-rmse:1.642101\n",
      "[14993]\ttrain-rmse:1.642094\n",
      "[14994]\ttrain-rmse:1.642089\n",
      "[14995]\ttrain-rmse:1.642081\n",
      "[14996]\ttrain-rmse:1.642070\n",
      "[14997]\ttrain-rmse:1.642064\n",
      "[14998]\ttrain-rmse:1.642056\n",
      "[14999]\ttrain-rmse:1.642049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Product_Info_4   Ins_Age        Ht        Wt       BMI  \\\n",
      "0            0.076923  0.641791  0.581818  0.148536  0.323008   \n",
      "1            0.076923  0.059701  0.600000  0.131799  0.272288   \n",
      "2            0.076923  0.029851  0.745455  0.288703  0.428780   \n",
      "3            0.487179  0.164179  0.672727  0.205021  0.352438   \n",
      "4            0.230769  0.417910  0.654545  0.234310  0.424046   \n",
      "5            0.230769  0.507463  0.836364  0.299163  0.364887   \n",
      "6            0.166194  0.373134  0.581818  0.173640  0.376587   \n",
      "7            0.076923  0.611940  0.781818  0.403766  0.571612   \n",
      "8            0.230769  0.522388  0.618182  0.184100  0.362643   \n",
      "9            0.076923  0.552239  0.600000  0.284519  0.587796   \n",
      "10           0.128205  0.537313  0.690909  0.309623  0.521668   \n",
      "11           0.230769  0.298507  0.690909  0.271967  0.455050   \n",
      "12           0.102564  0.567164  0.618182  0.163180  0.320784   \n",
      "13           0.487179  0.223881  0.781818  0.361925  0.507515   \n",
      "14           0.487179  0.328358  0.636364  0.142259  0.264648   \n",
      "15           0.000000  0.626866  0.672727  0.330544  0.581279   \n",
      "16           0.487179  0.208955  0.745455  0.246862  0.360969   \n",
      "17           0.384615  0.268657  0.636364  0.228033  0.430949   \n",
      "18           0.076923  0.388060  0.781818  0.309623  0.427394   \n",
      "19           0.487179  0.223881  0.600000  0.138075  0.285254   \n",
      "20           0.435897  0.388060  0.745455  0.246862  0.360969   \n",
      "21           1.000000  0.537313  0.709091  0.370293  0.605334   \n",
      "22           0.230769  0.179104  0.800000  0.539749  0.753765   \n",
      "23           0.179487  0.164179  0.745455  0.288703  0.428780   \n",
      "24           0.487179  0.164179  0.818182  0.435146  0.576961   \n",
      "25           0.230769  0.268657  0.781818  0.368201  0.517129   \n",
      "26           1.000000  0.507463  0.654545  0.299163  0.545946   \n",
      "27           0.230769  0.134328  0.763636  0.215481  0.296359   \n",
      "28           0.487179  0.492537  0.618182  0.276151  0.546823   \n",
      "29           1.000000  0.582090  0.654545  0.278243  0.506623   \n",
      "...               ...       ...       ...       ...       ...   \n",
      "59351        0.000000  0.134328  0.781818  0.351464  0.491491   \n",
      "59352        0.230769  0.358209  0.618182  0.246862  0.488220   \n",
      "59353        0.589744  0.179104  0.781818  0.382845  0.539563   \n",
      "59354        0.487179  0.402985  0.763636  0.341004  0.494104   \n",
      "59355        0.230769  0.223881  0.745455  0.361925  0.547451   \n",
      "59356        0.076923  0.522388  0.600000  0.299163  0.618050   \n",
      "59357        1.000000  0.582090  0.781818  0.351464  0.491491   \n",
      "59358        0.282051  0.238806  0.727273  0.372385  0.586182   \n",
      "59359        0.230769  0.447761  0.781818  0.424686  0.603660   \n",
      "59360        1.000000  0.194030  0.654545  0.146444  0.258890   \n",
      "59361        0.230769  0.268657  0.727273  0.267782  0.411703   \n",
      "59362        0.230769  0.253731  0.781818  0.351464  0.491491   \n",
      "59363        0.076923  0.746269  0.563636  0.205021  0.464570   \n",
      "59364        0.076923  0.552239  0.727273  0.177824  0.261651   \n",
      "59365        0.076923  0.641791  0.709091  0.284519  0.458023   \n",
      "59366        0.282051  0.582090  0.781818  0.320084  0.443418   \n",
      "59367        0.179487  0.373134  0.600000  0.320084  0.661270   \n",
      "59368        0.230769  0.417910  0.727273  0.299163  0.464047   \n",
      "59369        0.179487  0.611940  0.745455  0.451883  0.693246   \n",
      "59370        0.230769  0.238806  0.763636  0.330544  0.477625   \n",
      "59371        0.487179  0.537313  0.709091  0.343096  0.558626   \n",
      "59372        0.487179  0.477612  0.763636  0.305439  0.438076   \n",
      "59373        0.487179  0.208955  0.800000  0.257322  0.332885   \n",
      "59374        0.307692  0.164179  0.690909  0.288703  0.484658   \n",
      "59375        0.076923  0.477612  0.654545  0.271967  0.494827   \n",
      "59376        0.230769  0.074627  0.709091  0.320084  0.519103   \n",
      "59377        0.230769  0.432836  0.800000  0.403766  0.551119   \n",
      "59378        0.076923  0.104478  0.745455  0.246862  0.360969   \n",
      "59379        0.230769  0.507463  0.690909  0.276151  0.462452   \n",
      "59380        0.076923  0.447761  0.781818  0.382845  0.539563   \n",
      "\n",
      "       Employment_Info_1  Employment_Info_4  Employment_Info_6  \\\n",
      "0                 0.0280            0.00000         -9999.0000   \n",
      "1                 0.0000            0.00000             0.0018   \n",
      "2                 0.0300            0.00000             0.0300   \n",
      "3                 0.0420            0.00000             0.2000   \n",
      "4                 0.0270            0.00000             0.0500   \n",
      "5                 0.3250            0.00000             1.0000   \n",
      "6                 0.1100        -9999.00000             0.8000   \n",
      "7                 0.1200            0.00000             1.0000   \n",
      "8                 0.1650            0.00000             1.0000   \n",
      "9                 0.0250            0.00000             0.0500   \n",
      "10                0.0500        -9999.00000             0.1500   \n",
      "11                0.0900        -9999.00000             1.0000   \n",
      "12                0.0750            0.00000         -9999.0000   \n",
      "13                0.1000        -9999.00000             0.0750   \n",
      "14                0.1600            0.00000             0.6000   \n",
      "15                0.0750            0.00000         -9999.0000   \n",
      "16                0.1000            0.00000             0.2500   \n",
      "17                0.0378            0.00000             0.0360   \n",
      "18                0.0800            0.00000         -9999.0000   \n",
      "19                0.0550            0.00000             0.0000   \n",
      "20                0.0830            0.00000             0.5000   \n",
      "21                0.2100            0.00000             1.0000   \n",
      "22                0.0310            0.00000             0.0000   \n",
      "23                0.0650            0.00000             0.3500   \n",
      "24                0.0270            0.00000             0.1500   \n",
      "25                0.1000            0.00000             0.3500   \n",
      "26                0.1500            0.00000             1.0000   \n",
      "27                0.0420            0.00000         -9999.0000   \n",
      "28                0.1200            0.00000             0.1200   \n",
      "29                0.1150            0.00000             1.0000   \n",
      "...                  ...                ...                ...   \n",
      "59351             0.0700            0.00000             0.0700   \n",
      "59352             0.0200            0.00000             0.1000   \n",
      "59353             0.0800            0.00000             0.0000   \n",
      "59354             0.0500            0.02000             0.2500   \n",
      "59355             0.0580            0.00000             0.2250   \n",
      "59356             0.0400            0.00000         -9999.0000   \n",
      "59357             0.1250            0.02500             1.0000   \n",
      "59358             0.0900        -9999.00000             0.0050   \n",
      "59359             0.0600            0.00750             0.2250   \n",
      "59360             0.0800            0.00000             0.1500   \n",
      "59361             0.2500            0.00000             0.9000   \n",
      "59362             0.0540            0.00000             0.0250   \n",
      "59363             0.0000            0.00000             0.0000   \n",
      "59364             0.0500            0.00000             0.0000   \n",
      "59365             0.0450            0.00000             0.2000   \n",
      "59366             0.0580            0.00000             0.3000   \n",
      "59367             0.1600            0.00000             0.0000   \n",
      "59368             0.0900            0.00000             0.2000   \n",
      "59369             0.0920        -9999.00000             0.1600   \n",
      "59370             0.0650            0.00000             0.0500   \n",
      "59371             0.0650            0.00000         -9999.0000   \n",
      "59372             0.2000            0.00000             0.3000   \n",
      "59373             0.0320            0.00000         -9999.0000   \n",
      "59374             0.0590            0.00000             0.0200   \n",
      "59375             0.0450            0.00000             0.0450   \n",
      "59376             0.0200            0.00000             0.0250   \n",
      "59377             0.1000            0.00001             0.3500   \n",
      "59378             0.0350            0.00000         -9999.0000   \n",
      "59379             0.0380        -9999.00000         -9999.0000   \n",
      "59380             0.1230        -9999.00000             0.3000   \n",
      "\n",
      "       Insurance_History_5  Family_Hist_2 ...  73  74  75  76  77  78  79  80  \\\n",
      "0                 0.000667   -9999.000000 ...   0   0   0   1   0   0  10   2   \n",
      "1                 0.000133       0.188406 ...   0   0   0   0   0   0  26   2   \n",
      "2             -9999.000000       0.304348 ...   0   0   0   0   0   1  26   2   \n",
      "3             -9999.000000       0.420290 ...   0   0   0   0   1   0  10   2   \n",
      "4             -9999.000000       0.463768 ...   0   0   1   0   0   0  26   2   \n",
      "5                 0.005000   -9999.000000 ...   0   0   1   0   0   0  26   3   \n",
      "6                 0.001667       0.594203 ...   0   0   0   0   0   0  10   2   \n",
      "7                 0.000667   -9999.000000 ...   0   0   1   0   0   0  26   2   \n",
      "8                 0.007613   -9999.000000 ...   0   0   0   1   0   0  26   2   \n",
      "9                 0.000667       0.797101 ...   0   0   0   0   0   1  21   2   \n",
      "10                0.000587   -9999.000000 ...   0   0   0   1   0   0  26   2   \n",
      "11            -9999.000000       0.405797 ...   0   0   0   0   1   0  26   2   \n",
      "12                0.000667   -9999.000000 ...   0   0   0   0   0   0  26   2   \n",
      "13            -9999.000000       0.420290 ...   0   1   0   0   0   0  26   2   \n",
      "14                0.004000   -9999.000000 ...   0   0   0   0   1   0  26   2   \n",
      "15                0.000480   -9999.000000 ...   0   0   0   0   0   0  26   2   \n",
      "16            -9999.000000       0.275362 ...   0   0   0   0   1   0  26   2   \n",
      "17            -9999.000000   -9999.000000 ...   0   0   0   1   0   0  26   2   \n",
      "18                0.000400   -9999.000000 ...   0   0   0   1   0   0  26   2   \n",
      "19            -9999.000000       0.289855 ...   0   0   0   0   1   0  26   2   \n",
      "20                0.001107   -9999.000000 ...   0   0   1   0   0   0  26   2   \n",
      "21            -9999.000000   -9999.000000 ...   0   0   0   0   0   0  26   2   \n",
      "22            -9999.000000       0.434783 ...   0   0   0   0   1   0  26   2   \n",
      "23                0.003333   -9999.000000 ...   0   0   0   0   0   0  26   2   \n",
      "24                0.000133       0.376812 ...   0   1   0   0   0   0  26   2   \n",
      "25            -9999.000000   -9999.000000 ...   0   0   0   0   0   0  26   2   \n",
      "26                0.002333   -9999.000000 ...   0   0   0   0   0   0  26   2   \n",
      "27                0.000667   -9999.000000 ...   0   0   0   1   0   0  26   2   \n",
      "28                0.001000   -9999.000000 ...   0   0   0   0   1   0  26   2   \n",
      "29                0.004720   -9999.000000 ...   0   0   0   1   0   0  26   2   \n",
      "...                    ...            ... ...  ..  ..  ..  ..  ..  ..  ..  ..   \n",
      "59351         -9999.000000   -9999.000000 ...   0   0   0   0   0   0  26   2   \n",
      "59352         -9999.000000   -9999.000000 ...   0   0   1   0   0   0  10   2   \n",
      "59353         -9999.000000       0.275362 ...   0   0   0   0   1   0  26   2   \n",
      "59354             0.006667       0.608696 ...   0   0   1   0   0   0  26   2   \n",
      "59355             0.000333       0.405797 ...   0   0   0   1   0   0  26   2   \n",
      "59356             0.000067   -9999.000000 ...   0   0   0   1   0   0  10   2   \n",
      "59357         -9999.000000   -9999.000000 ...   0   0   0   1   0   0  26   2   \n",
      "59358         -9999.000000       0.275362 ...   0   0   0   0   1   0  26   2   \n",
      "59359             0.002067   -9999.000000 ...   0   0   0   1   0   0  26   2   \n",
      "59360         -9999.000000       0.304348 ...   0   0   0   0   1   0  26   2   \n",
      "59361         -9999.000000       0.478261 ...   0   0   0   0   0   0  26   2   \n",
      "59362             0.000667       0.449275 ...   0   0   0   0   1   0  26   2   \n",
      "59363         -9999.000000   -9999.000000 ...   0   0   1   0   0   0   4   2   \n",
      "59364         -9999.000000   -9999.000000 ...   0   0   1   0   0   0  26   2   \n",
      "59365         -9999.000000   -9999.000000 ...   0   1   0   0   0   0  29   2   \n",
      "59366             0.002000   -9999.000000 ...   0   1   0   0   0   0  26   2   \n",
      "59367             0.005667   -9999.000000 ...   0   0   0   0   0   1  26   2   \n",
      "59368             0.000733   -9999.000000 ...   0   0   0   0   1   0  26   2   \n",
      "59369         -9999.000000   -9999.000000 ...   0   1   0   0   0   0  26   2   \n",
      "59370             0.001333   -9999.000000 ...   0   0   0   1   0   0  26   2   \n",
      "59371         -9999.000000   -9999.000000 ...   0   0   0   1   0   0  26   2   \n",
      "59372         -9999.000000       0.681159 ...   0   0   0   1   0   0  26   2   \n",
      "59373         -9999.000000       0.275362 ...   0   0   0   0   1   0  29   2   \n",
      "59374         -9999.000000       0.405797 ...   0   0   0   0   1   0  26   2   \n",
      "59375         -9999.000000   -9999.000000 ...   0   0   0   0   0   0  26   2   \n",
      "59376         -9999.000000       0.217391 ...   0   1   0   0   0   0  10   2   \n",
      "59377             0.000267       0.565217 ...   0   0   0   1   0   0  26   2   \n",
      "59378         -9999.000000       0.173913 ...   0   0   0   0   0   1  26   2   \n",
      "59379         -9999.000000   -9999.000000 ...   0   0   1   0   0   0  10   2   \n",
      "59380         -9999.000000   -9999.000000 ...   0   0   0   0   0   0  26   2   \n",
      "\n",
      "       81  82  \n",
      "0       1   1  \n",
      "1       3   1  \n",
      "2       3   1  \n",
      "3       3   1  \n",
      "4       3   1  \n",
      "5       1   1  \n",
      "6       3   1  \n",
      "7       3   1  \n",
      "8       3   1  \n",
      "9       3   1  \n",
      "10      3   1  \n",
      "11      3   1  \n",
      "12      3   1  \n",
      "13      3   1  \n",
      "14      3   1  \n",
      "15      3   1  \n",
      "16      3   1  \n",
      "17      3   1  \n",
      "18      3   1  \n",
      "19      3   1  \n",
      "20      3   1  \n",
      "21      1   1  \n",
      "22      3   1  \n",
      "23      3   1  \n",
      "24      1   1  \n",
      "25      3   1  \n",
      "26      3   1  \n",
      "27      3   1  \n",
      "28      3   1  \n",
      "29      3   1  \n",
      "...    ..  ..  \n",
      "59351   3   1  \n",
      "59352   3   1  \n",
      "59353   1   1  \n",
      "59354   1   1  \n",
      "59355   3   1  \n",
      "59356   3   1  \n",
      "59357   1   3  \n",
      "59358   3   1  \n",
      "59359   3   1  \n",
      "59360   3   1  \n",
      "59361   1   1  \n",
      "59362   3   1  \n",
      "59363   3   1  \n",
      "59364   3   1  \n",
      "59365   3   1  \n",
      "59366   3   1  \n",
      "59367   3   3  \n",
      "59368   1   1  \n",
      "59369   3   1  \n",
      "59370   3   1  \n",
      "59371   1   1  \n",
      "59372   3   1  \n",
      "59373   3   1  \n",
      "59374   3   1  \n",
      "59375   3   1  \n",
      "59376   3   1  \n",
      "59377   3   1  \n",
      "59378   3   1  \n",
      "59379   3   1  \n",
      "59380   3   1  \n",
      "\n",
      "[59381 rows x 144 columns]\n",
      "Training the model\n"
     ]
    }
   ],
   "source": [
    "features=x_train.columns.tolist()\n",
    "features.remove(\"Id\")\n",
    "features.remove(\"Response\")\n",
    "\n",
    "train_features=x_train[features].fillna(-9999)\n",
    "test_features=x_test[features].fillna(-9999)\n",
    "print(train_features)\n",
    "#try to predict these missing values and then\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "#how about running a separate model for each response value? doing some sort of xgb with logistic regression\n",
    "#for each and then putting all of them together\n",
    "\n",
    "\n",
    "#You can improve parameters if you want, this solution is not perfect at all\n",
    "param = {'max_depth':6, 'eta':0.1, 'silent':1, 'min_child_weight':3, 'subsample' : 0.7 ,\"early_stopping_rounds\":10,\n",
    "          \"objective\"   : \"count:poisson\", 'eval_metric': 'rmse','colsample_bytree':0.65}\n",
    "\n",
    "#\"objective\" = \"multi:softprob\",    # multiclass classification \n",
    "#              \"num_class\" = num.class,    # number of classes \n",
    "#              \"eval_metric\" = \"merror\",    # evaluation metric \n",
    "#              \"nthread\" = 8,   # number of threads to be used \n",
    "#              \"max_depth\" = 16,    # maximum depth of tree \n",
    "#              \"eta\" = 0.3,    # step size shrinkage \n",
    "#              \"gamma\" = 0,    # minimum loss reduction \n",
    "#              \"subsample\" = 1,    # part of data instances to grow tree \n",
    "#              \"colsample_bytree\" = 1,  # subsample ratio of columns when constructing each tree \n",
    "#              \"min_child_weight\" = 12  # minimum sum of instance weight needed in a child \n",
    "#              )\n",
    "                \n",
    "                \n",
    "num_round=700\n",
    "\n",
    "dtrain=xgb.DMatrix(train_features,label=y_train)\n",
    "dtest=xgb.DMatrix(test_features)\n",
    "\n",
    "watchlist  = [(dtrain,'train')]\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_round, watchlist)\n",
    "\n",
    "print(\"Training the model\")\n",
    "y_test_bst=bst.predict(dtest)\n",
    "\n",
    "#how to build a model here where i can do an ensemble method doing xgboost enough times to get to a particular result (or is this what it is doing already?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.581203"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "y_test_bst[0]\n",
    "#y_response = []\n",
    "#for i in range(len(y_test_bst)):\n",
    "#     y_response.append([j+1 for j,x in enumerate(y_test_bst[i]) if x == max(y_test_bst[i])])\n",
    "    \n",
    "#chain = chain.from_iterable(y_response)\n",
    "#y_response = list(chain)\n",
    "\n",
    "#print min(y_test_bst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we need integers to fit the model\n",
    "\n",
    "def output_function(x):\n",
    "    if x<1:\n",
    "        return 1\n",
    "    elif x>8:\n",
    "        return 8\n",
    "    elif int(round(x))==3:\n",
    "        return 2\n",
    "    else:\n",
    "        return int(round(x))\n",
    "        \n",
    "y_test_bst_result=[output_function(y) for y in y_test_bst]\n",
    "\n",
    "#write results\n",
    "\n",
    "ids=test.Id.values.tolist()\n",
    "n_ids=len(ids)\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "prediction_file = open(\"pythonxgb.csv\", \"w\")\n",
    "prediction_file_object = csv.writer(prediction_file)\n",
    "prediction_file_object.writerow([\"Id\",\"Response\"])\n",
    "for i in range(0,n_ids):\n",
    "    prediction_file_object.writerow([ids[i],y_test_bst_result[i]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting here again... pk 1/6 @ 6:30pm\n",
    "#todo: using the train data as a train/test set to evaluate the model before submitting\n",
    "#use cross validation to really see how will perform in the wild\n",
    "#can i use xgboost in this way?\n",
    "#what about a bunch of log regressions with one vs all?\n",
    "\n",
    "#think about using a function or a pipeline (see bayesian-methods in-class exercise):\n",
    "#\n",
    "#\n",
    "#pipeline = Pipeline((\n",
    "#    ('vec', TfidfVectorizer(max_df = 0.8, ngram_range = (1, 2), use_idf=True)),\n",
    "#    ('clf', MultinomialNB(alpha = 0.01)),\n",
    "#))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "DATA_DIR = '/Users/patrickkennedy/Desktop'\n",
    "\n",
    "train = pd.read_csv(DATA_DIR + '/Project DATA/train.csv')\n",
    "test = pd.read_csv(DATA_DIR + '/Project DATA/test.csv')\n",
    "\n",
    "\n",
    "features = train.columns\n",
    "features.drop(\"Id\")\n",
    "features.drop(\"Response\")\n",
    "\n",
    "#let's split the train data into train/test so we can do some model validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[features], train.Response, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "#We transform categorical values to dummies 0/1\n",
    "categorical = ['Product_Info_1', 'Product_Info_2', 'Product_Info_3', 'Product_Info_5', 'Product_Info_6', 'Product_Info_7', 'Employment_Info_2', 'Employment_Info_3', 'Employment_Info_5', 'InsuredInfo_1', 'InsuredInfo_2', 'InsuredInfo_3', 'InsuredInfo_4', 'InsuredInfo_5', 'InsuredInfo_6', 'InsuredInfo_7', 'Insurance_History_1', 'Insurance_History_2', 'Insurance_History_3', 'Insurance_History_4', 'Insurance_History_7', 'Insurance_History_8', 'Insurance_History_9', 'Family_Hist_1', 'Medical_History_2', 'Medical_History_3', 'Medical_History_4', 'Medical_History_5', 'Medical_History_6', 'Medical_History_7', 'Medical_History_8', 'Medical_History_9', 'Medical_History_10', 'Medical_History_11', 'Medical_History_12', 'Medical_History_13', 'Medical_History_14', 'Medical_History_16', 'Medical_History_17', 'Medical_History_18', 'Medical_History_19', 'Medical_History_20', 'Medical_History_21', 'Medical_History_22', 'Medical_History_23', 'Medical_History_25', 'Medical_History_26', 'Medical_History_27', 'Medical_History_28', 'Medical_History_29', 'Medical_History_30', 'Medical_History_31', 'Medical_History_33', 'Medical_History_34', 'Medical_History_35', 'Medical_History_36', 'Medical_History_37', 'Medical_History_38', 'Medical_History_39', 'Medical_History_40', 'Medical_History_41','Medical_History_1', 'Medical_History_15', 'Medical_History_24', 'Medical_History_32']\n",
    "\n",
    "#cat_train_df = train[categorical]\n",
    "#cat_train_dict = cat_train_df.T.to_dict().values()\n",
    "\n",
    "#cat_test_df = test[categorical]\n",
    "#cat_test_dict = cat_test_df.T.to_dict().values()\n",
    "\n",
    "\n",
    "#x_train_no_cat = train.drop(categorical, axis=1)\n",
    "#x_test_no_cat = test.drop(categorical, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using dict vectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "\n",
    "vectorizer = DV(sparse=False)\n",
    "vec_x_cat_train = pd.DataFrame(vectorizer.fit_transform(cat_train_dict))\n",
    "vec_x_cat_test = pd.DataFrame(vectorizer.transform(cat_test_dict)) \n",
    "\n",
    "x_train = pd.concat([x_train_no_cat, vec_x_cat_train], axis=1)\n",
    "x_test = pd.concat([x_test_no_cat, vec_x_cat_test], axis=1)\n",
    "\n",
    "#how to ensemble xgboost given that the data has been vectorized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=x_train.columns.tolist()\n",
    "features.remove(\"Id\")\n",
    "features.remove(\"Response\")\n",
    "\n",
    "train_features=x_train[features].fillna(-9999)\n",
    "test_features=x_test[features].fillna(-9999)\n",
    "print(train_features)\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "#how about running a separate model for each response value? doing some sort of xgb with logistic regression\n",
    "#for each and then putting all of them together\n",
    "\n",
    "\n",
    "#You can improve parameters if you want, this solution is not perfect at all\n",
    "param = {'max_depth':6, 'eta':0.1, 'silent':1, 'min_child_weight':3, 'subsample' : 0.7 ,\"early_stopping_rounds\":10,\n",
    "          \"objective\"   : \"count:poisson\", 'eval_metric': 'rmse','colsample_bytree':0.65}\n",
    "\n",
    "#\"objective\" = \"multi:softprob\",    # multiclass classification \n",
    "#              \"num_class\" = num.class,    # number of classes \n",
    "#              \"eval_metric\" = \"merror\",    # evaluation metric \n",
    "#              \"nthread\" = 8,   # number of threads to be used \n",
    "#              \"max_depth\" = 16,    # maximum depth of tree \n",
    "#              \"eta\" = 0.3,    # step size shrinkage \n",
    "#              \"gamma\" = 0,    # minimum loss reduction \n",
    "#              \"subsample\" = 1,    # part of data instances to grow tree \n",
    "#              \"colsample_bytree\" = 1,  # subsample ratio of columns when constructing each tree \n",
    "#              \"min_child_weight\" = 12  # minimum sum of instance weight needed in a child \n",
    "#              )\n",
    "                \n",
    "                \n",
    "num_round=700\n",
    "\n",
    "dtrain=xgb.DMatrix(train_features,label=y_train)\n",
    "dtest=xgb.DMatrix(test_features)\n",
    "\n",
    "watchlist  = [(dtrain,'train')]\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_round, watchlist)\n",
    "\n",
    "print(\"Training the model\")\n",
    "y_test_bst=bst.predict(dtest)\n",
    "\n",
    "#how to build a model here where i can do an ensemble method doing xgboost enough times to get to a particular result (or is this what it is doing already?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dive into the code below... how are we getting a higher score? what is the code doing? what can i add to juke the score higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the data using pandas\n",
      "Eliminate missing values\n",
      "[('colsample_bytree', 0.9), ('silent', 1), ('min_child_weight', 50), ('subsample', 0.67), ('eta', 0.01), ('objective', 'reg:linear'), ('max_depth', 9)]\n",
      "('Train score is:', 0.6746139681398773)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.clock()\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import xgboost as xgb\n",
    "from scipy.optimize import fmin_powell\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "\n",
    "def eval_wrapper(yhat, y):  \n",
    "    y = np.array(y)\n",
    "    y = y.astype(int)\n",
    "    yhat = np.array(yhat)\n",
    "    yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)   \n",
    "    return quadratic_weighted_kappa(yhat, y)\n",
    "    \n",
    "def get_params():\n",
    "    \n",
    "    params = {}\n",
    "    params[\"objective\"] = \"reg:linear\"     \n",
    "    params[\"eta\"] = 0.01\n",
    "    params[\"min_child_weight\"] = 50\n",
    "    params[\"subsample\"] = 0.67\n",
    "    params[\"colsample_bytree\"] = 0.9\n",
    "    params[\"silent\"] = 1\n",
    "    params[\"max_depth\"] = 9\n",
    "    plst = list(params.items())\n",
    "\n",
    "    return plst\n",
    "    \n",
    "def apply_offset(data, bin_offset, sv, scorer=eval_wrapper):\n",
    "    # data has the format of pred=0, offset_pred=1, labels=2 in the first dim\n",
    "    data[1, data[0].astype(int)==sv] = data[0, data[0].astype(int)==sv] + bin_offset\n",
    "    score = scorer(data[1], data[2])\n",
    "    return score\n",
    "\n",
    "# global variables\n",
    "columns_to_drop = ['Id', 'Response']\n",
    "xgb_num_rounds = 500 #5000 gives me good score (~15 min), 10000 (~32 min) (no improvement)\n",
    "num_classes = 8\n",
    "\n",
    "print(\"Load the data using pandas\")\n",
    "train = pd.read_csv(DATA_DIR + '/Project DATA/train.csv')\n",
    "test = pd.read_csv(DATA_DIR + '/Project DATA/test.csv')\n",
    "\n",
    "# combine train and test\n",
    "all_data = train.append(test)\n",
    "\n",
    "print('Eliminate missing values')    \n",
    "# Use -1 for any others\n",
    "all_data.fillna(-1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# factorize categorical variables\n",
    "# add all categorical vars -pk\n",
    "# does this matter? and what of any feature selection beyond factorizing prod info 2?\n",
    "#categorical = ['Product_Info_1', 'Product_Info_2', 'Product_Info_3', 'Product_Info_5', 'Product_Info_6', 'Product_Info_7', 'Employment_Info_2', 'Employment_Info_3', 'Employment_Info_5', 'InsuredInfo_1', 'InsuredInfo_2', 'InsuredInfo_3', 'InsuredInfo_4', 'InsuredInfo_5', 'InsuredInfo_6', 'InsuredInfo_7', 'Insurance_History_1', 'Insurance_History_2', 'Insurance_History_3', 'Insurance_History_4', 'Insurance_History_7', 'Insurance_History_8', 'Insurance_History_9', 'Family_Hist_1', 'Medical_History_2', 'Medical_History_3', 'Medical_History_4', 'Medical_History_5', 'Medical_History_6', 'Medical_History_7', 'Medical_History_8', 'Medical_History_9', 'Medical_History_10', 'Medical_History_11', 'Medical_History_12', 'Medical_History_13', 'Medical_History_14', 'Medical_History_16', 'Medical_History_17', 'Medical_History_18', 'Medical_History_19', 'Medical_History_20', 'Medical_History_21', 'Medical_History_22', 'Medical_History_23', 'Medical_History_25', 'Medical_History_26', 'Medical_History_27', 'Medical_History_28', 'Medical_History_29', 'Medical_History_30', 'Medical_History_31', 'Medical_History_33', 'Medical_History_34', 'Medical_History_35', 'Medical_History_36', 'Medical_History_37', 'Medical_History_38', 'Medical_History_39', 'Medical_History_40', 'Medical_History_41','Medical_History_1', 'Medical_History_15', 'Medical_History_24', 'Medical_History_32']\n",
    "\n",
    "\n",
    "\n",
    "#does it matter that i am running on normalized variables? seems like it would obviate the need to do PCA...\n",
    "#doing PCA on continuous variables\n",
    "#continuous = ['Product_Info_4', 'Ins_Age', 'Ht', 'Wt', 'BMI', 'Employment_Info_1', 'Employment_Info_4', 'Employment_Info_6',\n",
    "#              'Insurance_History_5', 'Family_Hist_2', 'Family_Hist_3', 'Family_Hist_4', 'Family_Hist_5'] \n",
    "\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#X_scaled = StandardScaler().fit_transform(all_data[continuous])\n",
    "#pca = PCA(n_components=len(continuous), whiten=True).fit(X_scaled)\n",
    "#all_data_PCA = pca.transform(X_scaled)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#PCA_titles = ['PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5', 'PCA6', 'PCA7', 'PCA8', 'PCA9', 'PCA10', 'PCA11', 'PCA12', 'PCA13']\n",
    "#all_data_PCA_df = pd.DataFrame(all_data_PCA, index=all_data.index, columns=PCA_titles)\n",
    "    \n",
    "#add back into all_data and drop the continuous variables\n",
    "#all_data_new = pd.concat([all_data, all_data_PCA_df], axis=1)\n",
    "#all_data_new.drop(continuous, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "all_data['Product_Info_2'] = pd.factorize(all_data['Product_Info_2'])[0]\n",
    "\n",
    "# fix the dtype on the label column\n",
    "all_data['Response'] = all_data['Response'].astype(int)\n",
    "\n",
    "\n",
    "all_data_no_response = all_data.drop('Response')\n",
    "\n",
    "    \n",
    "# Provide split column\n",
    "#all_data_new['Split'] = np.random.randint(5, size=all_data_new.shape[0])\n",
    "\n",
    "# split train and test\n",
    "train = new_all_data[new_all_data['Response']>0].copy()\n",
    "test = new_all_data[new_all_data['Response']<1].copy()\n",
    "\n",
    "\n",
    "# convert data to xgb data structure\n",
    "xgtrain = xgb.DMatrix(train.drop(columns_to_drop, axis=1), train['Response'].values)\n",
    "xgtest = xgb.DMatrix(test.drop(columns_to_drop, axis=1), label=test['Response'].values)    \n",
    "\n",
    "# get the parameters for xgboost\n",
    "plst = get_params()\n",
    "print(plst)      \n",
    "\n",
    "# train model\n",
    "model = xgb.train(plst, xgtrain, xgb_num_rounds) \n",
    "\n",
    "# get preds\n",
    "train_preds = model.predict(xgtrain, ntree_limit=model.best_iteration)\n",
    "print('Train score is:', eval_wrapper(train_preds, train['Response'])) \n",
    "test_preds = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_preds_1 = np.clip(train_preds, -0.99, 8.99)\n",
    "test_preds_1 = np.clip(test_preds, -0.99, 8.99)\n",
    "\n",
    "# train offsets \n",
    "offsets = np.array([-5.1, -0.9, 0.1, -0.9, 1.0, 0.0, 0.0, 1.0])\n",
    "\n",
    "import itertools\n",
    "perms = [x for x in itertools.permutations([0, 1, 2, 3, 4, 5, 6, 7]) if x[0] != 7 and x[1] != 7]\n",
    "perms = perms[-15:]\n",
    "\n",
    "\n",
    "for i, perm in enumerate(reversed(perms)):\n",
    "\n",
    "\n",
    "    data = np.vstack((train_preds_1, train_preds_1, train['Response']))\n",
    "    for j in range(num_classes):\n",
    "        data[1, data[0].astype(int)==j] = data[0, data[0].astype(int)==j] + offsets[j] \n",
    "    for j in perm:\n",
    "        train_offset = lambda x: -apply_offset(data, x, j)\n",
    "        offsets[j] = fmin_powell(train_offset, offsets[j], disp=False)  \n",
    "    \n",
    "    # apply offsets to test\n",
    "    data = np.vstack((test_preds_1, test_preds_1, test['Response']))\n",
    "    for j in range(num_classes):\n",
    "        data[1, data[0].astype(int)==j] = data[0, data[0].astype(int)==j] + offsets[j] \n",
    "    final_test_preds = np.round(np.clip(data[1], 1, 8)).astype(int)\n",
    "\n",
    "preds_out = pd.DataFrame({\"Id\": test['Id'].values, \"Response\": final_test_preds})\n",
    "preds_out = preds_out.set_index('Id')\n",
    "preds_out.to_csv('xgb_offset_submission_more_cols.csv')\n",
    "\n",
    "#0.66738 for order [6,7,2,5,0,3,4,1]\n",
    "#0.667435 for order [6,5,7....]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#train_preds = np.clip(train_preds, -0.99, 8.99)\n",
    "#test_preds = np.clip(test_preds, -0.99, 8.99)\n",
    "\n",
    "# train offsets \n",
    "#offsets = np.ones(num_classes) * -0.5\n",
    "#offset_train_preds = np.vstack((train_preds, train_preds, train['Response'].values))\n",
    "\n",
    "#for j in range(num_classes):\n",
    "#    train_offset = lambda x: -apply_offset(offset_train_preds, x, j)\n",
    "#    offsets[j] = fmin_powell(train_offset, offsets[j])  \n",
    "\n",
    "\n",
    "# apply offsets to test\n",
    "#data = np.vstack((test_preds, test_preds, test['Response'].values))\n",
    "#for j in range(num_classes):\n",
    "#    data[1, data[0].astype(int)==j] = data[0, data[0].astype(int)==j] + offsets[j]    \n",
    "#final_test_preds = np.round(np.clip(data[1], 1, 8)).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "#preds_out = pd.DataFrame({\"Id\": test['Id'].values, \"Response\": final_test_preds})\n",
    "#preds_out = preds_out.set_index('Id')\n",
    "#preds_out.to_csv('xgb_offset_submission-fewer_cols.csv')\n",
    "\n",
    "#print time.clock()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = test[pred_cols]\n",
    "avg_pred = []\n",
    "\n",
    "avg_pred = preds.mean(axis=1)\n",
    "avg_pred = [int(x) for x in avg_pred]\n",
    "#print avg_pred\n",
    "\n",
    "\n",
    "final_preds = preds['preds2'].values\n",
    "\n",
    "\n",
    "preds_out = pd.DataFrame({\"Id\": test['Id'].values, \"Response\": final_preds})\n",
    "preds_out = preds_out.set_index('Id')\n",
    "preds_out.to_csv('xgb_offset_submission.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performs at: .66428\n",
    "\n",
    "position = 216/1455 -- 85th percentile\n",
    "\n",
    "\n",
    "Hi Muthu,\n",
    "I'd be happy to!\n",
    "The 'apply_offset' function applies an offset to select predictions generated from the XGBoost model. \n",
    "\n",
    "The 'data' variable it takes in is a numpy array with the dimensions of 3 by x, where x is the number of values \n",
    "on which to apply the offsets (i.e. the number of predictions). The values in 'data[0]' are the original \n",
    "predictions, the values in 'data[1]' are the same predictions where an offset can be applied, and the values \n",
    "in 'data[2]' are the labels against which to score the offset. There are eight offsets in total, as stored \n",
    "in the 'offsets' list variable. These apply to predictions which have as their integer value matching the \n",
    "position of the offset in the offsets list.\n",
    "\n",
    "So, for example, if you have a prediction of 0.956, and an offset of 1 in 'offsets[0]', \n",
    "the 'apply_offset' function (and specifically the line in question) will put 1.956 in 'data[1]' \n",
    "for that prediction.\n",
    "\n",
    "The line of code you're asking about takes the value from the original predictions (i.e. 'data[0]') \n",
    "where the offset applies (i.e. '.astype(int)==sv', which is the subset of the array where the prediction \n",
    "integer value matches 'sv', which is the position of the offset in the 'offsets' list) , applies the offset \n",
    "(i.e. ' + bin_offset'), and stores the result in the corresponding offset prediction \n",
    "(i.e. 'data[1, data[0].astype(int)==sv]').\n",
    "\n",
    "The upshot of all this is that the predictions generated from XGBoost are offset to a value which \n",
    "increases the score. My inspiration in creating this came from other discussions I saw where people \n",
    "were using values other than 0.5 as the cutoff to decide where a prediction belongs. So, for example, \n",
    "should a prediction of 3.54 be submitted as a 3 or a 4? Using simple rounding, it would be a 4. By \n",
    "adjusting this cutoff, higher scores have been achieved, but the value will always be a 3 or a 4. I took \n",
    "this a step further, and apply an offset before rounding, so a value like 3.54 might end up being 1.54 \n",
    "(if the offset was -2), which would be submitted as a 2 after rounding. I train the offsets against the \n",
    "train predictions, and then apply the resulting offsets to the test predictions before submitting.\n",
    "\n",
    "I hope this helps.\n",
    "\n",
    "Thanks, Michael\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "all_data_no_response = all_data.drop(['Response'],axis=1)\n",
    "all_data_multiplication = pd.DataFrame()\n",
    "\n",
    "for i, colx in enumerate(all_data_no_response.columns):\n",
    "    for j, coly in enumerate(all_data_no_response.columns):\n",
    "        if j <= i:\n",
    "            pass\n",
    "        else:\n",
    "            all_data_multiplication[colx+coly] = all_data_no_response[colx] * all_data_no_response[coly]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_all_data = pd.concat([all_data, all_data_multiplication], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59351</th>\n",
       "      <td>79115</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59352</th>\n",
       "      <td>79116</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59353</th>\n",
       "      <td>79117</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59354</th>\n",
       "      <td>79118</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59355</th>\n",
       "      <td>79119</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59356</th>\n",
       "      <td>79120</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59357</th>\n",
       "      <td>79121</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59358</th>\n",
       "      <td>79122</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59359</th>\n",
       "      <td>79123</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59360</th>\n",
       "      <td>79124</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59361</th>\n",
       "      <td>79126</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59362</th>\n",
       "      <td>79127</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59363</th>\n",
       "      <td>79128</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59364</th>\n",
       "      <td>79130</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59365</th>\n",
       "      <td>79131</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59366</th>\n",
       "      <td>79132</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59367</th>\n",
       "      <td>79133</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59368</th>\n",
       "      <td>79134</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59369</th>\n",
       "      <td>79135</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59370</th>\n",
       "      <td>79136</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59371</th>\n",
       "      <td>79137</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59372</th>\n",
       "      <td>79138</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59373</th>\n",
       "      <td>79139</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59374</th>\n",
       "      <td>79140</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59375</th>\n",
       "      <td>79141</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59376</th>\n",
       "      <td>79142</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59377</th>\n",
       "      <td>79143</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59378</th>\n",
       "      <td>79144</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59379</th>\n",
       "      <td>79145</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59380</th>\n",
       "      <td>79146</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59381 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id  Response\n",
       "0          2         8\n",
       "1          5         4\n",
       "2          6         8\n",
       "3          7         8\n",
       "4          8         8\n",
       "5         10         8\n",
       "6         11         8\n",
       "7         14         1\n",
       "8         15         8\n",
       "9         16         1\n",
       "10        17         6\n",
       "11        18         2\n",
       "12        19         7\n",
       "13        20         3\n",
       "14        22         8\n",
       "15        23         5\n",
       "16        24         8\n",
       "17        25         7\n",
       "18        26         2\n",
       "19        27         8\n",
       "20        29         8\n",
       "21        31         5\n",
       "22        32         5\n",
       "23        33         8\n",
       "24        34         6\n",
       "25        35         8\n",
       "26        37         6\n",
       "27        39         8\n",
       "28        40         7\n",
       "29        41         6\n",
       "...      ...       ...\n",
       "59351  79115         5\n",
       "59352  79116         6\n",
       "59353  79117         6\n",
       "59354  79118         6\n",
       "59355  79119         6\n",
       "59356  79120         6\n",
       "59357  79121         6\n",
       "59358  79122         6\n",
       "59359  79123         6\n",
       "59360  79124         5\n",
       "59361  79126         2\n",
       "59362  79127         7\n",
       "59363  79128         6\n",
       "59364  79130         1\n",
       "59365  79131         5\n",
       "59366  79132         8\n",
       "59367  79133         6\n",
       "59368  79134         8\n",
       "59369  79135         2\n",
       "59370  79136         4\n",
       "59371  79137         6\n",
       "59372  79138         2\n",
       "59373  79139         8\n",
       "59374  79140         7\n",
       "59375  79141         8\n",
       "59376  79142         4\n",
       "59377  79143         7\n",
       "59378  79144         8\n",
       "59379  79145         8\n",
       "59380  79146         7\n",
       "\n",
       "[59381 rows x 2 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = new_all_data[new_all_data['Response']>0]\n",
    "train[['Id', 'Response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#what about running xgboost's logistic objective on a one vs all for all 8 classes?\n",
    "#run it through with the offset too?\n",
    "\n",
    "#can use the same training data but the response variables can be iterated through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.104528\n",
      "[1]\ttrain-error:0.104528\n",
      "[2]\ttrain-error:0.104528\n",
      "[3]\ttrain-error:0.104528\n",
      "[4]\ttrain-error:0.104528\n",
      "[5]\ttrain-error:0.104528\n",
      "[6]\ttrain-error:0.104528\n",
      "[7]\ttrain-error:0.104528\n",
      "[8]\ttrain-error:0.104528\n",
      "[9]\ttrain-error:0.104528\n",
      "[10]\ttrain-error:0.104528\n",
      "[11]\ttrain-error:0.104528\n",
      "[12]\ttrain-error:0.104528\n",
      "[13]\ttrain-error:0.104528\n",
      "[14]\ttrain-error:0.104545\n",
      "[15]\ttrain-error:0.104545\n",
      "[16]\ttrain-error:0.104528\n",
      "[17]\ttrain-error:0.104394\n",
      "[18]\ttrain-error:0.104360\n",
      "[19]\ttrain-error:0.104276\n",
      "[20]\ttrain-error:0.104192\n",
      "[21]\ttrain-error:0.104158\n",
      "[22]\ttrain-error:0.104023\n",
      "[23]\ttrain-error:0.103872\n",
      "[24]\ttrain-error:0.103754\n",
      "[25]\ttrain-error:0.103670\n",
      "[26]\ttrain-error:0.103484\n",
      "[27]\ttrain-error:0.103400\n",
      "[28]\ttrain-error:0.103366\n",
      "[29]\ttrain-error:0.103350\n",
      "[30]\ttrain-error:0.103164\n",
      "[31]\ttrain-error:0.102962\n",
      "[32]\ttrain-error:0.102962\n",
      "[33]\ttrain-error:0.102828\n",
      "[34]\ttrain-error:0.102676\n",
      "[35]\ttrain-error:0.102592\n",
      "[36]\ttrain-error:0.102204\n",
      "[37]\ttrain-error:0.101985\n",
      "[38]\ttrain-error:0.101884\n",
      "[39]\ttrain-error:0.101834\n",
      "[40]\ttrain-error:0.101800\n",
      "[41]\ttrain-error:0.101581\n",
      "[42]\ttrain-error:0.101143\n",
      "[43]\ttrain-error:0.101076\n",
      "[44]\ttrain-error:0.100874\n",
      "[45]\ttrain-error:0.100739\n",
      "[46]\ttrain-error:0.100605\n",
      "[47]\ttrain-error:0.100621\n",
      "[48]\ttrain-error:0.100588\n",
      "[49]\ttrain-error:0.100554\n",
      "[50]\ttrain-error:0.100453\n",
      "[51]\ttrain-error:0.100268\n",
      "[52]\ttrain-error:0.100099\n",
      "[53]\ttrain-error:0.100049\n",
      "[54]\ttrain-error:0.099830\n",
      "[55]\ttrain-error:0.099746\n",
      "[56]\ttrain-error:0.099729\n",
      "[57]\ttrain-error:0.099594\n",
      "[58]\ttrain-error:0.099594\n",
      "[59]\ttrain-error:0.099695\n",
      "[60]\ttrain-error:0.099443\n",
      "[61]\ttrain-error:0.099123\n",
      "[62]\ttrain-error:0.099156\n",
      "[63]\ttrain-error:0.099240\n",
      "[64]\ttrain-error:0.099089\n",
      "[65]\ttrain-error:0.099072\n",
      "[66]\ttrain-error:0.098954\n",
      "[67]\ttrain-error:0.099089\n",
      "[68]\ttrain-error:0.099106\n",
      "[69]\ttrain-error:0.098904\n",
      "[70]\ttrain-error:0.098769\n",
      "[71]\ttrain-error:0.098702\n",
      "[72]\ttrain-error:0.098718\n",
      "[73]\ttrain-error:0.098382\n",
      "[74]\ttrain-error:0.098331\n",
      "[75]\ttrain-error:0.098449\n",
      "[76]\ttrain-error:0.098382\n",
      "[77]\ttrain-error:0.098382\n",
      "[78]\ttrain-error:0.098415\n",
      "[79]\ttrain-error:0.098449\n",
      "[80]\ttrain-error:0.098415\n",
      "[81]\ttrain-error:0.098247\n",
      "[82]\ttrain-error:0.098129\n",
      "[83]\ttrain-error:0.098095\n",
      "[84]\ttrain-error:0.098011\n",
      "[85]\ttrain-error:0.097977\n",
      "[86]\ttrain-error:0.097977\n",
      "[87]\ttrain-error:0.097977\n",
      "[88]\ttrain-error:0.097759\n",
      "[89]\ttrain-error:0.097809\n",
      "[90]\ttrain-error:0.097674\n",
      "[91]\ttrain-error:0.097540\n",
      "[92]\ttrain-error:0.097590\n",
      "[93]\ttrain-error:0.097439\n",
      "[94]\ttrain-error:0.097573\n",
      "[95]\ttrain-error:0.097506\n",
      "[96]\ttrain-error:0.097439\n",
      "[97]\ttrain-error:0.097203\n",
      "[98]\ttrain-error:0.097236\n",
      "[99]\ttrain-error:0.097321\n",
      "[100]\ttrain-error:0.097119\n",
      "[101]\ttrain-error:0.097119\n",
      "[102]\ttrain-error:0.097135\n",
      "[103]\ttrain-error:0.097051\n",
      "[104]\ttrain-error:0.096933\n",
      "[105]\ttrain-error:0.096900\n",
      "[106]\ttrain-error:0.096967\n",
      "[107]\ttrain-error:0.096883\n",
      "[108]\ttrain-error:0.096748\n",
      "[109]\ttrain-error:0.096647\n",
      "[110]\ttrain-error:0.096664\n",
      "[111]\ttrain-error:0.096546\n",
      "[112]\ttrain-error:0.096394\n",
      "[113]\ttrain-error:0.096344\n",
      "[114]\ttrain-error:0.096462\n",
      "[115]\ttrain-error:0.096310\n",
      "[116]\ttrain-error:0.096310\n",
      "[117]\ttrain-error:0.096277\n",
      "[118]\ttrain-error:0.096378\n",
      "[119]\ttrain-error:0.096243\n",
      "[120]\ttrain-error:0.096074\n",
      "[121]\ttrain-error:0.096159\n",
      "[122]\ttrain-error:0.096024\n",
      "[123]\ttrain-error:0.095872\n",
      "[124]\ttrain-error:0.095839\n",
      "[125]\ttrain-error:0.095771\n",
      "[126]\ttrain-error:0.095687\n",
      "[127]\ttrain-error:0.095704\n",
      "[128]\ttrain-error:0.095569\n",
      "[129]\ttrain-error:0.095586\n",
      "[130]\ttrain-error:0.095620\n",
      "[131]\ttrain-error:0.095519\n",
      "[132]\ttrain-error:0.095536\n",
      "[133]\ttrain-error:0.095620\n",
      "[134]\ttrain-error:0.095502\n",
      "[135]\ttrain-error:0.095519\n",
      "[136]\ttrain-error:0.095552\n",
      "[137]\ttrain-error:0.095401\n",
      "[138]\ttrain-error:0.095283\n",
      "[139]\ttrain-error:0.095216\n",
      "[140]\ttrain-error:0.095098\n",
      "[141]\ttrain-error:0.095199\n",
      "[142]\ttrain-error:0.095131\n",
      "[143]\ttrain-error:0.095064\n",
      "[144]\ttrain-error:0.095131\n",
      "[145]\ttrain-error:0.095115\n",
      "[146]\ttrain-error:0.094946\n",
      "[147]\ttrain-error:0.094862\n",
      "[148]\ttrain-error:0.094727\n",
      "[149]\ttrain-error:0.094542\n",
      "[150]\ttrain-error:0.094559\n",
      "[151]\ttrain-error:0.094424\n",
      "[152]\ttrain-error:0.094508\n",
      "[153]\ttrain-error:0.094441\n",
      "[154]\ttrain-error:0.094340\n",
      "[155]\ttrain-error:0.094306\n",
      "[156]\ttrain-error:0.094121\n",
      "[157]\ttrain-error:0.094222\n",
      "[158]\ttrain-error:0.094121\n",
      "[159]\ttrain-error:0.094155\n",
      "[160]\ttrain-error:0.093986\n",
      "[161]\ttrain-error:0.094104\n",
      "[162]\ttrain-error:0.093936\n",
      "[163]\ttrain-error:0.093902\n",
      "[164]\ttrain-error:0.093784\n",
      "[165]\ttrain-error:0.093801\n",
      "[166]\ttrain-error:0.093751\n",
      "[167]\ttrain-error:0.093666\n",
      "[168]\ttrain-error:0.093515\n",
      "[169]\ttrain-error:0.093666\n",
      "[170]\ttrain-error:0.093633\n",
      "[171]\ttrain-error:0.093767\n",
      "[172]\ttrain-error:0.093582\n",
      "[173]\ttrain-error:0.093582\n",
      "[174]\ttrain-error:0.093565\n",
      "[175]\ttrain-error:0.093599\n",
      "[176]\ttrain-error:0.093481\n",
      "[177]\ttrain-error:0.093431\n",
      "[178]\ttrain-error:0.093414\n",
      "[179]\ttrain-error:0.093464\n",
      "[180]\ttrain-error:0.093447\n",
      "[181]\ttrain-error:0.093464\n",
      "[182]\ttrain-error:0.093380\n",
      "[183]\ttrain-error:0.093515\n",
      "[184]\ttrain-error:0.093363\n",
      "[185]\ttrain-error:0.093262\n",
      "[186]\ttrain-error:0.093195\n",
      "[187]\ttrain-error:0.093144\n",
      "[188]\ttrain-error:0.093111\n",
      "[189]\ttrain-error:0.093212\n",
      "[190]\ttrain-error:0.093279\n",
      "[191]\ttrain-error:0.093178\n",
      "[192]\ttrain-error:0.093144\n",
      "[193]\ttrain-error:0.093111\n",
      "[194]\ttrain-error:0.092976\n",
      "[195]\ttrain-error:0.092841\n",
      "[196]\ttrain-error:0.092740\n",
      "[197]\ttrain-error:0.092774\n",
      "[198]\ttrain-error:0.092673\n",
      "[199]\ttrain-error:0.092706\n",
      "[200]\ttrain-error:0.092824\n",
      "[201]\ttrain-error:0.092723\n",
      "[202]\ttrain-error:0.092572\n",
      "[203]\ttrain-error:0.092589\n",
      "[204]\ttrain-error:0.092572\n",
      "[205]\ttrain-error:0.092487\n",
      "[206]\ttrain-error:0.092538\n",
      "[207]\ttrain-error:0.092403\n",
      "[208]\ttrain-error:0.092471\n",
      "[209]\ttrain-error:0.092386\n",
      "[210]\ttrain-error:0.092353\n",
      "[211]\ttrain-error:0.092269\n",
      "[212]\ttrain-error:0.092201\n",
      "[213]\ttrain-error:0.092269\n",
      "[214]\ttrain-error:0.092151\n",
      "[215]\ttrain-error:0.092151\n",
      "[216]\ttrain-error:0.092134\n",
      "[217]\ttrain-error:0.092235\n",
      "[218]\ttrain-error:0.092100\n",
      "[219]\ttrain-error:0.092083\n",
      "[220]\ttrain-error:0.092016\n",
      "[221]\ttrain-error:0.092050\n",
      "[222]\ttrain-error:0.091982\n",
      "[223]\ttrain-error:0.091982\n",
      "[224]\ttrain-error:0.091999\n",
      "[225]\ttrain-error:0.091881\n",
      "[226]\ttrain-error:0.091965\n",
      "[227]\ttrain-error:0.091780\n",
      "[228]\ttrain-error:0.091915\n",
      "[229]\ttrain-error:0.091814\n",
      "[230]\ttrain-error:0.091864\n",
      "[231]\ttrain-error:0.091898\n",
      "[232]\ttrain-error:0.091814\n",
      "[233]\ttrain-error:0.091747\n",
      "[234]\ttrain-error:0.091696\n",
      "[235]\ttrain-error:0.091561\n",
      "[236]\ttrain-error:0.091511\n",
      "[237]\ttrain-error:0.091561\n",
      "[238]\ttrain-error:0.091427\n",
      "[239]\ttrain-error:0.091342\n",
      "[240]\ttrain-error:0.091342\n",
      "[241]\ttrain-error:0.091494\n",
      "[242]\ttrain-error:0.091511\n",
      "[243]\ttrain-error:0.091595\n",
      "[244]\ttrain-error:0.091544\n",
      "[245]\ttrain-error:0.091477\n",
      "[246]\ttrain-error:0.091494\n",
      "[247]\ttrain-error:0.091511\n",
      "[248]\ttrain-error:0.091443\n",
      "[249]\ttrain-error:0.091595\n",
      "[250]\ttrain-error:0.091528\n",
      "[251]\ttrain-error:0.091511\n",
      "[252]\ttrain-error:0.091208\n",
      "[253]\ttrain-error:0.091006\n",
      "[254]\ttrain-error:0.090955\n",
      "[255]\ttrain-error:0.091022\n",
      "[256]\ttrain-error:0.091140\n",
      "[257]\ttrain-error:0.090989\n",
      "[258]\ttrain-error:0.090854\n",
      "[259]\ttrain-error:0.090955\n",
      "[260]\ttrain-error:0.091006\n",
      "[261]\ttrain-error:0.090854\n",
      "[262]\ttrain-error:0.090989\n",
      "[263]\ttrain-error:0.090854\n",
      "[264]\ttrain-error:0.090904\n",
      "[265]\ttrain-error:0.090921\n",
      "[266]\ttrain-error:0.090803\n",
      "[267]\ttrain-error:0.090837\n",
      "[268]\ttrain-error:0.090837\n",
      "[269]\ttrain-error:0.090904\n",
      "[270]\ttrain-error:0.090787\n",
      "[271]\ttrain-error:0.090702\n",
      "[272]\ttrain-error:0.090904\n",
      "[273]\ttrain-error:0.090753\n",
      "[274]\ttrain-error:0.090904\n",
      "[275]\ttrain-error:0.090736\n",
      "[276]\ttrain-error:0.090702\n",
      "[277]\ttrain-error:0.090635\n",
      "[278]\ttrain-error:0.090568\n",
      "[279]\ttrain-error:0.090517\n",
      "[280]\ttrain-error:0.090551\n",
      "[281]\ttrain-error:0.090500\n",
      "[282]\ttrain-error:0.090618\n",
      "[283]\ttrain-error:0.090635\n",
      "[284]\ttrain-error:0.090568\n",
      "[285]\ttrain-error:0.090467\n",
      "[286]\ttrain-error:0.090483\n",
      "[287]\ttrain-error:0.090399\n",
      "[288]\ttrain-error:0.090366\n",
      "[289]\ttrain-error:0.090416\n",
      "[290]\ttrain-error:0.090332\n",
      "[291]\ttrain-error:0.090349\n",
      "[292]\ttrain-error:0.090382\n",
      "[293]\ttrain-error:0.090467\n",
      "[294]\ttrain-error:0.090332\n",
      "[295]\ttrain-error:0.090467\n",
      "[296]\ttrain-error:0.090265\n",
      "[297]\ttrain-error:0.090315\n",
      "[298]\ttrain-error:0.090197\n",
      "[299]\ttrain-error:0.090113\n",
      "[300]\ttrain-error:0.090147\n",
      "[301]\ttrain-error:0.090164\n",
      "[302]\ttrain-error:0.090164\n",
      "[303]\ttrain-error:0.090079\n",
      "[304]\ttrain-error:0.089877\n",
      "[305]\ttrain-error:0.089928\n",
      "[306]\ttrain-error:0.090096\n",
      "[307]\ttrain-error:0.089978\n",
      "[308]\ttrain-error:0.089810\n",
      "[309]\ttrain-error:0.089911\n",
      "[310]\ttrain-error:0.089844\n",
      "[311]\ttrain-error:0.089759\n",
      "[312]\ttrain-error:0.089776\n",
      "[313]\ttrain-error:0.089759\n",
      "[314]\ttrain-error:0.089793\n",
      "[315]\ttrain-error:0.089675\n",
      "[316]\ttrain-error:0.089608\n",
      "[317]\ttrain-error:0.089759\n",
      "[318]\ttrain-error:0.089793\n",
      "[319]\ttrain-error:0.089692\n",
      "[320]\ttrain-error:0.089456\n",
      "[321]\ttrain-error:0.089406\n",
      "[322]\ttrain-error:0.089338\n",
      "[323]\ttrain-error:0.089389\n",
      "[324]\ttrain-error:0.089322\n",
      "[325]\ttrain-error:0.089288\n",
      "[326]\ttrain-error:0.089355\n",
      "[327]\ttrain-error:0.089423\n",
      "[328]\ttrain-error:0.089288\n",
      "[329]\ttrain-error:0.089423\n",
      "[330]\ttrain-error:0.089389\n",
      "[331]\ttrain-error:0.089372\n",
      "[332]\ttrain-error:0.089305\n",
      "[333]\ttrain-error:0.089355\n",
      "[334]\ttrain-error:0.089338\n",
      "[335]\ttrain-error:0.089322\n",
      "[336]\ttrain-error:0.089153\n",
      "[337]\ttrain-error:0.089086\n",
      "[338]\ttrain-error:0.089187\n",
      "[339]\ttrain-error:0.089187\n",
      "[340]\ttrain-error:0.088968\n",
      "[341]\ttrain-error:0.089136\n",
      "[342]\ttrain-error:0.089136\n",
      "[343]\ttrain-error:0.089086\n",
      "[344]\ttrain-error:0.089086\n",
      "[345]\ttrain-error:0.088850\n",
      "[346]\ttrain-error:0.088968\n",
      "[347]\ttrain-error:0.088985\n",
      "[348]\ttrain-error:0.088833\n",
      "[349]\ttrain-error:0.088783\n",
      "[350]\ttrain-error:0.088698\n",
      "[351]\ttrain-error:0.088682\n",
      "[352]\ttrain-error:0.088614\n",
      "[353]\ttrain-error:0.088631\n",
      "[354]\ttrain-error:0.088648\n",
      "[355]\ttrain-error:0.088597\n",
      "[356]\ttrain-error:0.088665\n",
      "[357]\ttrain-error:0.088530\n",
      "[358]\ttrain-error:0.088429\n",
      "[359]\ttrain-error:0.088463\n",
      "[360]\ttrain-error:0.088496\n",
      "[361]\ttrain-error:0.088378\n",
      "[362]\ttrain-error:0.088277\n",
      "[363]\ttrain-error:0.088328\n",
      "[364]\ttrain-error:0.088227\n",
      "[365]\ttrain-error:0.088378\n",
      "[366]\ttrain-error:0.088378\n",
      "[367]\ttrain-error:0.088227\n",
      "[368]\ttrain-error:0.088227\n",
      "[369]\ttrain-error:0.088058\n",
      "[370]\ttrain-error:0.088227\n",
      "[371]\ttrain-error:0.088210\n",
      "[372]\ttrain-error:0.088143\n",
      "[373]\ttrain-error:0.088277\n",
      "[374]\ttrain-error:0.088261\n",
      "[375]\ttrain-error:0.088058\n",
      "[376]\ttrain-error:0.087856\n",
      "[377]\ttrain-error:0.087806\n",
      "[378]\ttrain-error:0.087755\n",
      "[379]\ttrain-error:0.087840\n",
      "[380]\ttrain-error:0.087772\n",
      "[381]\ttrain-error:0.087907\n",
      "[382]\ttrain-error:0.087873\n",
      "[383]\ttrain-error:0.088042\n",
      "[384]\ttrain-error:0.087890\n",
      "[385]\ttrain-error:0.087840\n",
      "[386]\ttrain-error:0.087840\n",
      "[387]\ttrain-error:0.087789\n",
      "[388]\ttrain-error:0.087621\n",
      "[389]\ttrain-error:0.087570\n",
      "[390]\ttrain-error:0.087553\n",
      "[391]\ttrain-error:0.087486\n",
      "[392]\ttrain-error:0.087654\n",
      "[393]\ttrain-error:0.087435\n",
      "[394]\ttrain-error:0.087570\n",
      "[395]\ttrain-error:0.087486\n",
      "[396]\ttrain-error:0.087385\n",
      "[397]\ttrain-error:0.087267\n",
      "[398]\ttrain-error:0.087267\n",
      "[399]\ttrain-error:0.087317\n",
      "[400]\ttrain-error:0.087317\n",
      "[401]\ttrain-error:0.087301\n",
      "[402]\ttrain-error:0.087250\n",
      "[403]\ttrain-error:0.087233\n",
      "[404]\ttrain-error:0.087216\n",
      "[405]\ttrain-error:0.087233\n",
      "[406]\ttrain-error:0.087065\n",
      "[407]\ttrain-error:0.087082\n",
      "[408]\ttrain-error:0.087132\n",
      "[409]\ttrain-error:0.086964\n",
      "[410]\ttrain-error:0.086998\n",
      "[411]\ttrain-error:0.086880\n",
      "[412]\ttrain-error:0.086981\n",
      "[413]\ttrain-error:0.086896\n",
      "[414]\ttrain-error:0.087082\n",
      "[415]\ttrain-error:0.086812\n",
      "[416]\ttrain-error:0.086846\n",
      "[417]\ttrain-error:0.086863\n",
      "[418]\ttrain-error:0.086880\n",
      "[419]\ttrain-error:0.086779\n",
      "[420]\ttrain-error:0.086829\n",
      "[421]\ttrain-error:0.086812\n",
      "[422]\ttrain-error:0.086762\n",
      "[423]\ttrain-error:0.086795\n",
      "[424]\ttrain-error:0.086644\n",
      "[425]\ttrain-error:0.086711\n",
      "[426]\ttrain-error:0.086661\n",
      "[427]\ttrain-error:0.086577\n",
      "[428]\ttrain-error:0.086509\n",
      "[429]\ttrain-error:0.086526\n",
      "[430]\ttrain-error:0.086492\n",
      "[431]\ttrain-error:0.086526\n",
      "[432]\ttrain-error:0.086425\n",
      "[433]\ttrain-error:0.086442\n",
      "[434]\ttrain-error:0.086391\n",
      "[435]\ttrain-error:0.086459\n",
      "[436]\ttrain-error:0.086358\n",
      "[437]\ttrain-error:0.086442\n",
      "[438]\ttrain-error:0.086475\n",
      "[439]\ttrain-error:0.086307\n",
      "[440]\ttrain-error:0.086290\n",
      "[441]\ttrain-error:0.086442\n",
      "[442]\ttrain-error:0.086475\n",
      "[443]\ttrain-error:0.086526\n",
      "[444]\ttrain-error:0.086475\n",
      "[445]\ttrain-error:0.086341\n",
      "[446]\ttrain-error:0.086324\n",
      "[447]\ttrain-error:0.086358\n",
      "[448]\ttrain-error:0.086442\n",
      "[449]\ttrain-error:0.086475\n",
      "[450]\ttrain-error:0.086290\n",
      "[451]\ttrain-error:0.086273\n",
      "[452]\ttrain-error:0.086240\n",
      "[453]\ttrain-error:0.086038\n",
      "[454]\ttrain-error:0.086156\n",
      "[455]\ttrain-error:0.085869\n",
      "[456]\ttrain-error:0.085903\n",
      "[457]\ttrain-error:0.086004\n",
      "[458]\ttrain-error:0.085903\n",
      "[459]\ttrain-error:0.085869\n",
      "[460]\ttrain-error:0.085852\n",
      "[461]\ttrain-error:0.085701\n",
      "[462]\ttrain-error:0.085802\n",
      "[463]\ttrain-error:0.085667\n",
      "[464]\ttrain-error:0.085633\n",
      "[465]\ttrain-error:0.085718\n",
      "[466]\ttrain-error:0.085448\n",
      "[467]\ttrain-error:0.085600\n",
      "[468]\ttrain-error:0.085398\n",
      "[469]\ttrain-error:0.085297\n",
      "[470]\ttrain-error:0.085280\n",
      "[471]\ttrain-error:0.085431\n",
      "[472]\ttrain-error:0.085347\n",
      "[473]\ttrain-error:0.085600\n",
      "[474]\ttrain-error:0.085431\n",
      "[475]\ttrain-error:0.085280\n",
      "[476]\ttrain-error:0.085179\n",
      "[477]\ttrain-error:0.085179\n",
      "[478]\ttrain-error:0.085179\n",
      "[479]\ttrain-error:0.085111\n",
      "[480]\ttrain-error:0.085212\n",
      "[481]\ttrain-error:0.085179\n",
      "[482]\ttrain-error:0.085061\n",
      "[483]\ttrain-error:0.085111\n",
      "[484]\ttrain-error:0.085078\n",
      "[485]\ttrain-error:0.085078\n",
      "[486]\ttrain-error:0.085162\n",
      "[487]\ttrain-error:0.085027\n",
      "[488]\ttrain-error:0.085061\n",
      "[489]\ttrain-error:0.085010\n",
      "[490]\ttrain-error:0.084892\n",
      "[491]\ttrain-error:0.084892\n",
      "[492]\ttrain-error:0.084859\n",
      "[493]\ttrain-error:0.084859\n",
      "[494]\ttrain-error:0.084808\n",
      "[495]\ttrain-error:0.084690\n",
      "[496]\ttrain-error:0.084657\n",
      "[497]\ttrain-error:0.084623\n",
      "[498]\ttrain-error:0.084707\n",
      "[499]\ttrain-error:0.084522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Product_Info_1  Product_Info_2  Product_Info_3  Product_Info_4  \\\n",
      "0                   1               0              10        0.076923   \n",
      "1                   1               1              26        0.076923   \n",
      "2                   1               2              26        0.076923   \n",
      "3                   1               3              10        0.487179   \n",
      "4                   1               4              26        0.230769   \n",
      "5                   1               4              26        0.230769   \n",
      "6                   1               5              10        0.166194   \n",
      "7                   1               4              26        0.076923   \n",
      "8                   1               0              26        0.230769   \n",
      "9                   1               2              21        0.076923   \n",
      "10                  1               0              26        0.128205   \n",
      "11                  1               3              26        0.230769   \n",
      "12                  1               6              26        0.102564   \n",
      "13                  2               7              26        0.487179   \n",
      "14                  1               3              26        0.487179   \n",
      "15                  1               8              26        0.000000   \n",
      "16                  2               3              26        0.487179   \n",
      "17                  1               0              26        0.384615   \n",
      "18                  1               0              26        0.076923   \n",
      "19                  1               3              26        0.487179   \n",
      "20                  1               4              26        0.435897   \n",
      "21                  1               1              26        1.000000   \n",
      "22                  1               3              26        0.230769   \n",
      "23                  1               6              26        0.179487   \n",
      "24                  1               7              26        0.487179   \n",
      "25                  1               9              26        0.230769   \n",
      "26                  1               1              26        1.000000   \n",
      "27                  1               0              26        0.230769   \n",
      "28                  1               3              26        0.487179   \n",
      "29                  1               0              26        1.000000   \n",
      "...               ...             ...             ...             ...   \n",
      "59351               1               8              26        0.000000   \n",
      "59352               1               4              10        0.230769   \n",
      "59353               1               3              26        0.589744   \n",
      "59354               1               4              26        0.487179   \n",
      "59355               1               0              26        0.230769   \n",
      "59356               1               0              10        0.076923   \n",
      "59357               1               0              26        1.000000   \n",
      "59358               1               3              26        0.282051   \n",
      "59359               1               0              26        0.230769   \n",
      "59360               1               3              26        1.000000   \n",
      "59361               1               1              26        0.230769   \n",
      "59362               1               3              26        0.230769   \n",
      "59363               1               4               4        0.076923   \n",
      "59364               1               4              26        0.076923   \n",
      "59365               1               7              29        0.076923   \n",
      "59366               1               7              26        0.282051   \n",
      "59367               1               2              26        0.179487   \n",
      "59368               1               3              26        0.230769   \n",
      "59369               1               7              26        0.179487   \n",
      "59370               1               0              26        0.230769   \n",
      "59371               1               0              26        0.487179   \n",
      "59372               1               0              26        0.487179   \n",
      "59373               2               3              29        0.487179   \n",
      "59374               1               3              26        0.307692   \n",
      "59375               1              13              26        0.076923   \n",
      "59376               1               7              10        0.230769   \n",
      "59377               1               0              26        0.230769   \n",
      "59378               1               2              26        0.076923   \n",
      "59379               1               4              10        0.230769   \n",
      "59380               1               5              26        0.076923   \n",
      "\n",
      "       Product_Info_5  Product_Info_6  Product_Info_7   Ins_Age        Ht  \\\n",
      "0                   2               1               1  0.641791  0.581818   \n",
      "1                   2               3               1  0.059701  0.600000   \n",
      "2                   2               3               1  0.029851  0.745455   \n",
      "3                   2               3               1  0.164179  0.672727   \n",
      "4                   2               3               1  0.417910  0.654545   \n",
      "5                   3               1               1  0.507463  0.836364   \n",
      "6                   2               3               1  0.373134  0.581818   \n",
      "7                   2               3               1  0.611940  0.781818   \n",
      "8                   2               3               1  0.522388  0.618182   \n",
      "9                   2               3               1  0.552239  0.600000   \n",
      "10                  2               3               1  0.537313  0.690909   \n",
      "11                  2               3               1  0.298507  0.690909   \n",
      "12                  2               3               1  0.567164  0.618182   \n",
      "13                  2               3               1  0.223881  0.781818   \n",
      "14                  2               3               1  0.328358  0.636364   \n",
      "15                  2               3               1  0.626866  0.672727   \n",
      "16                  2               3               1  0.208955  0.745455   \n",
      "17                  2               3               1  0.268657  0.636364   \n",
      "18                  2               3               1  0.388060  0.781818   \n",
      "19                  2               3               1  0.223881  0.600000   \n",
      "20                  2               3               1  0.388060  0.745455   \n",
      "21                  2               1               1  0.537313  0.709091   \n",
      "22                  2               3               1  0.179104  0.800000   \n",
      "23                  2               3               1  0.164179  0.745455   \n",
      "24                  2               1               1  0.164179  0.818182   \n",
      "25                  2               3               1  0.268657  0.781818   \n",
      "26                  2               3               1  0.507463  0.654545   \n",
      "27                  2               3               1  0.134328  0.763636   \n",
      "28                  2               3               1  0.492537  0.618182   \n",
      "29                  2               3               1  0.582090  0.654545   \n",
      "...               ...             ...             ...       ...       ...   \n",
      "59351               2               3               1  0.134328  0.781818   \n",
      "59352               2               3               1  0.358209  0.618182   \n",
      "59353               2               1               1  0.179104  0.781818   \n",
      "59354               2               1               1  0.402985  0.763636   \n",
      "59355               2               3               1  0.223881  0.745455   \n",
      "59356               2               3               1  0.522388  0.600000   \n",
      "59357               2               1               3  0.582090  0.781818   \n",
      "59358               2               3               1  0.238806  0.727273   \n",
      "59359               2               3               1  0.447761  0.781818   \n",
      "59360               2               3               1  0.194030  0.654545   \n",
      "59361               2               1               1  0.268657  0.727273   \n",
      "59362               2               3               1  0.253731  0.781818   \n",
      "59363               2               3               1  0.746269  0.563636   \n",
      "59364               2               3               1  0.552239  0.727273   \n",
      "59365               2               3               1  0.641791  0.709091   \n",
      "59366               2               3               1  0.582090  0.781818   \n",
      "59367               2               3               3  0.373134  0.600000   \n",
      "59368               2               1               1  0.417910  0.727273   \n",
      "59369               2               3               1  0.611940  0.745455   \n",
      "59370               2               3               1  0.238806  0.763636   \n",
      "59371               2               1               1  0.537313  0.709091   \n",
      "59372               2               3               1  0.477612  0.763636   \n",
      "59373               2               3               1  0.208955  0.800000   \n",
      "59374               2               3               1  0.164179  0.690909   \n",
      "59375               2               3               1  0.477612  0.654545   \n",
      "59376               2               3               1  0.074627  0.709091   \n",
      "59377               2               3               1  0.432836  0.800000   \n",
      "59378               2               3               1  0.104478  0.745455   \n",
      "59379               2               3               1  0.507463  0.690909   \n",
      "59380               2               3               1  0.447761  0.781818   \n",
      "\n",
      "             Wt         ...          Medical_Keyword_39  Medical_Keyword_40  \\\n",
      "0      0.148536         ...                           0                   0   \n",
      "1      0.131799         ...                           0                   0   \n",
      "2      0.288703         ...                           0                   0   \n",
      "3      0.205021         ...                           0                   0   \n",
      "4      0.234310         ...                           0                   0   \n",
      "5      0.299163         ...                           0                   0   \n",
      "6      0.173640         ...                           0                   0   \n",
      "7      0.403766         ...                           0                   0   \n",
      "8      0.184100         ...                           0                   0   \n",
      "9      0.284519         ...                           0                   0   \n",
      "10     0.309623         ...                           0                   0   \n",
      "11     0.271967         ...                           0                   0   \n",
      "12     0.163180         ...                           0                   0   \n",
      "13     0.361925         ...                           0                   0   \n",
      "14     0.142259         ...                           0                   0   \n",
      "15     0.330544         ...                           0                   0   \n",
      "16     0.246862         ...                           0                   1   \n",
      "17     0.228033         ...                           0                   0   \n",
      "18     0.309623         ...                           0                   0   \n",
      "19     0.138075         ...                           0                   0   \n",
      "20     0.246862         ...                           0                   0   \n",
      "21     0.370293         ...                           0                   0   \n",
      "22     0.539749         ...                           0                   0   \n",
      "23     0.288703         ...                           0                   0   \n",
      "24     0.435146         ...                           0                   0   \n",
      "25     0.368201         ...                           0                   0   \n",
      "26     0.299163         ...                           0                   0   \n",
      "27     0.215481         ...                           0                   0   \n",
      "28     0.276151         ...                           0                   0   \n",
      "29     0.278243         ...                           0                   0   \n",
      "...         ...         ...                         ...                 ...   \n",
      "59351  0.351464         ...                           0                   0   \n",
      "59352  0.246862         ...                           0                   0   \n",
      "59353  0.382845         ...                           0                   0   \n",
      "59354  0.341004         ...                           0                   0   \n",
      "59355  0.361925         ...                           0                   0   \n",
      "59356  0.299163         ...                           0                   0   \n",
      "59357  0.351464         ...                           0                   0   \n",
      "59358  0.372385         ...                           0                   0   \n",
      "59359  0.424686         ...                           0                   0   \n",
      "59360  0.146444         ...                           0                   0   \n",
      "59361  0.267782         ...                           0                   0   \n",
      "59362  0.351464         ...                           0                   0   \n",
      "59363  0.205021         ...                           0                   0   \n",
      "59364  0.177824         ...                           0                   0   \n",
      "59365  0.284519         ...                           0                   0   \n",
      "59366  0.320084         ...                           0                   0   \n",
      "59367  0.320084         ...                           0                   0   \n",
      "59368  0.299163         ...                           0                   0   \n",
      "59369  0.451883         ...                           0                   0   \n",
      "59370  0.330544         ...                           0                   0   \n",
      "59371  0.343096         ...                           0                   0   \n",
      "59372  0.305439         ...                           0                   0   \n",
      "59373  0.257322         ...                           0                   0   \n",
      "59374  0.288703         ...                           0                   0   \n",
      "59375  0.271967         ...                           0                   0   \n",
      "59376  0.320084         ...                           0                   0   \n",
      "59377  0.403766         ...                           0                   0   \n",
      "59378  0.246862         ...                           0                   0   \n",
      "59379  0.276151         ...                           0                   1   \n",
      "59380  0.382845         ...                           0                   0   \n",
      "\n",
      "       Medical_Keyword_41  Medical_Keyword_42  Medical_Keyword_43  \\\n",
      "0                       0                   0                   0   \n",
      "1                       0                   0                   0   \n",
      "2                       0                   0                   0   \n",
      "3                       0                   0                   0   \n",
      "4                       0                   0                   0   \n",
      "5                       0                   0                   0   \n",
      "6                       0                   0                   0   \n",
      "7                       0                   0                   0   \n",
      "8                       0                   0                   0   \n",
      "9                       0                   0                   0   \n",
      "10                      0                   0                   0   \n",
      "11                      0                   0                   0   \n",
      "12                      0                   0                   0   \n",
      "13                      0                   0                   0   \n",
      "14                      0                   0                   0   \n",
      "15                      0                   0                   0   \n",
      "16                      0                   0                   0   \n",
      "17                      0                   0                   0   \n",
      "18                      0                   0                   0   \n",
      "19                      0                   0                   0   \n",
      "20                      0                   0                   0   \n",
      "21                      0                   0                   0   \n",
      "22                      0                   0                   0   \n",
      "23                      0                   0                   0   \n",
      "24                      0                   0                   0   \n",
      "25                      0                   0                   0   \n",
      "26                      0                   0                   0   \n",
      "27                      0                   0                   0   \n",
      "28                      0                   0                   0   \n",
      "29                      0                   0                   0   \n",
      "...                   ...                 ...                 ...   \n",
      "59351                   0                   0                   0   \n",
      "59352                   0                   0                   0   \n",
      "59353                   0                   0                   0   \n",
      "59354                   0                   0                   0   \n",
      "59355                   0                   1                   0   \n",
      "59356                   0                   0                   0   \n",
      "59357                   0                   0                   0   \n",
      "59358                   0                   0                   0   \n",
      "59359                   0                   0                   0   \n",
      "59360                   0                   0                   0   \n",
      "59361                   0                   0                   0   \n",
      "59362                   0                   0                   0   \n",
      "59363                   0                   0                   0   \n",
      "59364                   0                   0                   0   \n",
      "59365                   0                   0                   0   \n",
      "59366                   0                   0                   0   \n",
      "59367                   0                   0                   0   \n",
      "59368                   0                   0                   0   \n",
      "59369                   0                   0                   0   \n",
      "59370                   0                   0                   0   \n",
      "59371                   0                   0                   0   \n",
      "59372                   0                   1                   0   \n",
      "59373                   0                   0                   0   \n",
      "59374                   0                   0                   0   \n",
      "59375                   0                   0                   0   \n",
      "59376                   0                   0                   0   \n",
      "59377                   0                   0                   0   \n",
      "59378                   0                   0                   0   \n",
      "59379                   0                   0                   0   \n",
      "59380                   0                   0                   0   \n",
      "\n",
      "       Medical_Keyword_44  Medical_Keyword_45  Medical_Keyword_46  \\\n",
      "0                       0                   0                   0   \n",
      "1                       0                   0                   0   \n",
      "2                       0                   0                   0   \n",
      "3                       0                   0                   0   \n",
      "4                       0                   0                   0   \n",
      "5                       0                   0                   0   \n",
      "6                       0                   0                   0   \n",
      "7                       0                   0                   0   \n",
      "8                       0                   0                   0   \n",
      "9                       0                   0                   0   \n",
      "10                      1                   0                   0   \n",
      "11                      0                   0                   0   \n",
      "12                      0                   0                   0   \n",
      "13                      0                   0                   0   \n",
      "14                      0                   0                   0   \n",
      "15                      0                   0                   0   \n",
      "16                      0                   0                   0   \n",
      "17                      0                   0                   0   \n",
      "18                      0                   0                   0   \n",
      "19                      0                   0                   0   \n",
      "20                      0                   0                   0   \n",
      "21                      0                   0                   0   \n",
      "22                      0                   0                   0   \n",
      "23                      0                   0                   0   \n",
      "24                      0                   0                   0   \n",
      "25                      0                   0                   0   \n",
      "26                      0                   0                   0   \n",
      "27                      0                   0                   0   \n",
      "28                      0                   0                   0   \n",
      "29                      0                   0                   0   \n",
      "...                   ...                 ...                 ...   \n",
      "59351                   0                   0                   0   \n",
      "59352                   0                   0                   0   \n",
      "59353                   0                   0                   0   \n",
      "59354                   0                   0                   0   \n",
      "59355                   0                   0                   0   \n",
      "59356                   0                   0                   0   \n",
      "59357                   0                   0                   0   \n",
      "59358                   0                   0                   0   \n",
      "59359                   1                   0                   0   \n",
      "59360                   0                   0                   0   \n",
      "59361                   0                   0                   0   \n",
      "59362                   0                   0                   0   \n",
      "59363                   0                   0                   0   \n",
      "59364                   0                   0                   0   \n",
      "59365                   0                   0                   0   \n",
      "59366                   0                   0                   0   \n",
      "59367                   0                   0                   0   \n",
      "59368                   0                   0                   0   \n",
      "59369                   0                   0                   0   \n",
      "59370                   0                   0                   0   \n",
      "59371                   0                   0                   0   \n",
      "59372                   0                   0                   0   \n",
      "59373                   0                   0                   0   \n",
      "59374                   0                   0                   0   \n",
      "59375                   0                   0                   0   \n",
      "59376                   0                   0                   0   \n",
      "59377                   0                   0                   0   \n",
      "59378                   0                   0                   0   \n",
      "59379                   0                   0                   0   \n",
      "59380                   0                   0                   0   \n",
      "\n",
      "       Medical_Keyword_47  Medical_Keyword_48  \n",
      "0                       0                   0  \n",
      "1                       0                   0  \n",
      "2                       0                   0  \n",
      "3                       0                   0  \n",
      "4                       0                   0  \n",
      "5                       0                   0  \n",
      "6                       0                   0  \n",
      "7                       0                   0  \n",
      "8                       0                   0  \n",
      "9                       0                   0  \n",
      "10                      1                   1  \n",
      "11                      0                   0  \n",
      "12                      0                   0  \n",
      "13                      0                   0  \n",
      "14                      0                   0  \n",
      "15                      0                   0  \n",
      "16                      0                   0  \n",
      "17                      0                   0  \n",
      "18                      0                   0  \n",
      "19                      0                   0  \n",
      "20                      0                   0  \n",
      "21                      0                   0  \n",
      "22                      0                   0  \n",
      "23                      0                   0  \n",
      "24                      0                   0  \n",
      "25                      0                   0  \n",
      "26                      0                   0  \n",
      "27                      0                   0  \n",
      "28                      0                   0  \n",
      "29                      0                   0  \n",
      "...                   ...                 ...  \n",
      "59351                   0                   0  \n",
      "59352                   0                   0  \n",
      "59353                   0                   0  \n",
      "59354                   0                   0  \n",
      "59355                   0                   0  \n",
      "59356                   0                   0  \n",
      "59357                   0                   0  \n",
      "59358                   0                   0  \n",
      "59359                   0                   0  \n",
      "59360                   0                   0  \n",
      "59361                   0                   0  \n",
      "59362                   0                   0  \n",
      "59363                   0                   0  \n",
      "59364                   0                   0  \n",
      "59365                   0                   0  \n",
      "59366                   0                   0  \n",
      "59367                   0                   0  \n",
      "59368                   0                   0  \n",
      "59369                   0                   1  \n",
      "59370                   0                   0  \n",
      "59371                   0                   0  \n",
      "59372                   0                   1  \n",
      "59373                   0                   0  \n",
      "59374                   0                   0  \n",
      "59375                   0                   0  \n",
      "59376                   0                   0  \n",
      "59377                   0                   0  \n",
      "59378                   0                   0  \n",
      "59379                   0                   0  \n",
      "59380                   0                   0  \n",
      "\n",
      "[59381 rows x 126 columns]\n",
      "[('colsample_bytree', 0.3), ('silent', 1), ('min_child_weight', 50), ('subsample', 0.5), ('eta', 0.1), ('objective', 'binary:logistic'), ('max_depth', 9)]\n",
      "Training the model"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.110237\n",
      "[1]\ttrain-error:0.109463\n",
      "[2]\ttrain-error:0.108503\n",
      "[3]\ttrain-error:0.110136\n",
      "[4]\ttrain-error:0.109378\n",
      "[5]\ttrain-error:0.110204\n",
      "[6]\ttrain-error:0.110220\n",
      "[7]\ttrain-error:0.110237\n",
      "[8]\ttrain-error:0.110254\n",
      "[9]\ttrain-error:0.110288\n",
      "[10]\ttrain-error:0.110204\n",
      "[11]\ttrain-error:0.110187\n",
      "[12]\ttrain-error:0.110187\n",
      "[13]\ttrain-error:0.110204\n",
      "[14]\ttrain-error:0.110170\n",
      "[15]\ttrain-error:0.110103\n",
      "[16]\ttrain-error:0.110119\n",
      "[17]\ttrain-error:0.110103\n",
      "[18]\ttrain-error:0.110136\n",
      "[19]\ttrain-error:0.110119\n",
      "[20]\ttrain-error:0.110086\n",
      "[21]\ttrain-error:0.110103\n",
      "[22]\ttrain-error:0.110119\n",
      "[23]\ttrain-error:0.110103\n",
      "[24]\ttrain-error:0.110069\n",
      "[25]\ttrain-error:0.109985\n",
      "[26]\ttrain-error:0.110018\n",
      "[27]\ttrain-error:0.109951\n",
      "[28]\ttrain-error:0.110002\n",
      "[29]\ttrain-error:0.109884\n",
      "[30]\ttrain-error:0.109749\n",
      "[31]\ttrain-error:0.109496\n",
      "[32]\ttrain-error:0.109530\n",
      "[33]\ttrain-error:0.109479\n",
      "[34]\ttrain-error:0.109547\n",
      "[35]\ttrain-error:0.109412\n",
      "[36]\ttrain-error:0.109362\n",
      "[37]\ttrain-error:0.109328\n",
      "[38]\ttrain-error:0.109294\n",
      "[39]\ttrain-error:0.109042\n",
      "[40]\ttrain-error:0.108941\n",
      "[41]\ttrain-error:0.108604\n",
      "[42]\ttrain-error:0.108604\n",
      "[43]\ttrain-error:0.108486\n",
      "[44]\ttrain-error:0.108317\n",
      "[45]\ttrain-error:0.108233\n",
      "[46]\ttrain-error:0.108200\n",
      "[47]\ttrain-error:0.107728\n",
      "[48]\ttrain-error:0.107341\n",
      "[49]\ttrain-error:0.107155\n",
      "[50]\ttrain-error:0.107038\n",
      "[51]\ttrain-error:0.106785\n",
      "[52]\ttrain-error:0.106802\n",
      "[53]\ttrain-error:0.106802\n",
      "[54]\ttrain-error:0.106903\n",
      "[55]\ttrain-error:0.106785\n",
      "[56]\ttrain-error:0.106566\n",
      "[57]\ttrain-error:0.106465\n",
      "[58]\ttrain-error:0.106532\n",
      "[59]\ttrain-error:0.106415\n",
      "[60]\ttrain-error:0.106583\n",
      "[61]\ttrain-error:0.106263\n",
      "[62]\ttrain-error:0.106095\n",
      "[63]\ttrain-error:0.105791\n",
      "[64]\ttrain-error:0.105825\n",
      "[65]\ttrain-error:0.105606\n",
      "[66]\ttrain-error:0.105606\n",
      "[67]\ttrain-error:0.105606\n",
      "[68]\ttrain-error:0.105522\n",
      "[69]\ttrain-error:0.105421\n",
      "[70]\ttrain-error:0.105135\n",
      "[71]\ttrain-error:0.105118\n",
      "[72]\ttrain-error:0.105050\n",
      "[73]\ttrain-error:0.104832\n",
      "[74]\ttrain-error:0.104781\n",
      "[75]\ttrain-error:0.104613\n",
      "[76]\ttrain-error:0.104495\n",
      "[77]\ttrain-error:0.104444\n",
      "[78]\ttrain-error:0.104377\n",
      "[79]\ttrain-error:0.104276\n",
      "[80]\ttrain-error:0.104444\n",
      "[81]\ttrain-error:0.104192\n",
      "[82]\ttrain-error:0.104326\n",
      "[83]\ttrain-error:0.104326\n",
      "[84]\ttrain-error:0.104360\n",
      "[85]\ttrain-error:0.104192\n",
      "[86]\ttrain-error:0.104276\n",
      "[87]\ttrain-error:0.104360\n",
      "[88]\ttrain-error:0.104158\n",
      "[89]\ttrain-error:0.104023\n",
      "[90]\ttrain-error:0.104141\n",
      "[91]\ttrain-error:0.104107\n",
      "[92]\ttrain-error:0.103956\n",
      "[93]\ttrain-error:0.103905\n",
      "[94]\ttrain-error:0.103838\n",
      "[95]\ttrain-error:0.103989\n",
      "[96]\ttrain-error:0.103905\n",
      "[97]\ttrain-error:0.103821\n",
      "[98]\ttrain-error:0.103771\n",
      "[99]\ttrain-error:0.103737\n",
      "[100]\ttrain-error:0.103585\n",
      "[101]\ttrain-error:0.103501\n",
      "[102]\ttrain-error:0.103434\n",
      "[103]\ttrain-error:0.103501\n",
      "[104]\ttrain-error:0.103518\n",
      "[105]\ttrain-error:0.103535\n",
      "[106]\ttrain-error:0.103535\n",
      "[107]\ttrain-error:0.103417\n",
      "[108]\ttrain-error:0.103333\n",
      "[109]\ttrain-error:0.103451\n",
      "[110]\ttrain-error:0.103467\n",
      "[111]\ttrain-error:0.103249\n",
      "[112]\ttrain-error:0.103114\n",
      "[113]\ttrain-error:0.103232\n",
      "[114]\ttrain-error:0.103198\n",
      "[115]\ttrain-error:0.103147\n",
      "[116]\ttrain-error:0.103114\n",
      "[117]\ttrain-error:0.103147\n",
      "[118]\ttrain-error:0.103181\n",
      "[119]\ttrain-error:0.103215\n",
      "[120]\ttrain-error:0.103114\n",
      "[121]\ttrain-error:0.103114\n",
      "[122]\ttrain-error:0.103080\n",
      "[123]\ttrain-error:0.102996\n",
      "[124]\ttrain-error:0.102895\n",
      "[125]\ttrain-error:0.103013\n",
      "[126]\ttrain-error:0.102962\n",
      "[127]\ttrain-error:0.102895\n",
      "[128]\ttrain-error:0.102844\n",
      "[129]\ttrain-error:0.102710\n",
      "[130]\ttrain-error:0.102676\n",
      "[131]\ttrain-error:0.102406\n",
      "[132]\ttrain-error:0.102524\n",
      "[133]\ttrain-error:0.102406\n",
      "[134]\ttrain-error:0.102491\n",
      "[135]\ttrain-error:0.102625\n",
      "[136]\ttrain-error:0.102609\n",
      "[137]\ttrain-error:0.102440\n",
      "[138]\ttrain-error:0.102406\n",
      "[139]\ttrain-error:0.102541\n",
      "[140]\ttrain-error:0.102188\n",
      "[141]\ttrain-error:0.102171\n",
      "[142]\ttrain-error:0.102171\n",
      "[143]\ttrain-error:0.102204\n",
      "[144]\ttrain-error:0.102204\n",
      "[145]\ttrain-error:0.102103\n",
      "[146]\ttrain-error:0.101985\n",
      "[147]\ttrain-error:0.101868\n",
      "[148]\ttrain-error:0.101868\n",
      "[149]\ttrain-error:0.101767\n",
      "[150]\ttrain-error:0.101783\n",
      "[151]\ttrain-error:0.101952\n",
      "[152]\ttrain-error:0.101733\n",
      "[153]\ttrain-error:0.101851\n",
      "[154]\ttrain-error:0.101935\n",
      "[155]\ttrain-error:0.101750\n",
      "[156]\ttrain-error:0.101649\n",
      "[157]\ttrain-error:0.101666\n",
      "[158]\ttrain-error:0.101733\n",
      "[159]\ttrain-error:0.101548\n",
      "[160]\ttrain-error:0.101632\n",
      "[161]\ttrain-error:0.101564\n",
      "[162]\ttrain-error:0.101632\n",
      "[163]\ttrain-error:0.101379\n",
      "[164]\ttrain-error:0.101447\n",
      "[165]\ttrain-error:0.101447\n",
      "[166]\ttrain-error:0.101362\n",
      "[167]\ttrain-error:0.101127\n",
      "[168]\ttrain-error:0.101160\n",
      "[169]\ttrain-error:0.101127\n",
      "[170]\ttrain-error:0.101211\n",
      "[171]\ttrain-error:0.101143\n",
      "[172]\ttrain-error:0.100958\n",
      "[173]\ttrain-error:0.101076\n",
      "[174]\ttrain-error:0.100975\n",
      "[175]\ttrain-error:0.100857\n",
      "[176]\ttrain-error:0.100857\n",
      "[177]\ttrain-error:0.100739\n",
      "[178]\ttrain-error:0.100857\n",
      "[179]\ttrain-error:0.100689\n",
      "[180]\ttrain-error:0.100689\n",
      "[181]\ttrain-error:0.100655\n",
      "[182]\ttrain-error:0.100672\n",
      "[183]\ttrain-error:0.100554\n",
      "[184]\ttrain-error:0.100504\n",
      "[185]\ttrain-error:0.100487\n",
      "[186]\ttrain-error:0.100554\n",
      "[187]\ttrain-error:0.100470\n",
      "[188]\ttrain-error:0.100504\n",
      "[189]\ttrain-error:0.100520\n",
      "[190]\ttrain-error:0.100520\n",
      "[191]\ttrain-error:0.100453\n",
      "[192]\ttrain-error:0.100419\n",
      "[193]\ttrain-error:0.100487\n",
      "[194]\ttrain-error:0.100419\n",
      "[195]\ttrain-error:0.100402\n",
      "[196]\ttrain-error:0.100318\n",
      "[197]\ttrain-error:0.100318\n",
      "[198]\ttrain-error:0.100369\n",
      "[199]\ttrain-error:0.100335\n",
      "[200]\ttrain-error:0.100419\n",
      "[201]\ttrain-error:0.100268\n",
      "[202]\ttrain-error:0.100318\n",
      "[203]\ttrain-error:0.100386\n",
      "[204]\ttrain-error:0.100234\n",
      "[205]\ttrain-error:0.100318\n",
      "[206]\ttrain-error:0.100200\n",
      "[207]\ttrain-error:0.100318\n",
      "[208]\ttrain-error:0.100369\n",
      "[209]\ttrain-error:0.100184\n",
      "[210]\ttrain-error:0.100150\n",
      "[211]\ttrain-error:0.100200\n",
      "[212]\ttrain-error:0.100049\n",
      "[213]\ttrain-error:0.099864\n",
      "[214]\ttrain-error:0.099847\n",
      "[215]\ttrain-error:0.099729\n",
      "[216]\ttrain-error:0.099830\n",
      "[217]\ttrain-error:0.099729\n",
      "[218]\ttrain-error:0.099830\n",
      "[219]\ttrain-error:0.099746\n",
      "[220]\ttrain-error:0.099880\n",
      "[221]\ttrain-error:0.099813\n",
      "[222]\ttrain-error:0.099796\n",
      "[223]\ttrain-error:0.099779\n",
      "[224]\ttrain-error:0.099678\n",
      "[225]\ttrain-error:0.099796\n",
      "[226]\ttrain-error:0.099628\n",
      "[227]\ttrain-error:0.099577\n",
      "[228]\ttrain-error:0.099695\n",
      "[229]\ttrain-error:0.099510\n",
      "[230]\ttrain-error:0.099527\n",
      "[231]\ttrain-error:0.099560\n",
      "[232]\ttrain-error:0.099510\n",
      "[233]\ttrain-error:0.099712\n",
      "[234]\ttrain-error:0.099577\n",
      "[235]\ttrain-error:0.099560\n",
      "[236]\ttrain-error:0.099611\n",
      "[237]\ttrain-error:0.099544\n",
      "[238]\ttrain-error:0.099426\n",
      "[239]\ttrain-error:0.099426\n",
      "[240]\ttrain-error:0.099544\n",
      "[241]\ttrain-error:0.099493\n",
      "[242]\ttrain-error:0.099426\n",
      "[243]\ttrain-error:0.099139\n",
      "[244]\ttrain-error:0.099190\n",
      "[245]\ttrain-error:0.099291\n",
      "[246]\ttrain-error:0.099190\n",
      "[247]\ttrain-error:0.099173\n",
      "[248]\ttrain-error:0.099190\n",
      "[249]\ttrain-error:0.099274\n",
      "[250]\ttrain-error:0.099240\n",
      "[251]\ttrain-error:0.099190\n",
      "[252]\ttrain-error:0.099257\n",
      "[253]\ttrain-error:0.099156\n",
      "[254]\ttrain-error:0.098887\n",
      "[255]\ttrain-error:0.098853\n",
      "[256]\ttrain-error:0.098836\n",
      "[257]\ttrain-error:0.098954\n",
      "[258]\ttrain-error:0.098971\n",
      "[259]\ttrain-error:0.098819\n",
      "[260]\ttrain-error:0.098668\n",
      "[261]\ttrain-error:0.098466\n",
      "[262]\ttrain-error:0.098483\n",
      "[263]\ttrain-error:0.098752\n",
      "[264]\ttrain-error:0.098819\n",
      "[265]\ttrain-error:0.098685\n",
      "[266]\ttrain-error:0.098432\n",
      "[267]\ttrain-error:0.098449\n",
      "[268]\ttrain-error:0.098449\n",
      "[269]\ttrain-error:0.098516\n",
      "[270]\ttrain-error:0.098483\n",
      "[271]\ttrain-error:0.098365\n",
      "[272]\ttrain-error:0.098365\n",
      "[273]\ttrain-error:0.098247\n",
      "[274]\ttrain-error:0.098348\n",
      "[275]\ttrain-error:0.098297\n",
      "[276]\ttrain-error:0.098297\n",
      "[277]\ttrain-error:0.098297\n",
      "[278]\ttrain-error:0.098163\n",
      "[279]\ttrain-error:0.098297\n",
      "[280]\ttrain-error:0.098247\n",
      "[281]\ttrain-error:0.098247\n",
      "[282]\ttrain-error:0.098230\n",
      "[283]\ttrain-error:0.098045\n",
      "[284]\ttrain-error:0.097977\n",
      "[285]\ttrain-error:0.097876\n",
      "[286]\ttrain-error:0.097876\n",
      "[287]\ttrain-error:0.097759\n",
      "[288]\ttrain-error:0.097792\n",
      "[289]\ttrain-error:0.097775\n",
      "[290]\ttrain-error:0.097977\n",
      "[291]\ttrain-error:0.097775\n",
      "[292]\ttrain-error:0.097809\n",
      "[293]\ttrain-error:0.097691\n",
      "[294]\ttrain-error:0.097742\n",
      "[295]\ttrain-error:0.097590\n",
      "[296]\ttrain-error:0.097658\n",
      "[297]\ttrain-error:0.097523\n",
      "[298]\ttrain-error:0.097506\n",
      "[299]\ttrain-error:0.097540\n",
      "[300]\ttrain-error:0.097489\n",
      "[301]\ttrain-error:0.097371\n",
      "[302]\ttrain-error:0.097338\n",
      "[303]\ttrain-error:0.097354\n",
      "[304]\ttrain-error:0.097422\n",
      "[305]\ttrain-error:0.097455\n",
      "[306]\ttrain-error:0.097573\n",
      "[307]\ttrain-error:0.097506\n",
      "[308]\ttrain-error:0.097371\n",
      "[309]\ttrain-error:0.097354\n",
      "[310]\ttrain-error:0.097152\n",
      "[311]\ttrain-error:0.097321\n",
      "[312]\ttrain-error:0.097455\n",
      "[313]\ttrain-error:0.097354\n",
      "[314]\ttrain-error:0.097439\n",
      "[315]\ttrain-error:0.097405\n",
      "[316]\ttrain-error:0.097304\n",
      "[317]\ttrain-error:0.097253\n",
      "[318]\ttrain-error:0.097287\n",
      "[319]\ttrain-error:0.097152\n",
      "[320]\ttrain-error:0.097085\n",
      "[321]\ttrain-error:0.097051\n",
      "[322]\ttrain-error:0.097119\n",
      "[323]\ttrain-error:0.097119\n",
      "[324]\ttrain-error:0.096967\n",
      "[325]\ttrain-error:0.097018\n",
      "[326]\ttrain-error:0.097001\n",
      "[327]\ttrain-error:0.096832\n",
      "[328]\ttrain-error:0.096765\n",
      "[329]\ttrain-error:0.096731\n",
      "[330]\ttrain-error:0.096782\n",
      "[331]\ttrain-error:0.096799\n",
      "[332]\ttrain-error:0.096765\n",
      "[333]\ttrain-error:0.096731\n",
      "[334]\ttrain-error:0.096815\n",
      "[335]\ttrain-error:0.096664\n",
      "[336]\ttrain-error:0.096563\n",
      "[337]\ttrain-error:0.096815\n",
      "[338]\ttrain-error:0.096681\n",
      "[339]\ttrain-error:0.096546\n",
      "[340]\ttrain-error:0.096378\n",
      "[341]\ttrain-error:0.096394\n",
      "[342]\ttrain-error:0.096428\n",
      "[343]\ttrain-error:0.096226\n",
      "[344]\ttrain-error:0.096310\n",
      "[345]\ttrain-error:0.096091\n",
      "[346]\ttrain-error:0.096226\n",
      "[347]\ttrain-error:0.096074\n",
      "[348]\ttrain-error:0.096024\n",
      "[349]\ttrain-error:0.096125\n",
      "[350]\ttrain-error:0.096024\n",
      "[351]\ttrain-error:0.095906\n",
      "[352]\ttrain-error:0.095856\n",
      "[353]\ttrain-error:0.095771\n",
      "[354]\ttrain-error:0.095822\n",
      "[355]\ttrain-error:0.095738\n",
      "[356]\ttrain-error:0.095586\n",
      "[357]\ttrain-error:0.095687\n",
      "[358]\ttrain-error:0.095653\n",
      "[359]\ttrain-error:0.095569\n",
      "[360]\ttrain-error:0.095637\n",
      "[361]\ttrain-error:0.095620\n",
      "[362]\ttrain-error:0.095637\n",
      "[363]\ttrain-error:0.095637\n",
      "[364]\ttrain-error:0.095670\n",
      "[365]\ttrain-error:0.095536\n",
      "[366]\ttrain-error:0.095552\n",
      "[367]\ttrain-error:0.095502\n",
      "[368]\ttrain-error:0.095300\n",
      "[369]\ttrain-error:0.095401\n",
      "[370]\ttrain-error:0.095283\n",
      "[371]\ttrain-error:0.095350\n",
      "[372]\ttrain-error:0.095367\n",
      "[373]\ttrain-error:0.095485\n",
      "[374]\ttrain-error:0.095519\n",
      "[375]\ttrain-error:0.095485\n",
      "[376]\ttrain-error:0.095485\n",
      "[377]\ttrain-error:0.095384\n",
      "[378]\ttrain-error:0.095165\n",
      "[379]\ttrain-error:0.095266\n",
      "[380]\ttrain-error:0.095249\n",
      "[381]\ttrain-error:0.095334\n",
      "[382]\ttrain-error:0.095435\n",
      "[383]\ttrain-error:0.095283\n",
      "[384]\ttrain-error:0.095182\n",
      "[385]\ttrain-error:0.094980\n",
      "[386]\ttrain-error:0.095064\n",
      "[387]\ttrain-error:0.095030\n",
      "[388]\ttrain-error:0.094963\n",
      "[389]\ttrain-error:0.095047\n",
      "[390]\ttrain-error:0.094997\n",
      "[391]\ttrain-error:0.095115\n",
      "[392]\ttrain-error:0.094929\n",
      "[393]\ttrain-error:0.094795\n",
      "[394]\ttrain-error:0.094929\n",
      "[395]\ttrain-error:0.094761\n",
      "[396]\ttrain-error:0.094896\n",
      "[397]\ttrain-error:0.094778\n",
      "[398]\ttrain-error:0.094727\n",
      "[399]\ttrain-error:0.094761\n",
      "[400]\ttrain-error:0.094795\n",
      "[401]\ttrain-error:0.094828\n",
      "[402]\ttrain-error:0.094879\n",
      "[403]\ttrain-error:0.094795\n",
      "[404]\ttrain-error:0.094811\n",
      "[405]\ttrain-error:0.094593\n",
      "[406]\ttrain-error:0.094508\n",
      "[407]\ttrain-error:0.094441\n",
      "[408]\ttrain-error:0.094492\n",
      "[409]\ttrain-error:0.094441\n",
      "[410]\ttrain-error:0.094542\n",
      "[411]\ttrain-error:0.094542\n",
      "[412]\ttrain-error:0.094374\n",
      "[413]\ttrain-error:0.094390\n",
      "[414]\ttrain-error:0.094559\n",
      "[415]\ttrain-error:0.094340\n",
      "[416]\ttrain-error:0.094458\n",
      "[417]\ttrain-error:0.094390\n",
      "[418]\ttrain-error:0.094424\n",
      "[419]\ttrain-error:0.094289\n",
      "[420]\ttrain-error:0.094188\n",
      "[421]\ttrain-error:0.094205\n",
      "[422]\ttrain-error:0.094306\n",
      "[423]\ttrain-error:0.094323\n",
      "[424]\ttrain-error:0.094289\n",
      "[425]\ttrain-error:0.094273\n",
      "[426]\ttrain-error:0.094390\n",
      "[427]\ttrain-error:0.094205\n",
      "[428]\ttrain-error:0.094340\n",
      "[429]\ttrain-error:0.094340\n",
      "[430]\ttrain-error:0.094458\n",
      "[431]\ttrain-error:0.094306\n",
      "[432]\ttrain-error:0.094256\n",
      "[433]\ttrain-error:0.094188\n",
      "[434]\ttrain-error:0.094155\n",
      "[435]\ttrain-error:0.094104\n",
      "[436]\ttrain-error:0.093986\n",
      "[437]\ttrain-error:0.094172\n",
      "[438]\ttrain-error:0.094155\n",
      "[439]\ttrain-error:0.093936\n",
      "[440]\ttrain-error:0.093953\n",
      "[441]\ttrain-error:0.093902\n",
      "[442]\ttrain-error:0.093953\n",
      "[443]\ttrain-error:0.093953\n",
      "[444]\ttrain-error:0.094054\n",
      "[445]\ttrain-error:0.094003\n",
      "[446]\ttrain-error:0.093969\n",
      "[447]\ttrain-error:0.093919\n",
      "[448]\ttrain-error:0.093835\n",
      "[449]\ttrain-error:0.093818\n",
      "[450]\ttrain-error:0.093868\n",
      "[451]\ttrain-error:0.093616\n",
      "[452]\ttrain-error:0.093616\n",
      "[453]\ttrain-error:0.093599\n",
      "[454]\ttrain-error:0.093666\n",
      "[455]\ttrain-error:0.093616\n",
      "[456]\ttrain-error:0.093666\n",
      "[457]\ttrain-error:0.093565\n",
      "[458]\ttrain-error:0.093548\n",
      "[459]\ttrain-error:0.093616\n",
      "[460]\ttrain-error:0.093616\n",
      "[461]\ttrain-error:0.093649\n",
      "[462]\ttrain-error:0.093548\n",
      "[463]\ttrain-error:0.093599\n",
      "[464]\ttrain-error:0.093548\n",
      "[465]\ttrain-error:0.093447\n",
      "[466]\ttrain-error:0.093397\n",
      "[467]\ttrain-error:0.093346\n",
      "[468]\ttrain-error:0.093498\n",
      "[469]\ttrain-error:0.093414\n",
      "[470]\ttrain-error:0.093447\n",
      "[471]\ttrain-error:0.093397\n",
      "[472]\ttrain-error:0.093279\n",
      "[473]\ttrain-error:0.093397\n",
      "[474]\ttrain-error:0.093414\n",
      "[475]\ttrain-error:0.093279\n",
      "[476]\ttrain-error:0.093245\n",
      "[477]\ttrain-error:0.093161\n",
      "[478]\ttrain-error:0.093212\n",
      "[479]\ttrain-error:0.093077\n",
      "[480]\ttrain-error:0.092942\n",
      "[481]\ttrain-error:0.093178\n",
      "[482]\ttrain-error:0.093111\n",
      "[483]\ttrain-error:0.093060\n",
      "[484]\ttrain-error:0.093161\n",
      "[485]\ttrain-error:0.093127\n",
      "[486]\ttrain-error:0.093010\n",
      "[487]\ttrain-error:0.092925\n",
      "[488]\ttrain-error:0.092841\n",
      "[489]\ttrain-error:0.092824\n",
      "[490]\ttrain-error:0.092673\n",
      "[491]\ttrain-error:0.092572\n",
      "[492]\ttrain-error:0.092538\n",
      "[493]\ttrain-error:0.092504\n",
      "[494]\ttrain-error:0.092555\n",
      "[495]\ttrain-error:0.092504\n",
      "[496]\ttrain-error:0.092454\n",
      "[497]\ttrain-error:0.092386\n",
      "[498]\ttrain-error:0.092386\n",
      "[499]\ttrain-error:0.092386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.017059\n",
      "[1]\ttrain-error:0.017059\n",
      "[2]\ttrain-error:0.017059\n",
      "[3]\ttrain-error:0.017059\n",
      "[4]\ttrain-error:0.017059\n",
      "[5]\ttrain-error:0.017059\n",
      "[6]\ttrain-error:0.017059\n",
      "[7]\ttrain-error:0.017059\n",
      "[8]\ttrain-error:0.017059\n",
      "[9]\ttrain-error:0.017059\n",
      "[10]\ttrain-error:0.017059\n",
      "[11]\ttrain-error:0.017059\n",
      "[12]\ttrain-error:0.017059\n",
      "[13]\ttrain-error:0.017059\n",
      "[14]\ttrain-error:0.017059\n",
      "[15]\ttrain-error:0.017059\n",
      "[16]\ttrain-error:0.017059\n",
      "[17]\ttrain-error:0.017059\n",
      "[18]\ttrain-error:0.017059\n",
      "[19]\ttrain-error:0.017059\n",
      "[20]\ttrain-error:0.017059\n",
      "[21]\ttrain-error:0.017059\n",
      "[22]\ttrain-error:0.017059\n",
      "[23]\ttrain-error:0.017059\n",
      "[24]\ttrain-error:0.017059\n",
      "[25]\ttrain-error:0.017059\n",
      "[26]\ttrain-error:0.017059\n",
      "[27]\ttrain-error:0.017059\n",
      "[28]\ttrain-error:0.017059\n",
      "[29]\ttrain-error:0.017059\n",
      "[30]\ttrain-error:0.017059\n",
      "[31]\ttrain-error:0.017059\n",
      "[32]\ttrain-error:0.017059\n",
      "[33]\ttrain-error:0.017059\n",
      "[34]\ttrain-error:0.017059\n",
      "[35]\ttrain-error:0.017059\n",
      "[36]\ttrain-error:0.017059\n",
      "[37]\ttrain-error:0.017059\n",
      "[38]\ttrain-error:0.017059\n",
      "[39]\ttrain-error:0.017059\n",
      "[40]\ttrain-error:0.017059\n",
      "[41]\ttrain-error:0.017059\n",
      "[42]\ttrain-error:0.017059\n",
      "[43]\ttrain-error:0.017059\n",
      "[44]\ttrain-error:0.017059\n",
      "[45]\ttrain-error:0.017059\n",
      "[46]\ttrain-error:0.017059\n",
      "[47]\ttrain-error:0.017059\n",
      "[48]\ttrain-error:0.017026\n",
      "[49]\ttrain-error:0.017026\n",
      "[50]\ttrain-error:0.017009\n",
      "[51]\ttrain-error:0.016992\n",
      "[52]\ttrain-error:0.016992\n",
      "[53]\ttrain-error:0.016992\n",
      "[54]\ttrain-error:0.016992\n",
      "[55]\ttrain-error:0.016958\n",
      "[56]\ttrain-error:0.016958\n",
      "[57]\ttrain-error:0.016975\n",
      "[58]\ttrain-error:0.016941\n",
      "[59]\ttrain-error:0.016908\n",
      "[60]\ttrain-error:0.016908\n",
      "[61]\ttrain-error:0.016874\n",
      "[62]\ttrain-error:0.016824\n",
      "[63]\ttrain-error:0.016807\n",
      "[64]\ttrain-error:0.016790\n",
      "[65]\ttrain-error:0.016773\n",
      "[66]\ttrain-error:0.016790\n",
      "[67]\ttrain-error:0.016773\n",
      "[68]\ttrain-error:0.016756\n",
      "[69]\ttrain-error:0.016773\n",
      "[70]\ttrain-error:0.016756\n",
      "[71]\ttrain-error:0.016723\n",
      "[72]\ttrain-error:0.016655\n",
      "[73]\ttrain-error:0.016723\n",
      "[74]\ttrain-error:0.016689\n",
      "[75]\ttrain-error:0.016537\n",
      "[76]\ttrain-error:0.016504\n",
      "[77]\ttrain-error:0.016487\n",
      "[78]\ttrain-error:0.016470\n",
      "[79]\ttrain-error:0.016453\n",
      "[80]\ttrain-error:0.016419\n",
      "[81]\ttrain-error:0.016419\n",
      "[82]\ttrain-error:0.016352\n",
      "[83]\ttrain-error:0.016335\n",
      "[84]\ttrain-error:0.016318\n",
      "[85]\ttrain-error:0.016200\n",
      "[86]\ttrain-error:0.016133\n",
      "[87]\ttrain-error:0.016217\n",
      "[88]\ttrain-error:0.016099\n",
      "[89]\ttrain-error:0.016133\n",
      "[90]\ttrain-error:0.016049\n",
      "[91]\ttrain-error:0.016116\n",
      "[92]\ttrain-error:0.016066\n",
      "[93]\ttrain-error:0.016066\n",
      "[94]\ttrain-error:0.016015\n",
      "[95]\ttrain-error:0.015948\n",
      "[96]\ttrain-error:0.015965\n",
      "[97]\ttrain-error:0.015914\n",
      "[98]\ttrain-error:0.015864\n",
      "[99]\ttrain-error:0.015881\n",
      "[100]\ttrain-error:0.015729\n",
      "[101]\ttrain-error:0.015779\n",
      "[102]\ttrain-error:0.015746\n",
      "[103]\ttrain-error:0.015678\n",
      "[104]\ttrain-error:0.015695\n",
      "[105]\ttrain-error:0.015662\n",
      "[106]\ttrain-error:0.015577\n",
      "[107]\ttrain-error:0.015577\n",
      "[108]\ttrain-error:0.015577\n",
      "[109]\ttrain-error:0.015594\n",
      "[110]\ttrain-error:0.015611\n",
      "[111]\ttrain-error:0.015577\n",
      "[112]\ttrain-error:0.015527\n",
      "[113]\ttrain-error:0.015493\n",
      "[114]\ttrain-error:0.015561\n",
      "[115]\ttrain-error:0.015510\n",
      "[116]\ttrain-error:0.015375\n",
      "[117]\ttrain-error:0.015426\n",
      "[118]\ttrain-error:0.015342\n",
      "[119]\ttrain-error:0.015241\n",
      "[120]\ttrain-error:0.015207\n",
      "[121]\ttrain-error:0.015257\n",
      "[122]\ttrain-error:0.015257\n",
      "[123]\ttrain-error:0.015224\n",
      "[124]\ttrain-error:0.015241\n",
      "[125]\ttrain-error:0.015257\n",
      "[126]\ttrain-error:0.015173\n",
      "[127]\ttrain-error:0.015173\n",
      "[128]\ttrain-error:0.015140\n",
      "[129]\ttrain-error:0.015022\n",
      "[130]\ttrain-error:0.015022\n",
      "[131]\ttrain-error:0.014971\n",
      "[132]\ttrain-error:0.014988\n",
      "[133]\ttrain-error:0.015005\n",
      "[134]\ttrain-error:0.014971\n",
      "[135]\ttrain-error:0.014988\n",
      "[136]\ttrain-error:0.015005\n",
      "[137]\ttrain-error:0.015038\n",
      "[138]\ttrain-error:0.014988\n",
      "[139]\ttrain-error:0.014971\n",
      "[140]\ttrain-error:0.015038\n",
      "[141]\ttrain-error:0.014937\n",
      "[142]\ttrain-error:0.014988\n",
      "[143]\ttrain-error:0.015005\n",
      "[144]\ttrain-error:0.014954\n",
      "[145]\ttrain-error:0.014988\n",
      "[146]\ttrain-error:0.015022\n",
      "[147]\ttrain-error:0.014988\n",
      "[148]\ttrain-error:0.015005\n",
      "[149]\ttrain-error:0.014954\n",
      "[150]\ttrain-error:0.015005\n",
      "[151]\ttrain-error:0.014954\n",
      "[152]\ttrain-error:0.014921\n",
      "[153]\ttrain-error:0.014921\n",
      "[154]\ttrain-error:0.014937\n",
      "[155]\ttrain-error:0.014921\n",
      "[156]\ttrain-error:0.014870\n",
      "[157]\ttrain-error:0.014937\n",
      "[158]\ttrain-error:0.014904\n",
      "[159]\ttrain-error:0.014820\n",
      "[160]\ttrain-error:0.014836\n",
      "[161]\ttrain-error:0.014836\n",
      "[162]\ttrain-error:0.014820\n",
      "[163]\ttrain-error:0.014836\n",
      "[164]\ttrain-error:0.014820\n",
      "[165]\ttrain-error:0.014820\n",
      "[166]\ttrain-error:0.014786\n",
      "[167]\ttrain-error:0.014853\n",
      "[168]\ttrain-error:0.014803\n",
      "[169]\ttrain-error:0.014836\n",
      "[170]\ttrain-error:0.014786\n",
      "[171]\ttrain-error:0.014836\n",
      "[172]\ttrain-error:0.014752\n",
      "[173]\ttrain-error:0.014651\n",
      "[174]\ttrain-error:0.014735\n",
      "[175]\ttrain-error:0.014735\n",
      "[176]\ttrain-error:0.014786\n",
      "[177]\ttrain-error:0.014719\n",
      "[178]\ttrain-error:0.014719\n",
      "[179]\ttrain-error:0.014685\n",
      "[180]\ttrain-error:0.014634\n",
      "[181]\ttrain-error:0.014668\n",
      "[182]\ttrain-error:0.014702\n",
      "[183]\ttrain-error:0.014735\n",
      "[184]\ttrain-error:0.014702\n",
      "[185]\ttrain-error:0.014668\n",
      "[186]\ttrain-error:0.014685\n",
      "[187]\ttrain-error:0.014617\n",
      "[188]\ttrain-error:0.014668\n",
      "[189]\ttrain-error:0.014651\n",
      "[190]\ttrain-error:0.014702\n",
      "[191]\ttrain-error:0.014584\n",
      "[192]\ttrain-error:0.014584\n",
      "[193]\ttrain-error:0.014617\n",
      "[194]\ttrain-error:0.014634\n",
      "[195]\ttrain-error:0.014735\n",
      "[196]\ttrain-error:0.014685\n",
      "[197]\ttrain-error:0.014584\n",
      "[198]\ttrain-error:0.014533\n",
      "[199]\ttrain-error:0.014584\n",
      "[200]\ttrain-error:0.014516\n",
      "[201]\ttrain-error:0.014533\n",
      "[202]\ttrain-error:0.014516\n",
      "[203]\ttrain-error:0.014567\n",
      "[204]\ttrain-error:0.014634\n",
      "[205]\ttrain-error:0.014516\n",
      "[206]\ttrain-error:0.014567\n",
      "[207]\ttrain-error:0.014550\n",
      "[208]\ttrain-error:0.014584\n",
      "[209]\ttrain-error:0.014516\n",
      "[210]\ttrain-error:0.014533\n",
      "[211]\ttrain-error:0.014567\n",
      "[212]\ttrain-error:0.014533\n",
      "[213]\ttrain-error:0.014550\n",
      "[214]\ttrain-error:0.014533\n",
      "[215]\ttrain-error:0.014500\n",
      "[216]\ttrain-error:0.014533\n",
      "[217]\ttrain-error:0.014516\n",
      "[218]\ttrain-error:0.014483\n",
      "[219]\ttrain-error:0.014399\n",
      "[220]\ttrain-error:0.014399\n",
      "[221]\ttrain-error:0.014466\n",
      "[222]\ttrain-error:0.014466\n",
      "[223]\ttrain-error:0.014399\n",
      "[224]\ttrain-error:0.014500\n",
      "[225]\ttrain-error:0.014466\n",
      "[226]\ttrain-error:0.014449\n",
      "[227]\ttrain-error:0.014432\n",
      "[228]\ttrain-error:0.014399\n",
      "[229]\ttrain-error:0.014466\n",
      "[230]\ttrain-error:0.014382\n",
      "[231]\ttrain-error:0.014415\n",
      "[232]\ttrain-error:0.014449\n",
      "[233]\ttrain-error:0.014432\n",
      "[234]\ttrain-error:0.014415\n",
      "[235]\ttrain-error:0.014314\n",
      "[236]\ttrain-error:0.014348\n",
      "[237]\ttrain-error:0.014348\n",
      "[238]\ttrain-error:0.014432\n",
      "[239]\ttrain-error:0.014348\n",
      "[240]\ttrain-error:0.014415\n",
      "[241]\ttrain-error:0.014399\n",
      "[242]\ttrain-error:0.014365\n",
      "[243]\ttrain-error:0.014382\n",
      "[244]\ttrain-error:0.014382\n",
      "[245]\ttrain-error:0.014365\n",
      "[246]\ttrain-error:0.014331\n",
      "[247]\ttrain-error:0.014382\n",
      "[248]\ttrain-error:0.014348\n",
      "[249]\ttrain-error:0.014415\n",
      "[250]\ttrain-error:0.014432\n",
      "[251]\ttrain-error:0.014415\n",
      "[252]\ttrain-error:0.014365\n",
      "[253]\ttrain-error:0.014382\n",
      "[254]\ttrain-error:0.014331\n",
      "[255]\ttrain-error:0.014382\n",
      "[256]\ttrain-error:0.014365\n",
      "[257]\ttrain-error:0.014331\n",
      "[258]\ttrain-error:0.014331\n",
      "[259]\ttrain-error:0.014348\n",
      "[260]\ttrain-error:0.014365\n",
      "[261]\ttrain-error:0.014348\n",
      "[262]\ttrain-error:0.014348\n",
      "[263]\ttrain-error:0.014382\n",
      "[264]\ttrain-error:0.014399\n",
      "[265]\ttrain-error:0.014314\n",
      "[266]\ttrain-error:0.014298\n",
      "[267]\ttrain-error:0.014281\n",
      "[268]\ttrain-error:0.014298\n",
      "[269]\ttrain-error:0.014314\n",
      "[270]\ttrain-error:0.014298\n",
      "[271]\ttrain-error:0.014264\n",
      "[272]\ttrain-error:0.014348\n",
      "[273]\ttrain-error:0.014415\n",
      "[274]\ttrain-error:0.014331\n",
      "[275]\ttrain-error:0.014314\n",
      "[276]\ttrain-error:0.014281\n",
      "[277]\ttrain-error:0.014331\n",
      "[278]\ttrain-error:0.014281\n",
      "[279]\ttrain-error:0.014230\n",
      "[280]\ttrain-error:0.014298\n",
      "[281]\ttrain-error:0.014331\n",
      "[282]\ttrain-error:0.014314\n",
      "[283]\ttrain-error:0.014298\n",
      "[284]\ttrain-error:0.014230\n",
      "[285]\ttrain-error:0.014196\n",
      "[286]\ttrain-error:0.014196\n",
      "[287]\ttrain-error:0.014180\n",
      "[288]\ttrain-error:0.014213\n",
      "[289]\ttrain-error:0.014314\n",
      "[290]\ttrain-error:0.014281\n",
      "[291]\ttrain-error:0.014298\n",
      "[292]\ttrain-error:0.014264\n",
      "[293]\ttrain-error:0.014247\n",
      "[294]\ttrain-error:0.014180\n",
      "[295]\ttrain-error:0.014281\n",
      "[296]\ttrain-error:0.014281\n",
      "[297]\ttrain-error:0.014281\n",
      "[298]\ttrain-error:0.014230\n",
      "[299]\ttrain-error:0.014230\n",
      "[300]\ttrain-error:0.014264\n",
      "[301]\ttrain-error:0.014247\n",
      "[302]\ttrain-error:0.014365\n",
      "[303]\ttrain-error:0.014298\n",
      "[304]\ttrain-error:0.014264\n",
      "[305]\ttrain-error:0.014331\n",
      "[306]\ttrain-error:0.014298\n",
      "[307]\ttrain-error:0.014314\n",
      "[308]\ttrain-error:0.014298\n",
      "[309]\ttrain-error:0.014365\n",
      "[310]\ttrain-error:0.014281\n",
      "[311]\ttrain-error:0.014298\n",
      "[312]\ttrain-error:0.014281\n",
      "[313]\ttrain-error:0.014298\n",
      "[314]\ttrain-error:0.014331\n",
      "[315]\ttrain-error:0.014331\n",
      "[316]\ttrain-error:0.014382\n",
      "[317]\ttrain-error:0.014382\n",
      "[318]\ttrain-error:0.014399\n",
      "[319]\ttrain-error:0.014331\n",
      "[320]\ttrain-error:0.014230\n",
      "[321]\ttrain-error:0.014264\n",
      "[322]\ttrain-error:0.014264\n",
      "[323]\ttrain-error:0.014213\n",
      "[324]\ttrain-error:0.014213\n",
      "[325]\ttrain-error:0.014196\n",
      "[326]\ttrain-error:0.014146\n",
      "[327]\ttrain-error:0.014163\n",
      "[328]\ttrain-error:0.014112\n",
      "[329]\ttrain-error:0.014180\n",
      "[330]\ttrain-error:0.014095\n",
      "[331]\ttrain-error:0.014196\n",
      "[332]\ttrain-error:0.014196\n",
      "[333]\ttrain-error:0.014213\n",
      "[334]\ttrain-error:0.014112\n",
      "[335]\ttrain-error:0.014079\n",
      "[336]\ttrain-error:0.014146\n",
      "[337]\ttrain-error:0.014146\n",
      "[338]\ttrain-error:0.014095\n",
      "[339]\ttrain-error:0.014062\n",
      "[340]\ttrain-error:0.014045\n",
      "[341]\ttrain-error:0.014079\n",
      "[342]\ttrain-error:0.014112\n",
      "[343]\ttrain-error:0.014146\n",
      "[344]\ttrain-error:0.014163\n",
      "[345]\ttrain-error:0.014129\n",
      "[346]\ttrain-error:0.014129\n",
      "[347]\ttrain-error:0.014079\n",
      "[348]\ttrain-error:0.014095\n",
      "[349]\ttrain-error:0.014062\n",
      "[350]\ttrain-error:0.013961\n",
      "[351]\ttrain-error:0.013994\n",
      "[352]\ttrain-error:0.013961\n",
      "[353]\ttrain-error:0.013994\n",
      "[354]\ttrain-error:0.013978\n",
      "[355]\ttrain-error:0.014045\n",
      "[356]\ttrain-error:0.013994\n",
      "[357]\ttrain-error:0.013978\n",
      "[358]\ttrain-error:0.014079\n",
      "[359]\ttrain-error:0.014129\n",
      "[360]\ttrain-error:0.014129\n",
      "[361]\ttrain-error:0.014095\n",
      "[362]\ttrain-error:0.014112\n",
      "[363]\ttrain-error:0.014112\n",
      "[364]\ttrain-error:0.014079\n",
      "[365]\ttrain-error:0.014095\n",
      "[366]\ttrain-error:0.014129\n",
      "[367]\ttrain-error:0.014146\n",
      "[368]\ttrain-error:0.014129\n",
      "[369]\ttrain-error:0.014062\n",
      "[370]\ttrain-error:0.014045\n",
      "[371]\ttrain-error:0.014045\n",
      "[372]\ttrain-error:0.014011\n",
      "[373]\ttrain-error:0.013978\n",
      "[374]\ttrain-error:0.014028\n",
      "[375]\ttrain-error:0.013961\n",
      "[376]\ttrain-error:0.013893\n",
      "[377]\ttrain-error:0.013910\n",
      "[378]\ttrain-error:0.013927\n",
      "[379]\ttrain-error:0.013910\n",
      "[380]\ttrain-error:0.013910\n",
      "[381]\ttrain-error:0.013910\n",
      "[382]\ttrain-error:0.013961\n",
      "[383]\ttrain-error:0.013860\n",
      "[384]\ttrain-error:0.013843\n",
      "[385]\ttrain-error:0.013843\n",
      "[386]\ttrain-error:0.013944\n",
      "[387]\ttrain-error:0.013944\n",
      "[388]\ttrain-error:0.013893\n",
      "[389]\ttrain-error:0.013876\n",
      "[390]\ttrain-error:0.013927\n",
      "[391]\ttrain-error:0.013893\n",
      "[392]\ttrain-error:0.013860\n",
      "[393]\ttrain-error:0.013910\n",
      "[394]\ttrain-error:0.013826\n",
      "[395]\ttrain-error:0.013843\n",
      "[396]\ttrain-error:0.013826\n",
      "[397]\ttrain-error:0.013893\n",
      "[398]\ttrain-error:0.014045\n",
      "[399]\ttrain-error:0.013927\n",
      "[400]\ttrain-error:0.013994\n",
      "[401]\ttrain-error:0.014045\n",
      "[402]\ttrain-error:0.013944\n",
      "[403]\ttrain-error:0.013994\n",
      "[404]\ttrain-error:0.013893\n",
      "[405]\ttrain-error:0.013927\n",
      "[406]\ttrain-error:0.013843\n",
      "[407]\ttrain-error:0.013876\n",
      "[408]\ttrain-error:0.013826\n",
      "[409]\ttrain-error:0.013893\n",
      "[410]\ttrain-error:0.013843\n",
      "[411]\ttrain-error:0.013893\n",
      "[412]\ttrain-error:0.013944\n",
      "[413]\ttrain-error:0.013910\n",
      "[414]\ttrain-error:0.013961\n",
      "[415]\ttrain-error:0.013944\n",
      "[416]\ttrain-error:0.013927\n",
      "[417]\ttrain-error:0.013893\n",
      "[418]\ttrain-error:0.013876\n",
      "[419]\ttrain-error:0.013860\n",
      "[420]\ttrain-error:0.013826\n",
      "[421]\ttrain-error:0.013843\n",
      "[422]\ttrain-error:0.013826\n",
      "[423]\ttrain-error:0.013910\n",
      "[424]\ttrain-error:0.013860\n",
      "[425]\ttrain-error:0.013893\n",
      "[426]\ttrain-error:0.013944\n",
      "[427]\ttrain-error:0.013910\n",
      "[428]\ttrain-error:0.013876\n",
      "[429]\ttrain-error:0.013944\n",
      "[430]\ttrain-error:0.013893\n",
      "[431]\ttrain-error:0.013994\n",
      "[432]\ttrain-error:0.013961\n",
      "[433]\ttrain-error:0.013961\n",
      "[434]\ttrain-error:0.013994\n",
      "[435]\ttrain-error:0.014028\n",
      "[436]\ttrain-error:0.014011\n",
      "[437]\ttrain-error:0.014028\n",
      "[438]\ttrain-error:0.014011\n",
      "[439]\ttrain-error:0.013994\n",
      "[440]\ttrain-error:0.013910\n",
      "[441]\ttrain-error:0.013961\n",
      "[442]\ttrain-error:0.013978\n",
      "[443]\ttrain-error:0.013927\n",
      "[444]\ttrain-error:0.013961\n",
      "[445]\ttrain-error:0.013893\n",
      "[446]\ttrain-error:0.013944\n",
      "[447]\ttrain-error:0.013860\n",
      "[448]\ttrain-error:0.013893\n",
      "[449]\ttrain-error:0.013876\n",
      "[450]\ttrain-error:0.013910\n",
      "[451]\ttrain-error:0.013893\n",
      "[452]\ttrain-error:0.013876\n",
      "[453]\ttrain-error:0.013809\n",
      "[454]\ttrain-error:0.013809\n",
      "[455]\ttrain-error:0.013843\n",
      "[456]\ttrain-error:0.013826\n",
      "[457]\ttrain-error:0.013759\n",
      "[458]\ttrain-error:0.013826\n",
      "[459]\ttrain-error:0.013775\n",
      "[460]\ttrain-error:0.013775\n",
      "[461]\ttrain-error:0.013759\n",
      "[462]\ttrain-error:0.013759\n",
      "[463]\ttrain-error:0.013725\n",
      "[464]\ttrain-error:0.013809\n",
      "[465]\ttrain-error:0.013775\n",
      "[466]\ttrain-error:0.013742\n",
      "[467]\ttrain-error:0.013725\n",
      "[468]\ttrain-error:0.013708\n",
      "[469]\ttrain-error:0.013742\n",
      "[470]\ttrain-error:0.013725\n",
      "[471]\ttrain-error:0.013708\n",
      "[472]\ttrain-error:0.013658\n",
      "[473]\ttrain-error:0.013691\n",
      "[474]\ttrain-error:0.013624\n",
      "[475]\ttrain-error:0.013658\n",
      "[476]\ttrain-error:0.013624\n",
      "[477]\ttrain-error:0.013641\n",
      "[478]\ttrain-error:0.013607\n",
      "[479]\ttrain-error:0.013624\n",
      "[480]\ttrain-error:0.013691\n",
      "[481]\ttrain-error:0.013691\n",
      "[482]\ttrain-error:0.013725\n",
      "[483]\ttrain-error:0.013708\n",
      "[484]\ttrain-error:0.013775\n",
      "[485]\ttrain-error:0.013759\n",
      "[486]\ttrain-error:0.013759\n",
      "[487]\ttrain-error:0.013775\n",
      "[488]\ttrain-error:0.013725\n",
      "[489]\ttrain-error:0.013742\n",
      "[490]\ttrain-error:0.013658\n",
      "[491]\ttrain-error:0.013641\n",
      "[492]\ttrain-error:0.013641\n",
      "[493]\ttrain-error:0.013641\n",
      "[494]\ttrain-error:0.013725\n",
      "[495]\ttrain-error:0.013641\n",
      "[496]\ttrain-error:0.013658\n",
      "[497]\ttrain-error:0.013590\n",
      "[498]\ttrain-error:0.013658\n",
      "[499]\ttrain-error:0.013641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.022953\n",
      "[1]\ttrain-error:0.021775\n",
      "[2]\ttrain-error:0.021000\n",
      "[3]\ttrain-error:0.024048\n",
      "[4]\ttrain-error:0.023560\n",
      "[5]\ttrain-error:0.023577\n",
      "[6]\ttrain-error:0.024048\n",
      "[7]\ttrain-error:0.024048\n",
      "[8]\ttrain-error:0.024048\n",
      "[9]\ttrain-error:0.024048\n",
      "[10]\ttrain-error:0.024048\n",
      "[11]\ttrain-error:0.024048\n",
      "[12]\ttrain-error:0.024048\n",
      "[13]\ttrain-error:0.024048\n",
      "[14]\ttrain-error:0.024048\n",
      "[15]\ttrain-error:0.024048\n",
      "[16]\ttrain-error:0.024048\n",
      "[17]\ttrain-error:0.024048\n",
      "[18]\ttrain-error:0.024048\n",
      "[19]\ttrain-error:0.024048\n",
      "[20]\ttrain-error:0.024048\n",
      "[21]\ttrain-error:0.024048\n",
      "[22]\ttrain-error:0.024048\n",
      "[23]\ttrain-error:0.024048\n",
      "[24]\ttrain-error:0.024048\n",
      "[25]\ttrain-error:0.024048\n",
      "[26]\ttrain-error:0.024048\n",
      "[27]\ttrain-error:0.024048\n",
      "[28]\ttrain-error:0.024048\n",
      "[29]\ttrain-error:0.024048\n",
      "[30]\ttrain-error:0.024048\n",
      "[31]\ttrain-error:0.024048\n",
      "[32]\ttrain-error:0.024048\n",
      "[33]\ttrain-error:0.024048\n",
      "[34]\ttrain-error:0.024048\n",
      "[35]\ttrain-error:0.024048\n",
      "[36]\ttrain-error:0.024048\n",
      "[37]\ttrain-error:0.024048\n",
      "[38]\ttrain-error:0.024048\n",
      "[39]\ttrain-error:0.024048\n",
      "[40]\ttrain-error:0.024014\n",
      "[41]\ttrain-error:0.023998\n",
      "[42]\ttrain-error:0.023998\n",
      "[43]\ttrain-error:0.023913\n",
      "[44]\ttrain-error:0.023897\n",
      "[45]\ttrain-error:0.023880\n",
      "[46]\ttrain-error:0.023880\n",
      "[47]\ttrain-error:0.023897\n",
      "[48]\ttrain-error:0.023880\n",
      "[49]\ttrain-error:0.023897\n",
      "[50]\ttrain-error:0.023897\n",
      "[51]\ttrain-error:0.023678\n",
      "[52]\ttrain-error:0.023678\n",
      "[53]\ttrain-error:0.023678\n",
      "[54]\ttrain-error:0.023627\n",
      "[55]\ttrain-error:0.023492\n",
      "[56]\ttrain-error:0.023374\n",
      "[57]\ttrain-error:0.023374\n",
      "[58]\ttrain-error:0.023273\n",
      "[59]\ttrain-error:0.022970\n",
      "[60]\ttrain-error:0.022314\n",
      "[61]\ttrain-error:0.022263\n",
      "[62]\ttrain-error:0.022044\n",
      "[63]\ttrain-error:0.021943\n",
      "[64]\ttrain-error:0.021926\n",
      "[65]\ttrain-error:0.021859\n",
      "[66]\ttrain-error:0.021825\n",
      "[67]\ttrain-error:0.021893\n",
      "[68]\ttrain-error:0.021724\n",
      "[69]\ttrain-error:0.021623\n",
      "[70]\ttrain-error:0.021472\n",
      "[71]\ttrain-error:0.021202\n",
      "[72]\ttrain-error:0.020966\n",
      "[73]\ttrain-error:0.020731\n",
      "[74]\ttrain-error:0.020747\n",
      "[75]\ttrain-error:0.020512\n",
      "[76]\ttrain-error:0.020040\n",
      "[77]\ttrain-error:0.020006\n",
      "[78]\ttrain-error:0.019569\n",
      "[79]\ttrain-error:0.019164\n",
      "[80]\ttrain-error:0.018979\n",
      "[81]\ttrain-error:0.019063\n",
      "[82]\ttrain-error:0.018844\n",
      "[83]\ttrain-error:0.018878\n",
      "[84]\ttrain-error:0.018861\n",
      "[85]\ttrain-error:0.018760\n",
      "[86]\ttrain-error:0.018659\n",
      "[87]\ttrain-error:0.018676\n",
      "[88]\ttrain-error:0.018457\n",
      "[89]\ttrain-error:0.018524\n",
      "[90]\ttrain-error:0.018541\n",
      "[91]\ttrain-error:0.018440\n",
      "[92]\ttrain-error:0.018306\n",
      "[93]\ttrain-error:0.018255\n",
      "[94]\ttrain-error:0.018306\n",
      "[95]\ttrain-error:0.018238\n",
      "[96]\ttrain-error:0.018103\n",
      "[97]\ttrain-error:0.018204\n",
      "[98]\ttrain-error:0.018137\n",
      "[99]\ttrain-error:0.018036\n",
      "[100]\ttrain-error:0.017918\n",
      "[101]\ttrain-error:0.017750\n",
      "[102]\ttrain-error:0.017733\n",
      "[103]\ttrain-error:0.017783\n",
      "[104]\ttrain-error:0.017615\n",
      "[105]\ttrain-error:0.017598\n",
      "[106]\ttrain-error:0.017565\n",
      "[107]\ttrain-error:0.017430\n",
      "[108]\ttrain-error:0.017346\n",
      "[109]\ttrain-error:0.017278\n",
      "[110]\ttrain-error:0.017312\n",
      "[111]\ttrain-error:0.017295\n",
      "[112]\ttrain-error:0.017228\n",
      "[113]\ttrain-error:0.017245\n",
      "[114]\ttrain-error:0.017144\n",
      "[115]\ttrain-error:0.017245\n",
      "[116]\ttrain-error:0.017211\n",
      "[117]\ttrain-error:0.017211\n",
      "[118]\ttrain-error:0.017160\n",
      "[119]\ttrain-error:0.017228\n",
      "[120]\ttrain-error:0.017127\n",
      "[121]\ttrain-error:0.017009\n",
      "[122]\ttrain-error:0.016975\n",
      "[123]\ttrain-error:0.016975\n",
      "[124]\ttrain-error:0.016958\n",
      "[125]\ttrain-error:0.017093\n",
      "[126]\ttrain-error:0.017127\n",
      "[127]\ttrain-error:0.017059\n",
      "[128]\ttrain-error:0.017076\n",
      "[129]\ttrain-error:0.017059\n",
      "[130]\ttrain-error:0.017026\n",
      "[131]\ttrain-error:0.016958\n",
      "[132]\ttrain-error:0.016958\n",
      "[133]\ttrain-error:0.016925\n",
      "[134]\ttrain-error:0.017009\n",
      "[135]\ttrain-error:0.016992\n",
      "[136]\ttrain-error:0.017009\n",
      "[137]\ttrain-error:0.016975\n",
      "[138]\ttrain-error:0.016992\n",
      "[139]\ttrain-error:0.016992\n",
      "[140]\ttrain-error:0.016958\n",
      "[141]\ttrain-error:0.016975\n",
      "[142]\ttrain-error:0.016925\n",
      "[143]\ttrain-error:0.017026\n",
      "[144]\ttrain-error:0.017059\n",
      "[145]\ttrain-error:0.017059\n",
      "[146]\ttrain-error:0.017026\n",
      "[147]\ttrain-error:0.017042\n",
      "[148]\ttrain-error:0.016941\n",
      "[149]\ttrain-error:0.016824\n",
      "[150]\ttrain-error:0.016807\n",
      "[151]\ttrain-error:0.016756\n",
      "[152]\ttrain-error:0.016790\n",
      "[153]\ttrain-error:0.016739\n",
      "[154]\ttrain-error:0.016756\n",
      "[155]\ttrain-error:0.016790\n",
      "[156]\ttrain-error:0.016756\n",
      "[157]\ttrain-error:0.016723\n",
      "[158]\ttrain-error:0.016773\n",
      "[159]\ttrain-error:0.016824\n",
      "[160]\ttrain-error:0.016824\n",
      "[161]\ttrain-error:0.016706\n",
      "[162]\ttrain-error:0.016874\n",
      "[163]\ttrain-error:0.016891\n",
      "[164]\ttrain-error:0.016908\n",
      "[165]\ttrain-error:0.016840\n",
      "[166]\ttrain-error:0.016756\n",
      "[167]\ttrain-error:0.016807\n",
      "[168]\ttrain-error:0.016756\n",
      "[169]\ttrain-error:0.016571\n",
      "[170]\ttrain-error:0.016621\n",
      "[171]\ttrain-error:0.016689\n",
      "[172]\ttrain-error:0.016672\n",
      "[173]\ttrain-error:0.016520\n",
      "[174]\ttrain-error:0.016504\n",
      "[175]\ttrain-error:0.016504\n",
      "[176]\ttrain-error:0.016605\n",
      "[177]\ttrain-error:0.016571\n",
      "[178]\ttrain-error:0.016487\n",
      "[179]\ttrain-error:0.016419\n",
      "[180]\ttrain-error:0.016554\n",
      "[181]\ttrain-error:0.016537\n",
      "[182]\ttrain-error:0.016504\n",
      "[183]\ttrain-error:0.016672\n",
      "[184]\ttrain-error:0.016638\n",
      "[185]\ttrain-error:0.016588\n",
      "[186]\ttrain-error:0.016487\n",
      "[187]\ttrain-error:0.016504\n",
      "[188]\ttrain-error:0.016487\n",
      "[189]\ttrain-error:0.016504\n",
      "[190]\ttrain-error:0.016537\n",
      "[191]\ttrain-error:0.016554\n",
      "[192]\ttrain-error:0.016537\n",
      "[193]\ttrain-error:0.016554\n",
      "[194]\ttrain-error:0.016655\n",
      "[195]\ttrain-error:0.016605\n",
      "[196]\ttrain-error:0.016537\n",
      "[197]\ttrain-error:0.016554\n",
      "[198]\ttrain-error:0.016520\n",
      "[199]\ttrain-error:0.016605\n",
      "[200]\ttrain-error:0.016588\n",
      "[201]\ttrain-error:0.016605\n",
      "[202]\ttrain-error:0.016672\n",
      "[203]\ttrain-error:0.016672\n",
      "[204]\ttrain-error:0.016638\n",
      "[205]\ttrain-error:0.016621\n",
      "[206]\ttrain-error:0.016554\n",
      "[207]\ttrain-error:0.016554\n",
      "[208]\ttrain-error:0.016504\n",
      "[209]\ttrain-error:0.016621\n",
      "[210]\ttrain-error:0.016605\n",
      "[211]\ttrain-error:0.016638\n",
      "[212]\ttrain-error:0.016588\n",
      "[213]\ttrain-error:0.016571\n",
      "[214]\ttrain-error:0.016537\n",
      "[215]\ttrain-error:0.016605\n",
      "[216]\ttrain-error:0.016588\n",
      "[217]\ttrain-error:0.016554\n",
      "[218]\ttrain-error:0.016453\n",
      "[219]\ttrain-error:0.016520\n",
      "[220]\ttrain-error:0.016588\n",
      "[221]\ttrain-error:0.016470\n",
      "[222]\ttrain-error:0.016504\n",
      "[223]\ttrain-error:0.016621\n",
      "[224]\ttrain-error:0.016487\n",
      "[225]\ttrain-error:0.016453\n",
      "[226]\ttrain-error:0.016369\n",
      "[227]\ttrain-error:0.016453\n",
      "[228]\ttrain-error:0.016487\n",
      "[229]\ttrain-error:0.016504\n",
      "[230]\ttrain-error:0.016487\n",
      "[231]\ttrain-error:0.016470\n",
      "[232]\ttrain-error:0.016537\n",
      "[233]\ttrain-error:0.016470\n",
      "[234]\ttrain-error:0.016487\n",
      "[235]\ttrain-error:0.016386\n",
      "[236]\ttrain-error:0.016335\n",
      "[237]\ttrain-error:0.016302\n",
      "[238]\ttrain-error:0.016352\n",
      "[239]\ttrain-error:0.016335\n",
      "[240]\ttrain-error:0.016386\n",
      "[241]\ttrain-error:0.016302\n",
      "[242]\ttrain-error:0.016318\n",
      "[243]\ttrain-error:0.016386\n",
      "[244]\ttrain-error:0.016369\n",
      "[245]\ttrain-error:0.016318\n",
      "[246]\ttrain-error:0.016302\n",
      "[247]\ttrain-error:0.016352\n",
      "[248]\ttrain-error:0.016318\n",
      "[249]\ttrain-error:0.016200\n",
      "[250]\ttrain-error:0.016251\n",
      "[251]\ttrain-error:0.016268\n",
      "[252]\ttrain-error:0.016302\n",
      "[253]\ttrain-error:0.016234\n",
      "[254]\ttrain-error:0.016285\n",
      "[255]\ttrain-error:0.016268\n",
      "[256]\ttrain-error:0.016234\n",
      "[257]\ttrain-error:0.016285\n",
      "[258]\ttrain-error:0.016150\n",
      "[259]\ttrain-error:0.016049\n",
      "[260]\ttrain-error:0.016083\n",
      "[261]\ttrain-error:0.016066\n",
      "[262]\ttrain-error:0.016083\n",
      "[263]\ttrain-error:0.016083\n",
      "[264]\ttrain-error:0.016116\n",
      "[265]\ttrain-error:0.016066\n",
      "[266]\ttrain-error:0.016015\n",
      "[267]\ttrain-error:0.016049\n",
      "[268]\ttrain-error:0.015982\n",
      "[269]\ttrain-error:0.016099\n",
      "[270]\ttrain-error:0.016015\n",
      "[271]\ttrain-error:0.016015\n",
      "[272]\ttrain-error:0.016049\n",
      "[273]\ttrain-error:0.015948\n",
      "[274]\ttrain-error:0.016015\n",
      "[275]\ttrain-error:0.015998\n",
      "[276]\ttrain-error:0.015881\n",
      "[277]\ttrain-error:0.015813\n",
      "[278]\ttrain-error:0.015881\n",
      "[279]\ttrain-error:0.015864\n",
      "[280]\ttrain-error:0.015948\n",
      "[281]\ttrain-error:0.015931\n",
      "[282]\ttrain-error:0.015914\n",
      "[283]\ttrain-error:0.015965\n",
      "[284]\ttrain-error:0.015982\n",
      "[285]\ttrain-error:0.016066\n",
      "[286]\ttrain-error:0.016066\n",
      "[287]\ttrain-error:0.016066\n",
      "[288]\ttrain-error:0.016099\n",
      "[289]\ttrain-error:0.016032\n",
      "[290]\ttrain-error:0.016066\n",
      "[291]\ttrain-error:0.016066\n",
      "[292]\ttrain-error:0.016049\n",
      "[293]\ttrain-error:0.016049\n",
      "[294]\ttrain-error:0.016032\n",
      "[295]\ttrain-error:0.015998\n",
      "[296]\ttrain-error:0.016049\n",
      "[297]\ttrain-error:0.016049\n",
      "[298]\ttrain-error:0.016150\n",
      "[299]\ttrain-error:0.016167\n",
      "[300]\ttrain-error:0.016167\n",
      "[301]\ttrain-error:0.016066\n",
      "[302]\ttrain-error:0.016083\n",
      "[303]\ttrain-error:0.016032\n",
      "[304]\ttrain-error:0.016032\n",
      "[305]\ttrain-error:0.016015\n",
      "[306]\ttrain-error:0.015965\n",
      "[307]\ttrain-error:0.015982\n",
      "[308]\ttrain-error:0.016032\n",
      "[309]\ttrain-error:0.016099\n",
      "[310]\ttrain-error:0.016184\n",
      "[311]\ttrain-error:0.016032\n",
      "[312]\ttrain-error:0.015998\n",
      "[313]\ttrain-error:0.016083\n",
      "[314]\ttrain-error:0.016099\n",
      "[315]\ttrain-error:0.016150\n",
      "[316]\ttrain-error:0.016200\n",
      "[317]\ttrain-error:0.016116\n",
      "[318]\ttrain-error:0.016049\n",
      "[319]\ttrain-error:0.016049\n",
      "[320]\ttrain-error:0.015998\n",
      "[321]\ttrain-error:0.016015\n",
      "[322]\ttrain-error:0.016116\n",
      "[323]\ttrain-error:0.016099\n",
      "[324]\ttrain-error:0.016066\n",
      "[325]\ttrain-error:0.016099\n",
      "[326]\ttrain-error:0.016083\n",
      "[327]\ttrain-error:0.016083\n",
      "[328]\ttrain-error:0.016116\n",
      "[329]\ttrain-error:0.015998\n",
      "[330]\ttrain-error:0.015914\n",
      "[331]\ttrain-error:0.015881\n",
      "[332]\ttrain-error:0.015897\n",
      "[333]\ttrain-error:0.015813\n",
      "[334]\ttrain-error:0.015864\n",
      "[335]\ttrain-error:0.015897\n",
      "[336]\ttrain-error:0.015931\n",
      "[337]\ttrain-error:0.015931\n",
      "[338]\ttrain-error:0.015914\n",
      "[339]\ttrain-error:0.015948\n",
      "[340]\ttrain-error:0.015948\n",
      "[341]\ttrain-error:0.015931\n",
      "[342]\ttrain-error:0.015830\n",
      "[343]\ttrain-error:0.015931\n",
      "[344]\ttrain-error:0.015897\n",
      "[345]\ttrain-error:0.015864\n",
      "[346]\ttrain-error:0.015931\n",
      "[347]\ttrain-error:0.015881\n",
      "[348]\ttrain-error:0.015914\n",
      "[349]\ttrain-error:0.015881\n",
      "[350]\ttrain-error:0.015881\n",
      "[351]\ttrain-error:0.015931\n",
      "[352]\ttrain-error:0.015864\n",
      "[353]\ttrain-error:0.015897\n",
      "[354]\ttrain-error:0.015897\n",
      "[355]\ttrain-error:0.015796\n",
      "[356]\ttrain-error:0.015796\n",
      "[357]\ttrain-error:0.015830\n",
      "[358]\ttrain-error:0.015864\n",
      "[359]\ttrain-error:0.015813\n",
      "[360]\ttrain-error:0.015830\n",
      "[361]\ttrain-error:0.015881\n",
      "[362]\ttrain-error:0.015931\n",
      "[363]\ttrain-error:0.015864\n",
      "[364]\ttrain-error:0.015914\n",
      "[365]\ttrain-error:0.015914\n",
      "[366]\ttrain-error:0.015881\n",
      "[367]\ttrain-error:0.015847\n",
      "[368]\ttrain-error:0.015830\n",
      "[369]\ttrain-error:0.015897\n",
      "[370]\ttrain-error:0.015931\n",
      "[371]\ttrain-error:0.015982\n",
      "[372]\ttrain-error:0.015830\n",
      "[373]\ttrain-error:0.015830\n",
      "[374]\ttrain-error:0.015864\n",
      "[375]\ttrain-error:0.015965\n",
      "[376]\ttrain-error:0.015830\n",
      "[377]\ttrain-error:0.015830\n",
      "[378]\ttrain-error:0.015847\n",
      "[379]\ttrain-error:0.015965\n",
      "[380]\ttrain-error:0.015864\n",
      "[381]\ttrain-error:0.015864\n",
      "[382]\ttrain-error:0.015779\n",
      "[383]\ttrain-error:0.015813\n",
      "[384]\ttrain-error:0.015847\n",
      "[385]\ttrain-error:0.015779\n",
      "[386]\ttrain-error:0.015729\n",
      "[387]\ttrain-error:0.015779\n",
      "[388]\ttrain-error:0.015779\n",
      "[389]\ttrain-error:0.015830\n",
      "[390]\ttrain-error:0.015746\n",
      "[391]\ttrain-error:0.015712\n",
      "[392]\ttrain-error:0.015729\n",
      "[393]\ttrain-error:0.015779\n",
      "[394]\ttrain-error:0.015746\n",
      "[395]\ttrain-error:0.015695\n",
      "[396]\ttrain-error:0.015662\n",
      "[397]\ttrain-error:0.015594\n",
      "[398]\ttrain-error:0.015628\n",
      "[399]\ttrain-error:0.015493\n",
      "[400]\ttrain-error:0.015628\n",
      "[401]\ttrain-error:0.015645\n",
      "[402]\ttrain-error:0.015678\n",
      "[403]\ttrain-error:0.015594\n",
      "[404]\ttrain-error:0.015577\n",
      "[405]\ttrain-error:0.015577\n",
      "[406]\ttrain-error:0.015594\n",
      "[407]\ttrain-error:0.015527\n",
      "[408]\ttrain-error:0.015493\n",
      "[409]\ttrain-error:0.015577\n",
      "[410]\ttrain-error:0.015493\n",
      "[411]\ttrain-error:0.015476\n",
      "[412]\ttrain-error:0.015443\n",
      "[413]\ttrain-error:0.015426\n",
      "[414]\ttrain-error:0.015342\n",
      "[415]\ttrain-error:0.015409\n",
      "[416]\ttrain-error:0.015375\n",
      "[417]\ttrain-error:0.015375\n",
      "[418]\ttrain-error:0.015325\n",
      "[419]\ttrain-error:0.015241\n",
      "[420]\ttrain-error:0.015274\n",
      "[421]\ttrain-error:0.015325\n",
      "[422]\ttrain-error:0.015308\n",
      "[423]\ttrain-error:0.015241\n",
      "[424]\ttrain-error:0.015325\n",
      "[425]\ttrain-error:0.015291\n",
      "[426]\ttrain-error:0.015409\n",
      "[427]\ttrain-error:0.015308\n",
      "[428]\ttrain-error:0.015291\n",
      "[429]\ttrain-error:0.015308\n",
      "[430]\ttrain-error:0.015325\n",
      "[431]\ttrain-error:0.015375\n",
      "[432]\ttrain-error:0.015493\n",
      "[433]\ttrain-error:0.015459\n",
      "[434]\ttrain-error:0.015409\n",
      "[435]\ttrain-error:0.015375\n",
      "[436]\ttrain-error:0.015443\n",
      "[437]\ttrain-error:0.015443\n",
      "[438]\ttrain-error:0.015443\n",
      "[439]\ttrain-error:0.015493\n",
      "[440]\ttrain-error:0.015443\n",
      "[441]\ttrain-error:0.015561\n",
      "[442]\ttrain-error:0.015493\n",
      "[443]\ttrain-error:0.015577\n",
      "[444]\ttrain-error:0.015561\n",
      "[445]\ttrain-error:0.015527\n",
      "[446]\ttrain-error:0.015459\n",
      "[447]\ttrain-error:0.015493\n",
      "[448]\ttrain-error:0.015459\n",
      "[449]\ttrain-error:0.015443\n",
      "[450]\ttrain-error:0.015392\n",
      "[451]\ttrain-error:0.015375\n",
      "[452]\ttrain-error:0.015392\n",
      "[453]\ttrain-error:0.015392\n",
      "[454]\ttrain-error:0.015510\n",
      "[455]\ttrain-error:0.015443\n",
      "[456]\ttrain-error:0.015443\n",
      "[457]\ttrain-error:0.015459\n",
      "[458]\ttrain-error:0.015426\n",
      "[459]\ttrain-error:0.015459\n",
      "[460]\ttrain-error:0.015510\n",
      "[461]\ttrain-error:0.015459\n",
      "[462]\ttrain-error:0.015409\n",
      "[463]\ttrain-error:0.015510\n",
      "[464]\ttrain-error:0.015493\n",
      "[465]\ttrain-error:0.015443\n",
      "[466]\ttrain-error:0.015459\n",
      "[467]\ttrain-error:0.015375\n",
      "[468]\ttrain-error:0.015392\n",
      "[469]\ttrain-error:0.015409\n",
      "[470]\ttrain-error:0.015426\n",
      "[471]\ttrain-error:0.015426\n",
      "[472]\ttrain-error:0.015459\n",
      "[473]\ttrain-error:0.015561\n",
      "[474]\ttrain-error:0.015459\n",
      "[475]\ttrain-error:0.015459\n",
      "[476]\ttrain-error:0.015476\n",
      "[477]\ttrain-error:0.015493\n",
      "[478]\ttrain-error:0.015426\n",
      "[479]\ttrain-error:0.015375\n",
      "[480]\ttrain-error:0.015392\n",
      "[481]\ttrain-error:0.015392\n",
      "[482]\ttrain-error:0.015358\n",
      "[483]\ttrain-error:0.015426\n",
      "[484]\ttrain-error:0.015325\n",
      "[485]\ttrain-error:0.015476\n",
      "[486]\ttrain-error:0.015409\n",
      "[487]\ttrain-error:0.015426\n",
      "[488]\ttrain-error:0.015375\n",
      "[489]\ttrain-error:0.015426\n",
      "[490]\ttrain-error:0.015459\n",
      "[491]\ttrain-error:0.015459\n",
      "[492]\ttrain-error:0.015493\n",
      "[493]\ttrain-error:0.015459\n",
      "[494]\ttrain-error:0.015476\n",
      "[495]\ttrain-error:0.015493\n",
      "[496]\ttrain-error:0.015493\n",
      "[497]\ttrain-error:0.015476\n",
      "[498]\ttrain-error:0.015409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[499]\ttrain-error:0.015392\n",
      "[0]\ttrain-error:0.091410\n",
      "[1]\ttrain-error:0.091208\n",
      "[2]\ttrain-error:0.089069\n",
      "[3]\ttrain-error:0.090803\n",
      "[4]\ttrain-error:0.091056\n",
      "[5]\ttrain-error:0.091494\n",
      "[6]\ttrain-error:0.091477\n",
      "[7]\ttrain-error:0.091477\n",
      "[8]\ttrain-error:0.091326\n",
      "[9]\ttrain-error:0.091292\n",
      "[10]\ttrain-error:0.091427\n",
      "[11]\ttrain-error:0.091342\n",
      "[12]\ttrain-error:0.091056\n",
      "[13]\ttrain-error:0.091309\n",
      "[14]\ttrain-error:0.090517\n",
      "[15]\ttrain-error:0.090888\n",
      "[16]\ttrain-error:0.091140\n",
      "[17]\ttrain-error:0.091157\n",
      "[18]\ttrain-error:0.091258\n",
      "[19]\ttrain-error:0.090989\n",
      "[20]\ttrain-error:0.090517\n",
      "[21]\ttrain-error:0.090803\n",
      "[22]\ttrain-error:0.091022\n",
      "[23]\ttrain-error:0.090467\n",
      "[24]\ttrain-error:0.089894\n",
      "[25]\ttrain-error:0.090062\n",
      "[26]\ttrain-error:0.090366\n",
      "[27]\ttrain-error:0.090450\n",
      "[28]\ttrain-error:0.089945\n",
      "[29]\ttrain-error:0.088581\n",
      "[30]\ttrain-error:0.088749\n",
      "[31]\ttrain-error:0.089069\n",
      "[32]\ttrain-error:0.088496\n",
      "[33]\ttrain-error:0.088732\n",
      "[34]\ttrain-error:0.088244\n",
      "[35]\ttrain-error:0.087789\n",
      "[36]\ttrain-error:0.087402\n",
      "[37]\ttrain-error:0.087385\n",
      "[38]\ttrain-error:0.087166\n",
      "[39]\ttrain-error:0.087031\n",
      "[40]\ttrain-error:0.087536\n",
      "[41]\ttrain-error:0.087570\n",
      "[42]\ttrain-error:0.087402\n",
      "[43]\ttrain-error:0.087065\n",
      "[44]\ttrain-error:0.085499\n",
      "[45]\ttrain-error:0.085398\n",
      "[46]\ttrain-error:0.085280\n",
      "[47]\ttrain-error:0.083377\n",
      "[48]\ttrain-error:0.081929\n",
      "[49]\ttrain-error:0.081828\n",
      "[50]\ttrain-error:0.081508\n",
      "[51]\ttrain-error:0.081103\n",
      "[52]\ttrain-error:0.080918\n",
      "[53]\ttrain-error:0.080834\n",
      "[54]\ttrain-error:0.080564\n",
      "[55]\ttrain-error:0.080430\n",
      "[56]\ttrain-error:0.079807\n",
      "[57]\ttrain-error:0.079251\n",
      "[58]\ttrain-error:0.078527\n",
      "[59]\ttrain-error:0.078560\n",
      "[60]\ttrain-error:0.078493\n",
      "[61]\ttrain-error:0.077617\n",
      "[62]\ttrain-error:0.077398\n",
      "[63]\ttrain-error:0.076860\n",
      "[64]\ttrain-error:0.076708\n",
      "[65]\ttrain-error:0.076472\n",
      "[66]\ttrain-error:0.076253\n",
      "[67]\ttrain-error:0.076203\n",
      "[68]\ttrain-error:0.075378\n",
      "[69]\ttrain-error:0.075209\n",
      "[70]\ttrain-error:0.074738\n",
      "[71]\ttrain-error:0.074435\n",
      "[72]\ttrain-error:0.074165\n",
      "[73]\ttrain-error:0.073913\n",
      "[74]\ttrain-error:0.073862\n",
      "[75]\ttrain-error:0.073357\n",
      "[76]\ttrain-error:0.073256\n",
      "[77]\ttrain-error:0.073222\n",
      "[78]\ttrain-error:0.073104\n",
      "[79]\ttrain-error:0.072885\n",
      "[80]\ttrain-error:0.072700\n",
      "[81]\ttrain-error:0.072616\n",
      "[82]\ttrain-error:0.071824\n",
      "[83]\ttrain-error:0.071807\n",
      "[84]\ttrain-error:0.071774\n",
      "[85]\ttrain-error:0.071538\n",
      "[86]\ttrain-error:0.071016\n",
      "[87]\ttrain-error:0.070831\n",
      "[88]\ttrain-error:0.070629\n",
      "[89]\ttrain-error:0.070376\n",
      "[90]\ttrain-error:0.070292\n",
      "[91]\ttrain-error:0.069955\n",
      "[92]\ttrain-error:0.069921\n",
      "[93]\ttrain-error:0.069635\n",
      "[94]\ttrain-error:0.069399\n",
      "[95]\ttrain-error:0.069281\n",
      "[96]\ttrain-error:0.069366\n",
      "[97]\ttrain-error:0.069248\n",
      "[98]\ttrain-error:0.069265\n",
      "[99]\ttrain-error:0.069012\n",
      "[100]\ttrain-error:0.069029\n",
      "[101]\ttrain-error:0.069029\n",
      "[102]\ttrain-error:0.068978\n",
      "[103]\ttrain-error:0.068793\n",
      "[104]\ttrain-error:0.068776\n",
      "[105]\ttrain-error:0.068726\n",
      "[106]\ttrain-error:0.068574\n",
      "[107]\ttrain-error:0.068456\n",
      "[108]\ttrain-error:0.068423\n",
      "[109]\ttrain-error:0.068641\n",
      "[110]\ttrain-error:0.068574\n",
      "[111]\ttrain-error:0.068389\n",
      "[112]\ttrain-error:0.068372\n",
      "[113]\ttrain-error:0.068271\n",
      "[114]\ttrain-error:0.068136\n",
      "[115]\ttrain-error:0.068119\n",
      "[116]\ttrain-error:0.068103\n",
      "[117]\ttrain-error:0.068153\n",
      "[118]\ttrain-error:0.068069\n",
      "[119]\ttrain-error:0.067968\n",
      "[120]\ttrain-error:0.067698\n",
      "[121]\ttrain-error:0.067513\n",
      "[122]\ttrain-error:0.067479\n",
      "[123]\ttrain-error:0.067564\n",
      "[124]\ttrain-error:0.067463\n",
      "[125]\ttrain-error:0.067429\n",
      "[126]\ttrain-error:0.067547\n",
      "[127]\ttrain-error:0.067395\n",
      "[128]\ttrain-error:0.067395\n",
      "[129]\ttrain-error:0.067345\n",
      "[130]\ttrain-error:0.067294\n",
      "[131]\ttrain-error:0.067328\n",
      "[132]\ttrain-error:0.067176\n",
      "[133]\ttrain-error:0.067176\n",
      "[134]\ttrain-error:0.067143\n",
      "[135]\ttrain-error:0.067126\n",
      "[136]\ttrain-error:0.066957\n",
      "[137]\ttrain-error:0.067042\n",
      "[138]\ttrain-error:0.067075\n",
      "[139]\ttrain-error:0.067008\n",
      "[140]\ttrain-error:0.067025\n",
      "[141]\ttrain-error:0.067176\n",
      "[142]\ttrain-error:0.067058\n",
      "[143]\ttrain-error:0.066974\n",
      "[144]\ttrain-error:0.066924\n",
      "[145]\ttrain-error:0.066974\n",
      "[146]\ttrain-error:0.066688\n",
      "[147]\ttrain-error:0.066604\n",
      "[148]\ttrain-error:0.066503\n",
      "[149]\ttrain-error:0.066486\n",
      "[150]\ttrain-error:0.066536\n",
      "[151]\ttrain-error:0.066284\n",
      "[152]\ttrain-error:0.066385\n",
      "[153]\ttrain-error:0.066267\n",
      "[154]\ttrain-error:0.066099\n",
      "[155]\ttrain-error:0.065947\n",
      "[156]\ttrain-error:0.065896\n",
      "[157]\ttrain-error:0.065981\n",
      "[158]\ttrain-error:0.065896\n",
      "[159]\ttrain-error:0.065745\n",
      "[160]\ttrain-error:0.065745\n",
      "[161]\ttrain-error:0.065577\n",
      "[162]\ttrain-error:0.065745\n",
      "[163]\ttrain-error:0.065694\n",
      "[164]\ttrain-error:0.065644\n",
      "[165]\ttrain-error:0.065661\n",
      "[166]\ttrain-error:0.065745\n",
      "[167]\ttrain-error:0.065661\n",
      "[168]\ttrain-error:0.065779\n",
      "[169]\ttrain-error:0.065610\n",
      "[170]\ttrain-error:0.065678\n",
      "[171]\ttrain-error:0.065526\n",
      "[172]\ttrain-error:0.065509\n",
      "[173]\ttrain-error:0.065526\n",
      "[174]\ttrain-error:0.065526\n",
      "[175]\ttrain-error:0.065391\n",
      "[176]\ttrain-error:0.065374\n",
      "[177]\ttrain-error:0.065442\n",
      "[178]\ttrain-error:0.065408\n",
      "[179]\ttrain-error:0.065273\n",
      "[180]\ttrain-error:0.065358\n",
      "[181]\ttrain-error:0.065257\n",
      "[182]\ttrain-error:0.065324\n",
      "[183]\ttrain-error:0.065257\n",
      "[184]\ttrain-error:0.065307\n",
      "[185]\ttrain-error:0.065459\n",
      "[186]\ttrain-error:0.065442\n",
      "[187]\ttrain-error:0.065122\n",
      "[188]\ttrain-error:0.065071\n",
      "[189]\ttrain-error:0.065189\n",
      "[190]\ttrain-error:0.065156\n",
      "[191]\ttrain-error:0.065139\n",
      "[192]\ttrain-error:0.065105\n",
      "[193]\ttrain-error:0.065021\n",
      "[194]\ttrain-error:0.064852\n",
      "[195]\ttrain-error:0.064718\n",
      "[196]\ttrain-error:0.064819\n",
      "[197]\ttrain-error:0.064869\n",
      "[198]\ttrain-error:0.064852\n",
      "[199]\ttrain-error:0.064903\n",
      "[200]\ttrain-error:0.064667\n",
      "[201]\ttrain-error:0.064549\n",
      "[202]\ttrain-error:0.064583\n",
      "[203]\ttrain-error:0.064532\n",
      "[204]\ttrain-error:0.064633\n",
      "[205]\ttrain-error:0.064516\n",
      "[206]\ttrain-error:0.064330\n",
      "[207]\ttrain-error:0.064280\n",
      "[208]\ttrain-error:0.064448\n",
      "[209]\ttrain-error:0.064465\n",
      "[210]\ttrain-error:0.064381\n",
      "[211]\ttrain-error:0.064364\n",
      "[212]\ttrain-error:0.064280\n",
      "[213]\ttrain-error:0.064297\n",
      "[214]\ttrain-error:0.064280\n",
      "[215]\ttrain-error:0.064196\n",
      "[216]\ttrain-error:0.064280\n",
      "[217]\ttrain-error:0.064229\n",
      "[218]\ttrain-error:0.064078\n",
      "[219]\ttrain-error:0.064212\n",
      "[220]\ttrain-error:0.064128\n",
      "[221]\ttrain-error:0.064078\n",
      "[222]\ttrain-error:0.063943\n",
      "[223]\ttrain-error:0.063775\n",
      "[224]\ttrain-error:0.063960\n",
      "[225]\ttrain-error:0.063808\n",
      "[226]\ttrain-error:0.063926\n",
      "[227]\ttrain-error:0.064044\n",
      "[228]\ttrain-error:0.063926\n",
      "[229]\ttrain-error:0.064061\n",
      "[230]\ttrain-error:0.063876\n",
      "[231]\ttrain-error:0.063892\n",
      "[232]\ttrain-error:0.063741\n",
      "[233]\ttrain-error:0.063606\n",
      "[234]\ttrain-error:0.063522\n",
      "[235]\ttrain-error:0.063657\n",
      "[236]\ttrain-error:0.063640\n",
      "[237]\ttrain-error:0.063556\n",
      "[238]\ttrain-error:0.063623\n",
      "[239]\ttrain-error:0.063556\n",
      "[240]\ttrain-error:0.063539\n",
      "[241]\ttrain-error:0.063455\n",
      "[242]\ttrain-error:0.063404\n",
      "[243]\ttrain-error:0.063471\n",
      "[244]\ttrain-error:0.063253\n",
      "[245]\ttrain-error:0.063337\n",
      "[246]\ttrain-error:0.063303\n",
      "[247]\ttrain-error:0.063185\n",
      "[248]\ttrain-error:0.063185\n",
      "[249]\ttrain-error:0.063286\n",
      "[250]\ttrain-error:0.063101\n",
      "[251]\ttrain-error:0.063050\n",
      "[252]\ttrain-error:0.062983\n",
      "[253]\ttrain-error:0.062916\n",
      "[254]\ttrain-error:0.062848\n",
      "[255]\ttrain-error:0.062865\n",
      "[256]\ttrain-error:0.062747\n",
      "[257]\ttrain-error:0.062899\n",
      "[258]\ttrain-error:0.062764\n",
      "[259]\ttrain-error:0.062714\n",
      "[260]\ttrain-error:0.062764\n",
      "[261]\ttrain-error:0.062461\n",
      "[262]\ttrain-error:0.062512\n",
      "[263]\ttrain-error:0.062360\n",
      "[264]\ttrain-error:0.062444\n",
      "[265]\ttrain-error:0.062377\n",
      "[266]\ttrain-error:0.062461\n",
      "[267]\ttrain-error:0.062309\n",
      "[268]\ttrain-error:0.062225\n",
      "[269]\ttrain-error:0.062360\n",
      "[270]\ttrain-error:0.062326\n",
      "[271]\ttrain-error:0.062326\n",
      "[272]\ttrain-error:0.062394\n",
      "[273]\ttrain-error:0.062360\n",
      "[274]\ttrain-error:0.062394\n",
      "[275]\ttrain-error:0.062461\n",
      "[276]\ttrain-error:0.062394\n",
      "[277]\ttrain-error:0.062293\n",
      "[278]\ttrain-error:0.062175\n",
      "[279]\ttrain-error:0.062040\n",
      "[280]\ttrain-error:0.061990\n",
      "[281]\ttrain-error:0.062074\n",
      "[282]\ttrain-error:0.062091\n",
      "[283]\ttrain-error:0.061855\n",
      "[284]\ttrain-error:0.061973\n",
      "[285]\ttrain-error:0.061956\n",
      "[286]\ttrain-error:0.061922\n",
      "[287]\ttrain-error:0.061973\n",
      "[288]\ttrain-error:0.062057\n",
      "[289]\ttrain-error:0.061990\n",
      "[290]\ttrain-error:0.062074\n",
      "[291]\ttrain-error:0.061838\n",
      "[292]\ttrain-error:0.061905\n",
      "[293]\ttrain-error:0.061686\n",
      "[294]\ttrain-error:0.061434\n",
      "[295]\ttrain-error:0.061670\n",
      "[296]\ttrain-error:0.061636\n",
      "[297]\ttrain-error:0.061670\n",
      "[298]\ttrain-error:0.061686\n",
      "[299]\ttrain-error:0.061569\n",
      "[300]\ttrain-error:0.061535\n",
      "[301]\ttrain-error:0.061552\n",
      "[302]\ttrain-error:0.061552\n",
      "[303]\ttrain-error:0.061552\n",
      "[304]\ttrain-error:0.061467\n",
      "[305]\ttrain-error:0.061366\n",
      "[306]\ttrain-error:0.061232\n",
      "[307]\ttrain-error:0.061316\n",
      "[308]\ttrain-error:0.061181\n",
      "[309]\ttrain-error:0.060962\n",
      "[310]\ttrain-error:0.060996\n",
      "[311]\ttrain-error:0.060726\n",
      "[312]\ttrain-error:0.060844\n",
      "[313]\ttrain-error:0.060794\n",
      "[314]\ttrain-error:0.060794\n",
      "[315]\ttrain-error:0.060794\n",
      "[316]\ttrain-error:0.060592\n",
      "[317]\ttrain-error:0.060828\n",
      "[318]\ttrain-error:0.060861\n",
      "[319]\ttrain-error:0.060878\n",
      "[320]\ttrain-error:0.060861\n",
      "[321]\ttrain-error:0.060945\n",
      "[322]\ttrain-error:0.060878\n",
      "[323]\ttrain-error:0.060726\n",
      "[324]\ttrain-error:0.060811\n",
      "[325]\ttrain-error:0.060828\n",
      "[326]\ttrain-error:0.060710\n",
      "[327]\ttrain-error:0.060642\n",
      "[328]\ttrain-error:0.060541\n",
      "[329]\ttrain-error:0.060726\n",
      "[330]\ttrain-error:0.060726\n",
      "[331]\ttrain-error:0.060659\n",
      "[332]\ttrain-error:0.060743\n",
      "[333]\ttrain-error:0.060861\n",
      "[334]\ttrain-error:0.060777\n",
      "[335]\ttrain-error:0.060625\n",
      "[336]\ttrain-error:0.060811\n",
      "[337]\ttrain-error:0.060811\n",
      "[338]\ttrain-error:0.060811\n",
      "[339]\ttrain-error:0.060592\n",
      "[340]\ttrain-error:0.060491\n",
      "[341]\ttrain-error:0.060592\n",
      "[342]\ttrain-error:0.060541\n",
      "[343]\ttrain-error:0.060356\n",
      "[344]\ttrain-error:0.060592\n",
      "[345]\ttrain-error:0.060339\n",
      "[346]\ttrain-error:0.060238\n",
      "[347]\ttrain-error:0.060120\n",
      "[348]\ttrain-error:0.060087\n",
      "[349]\ttrain-error:0.060154\n",
      "[350]\ttrain-error:0.060087\n",
      "[351]\ttrain-error:0.060019\n",
      "[352]\ttrain-error:0.060137\n",
      "[353]\ttrain-error:0.059935\n",
      "[354]\ttrain-error:0.060019\n",
      "[355]\ttrain-error:0.060070\n",
      "[356]\ttrain-error:0.059952\n",
      "[357]\ttrain-error:0.059834\n",
      "[358]\ttrain-error:0.059733\n",
      "[359]\ttrain-error:0.059834\n",
      "[360]\ttrain-error:0.059817\n",
      "[361]\ttrain-error:0.059733\n",
      "[362]\ttrain-error:0.059699\n",
      "[363]\ttrain-error:0.059632\n",
      "[364]\ttrain-error:0.059666\n",
      "[365]\ttrain-error:0.059531\n",
      "[366]\ttrain-error:0.059800\n",
      "[367]\ttrain-error:0.059581\n",
      "[368]\ttrain-error:0.059514\n",
      "[369]\ttrain-error:0.059565\n",
      "[370]\ttrain-error:0.059396\n",
      "[371]\ttrain-error:0.059581\n",
      "[372]\ttrain-error:0.059514\n",
      "[373]\ttrain-error:0.059531\n",
      "[374]\ttrain-error:0.059514\n",
      "[375]\ttrain-error:0.059581\n",
      "[376]\ttrain-error:0.059497\n",
      "[377]\ttrain-error:0.059480\n",
      "[378]\ttrain-error:0.059666\n",
      "[379]\ttrain-error:0.059480\n",
      "[380]\ttrain-error:0.059413\n",
      "[381]\ttrain-error:0.059480\n",
      "[382]\ttrain-error:0.059278\n",
      "[383]\ttrain-error:0.059160\n",
      "[384]\ttrain-error:0.059042\n",
      "[385]\ttrain-error:0.059026\n",
      "[386]\ttrain-error:0.059211\n",
      "[387]\ttrain-error:0.059127\n",
      "[388]\ttrain-error:0.059177\n",
      "[389]\ttrain-error:0.058958\n",
      "[390]\ttrain-error:0.059076\n",
      "[391]\ttrain-error:0.059076\n",
      "[392]\ttrain-error:0.058891\n",
      "[393]\ttrain-error:0.058807\n",
      "[394]\ttrain-error:0.058891\n",
      "[395]\ttrain-error:0.058958\n",
      "[396]\ttrain-error:0.059009\n",
      "[397]\ttrain-error:0.058975\n",
      "[398]\ttrain-error:0.058857\n",
      "[399]\ttrain-error:0.058891\n",
      "[400]\ttrain-error:0.058840\n",
      "[401]\ttrain-error:0.058874\n",
      "[402]\ttrain-error:0.058908\n",
      "[403]\ttrain-error:0.058925\n",
      "[404]\ttrain-error:0.058840\n",
      "[405]\ttrain-error:0.058958\n",
      "[406]\ttrain-error:0.058706\n",
      "[407]\ttrain-error:0.058706\n",
      "[408]\ttrain-error:0.058807\n",
      "[409]\ttrain-error:0.058655\n",
      "[410]\ttrain-error:0.058638\n",
      "[411]\ttrain-error:0.058722\n",
      "[412]\ttrain-error:0.058756\n",
      "[413]\ttrain-error:0.058621\n",
      "[414]\ttrain-error:0.058487\n",
      "[415]\ttrain-error:0.058520\n",
      "[416]\ttrain-error:0.058605\n",
      "[417]\ttrain-error:0.058453\n",
      "[418]\ttrain-error:0.058504\n",
      "[419]\ttrain-error:0.058470\n",
      "[420]\ttrain-error:0.058436\n",
      "[421]\ttrain-error:0.058369\n",
      "[422]\ttrain-error:0.058436\n",
      "[423]\ttrain-error:0.058234\n",
      "[424]\ttrain-error:0.058251\n",
      "[425]\ttrain-error:0.058049\n",
      "[426]\ttrain-error:0.058099\n",
      "[427]\ttrain-error:0.057982\n",
      "[428]\ttrain-error:0.058015\n",
      "[429]\ttrain-error:0.058099\n",
      "[430]\ttrain-error:0.057948\n",
      "[431]\ttrain-error:0.057931\n",
      "[432]\ttrain-error:0.057830\n",
      "[433]\ttrain-error:0.057880\n",
      "[434]\ttrain-error:0.057746\n",
      "[435]\ttrain-error:0.057796\n",
      "[436]\ttrain-error:0.057880\n",
      "[437]\ttrain-error:0.057813\n",
      "[438]\ttrain-error:0.057746\n",
      "[439]\ttrain-error:0.057763\n",
      "[440]\ttrain-error:0.057847\n",
      "[441]\ttrain-error:0.057880\n",
      "[442]\ttrain-error:0.057662\n",
      "[443]\ttrain-error:0.057779\n",
      "[444]\ttrain-error:0.057746\n",
      "[445]\ttrain-error:0.057662\n",
      "[446]\ttrain-error:0.057695\n",
      "[447]\ttrain-error:0.057577\n",
      "[448]\ttrain-error:0.057611\n",
      "[449]\ttrain-error:0.057746\n",
      "[450]\ttrain-error:0.057695\n",
      "[451]\ttrain-error:0.057830\n",
      "[452]\ttrain-error:0.057527\n",
      "[453]\ttrain-error:0.057544\n",
      "[454]\ttrain-error:0.057476\n",
      "[455]\ttrain-error:0.057409\n",
      "[456]\ttrain-error:0.057257\n",
      "[457]\ttrain-error:0.057207\n",
      "[458]\ttrain-error:0.057139\n",
      "[459]\ttrain-error:0.057123\n",
      "[460]\ttrain-error:0.057139\n",
      "[461]\ttrain-error:0.057224\n",
      "[462]\ttrain-error:0.057274\n",
      "[463]\ttrain-error:0.057325\n",
      "[464]\ttrain-error:0.057325\n",
      "[465]\ttrain-error:0.057190\n",
      "[466]\ttrain-error:0.057325\n",
      "[467]\ttrain-error:0.057207\n",
      "[468]\ttrain-error:0.057139\n",
      "[469]\ttrain-error:0.057156\n",
      "[470]\ttrain-error:0.057139\n",
      "[471]\ttrain-error:0.057139\n",
      "[472]\ttrain-error:0.057123\n",
      "[473]\ttrain-error:0.057038\n",
      "[474]\ttrain-error:0.056769\n",
      "[475]\ttrain-error:0.056617\n",
      "[476]\ttrain-error:0.056601\n",
      "[477]\ttrain-error:0.056769\n",
      "[478]\ttrain-error:0.056617\n",
      "[479]\ttrain-error:0.056685\n",
      "[480]\ttrain-error:0.056617\n",
      "[481]\ttrain-error:0.056601\n",
      "[482]\ttrain-error:0.056516\n",
      "[483]\ttrain-error:0.056533\n",
      "[484]\ttrain-error:0.056651\n",
      "[485]\ttrain-error:0.056415\n",
      "[486]\ttrain-error:0.056466\n",
      "[487]\ttrain-error:0.056516\n",
      "[488]\ttrain-error:0.056483\n",
      "[489]\ttrain-error:0.056500\n",
      "[490]\ttrain-error:0.056365\n",
      "[491]\ttrain-error:0.056281\n",
      "[492]\ttrain-error:0.056230\n",
      "[493]\ttrain-error:0.056264\n",
      "[494]\ttrain-error:0.056331\n",
      "[495]\ttrain-error:0.056264\n",
      "[496]\ttrain-error:0.056079\n",
      "[497]\ttrain-error:0.056163\n",
      "[498]\ttrain-error:0.056196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[499]\ttrain-error:0.056196\n",
      "[0]\ttrain-error:0.185733\n",
      "[1]\ttrain-error:0.186221\n",
      "[2]\ttrain-error:0.185699\n",
      "[3]\ttrain-error:0.186188\n",
      "[4]\ttrain-error:0.185918\n",
      "[5]\ttrain-error:0.185750\n",
      "[6]\ttrain-error:0.186305\n",
      "[7]\ttrain-error:0.186103\n",
      "[8]\ttrain-error:0.186743\n",
      "[9]\ttrain-error:0.186036\n",
      "[10]\ttrain-error:0.186474\n",
      "[11]\ttrain-error:0.187568\n",
      "[12]\ttrain-error:0.187265\n",
      "[13]\ttrain-error:0.187922\n",
      "[14]\ttrain-error:0.186912\n",
      "[15]\ttrain-error:0.186221\n",
      "[16]\ttrain-error:0.185985\n",
      "[17]\ttrain-error:0.186390\n",
      "[18]\ttrain-error:0.186204\n",
      "[19]\ttrain-error:0.186188\n",
      "[20]\ttrain-error:0.186036\n",
      "[21]\ttrain-error:0.185834\n",
      "[22]\ttrain-error:0.185665\n",
      "[23]\ttrain-error:0.185750\n",
      "[24]\ttrain-error:0.185851\n",
      "[25]\ttrain-error:0.184840\n",
      "[26]\ttrain-error:0.184588\n",
      "[27]\ttrain-error:0.184689\n",
      "[28]\ttrain-error:0.184487\n",
      "[29]\ttrain-error:0.184268\n",
      "[30]\ttrain-error:0.184335\n",
      "[31]\ttrain-error:0.183864\n",
      "[32]\ttrain-error:0.183678\n",
      "[33]\ttrain-error:0.182584\n",
      "[34]\ttrain-error:0.181203\n",
      "[35]\ttrain-error:0.181253\n",
      "[36]\ttrain-error:0.181186\n",
      "[37]\ttrain-error:0.181169\n",
      "[38]\ttrain-error:0.181186\n",
      "[39]\ttrain-error:0.179586\n",
      "[40]\ttrain-error:0.179115\n",
      "[41]\ttrain-error:0.178896\n",
      "[42]\ttrain-error:0.178475\n",
      "[43]\ttrain-error:0.178626\n",
      "[44]\ttrain-error:0.178037\n",
      "[45]\ttrain-error:0.177767\n",
      "[46]\ttrain-error:0.177498\n",
      "[47]\ttrain-error:0.177060\n",
      "[48]\ttrain-error:0.176841\n",
      "[49]\ttrain-error:0.176757\n",
      "[50]\ttrain-error:0.176487\n",
      "[51]\ttrain-error:0.175595\n",
      "[52]\ttrain-error:0.175426\n",
      "[53]\ttrain-error:0.175359\n",
      "[54]\ttrain-error:0.175359\n",
      "[55]\ttrain-error:0.174904\n",
      "[56]\ttrain-error:0.174568\n",
      "[57]\ttrain-error:0.174382\n",
      "[58]\ttrain-error:0.174450\n",
      "[59]\ttrain-error:0.174315\n",
      "[60]\ttrain-error:0.173658\n",
      "[61]\ttrain-error:0.173608\n",
      "[62]\ttrain-error:0.172833\n",
      "[63]\ttrain-error:0.172648\n",
      "[64]\ttrain-error:0.172008\n",
      "[65]\ttrain-error:0.172193\n",
      "[66]\ttrain-error:0.171974\n",
      "[67]\ttrain-error:0.171856\n",
      "[68]\ttrain-error:0.171486\n",
      "[69]\ttrain-error:0.171216\n",
      "[70]\ttrain-error:0.170846\n",
      "[71]\ttrain-error:0.170004\n",
      "[72]\ttrain-error:0.169650\n",
      "[73]\ttrain-error:0.169111\n",
      "[74]\ttrain-error:0.168876\n",
      "[75]\ttrain-error:0.168673\n",
      "[76]\ttrain-error:0.168202\n",
      "[77]\ttrain-error:0.167882\n",
      "[78]\ttrain-error:0.167394\n",
      "[79]\ttrain-error:0.167360\n",
      "[80]\ttrain-error:0.167192\n",
      "[81]\ttrain-error:0.167175\n",
      "[82]\ttrain-error:0.166838\n",
      "[83]\ttrain-error:0.166720\n",
      "[84]\ttrain-error:0.166383\n",
      "[85]\ttrain-error:0.166080\n",
      "[86]\ttrain-error:0.165996\n",
      "[87]\ttrain-error:0.165861\n",
      "[88]\ttrain-error:0.165575\n",
      "[89]\ttrain-error:0.165255\n",
      "[90]\ttrain-error:0.165154\n",
      "[91]\ttrain-error:0.164969\n",
      "[92]\ttrain-error:0.165086\n",
      "[93]\ttrain-error:0.164682\n",
      "[94]\ttrain-error:0.164480\n",
      "[95]\ttrain-error:0.163790\n",
      "[96]\ttrain-error:0.163773\n",
      "[97]\ttrain-error:0.163790\n",
      "[98]\ttrain-error:0.163520\n",
      "[99]\ttrain-error:0.163150\n",
      "[100]\ttrain-error:0.161516\n",
      "[101]\ttrain-error:0.161078\n",
      "[102]\ttrain-error:0.161045\n",
      "[103]\ttrain-error:0.161196\n",
      "[104]\ttrain-error:0.161129\n",
      "[105]\ttrain-error:0.160961\n",
      "[106]\ttrain-error:0.160809\n",
      "[107]\ttrain-error:0.160758\n",
      "[108]\ttrain-error:0.160977\n",
      "[109]\ttrain-error:0.160657\n",
      "[110]\ttrain-error:0.159849\n",
      "[111]\ttrain-error:0.159630\n",
      "[112]\ttrain-error:0.159226\n",
      "[113]\ttrain-error:0.159378\n",
      "[114]\ttrain-error:0.158906\n",
      "[115]\ttrain-error:0.158872\n",
      "[116]\ttrain-error:0.158788\n",
      "[117]\ttrain-error:0.158384\n",
      "[118]\ttrain-error:0.158283\n",
      "[119]\ttrain-error:0.158333\n",
      "[120]\ttrain-error:0.158317\n",
      "[121]\ttrain-error:0.158199\n",
      "[122]\ttrain-error:0.158081\n",
      "[123]\ttrain-error:0.157912\n",
      "[124]\ttrain-error:0.157542\n",
      "[125]\ttrain-error:0.157424\n",
      "[126]\ttrain-error:0.157239\n",
      "[127]\ttrain-error:0.157205\n",
      "[128]\ttrain-error:0.157070\n",
      "[129]\ttrain-error:0.157087\n",
      "[130]\ttrain-error:0.156902\n",
      "[131]\ttrain-error:0.156717\n",
      "[132]\ttrain-error:0.156397\n",
      "[133]\ttrain-error:0.156616\n",
      "[134]\ttrain-error:0.156329\n",
      "[135]\ttrain-error:0.155993\n",
      "[136]\ttrain-error:0.156178\n",
      "[137]\ttrain-error:0.156111\n",
      "[138]\ttrain-error:0.155841\n",
      "[139]\ttrain-error:0.155521\n",
      "[140]\ttrain-error:0.155555\n",
      "[141]\ttrain-error:0.155235\n",
      "[142]\ttrain-error:0.154982\n",
      "[143]\ttrain-error:0.154780\n",
      "[144]\ttrain-error:0.154864\n",
      "[145]\ttrain-error:0.154915\n",
      "[146]\ttrain-error:0.154393\n",
      "[147]\ttrain-error:0.154410\n",
      "[148]\ttrain-error:0.154258\n",
      "[149]\ttrain-error:0.153955\n",
      "[150]\ttrain-error:0.153871\n",
      "[151]\ttrain-error:0.153601\n",
      "[152]\ttrain-error:0.153568\n",
      "[153]\ttrain-error:0.153332\n",
      "[154]\ttrain-error:0.153046\n",
      "[155]\ttrain-error:0.152877\n",
      "[156]\ttrain-error:0.152625\n",
      "[157]\ttrain-error:0.152641\n",
      "[158]\ttrain-error:0.152490\n",
      "[159]\ttrain-error:0.152187\n",
      "[160]\ttrain-error:0.151951\n",
      "[161]\ttrain-error:0.151850\n",
      "[162]\ttrain-error:0.151648\n",
      "[163]\ttrain-error:0.151446\n",
      "[164]\ttrain-error:0.151479\n",
      "[165]\ttrain-error:0.151193\n",
      "[166]\ttrain-error:0.150991\n",
      "[167]\ttrain-error:0.151075\n",
      "[168]\ttrain-error:0.150755\n",
      "[169]\ttrain-error:0.150671\n",
      "[170]\ttrain-error:0.150621\n",
      "[171]\ttrain-error:0.150536\n",
      "[172]\ttrain-error:0.150520\n",
      "[173]\ttrain-error:0.150334\n",
      "[174]\ttrain-error:0.150233\n",
      "[175]\ttrain-error:0.149896\n",
      "[176]\ttrain-error:0.149997\n",
      "[177]\ttrain-error:0.149846\n",
      "[178]\ttrain-error:0.149661\n",
      "[179]\ttrain-error:0.149779\n",
      "[180]\ttrain-error:0.149694\n",
      "[181]\ttrain-error:0.149576\n",
      "[182]\ttrain-error:0.149492\n",
      "[183]\ttrain-error:0.149425\n",
      "[184]\ttrain-error:0.149391\n",
      "[185]\ttrain-error:0.149290\n",
      "[186]\ttrain-error:0.149240\n",
      "[187]\ttrain-error:0.149223\n",
      "[188]\ttrain-error:0.149189\n",
      "[189]\ttrain-error:0.149206\n",
      "[190]\ttrain-error:0.148953\n",
      "[191]\ttrain-error:0.148869\n",
      "[192]\ttrain-error:0.148414\n",
      "[193]\ttrain-error:0.148229\n",
      "[194]\ttrain-error:0.148229\n",
      "[195]\ttrain-error:0.148162\n",
      "[196]\ttrain-error:0.148330\n",
      "[197]\ttrain-error:0.148061\n",
      "[198]\ttrain-error:0.148179\n",
      "[199]\ttrain-error:0.148162\n",
      "[200]\ttrain-error:0.147471\n",
      "[201]\ttrain-error:0.147657\n",
      "[202]\ttrain-error:0.147522\n",
      "[203]\ttrain-error:0.147522\n",
      "[204]\ttrain-error:0.147370\n",
      "[205]\ttrain-error:0.147337\n",
      "[206]\ttrain-error:0.147370\n",
      "[207]\ttrain-error:0.147168\n",
      "[208]\ttrain-error:0.147286\n",
      "[209]\ttrain-error:0.146933\n",
      "[210]\ttrain-error:0.146730\n",
      "[211]\ttrain-error:0.146562\n",
      "[212]\ttrain-error:0.146596\n",
      "[213]\ttrain-error:0.146646\n",
      "[214]\ttrain-error:0.146512\n",
      "[215]\ttrain-error:0.146394\n",
      "[216]\ttrain-error:0.146461\n",
      "[217]\ttrain-error:0.146276\n",
      "[218]\ttrain-error:0.146225\n",
      "[219]\ttrain-error:0.146242\n",
      "[220]\ttrain-error:0.146360\n",
      "[221]\ttrain-error:0.146091\n",
      "[222]\ttrain-error:0.146040\n",
      "[223]\ttrain-error:0.145989\n",
      "[224]\ttrain-error:0.146091\n",
      "[225]\ttrain-error:0.146057\n",
      "[226]\ttrain-error:0.145989\n",
      "[227]\ttrain-error:0.145821\n",
      "[228]\ttrain-error:0.145771\n",
      "[229]\ttrain-error:0.145501\n",
      "[230]\ttrain-error:0.145501\n",
      "[231]\ttrain-error:0.145063\n",
      "[232]\ttrain-error:0.145248\n",
      "[233]\ttrain-error:0.145198\n",
      "[234]\ttrain-error:0.145164\n",
      "[235]\ttrain-error:0.145046\n",
      "[236]\ttrain-error:0.144962\n",
      "[237]\ttrain-error:0.144996\n",
      "[238]\ttrain-error:0.144945\n",
      "[239]\ttrain-error:0.144895\n",
      "[240]\ttrain-error:0.144844\n",
      "[241]\ttrain-error:0.144693\n",
      "[242]\ttrain-error:0.144726\n",
      "[243]\ttrain-error:0.144676\n",
      "[244]\ttrain-error:0.144541\n",
      "[245]\ttrain-error:0.144693\n",
      "[246]\ttrain-error:0.144524\n",
      "[247]\ttrain-error:0.144423\n",
      "[248]\ttrain-error:0.144238\n",
      "[249]\ttrain-error:0.144255\n",
      "[250]\ttrain-error:0.144390\n",
      "[251]\ttrain-error:0.144086\n",
      "[252]\ttrain-error:0.143918\n",
      "[253]\ttrain-error:0.143767\n",
      "[254]\ttrain-error:0.143682\n",
      "[255]\ttrain-error:0.143598\n",
      "[256]\ttrain-error:0.143665\n",
      "[257]\ttrain-error:0.143783\n",
      "[258]\ttrain-error:0.143716\n",
      "[259]\ttrain-error:0.143413\n",
      "[260]\ttrain-error:0.143177\n",
      "[261]\ttrain-error:0.143295\n",
      "[262]\ttrain-error:0.142706\n",
      "[263]\ttrain-error:0.142722\n",
      "[264]\ttrain-error:0.142638\n",
      "[265]\ttrain-error:0.142335\n",
      "[266]\ttrain-error:0.142369\n",
      "[267]\ttrain-error:0.142318\n",
      "[268]\ttrain-error:0.142133\n",
      "[269]\ttrain-error:0.142251\n",
      "[270]\ttrain-error:0.141729\n",
      "[271]\ttrain-error:0.142099\n",
      "[272]\ttrain-error:0.141897\n",
      "[273]\ttrain-error:0.141729\n",
      "[274]\ttrain-error:0.141830\n",
      "[275]\ttrain-error:0.141678\n",
      "[276]\ttrain-error:0.141712\n",
      "[277]\ttrain-error:0.141779\n",
      "[278]\ttrain-error:0.141560\n",
      "[279]\ttrain-error:0.141628\n",
      "[280]\ttrain-error:0.141527\n",
      "[281]\ttrain-error:0.141426\n",
      "[282]\ttrain-error:0.141426\n",
      "[283]\ttrain-error:0.141476\n",
      "[284]\ttrain-error:0.141493\n",
      "[285]\ttrain-error:0.141224\n",
      "[286]\ttrain-error:0.140937\n",
      "[287]\ttrain-error:0.141190\n",
      "[288]\ttrain-error:0.141156\n",
      "[289]\ttrain-error:0.141224\n",
      "[290]\ttrain-error:0.141224\n",
      "[291]\ttrain-error:0.141173\n",
      "[292]\ttrain-error:0.141005\n",
      "[293]\ttrain-error:0.140786\n",
      "[294]\ttrain-error:0.140920\n",
      "[295]\ttrain-error:0.140920\n",
      "[296]\ttrain-error:0.140752\n",
      "[297]\ttrain-error:0.140533\n",
      "[298]\ttrain-error:0.140449\n",
      "[299]\ttrain-error:0.140483\n",
      "[300]\ttrain-error:0.140281\n",
      "[301]\ttrain-error:0.140314\n",
      "[302]\ttrain-error:0.140230\n",
      "[303]\ttrain-error:0.140213\n",
      "[304]\ttrain-error:0.140264\n",
      "[305]\ttrain-error:0.140045\n",
      "[306]\ttrain-error:0.139961\n",
      "[307]\ttrain-error:0.140230\n",
      "[308]\ttrain-error:0.139708\n",
      "[309]\ttrain-error:0.139607\n",
      "[310]\ttrain-error:0.139708\n",
      "[311]\ttrain-error:0.139775\n",
      "[312]\ttrain-error:0.139742\n",
      "[313]\ttrain-error:0.139691\n",
      "[314]\ttrain-error:0.139506\n",
      "[315]\ttrain-error:0.139455\n",
      "[316]\ttrain-error:0.139253\n",
      "[317]\ttrain-error:0.139253\n",
      "[318]\ttrain-error:0.139102\n",
      "[319]\ttrain-error:0.138916\n",
      "[320]\ttrain-error:0.139085\n",
      "[321]\ttrain-error:0.139068\n",
      "[322]\ttrain-error:0.139152\n",
      "[323]\ttrain-error:0.138967\n",
      "[324]\ttrain-error:0.139001\n",
      "[325]\ttrain-error:0.138933\n",
      "[326]\ttrain-error:0.138849\n",
      "[327]\ttrain-error:0.138613\n",
      "[328]\ttrain-error:0.138344\n",
      "[329]\ttrain-error:0.138597\n",
      "[330]\ttrain-error:0.138260\n",
      "[331]\ttrain-error:0.138209\n",
      "[332]\ttrain-error:0.138226\n",
      "[333]\ttrain-error:0.138310\n",
      "[334]\ttrain-error:0.138260\n",
      "[335]\ttrain-error:0.138058\n",
      "[336]\ttrain-error:0.138260\n",
      "[337]\ttrain-error:0.138125\n",
      "[338]\ttrain-error:0.138007\n",
      "[339]\ttrain-error:0.138108\n",
      "[340]\ttrain-error:0.137973\n",
      "[341]\ttrain-error:0.137889\n",
      "[342]\ttrain-error:0.137805\n",
      "[343]\ttrain-error:0.137670\n",
      "[344]\ttrain-error:0.137687\n",
      "[345]\ttrain-error:0.137451\n",
      "[346]\ttrain-error:0.137485\n",
      "[347]\ttrain-error:0.137317\n",
      "[348]\ttrain-error:0.137249\n",
      "[349]\ttrain-error:0.136997\n",
      "[350]\ttrain-error:0.137266\n",
      "[351]\ttrain-error:0.137333\n",
      "[352]\ttrain-error:0.137350\n",
      "[353]\ttrain-error:0.137115\n",
      "[354]\ttrain-error:0.137115\n",
      "[355]\ttrain-error:0.137317\n",
      "[356]\ttrain-error:0.137418\n",
      "[357]\ttrain-error:0.136946\n",
      "[358]\ttrain-error:0.136778\n",
      "[359]\ttrain-error:0.136626\n",
      "[360]\ttrain-error:0.136390\n",
      "[361]\ttrain-error:0.136239\n",
      "[362]\ttrain-error:0.136407\n",
      "[363]\ttrain-error:0.136172\n",
      "[364]\ttrain-error:0.136323\n",
      "[365]\ttrain-error:0.136205\n",
      "[366]\ttrain-error:0.136155\n",
      "[367]\ttrain-error:0.136138\n",
      "[368]\ttrain-error:0.135852\n",
      "[369]\ttrain-error:0.135784\n",
      "[370]\ttrain-error:0.135784\n",
      "[371]\ttrain-error:0.135599\n",
      "[372]\ttrain-error:0.135498\n",
      "[373]\ttrain-error:0.135127\n",
      "[374]\ttrain-error:0.135195\n",
      "[375]\ttrain-error:0.135212\n",
      "[376]\ttrain-error:0.135127\n",
      "[377]\ttrain-error:0.135111\n",
      "[378]\ttrain-error:0.134993\n",
      "[379]\ttrain-error:0.135178\n",
      "[380]\ttrain-error:0.134858\n",
      "[381]\ttrain-error:0.134690\n",
      "[382]\ttrain-error:0.134723\n",
      "[383]\ttrain-error:0.134504\n",
      "[384]\ttrain-error:0.134589\n",
      "[385]\ttrain-error:0.134386\n",
      "[386]\ttrain-error:0.134386\n",
      "[387]\ttrain-error:0.134521\n",
      "[388]\ttrain-error:0.134218\n",
      "[389]\ttrain-error:0.134471\n",
      "[390]\ttrain-error:0.134555\n",
      "[391]\ttrain-error:0.134639\n",
      "[392]\ttrain-error:0.134572\n",
      "[393]\ttrain-error:0.134471\n",
      "[394]\ttrain-error:0.134420\n",
      "[395]\ttrain-error:0.134336\n",
      "[396]\ttrain-error:0.134033\n",
      "[397]\ttrain-error:0.134050\n",
      "[398]\ttrain-error:0.134134\n",
      "[399]\ttrain-error:0.134302\n",
      "[400]\ttrain-error:0.134151\n",
      "[401]\ttrain-error:0.134016\n",
      "[402]\ttrain-error:0.134083\n",
      "[403]\ttrain-error:0.133814\n",
      "[404]\ttrain-error:0.133999\n",
      "[405]\ttrain-error:0.134050\n",
      "[406]\ttrain-error:0.134016\n",
      "[407]\ttrain-error:0.133713\n",
      "[408]\ttrain-error:0.133561\n",
      "[409]\ttrain-error:0.133460\n",
      "[410]\ttrain-error:0.133376\n",
      "[411]\ttrain-error:0.133393\n",
      "[412]\ttrain-error:0.133208\n",
      "[413]\ttrain-error:0.133140\n",
      "[414]\ttrain-error:0.133073\n",
      "[415]\ttrain-error:0.133174\n",
      "[416]\ttrain-error:0.133325\n",
      "[417]\ttrain-error:0.133275\n",
      "[418]\ttrain-error:0.133208\n",
      "[419]\ttrain-error:0.133241\n",
      "[420]\ttrain-error:0.133275\n",
      "[421]\ttrain-error:0.133393\n",
      "[422]\ttrain-error:0.133157\n",
      "[423]\ttrain-error:0.133208\n",
      "[424]\ttrain-error:0.133107\n",
      "[425]\ttrain-error:0.132803\n",
      "[426]\ttrain-error:0.132702\n",
      "[427]\ttrain-error:0.132584\n",
      "[428]\ttrain-error:0.132551\n",
      "[429]\ttrain-error:0.132450\n",
      "[430]\ttrain-error:0.132163\n",
      "[431]\ttrain-error:0.132298\n",
      "[432]\ttrain-error:0.132349\n",
      "[433]\ttrain-error:0.132113\n",
      "[434]\ttrain-error:0.132062\n",
      "[435]\ttrain-error:0.132130\n",
      "[436]\ttrain-error:0.132113\n",
      "[437]\ttrain-error:0.132096\n",
      "[438]\ttrain-error:0.132197\n",
      "[439]\ttrain-error:0.132265\n",
      "[440]\ttrain-error:0.132130\n",
      "[441]\ttrain-error:0.132012\n",
      "[442]\ttrain-error:0.131793\n",
      "[443]\ttrain-error:0.131709\n",
      "[444]\ttrain-error:0.131507\n",
      "[445]\ttrain-error:0.131423\n",
      "[446]\ttrain-error:0.131069\n",
      "[447]\ttrain-error:0.131305\n",
      "[448]\ttrain-error:0.131237\n",
      "[449]\ttrain-error:0.131254\n",
      "[450]\ttrain-error:0.131321\n",
      "[451]\ttrain-error:0.131103\n",
      "[452]\ttrain-error:0.131305\n",
      "[453]\ttrain-error:0.131423\n",
      "[454]\ttrain-error:0.131170\n",
      "[455]\ttrain-error:0.131372\n",
      "[456]\ttrain-error:0.131355\n",
      "[457]\ttrain-error:0.131338\n",
      "[458]\ttrain-error:0.131372\n",
      "[459]\ttrain-error:0.131103\n",
      "[460]\ttrain-error:0.131288\n",
      "[461]\ttrain-error:0.131136\n",
      "[462]\ttrain-error:0.130985\n",
      "[463]\ttrain-error:0.131035\n",
      "[464]\ttrain-error:0.130850\n",
      "[465]\ttrain-error:0.130783\n",
      "[466]\ttrain-error:0.130530\n",
      "[467]\ttrain-error:0.130682\n",
      "[468]\ttrain-error:0.130631\n",
      "[469]\ttrain-error:0.130446\n",
      "[470]\ttrain-error:0.130412\n",
      "[471]\ttrain-error:0.130597\n",
      "[472]\ttrain-error:0.130698\n",
      "[473]\ttrain-error:0.130682\n",
      "[474]\ttrain-error:0.130378\n",
      "[475]\ttrain-error:0.130143\n",
      "[476]\ttrain-error:0.130008\n",
      "[477]\ttrain-error:0.129957\n",
      "[478]\ttrain-error:0.130025\n",
      "[479]\ttrain-error:0.130244\n",
      "[480]\ttrain-error:0.130244\n",
      "[481]\ttrain-error:0.130176\n",
      "[482]\ttrain-error:0.129957\n",
      "[483]\ttrain-error:0.130025\n",
      "[484]\ttrain-error:0.129890\n",
      "[485]\ttrain-error:0.129553\n",
      "[486]\ttrain-error:0.129435\n",
      "[487]\ttrain-error:0.129520\n",
      "[488]\ttrain-error:0.129570\n",
      "[489]\ttrain-error:0.129536\n",
      "[490]\ttrain-error:0.129553\n",
      "[491]\ttrain-error:0.129301\n",
      "[492]\ttrain-error:0.128947\n",
      "[493]\ttrain-error:0.129082\n",
      "[494]\ttrain-error:0.128964\n",
      "[495]\ttrain-error:0.128812\n",
      "[496]\ttrain-error:0.128913\n",
      "[497]\ttrain-error:0.128997\n",
      "[498]\ttrain-error:0.128880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[499]\ttrain-error:0.128981\n",
      "[0]\ttrain-error:0.134740\n",
      "[1]\ttrain-error:0.134976\n",
      "[2]\ttrain-error:0.134403\n",
      "[3]\ttrain-error:0.135178\n",
      "[4]\ttrain-error:0.135178\n",
      "[5]\ttrain-error:0.135178\n",
      "[6]\ttrain-error:0.135178\n",
      "[7]\ttrain-error:0.135178\n",
      "[8]\ttrain-error:0.135178\n",
      "[9]\ttrain-error:0.135178\n",
      "[10]\ttrain-error:0.135178\n",
      "[11]\ttrain-error:0.135178\n",
      "[12]\ttrain-error:0.135178\n",
      "[13]\ttrain-error:0.135178\n",
      "[14]\ttrain-error:0.135178\n",
      "[15]\ttrain-error:0.135178\n",
      "[16]\ttrain-error:0.135161\n",
      "[17]\ttrain-error:0.135161\n",
      "[18]\ttrain-error:0.135161\n",
      "[19]\ttrain-error:0.135161\n",
      "[20]\ttrain-error:0.135161\n",
      "[21]\ttrain-error:0.135161\n",
      "[22]\ttrain-error:0.135178\n",
      "[23]\ttrain-error:0.135161\n",
      "[24]\ttrain-error:0.135144\n",
      "[25]\ttrain-error:0.135144\n",
      "[26]\ttrain-error:0.135144\n",
      "[27]\ttrain-error:0.135144\n",
      "[28]\ttrain-error:0.135144\n",
      "[29]\ttrain-error:0.135144\n",
      "[30]\ttrain-error:0.135094\n",
      "[31]\ttrain-error:0.134993\n",
      "[32]\ttrain-error:0.135026\n",
      "[33]\ttrain-error:0.134959\n",
      "[34]\ttrain-error:0.134875\n",
      "[35]\ttrain-error:0.134841\n",
      "[36]\ttrain-error:0.134757\n",
      "[37]\ttrain-error:0.134673\n",
      "[38]\ttrain-error:0.134538\n",
      "[39]\ttrain-error:0.134370\n",
      "[40]\ttrain-error:0.134100\n",
      "[41]\ttrain-error:0.134285\n",
      "[42]\ttrain-error:0.134201\n",
      "[43]\ttrain-error:0.133982\n",
      "[44]\ttrain-error:0.133662\n",
      "[45]\ttrain-error:0.133662\n",
      "[46]\ttrain-error:0.133460\n",
      "[47]\ttrain-error:0.133208\n",
      "[48]\ttrain-error:0.133022\n",
      "[49]\ttrain-error:0.132686\n",
      "[50]\ttrain-error:0.132467\n",
      "[51]\ttrain-error:0.132399\n",
      "[52]\ttrain-error:0.132315\n",
      "[53]\ttrain-error:0.132281\n",
      "[54]\ttrain-error:0.132349\n",
      "[55]\ttrain-error:0.132180\n",
      "[56]\ttrain-error:0.132113\n",
      "[57]\ttrain-error:0.131995\n",
      "[58]\ttrain-error:0.131877\n",
      "[59]\ttrain-error:0.131524\n",
      "[60]\ttrain-error:0.131608\n",
      "[61]\ttrain-error:0.131355\n",
      "[62]\ttrain-error:0.131254\n",
      "[63]\ttrain-error:0.130900\n",
      "[64]\ttrain-error:0.130698\n",
      "[65]\ttrain-error:0.130665\n",
      "[66]\ttrain-error:0.130530\n",
      "[67]\ttrain-error:0.130580\n",
      "[68]\ttrain-error:0.130564\n",
      "[69]\ttrain-error:0.130345\n",
      "[70]\ttrain-error:0.129755\n",
      "[71]\ttrain-error:0.129941\n",
      "[72]\ttrain-error:0.129806\n",
      "[73]\ttrain-error:0.129435\n",
      "[74]\ttrain-error:0.129149\n",
      "[75]\ttrain-error:0.128997\n",
      "[76]\ttrain-error:0.128610\n",
      "[77]\ttrain-error:0.128829\n",
      "[78]\ttrain-error:0.128863\n",
      "[79]\ttrain-error:0.128593\n",
      "[80]\ttrain-error:0.128341\n",
      "[81]\ttrain-error:0.128492\n",
      "[82]\ttrain-error:0.128442\n",
      "[83]\ttrain-error:0.128324\n",
      "[84]\ttrain-error:0.127987\n",
      "[85]\ttrain-error:0.128071\n",
      "[86]\ttrain-error:0.128004\n",
      "[87]\ttrain-error:0.127835\n",
      "[88]\ttrain-error:0.127734\n",
      "[89]\ttrain-error:0.127465\n",
      "[90]\ttrain-error:0.127465\n",
      "[91]\ttrain-error:0.127145\n",
      "[92]\ttrain-error:0.127027\n",
      "[93]\ttrain-error:0.126909\n",
      "[94]\ttrain-error:0.126556\n",
      "[95]\ttrain-error:0.126421\n",
      "[96]\ttrain-error:0.126337\n",
      "[97]\ttrain-error:0.126219\n",
      "[98]\ttrain-error:0.125966\n",
      "[99]\ttrain-error:0.125848\n",
      "[100]\ttrain-error:0.125596\n",
      "[101]\ttrain-error:0.125798\n",
      "[102]\ttrain-error:0.125562\n",
      "[103]\ttrain-error:0.125309\n",
      "[104]\ttrain-error:0.125242\n",
      "[105]\ttrain-error:0.125276\n",
      "[106]\ttrain-error:0.125023\n",
      "[107]\ttrain-error:0.125091\n",
      "[108]\ttrain-error:0.124804\n",
      "[109]\ttrain-error:0.124888\n",
      "[110]\ttrain-error:0.124434\n",
      "[111]\ttrain-error:0.124316\n",
      "[112]\ttrain-error:0.124147\n",
      "[113]\ttrain-error:0.124164\n",
      "[114]\ttrain-error:0.123929\n",
      "[115]\ttrain-error:0.123794\n",
      "[116]\ttrain-error:0.123710\n",
      "[117]\ttrain-error:0.123693\n",
      "[118]\ttrain-error:0.123541\n",
      "[119]\ttrain-error:0.123440\n",
      "[120]\ttrain-error:0.123423\n",
      "[121]\ttrain-error:0.123524\n",
      "[122]\ttrain-error:0.123221\n",
      "[123]\ttrain-error:0.123070\n",
      "[124]\ttrain-error:0.123120\n",
      "[125]\ttrain-error:0.122868\n",
      "[126]\ttrain-error:0.122800\n",
      "[127]\ttrain-error:0.122632\n",
      "[128]\ttrain-error:0.122666\n",
      "[129]\ttrain-error:0.122564\n",
      "[130]\ttrain-error:0.122564\n",
      "[131]\ttrain-error:0.122379\n",
      "[132]\ttrain-error:0.122228\n",
      "[133]\ttrain-error:0.122228\n",
      "[134]\ttrain-error:0.122009\n",
      "[135]\ttrain-error:0.121925\n",
      "[136]\ttrain-error:0.121925\n",
      "[137]\ttrain-error:0.122110\n",
      "[138]\ttrain-error:0.122076\n",
      "[139]\ttrain-error:0.122177\n",
      "[140]\ttrain-error:0.121823\n",
      "[141]\ttrain-error:0.121689\n",
      "[142]\ttrain-error:0.121689\n",
      "[143]\ttrain-error:0.121554\n",
      "[144]\ttrain-error:0.121487\n",
      "[145]\ttrain-error:0.121504\n",
      "[146]\ttrain-error:0.121301\n",
      "[147]\ttrain-error:0.121150\n",
      "[148]\ttrain-error:0.121133\n",
      "[149]\ttrain-error:0.121150\n",
      "[150]\ttrain-error:0.120914\n",
      "[151]\ttrain-error:0.120864\n",
      "[152]\ttrain-error:0.120897\n",
      "[153]\ttrain-error:0.120948\n",
      "[154]\ttrain-error:0.120763\n",
      "[155]\ttrain-error:0.120746\n",
      "[156]\ttrain-error:0.120746\n",
      "[157]\ttrain-error:0.120880\n",
      "[158]\ttrain-error:0.120645\n",
      "[159]\ttrain-error:0.120459\n",
      "[160]\ttrain-error:0.120544\n",
      "[161]\ttrain-error:0.120779\n",
      "[162]\ttrain-error:0.120577\n",
      "[163]\ttrain-error:0.120325\n",
      "[164]\ttrain-error:0.120274\n",
      "[165]\ttrain-error:0.120257\n",
      "[166]\ttrain-error:0.120224\n",
      "[167]\ttrain-error:0.120308\n",
      "[168]\ttrain-error:0.120392\n",
      "[169]\ttrain-error:0.120291\n",
      "[170]\ttrain-error:0.120072\n",
      "[171]\ttrain-error:0.120207\n",
      "[172]\ttrain-error:0.119971\n",
      "[173]\ttrain-error:0.119769\n",
      "[174]\ttrain-error:0.119853\n",
      "[175]\ttrain-error:0.119668\n",
      "[176]\ttrain-error:0.119651\n",
      "[177]\ttrain-error:0.119668\n",
      "[178]\ttrain-error:0.119550\n",
      "[179]\ttrain-error:0.119331\n",
      "[180]\ttrain-error:0.119466\n",
      "[181]\ttrain-error:0.119415\n",
      "[182]\ttrain-error:0.119365\n",
      "[183]\ttrain-error:0.119264\n",
      "[184]\ttrain-error:0.119129\n",
      "[185]\ttrain-error:0.119146\n",
      "[186]\ttrain-error:0.119129\n",
      "[187]\ttrain-error:0.119129\n",
      "[188]\ttrain-error:0.119078\n",
      "[189]\ttrain-error:0.119095\n",
      "[190]\ttrain-error:0.118860\n",
      "[191]\ttrain-error:0.118961\n",
      "[192]\ttrain-error:0.118961\n",
      "[193]\ttrain-error:0.118742\n",
      "[194]\ttrain-error:0.118876\n",
      "[195]\ttrain-error:0.118657\n",
      "[196]\ttrain-error:0.118759\n",
      "[197]\ttrain-error:0.118876\n",
      "[198]\ttrain-error:0.118792\n",
      "[199]\ttrain-error:0.118472\n",
      "[200]\ttrain-error:0.118422\n",
      "[201]\ttrain-error:0.118405\n",
      "[202]\ttrain-error:0.118388\n",
      "[203]\ttrain-error:0.118388\n",
      "[204]\ttrain-error:0.118102\n",
      "[205]\ttrain-error:0.118135\n",
      "[206]\ttrain-error:0.118135\n",
      "[207]\ttrain-error:0.118102\n",
      "[208]\ttrain-error:0.117984\n",
      "[209]\ttrain-error:0.117967\n",
      "[210]\ttrain-error:0.117799\n",
      "[211]\ttrain-error:0.117681\n",
      "[212]\ttrain-error:0.117883\n",
      "[213]\ttrain-error:0.117580\n",
      "[214]\ttrain-error:0.117495\n",
      "[215]\ttrain-error:0.117445\n",
      "[216]\ttrain-error:0.117563\n",
      "[217]\ttrain-error:0.117411\n",
      "[218]\ttrain-error:0.117260\n",
      "[219]\ttrain-error:0.117277\n",
      "[220]\ttrain-error:0.117091\n",
      "[221]\ttrain-error:0.116973\n",
      "[222]\ttrain-error:0.116940\n",
      "[223]\ttrain-error:0.117058\n",
      "[224]\ttrain-error:0.116805\n",
      "[225]\ttrain-error:0.116738\n",
      "[226]\ttrain-error:0.116536\n",
      "[227]\ttrain-error:0.116401\n",
      "[228]\ttrain-error:0.116384\n",
      "[229]\ttrain-error:0.116266\n",
      "[230]\ttrain-error:0.116014\n",
      "[231]\ttrain-error:0.116283\n",
      "[232]\ttrain-error:0.116131\n",
      "[233]\ttrain-error:0.116098\n",
      "[234]\ttrain-error:0.115862\n",
      "[235]\ttrain-error:0.115660\n",
      "[236]\ttrain-error:0.115576\n",
      "[237]\ttrain-error:0.115778\n",
      "[238]\ttrain-error:0.115593\n",
      "[239]\ttrain-error:0.115643\n",
      "[240]\ttrain-error:0.115576\n",
      "[241]\ttrain-error:0.115576\n",
      "[242]\ttrain-error:0.115475\n",
      "[243]\ttrain-error:0.115340\n",
      "[244]\ttrain-error:0.115475\n",
      "[245]\ttrain-error:0.115559\n",
      "[246]\ttrain-error:0.115508\n",
      "[247]\ttrain-error:0.115306\n",
      "[248]\ttrain-error:0.115256\n",
      "[249]\ttrain-error:0.115070\n",
      "[250]\ttrain-error:0.115222\n",
      "[251]\ttrain-error:0.114885\n",
      "[252]\ttrain-error:0.114919\n",
      "[253]\ttrain-error:0.114902\n",
      "[254]\ttrain-error:0.114902\n",
      "[255]\ttrain-error:0.114969\n",
      "[256]\ttrain-error:0.114565\n",
      "[257]\ttrain-error:0.114548\n",
      "[258]\ttrain-error:0.114447\n",
      "[259]\ttrain-error:0.114245\n",
      "[260]\ttrain-error:0.114296\n",
      "[261]\ttrain-error:0.114228\n",
      "[262]\ttrain-error:0.114262\n",
      "[263]\ttrain-error:0.114330\n",
      "[264]\ttrain-error:0.114178\n",
      "[265]\ttrain-error:0.114060\n",
      "[266]\ttrain-error:0.114060\n",
      "[267]\ttrain-error:0.113791\n",
      "[268]\ttrain-error:0.113959\n",
      "[269]\ttrain-error:0.113858\n",
      "[270]\ttrain-error:0.113892\n",
      "[271]\ttrain-error:0.113690\n",
      "[272]\ttrain-error:0.113723\n",
      "[273]\ttrain-error:0.113673\n",
      "[274]\ttrain-error:0.113706\n",
      "[275]\ttrain-error:0.113572\n",
      "[276]\ttrain-error:0.113336\n",
      "[277]\ttrain-error:0.113471\n",
      "[278]\ttrain-error:0.113555\n",
      "[279]\ttrain-error:0.113589\n",
      "[280]\ttrain-error:0.113504\n",
      "[281]\ttrain-error:0.113336\n",
      "[282]\ttrain-error:0.113235\n",
      "[283]\ttrain-error:0.113386\n",
      "[284]\ttrain-error:0.113302\n",
      "[285]\ttrain-error:0.113083\n",
      "[286]\ttrain-error:0.113066\n",
      "[287]\ttrain-error:0.112797\n",
      "[288]\ttrain-error:0.112881\n",
      "[289]\ttrain-error:0.112477\n",
      "[290]\ttrain-error:0.112410\n",
      "[291]\ttrain-error:0.112325\n",
      "[292]\ttrain-error:0.112191\n",
      "[293]\ttrain-error:0.112157\n",
      "[294]\ttrain-error:0.112123\n",
      "[295]\ttrain-error:0.112157\n",
      "[296]\ttrain-error:0.111938\n",
      "[297]\ttrain-error:0.111837\n",
      "[298]\ttrain-error:0.111972\n",
      "[299]\ttrain-error:0.111787\n",
      "[300]\ttrain-error:0.111585\n",
      "[301]\ttrain-error:0.111467\n",
      "[302]\ttrain-error:0.111332\n",
      "[303]\ttrain-error:0.111231\n",
      "[304]\ttrain-error:0.111096\n",
      "[305]\ttrain-error:0.111012\n",
      "[306]\ttrain-error:0.110860\n",
      "[307]\ttrain-error:0.110877\n",
      "[308]\ttrain-error:0.110978\n",
      "[309]\ttrain-error:0.110810\n",
      "[310]\ttrain-error:0.110776\n",
      "[311]\ttrain-error:0.110608\n",
      "[312]\ttrain-error:0.110524\n",
      "[313]\ttrain-error:0.110439\n",
      "[314]\ttrain-error:0.110305\n",
      "[315]\ttrain-error:0.110423\n",
      "[316]\ttrain-error:0.110103\n",
      "[317]\ttrain-error:0.110086\n",
      "[318]\ttrain-error:0.110069\n",
      "[319]\ttrain-error:0.109934\n",
      "[320]\ttrain-error:0.109900\n",
      "[321]\ttrain-error:0.109597\n",
      "[322]\ttrain-error:0.109530\n",
      "[323]\ttrain-error:0.109597\n",
      "[324]\ttrain-error:0.109816\n",
      "[325]\ttrain-error:0.109530\n",
      "[326]\ttrain-error:0.109496\n",
      "[327]\ttrain-error:0.109193\n",
      "[328]\ttrain-error:0.109176\n",
      "[329]\ttrain-error:0.109159\n",
      "[330]\ttrain-error:0.108907\n",
      "[331]\ttrain-error:0.108890\n",
      "[332]\ttrain-error:0.108890\n",
      "[333]\ttrain-error:0.108907\n",
      "[334]\ttrain-error:0.108722\n",
      "[335]\ttrain-error:0.108806\n",
      "[336]\ttrain-error:0.108890\n",
      "[337]\ttrain-error:0.108941\n",
      "[338]\ttrain-error:0.109025\n",
      "[339]\ttrain-error:0.108621\n",
      "[340]\ttrain-error:0.108654\n",
      "[341]\ttrain-error:0.108570\n",
      "[342]\ttrain-error:0.108738\n",
      "[343]\ttrain-error:0.108840\n",
      "[344]\ttrain-error:0.108738\n",
      "[345]\ttrain-error:0.108621\n",
      "[346]\ttrain-error:0.108621\n",
      "[347]\ttrain-error:0.108368\n",
      "[348]\ttrain-error:0.108284\n",
      "[349]\ttrain-error:0.108267\n",
      "[350]\ttrain-error:0.108301\n",
      "[351]\ttrain-error:0.108216\n",
      "[352]\ttrain-error:0.108233\n",
      "[353]\ttrain-error:0.108216\n",
      "[354]\ttrain-error:0.108149\n",
      "[355]\ttrain-error:0.108183\n",
      "[356]\ttrain-error:0.107998\n",
      "[357]\ttrain-error:0.107795\n",
      "[358]\ttrain-error:0.107998\n",
      "[359]\ttrain-error:0.107863\n",
      "[360]\ttrain-error:0.107812\n",
      "[361]\ttrain-error:0.107779\n",
      "[362]\ttrain-error:0.107812\n",
      "[363]\ttrain-error:0.107610\n",
      "[364]\ttrain-error:0.107846\n",
      "[365]\ttrain-error:0.107678\n",
      "[366]\ttrain-error:0.107711\n",
      "[367]\ttrain-error:0.107745\n",
      "[368]\ttrain-error:0.107475\n",
      "[369]\ttrain-error:0.107475\n",
      "[370]\ttrain-error:0.107324\n",
      "[371]\ttrain-error:0.107341\n",
      "[372]\ttrain-error:0.107223\n",
      "[373]\ttrain-error:0.107358\n",
      "[374]\ttrain-error:0.107290\n",
      "[375]\ttrain-error:0.107341\n",
      "[376]\ttrain-error:0.107408\n",
      "[377]\ttrain-error:0.107425\n",
      "[378]\ttrain-error:0.107341\n",
      "[379]\ttrain-error:0.107324\n",
      "[380]\ttrain-error:0.107206\n",
      "[381]\ttrain-error:0.107139\n",
      "[382]\ttrain-error:0.107223\n",
      "[383]\ttrain-error:0.107139\n",
      "[384]\ttrain-error:0.107341\n",
      "[385]\ttrain-error:0.106953\n",
      "[386]\ttrain-error:0.106768\n",
      "[387]\ttrain-error:0.107021\n",
      "[388]\ttrain-error:0.106768\n",
      "[389]\ttrain-error:0.107071\n",
      "[390]\ttrain-error:0.106886\n",
      "[391]\ttrain-error:0.106701\n",
      "[392]\ttrain-error:0.106718\n",
      "[393]\ttrain-error:0.106903\n",
      "[394]\ttrain-error:0.106937\n",
      "[395]\ttrain-error:0.106600\n",
      "[396]\ttrain-error:0.106448\n",
      "[397]\ttrain-error:0.106415\n",
      "[398]\ttrain-error:0.106600\n",
      "[399]\ttrain-error:0.106347\n",
      "[400]\ttrain-error:0.106600\n",
      "[401]\ttrain-error:0.106600\n",
      "[402]\ttrain-error:0.106415\n",
      "[403]\ttrain-error:0.106398\n",
      "[404]\ttrain-error:0.106297\n",
      "[405]\ttrain-error:0.106196\n",
      "[406]\ttrain-error:0.105943\n",
      "[407]\ttrain-error:0.106061\n",
      "[408]\ttrain-error:0.105960\n",
      "[409]\ttrain-error:0.105926\n",
      "[410]\ttrain-error:0.105539\n",
      "[411]\ttrain-error:0.105674\n",
      "[412]\ttrain-error:0.105741\n",
      "[413]\ttrain-error:0.105556\n",
      "[414]\ttrain-error:0.105572\n",
      "[415]\ttrain-error:0.105438\n",
      "[416]\ttrain-error:0.105387\n",
      "[417]\ttrain-error:0.105337\n",
      "[418]\ttrain-error:0.105471\n",
      "[419]\ttrain-error:0.105286\n",
      "[420]\ttrain-error:0.105084\n",
      "[421]\ttrain-error:0.105286\n",
      "[422]\ttrain-error:0.105320\n",
      "[423]\ttrain-error:0.105118\n",
      "[424]\ttrain-error:0.105050\n",
      "[425]\ttrain-error:0.105101\n",
      "[426]\ttrain-error:0.105320\n",
      "[427]\ttrain-error:0.105337\n",
      "[428]\ttrain-error:0.105421\n",
      "[429]\ttrain-error:0.105539\n",
      "[430]\ttrain-error:0.105269\n",
      "[431]\ttrain-error:0.105236\n",
      "[432]\ttrain-error:0.105151\n",
      "[433]\ttrain-error:0.105151\n",
      "[434]\ttrain-error:0.105320\n",
      "[435]\ttrain-error:0.105135\n",
      "[436]\ttrain-error:0.104966\n",
      "[437]\ttrain-error:0.104764\n",
      "[438]\ttrain-error:0.104663\n",
      "[439]\ttrain-error:0.104663\n",
      "[440]\ttrain-error:0.104528\n",
      "[441]\ttrain-error:0.104410\n",
      "[442]\ttrain-error:0.104427\n",
      "[443]\ttrain-error:0.104293\n",
      "[444]\ttrain-error:0.104343\n",
      "[445]\ttrain-error:0.104444\n",
      "[446]\ttrain-error:0.104427\n",
      "[447]\ttrain-error:0.104276\n",
      "[448]\ttrain-error:0.104158\n",
      "[449]\ttrain-error:0.103973\n",
      "[450]\ttrain-error:0.103905\n",
      "[451]\ttrain-error:0.103670\n",
      "[452]\ttrain-error:0.103922\n",
      "[453]\ttrain-error:0.103939\n",
      "[454]\ttrain-error:0.103754\n",
      "[455]\ttrain-error:0.103434\n",
      "[456]\ttrain-error:0.103215\n",
      "[457]\ttrain-error:0.103097\n",
      "[458]\ttrain-error:0.103080\n",
      "[459]\ttrain-error:0.103249\n",
      "[460]\ttrain-error:0.103147\n",
      "[461]\ttrain-error:0.103164\n",
      "[462]\ttrain-error:0.102945\n",
      "[463]\ttrain-error:0.103147\n",
      "[464]\ttrain-error:0.102962\n",
      "[465]\ttrain-error:0.102979\n",
      "[466]\ttrain-error:0.103282\n",
      "[467]\ttrain-error:0.103114\n",
      "[468]\ttrain-error:0.103164\n",
      "[469]\ttrain-error:0.103164\n",
      "[470]\ttrain-error:0.103366\n",
      "[471]\ttrain-error:0.103333\n",
      "[472]\ttrain-error:0.103198\n",
      "[473]\ttrain-error:0.103350\n",
      "[474]\ttrain-error:0.102508\n",
      "[475]\ttrain-error:0.102322\n",
      "[476]\ttrain-error:0.102457\n",
      "[477]\ttrain-error:0.102373\n",
      "[478]\ttrain-error:0.102221\n",
      "[479]\ttrain-error:0.102204\n",
      "[480]\ttrain-error:0.102070\n",
      "[481]\ttrain-error:0.101952\n",
      "[482]\ttrain-error:0.101985\n",
      "[483]\ttrain-error:0.101868\n",
      "[484]\ttrain-error:0.101918\n",
      "[485]\ttrain-error:0.101952\n",
      "[486]\ttrain-error:0.101868\n",
      "[487]\ttrain-error:0.101952\n",
      "[488]\ttrain-error:0.102120\n",
      "[489]\ttrain-error:0.102019\n",
      "[490]\ttrain-error:0.101901\n",
      "[491]\ttrain-error:0.101884\n",
      "[492]\ttrain-error:0.102002\n",
      "[493]\ttrain-error:0.101800\n",
      "[494]\ttrain-error:0.101901\n",
      "[495]\ttrain-error:0.101581\n",
      "[496]\ttrain-error:0.101514\n",
      "[497]\ttrain-error:0.101884\n",
      "[498]\ttrain-error:0.101800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[499]\ttrain-error:0.101716\n",
      "[0]\ttrain-error:0.223354\n",
      "[1]\ttrain-error:0.216450\n",
      "[2]\ttrain-error:0.203078\n",
      "[3]\ttrain-error:0.198616\n",
      "[4]\ttrain-error:0.197875\n",
      "[5]\ttrain-error:0.198329\n",
      "[6]\ttrain-error:0.200165\n",
      "[7]\ttrain-error:0.198919\n",
      "[8]\ttrain-error:0.198582\n",
      "[9]\ttrain-error:0.193732\n",
      "[10]\ttrain-error:0.194726\n",
      "[11]\ttrain-error:0.192991\n",
      "[12]\ttrain-error:0.191290\n",
      "[13]\ttrain-error:0.191947\n",
      "[14]\ttrain-error:0.190583\n",
      "[15]\ttrain-error:0.188528\n",
      "[16]\ttrain-error:0.187383\n",
      "[17]\ttrain-error:0.187400\n",
      "[18]\ttrain-error:0.185783\n",
      "[19]\ttrain-error:0.184857\n",
      "[20]\ttrain-error:0.184823\n",
      "[21]\ttrain-error:0.183544\n",
      "[22]\ttrain-error:0.183645\n",
      "[23]\ttrain-error:0.183240\n",
      "[24]\ttrain-error:0.180462\n",
      "[25]\ttrain-error:0.176993\n",
      "[26]\ttrain-error:0.174601\n",
      "[27]\ttrain-error:0.173675\n",
      "[28]\ttrain-error:0.173456\n",
      "[29]\ttrain-error:0.171267\n",
      "[30]\ttrain-error:0.171099\n",
      "[31]\ttrain-error:0.168909\n",
      "[32]\ttrain-error:0.168370\n",
      "[33]\ttrain-error:0.166855\n",
      "[34]\ttrain-error:0.166164\n",
      "[35]\ttrain-error:0.165558\n",
      "[36]\ttrain-error:0.164868\n",
      "[37]\ttrain-error:0.164918\n",
      "[38]\ttrain-error:0.164312\n",
      "[39]\ttrain-error:0.163773\n",
      "[40]\ttrain-error:0.162948\n",
      "[41]\ttrain-error:0.162207\n",
      "[42]\ttrain-error:0.161718\n",
      "[43]\ttrain-error:0.160961\n",
      "[44]\ttrain-error:0.160792\n",
      "[45]\ttrain-error:0.160152\n",
      "[46]\ttrain-error:0.159698\n",
      "[47]\ttrain-error:0.159226\n",
      "[48]\ttrain-error:0.159394\n",
      "[49]\ttrain-error:0.159378\n",
      "[50]\ttrain-error:0.158771\n",
      "[51]\ttrain-error:0.158014\n",
      "[52]\ttrain-error:0.157929\n",
      "[53]\ttrain-error:0.157727\n",
      "[54]\ttrain-error:0.157340\n",
      "[55]\ttrain-error:0.157458\n",
      "[56]\ttrain-error:0.156599\n",
      "[57]\ttrain-error:0.156565\n",
      "[58]\ttrain-error:0.156767\n",
      "[59]\ttrain-error:0.156649\n",
      "[60]\ttrain-error:0.156178\n",
      "[61]\ttrain-error:0.155757\n",
      "[62]\ttrain-error:0.155370\n",
      "[63]\ttrain-error:0.155235\n",
      "[64]\ttrain-error:0.155454\n",
      "[65]\ttrain-error:0.154915\n",
      "[66]\ttrain-error:0.154949\n",
      "[67]\ttrain-error:0.154915\n",
      "[68]\ttrain-error:0.155050\n",
      "[69]\ttrain-error:0.154814\n",
      "[70]\ttrain-error:0.154275\n",
      "[71]\ttrain-error:0.154107\n",
      "[72]\ttrain-error:0.153904\n",
      "[73]\ttrain-error:0.153820\n",
      "[74]\ttrain-error:0.153854\n",
      "[75]\ttrain-error:0.153500\n",
      "[76]\ttrain-error:0.153147\n",
      "[77]\ttrain-error:0.153029\n",
      "[78]\ttrain-error:0.152961\n",
      "[79]\ttrain-error:0.152692\n",
      "[80]\ttrain-error:0.152692\n",
      "[81]\ttrain-error:0.152254\n",
      "[82]\ttrain-error:0.152103\n",
      "[83]\ttrain-error:0.152321\n",
      "[84]\ttrain-error:0.152170\n",
      "[85]\ttrain-error:0.151631\n",
      "[86]\ttrain-error:0.151412\n",
      "[87]\ttrain-error:0.151395\n",
      "[88]\ttrain-error:0.151126\n",
      "[89]\ttrain-error:0.151227\n",
      "[90]\ttrain-error:0.151513\n",
      "[91]\ttrain-error:0.151328\n",
      "[92]\ttrain-error:0.151261\n",
      "[93]\ttrain-error:0.150974\n",
      "[94]\ttrain-error:0.150856\n",
      "[95]\ttrain-error:0.150806\n",
      "[96]\ttrain-error:0.150469\n",
      "[97]\ttrain-error:0.150351\n",
      "[98]\ttrain-error:0.150402\n",
      "[99]\ttrain-error:0.150115\n",
      "[100]\ttrain-error:0.149964\n",
      "[101]\ttrain-error:0.149846\n",
      "[102]\ttrain-error:0.149728\n",
      "[103]\ttrain-error:0.149459\n",
      "[104]\ttrain-error:0.149273\n",
      "[105]\ttrain-error:0.149172\n",
      "[106]\ttrain-error:0.148970\n",
      "[107]\ttrain-error:0.148633\n",
      "[108]\ttrain-error:0.148667\n",
      "[109]\ttrain-error:0.148482\n",
      "[110]\ttrain-error:0.148162\n",
      "[111]\ttrain-error:0.147657\n",
      "[112]\ttrain-error:0.147539\n",
      "[113]\ttrain-error:0.147438\n",
      "[114]\ttrain-error:0.147286\n",
      "[115]\ttrain-error:0.147168\n",
      "[116]\ttrain-error:0.146983\n",
      "[117]\ttrain-error:0.147421\n",
      "[118]\ttrain-error:0.147455\n",
      "[119]\ttrain-error:0.147269\n",
      "[120]\ttrain-error:0.147118\n",
      "[121]\ttrain-error:0.146848\n",
      "[122]\ttrain-error:0.146394\n",
      "[123]\ttrain-error:0.146444\n",
      "[124]\ttrain-error:0.146613\n",
      "[125]\ttrain-error:0.146528\n",
      "[126]\ttrain-error:0.146512\n",
      "[127]\ttrain-error:0.146596\n",
      "[128]\ttrain-error:0.146478\n",
      "[129]\ttrain-error:0.146427\n",
      "[130]\ttrain-error:0.146427\n",
      "[131]\ttrain-error:0.146394\n",
      "[132]\ttrain-error:0.146343\n",
      "[133]\ttrain-error:0.145989\n",
      "[134]\ttrain-error:0.145989\n",
      "[135]\ttrain-error:0.146107\n",
      "[136]\ttrain-error:0.145686\n",
      "[137]\ttrain-error:0.145669\n",
      "[138]\ttrain-error:0.145619\n",
      "[139]\ttrain-error:0.145585\n",
      "[140]\ttrain-error:0.145585\n",
      "[141]\ttrain-error:0.145451\n",
      "[142]\ttrain-error:0.145417\n",
      "[143]\ttrain-error:0.145282\n",
      "[144]\ttrain-error:0.145147\n",
      "[145]\ttrain-error:0.145366\n",
      "[146]\ttrain-error:0.145400\n",
      "[147]\ttrain-error:0.145282\n",
      "[148]\ttrain-error:0.144878\n",
      "[149]\ttrain-error:0.144575\n",
      "[150]\ttrain-error:0.144322\n",
      "[151]\ttrain-error:0.144457\n",
      "[152]\ttrain-error:0.144188\n",
      "[153]\ttrain-error:0.144255\n",
      "[154]\ttrain-error:0.144272\n",
      "[155]\ttrain-error:0.144221\n",
      "[156]\ttrain-error:0.144103\n",
      "[157]\ttrain-error:0.143733\n",
      "[158]\ttrain-error:0.143733\n",
      "[159]\ttrain-error:0.143817\n",
      "[160]\ttrain-error:0.143430\n",
      "[161]\ttrain-error:0.143514\n",
      "[162]\ttrain-error:0.143497\n",
      "[163]\ttrain-error:0.143581\n",
      "[164]\ttrain-error:0.143177\n",
      "[165]\ttrain-error:0.143009\n",
      "[166]\ttrain-error:0.142722\n",
      "[167]\ttrain-error:0.142756\n",
      "[168]\ttrain-error:0.142605\n",
      "[169]\ttrain-error:0.142251\n",
      "[170]\ttrain-error:0.142268\n",
      "[171]\ttrain-error:0.142335\n",
      "[172]\ttrain-error:0.142402\n",
      "[173]\ttrain-error:0.141948\n",
      "[174]\ttrain-error:0.141847\n",
      "[175]\ttrain-error:0.141763\n",
      "[176]\ttrain-error:0.141392\n",
      "[177]\ttrain-error:0.141207\n",
      "[178]\ttrain-error:0.141291\n",
      "[179]\ttrain-error:0.141055\n",
      "[180]\ttrain-error:0.140937\n",
      "[181]\ttrain-error:0.140904\n",
      "[182]\ttrain-error:0.140516\n",
      "[183]\ttrain-error:0.140365\n",
      "[184]\ttrain-error:0.140264\n",
      "[185]\ttrain-error:0.140584\n",
      "[186]\ttrain-error:0.140382\n",
      "[187]\ttrain-error:0.140365\n",
      "[188]\ttrain-error:0.139961\n",
      "[189]\ttrain-error:0.139843\n",
      "[190]\ttrain-error:0.139944\n",
      "[191]\ttrain-error:0.139860\n",
      "[192]\ttrain-error:0.139691\n",
      "[193]\ttrain-error:0.139556\n",
      "[194]\ttrain-error:0.139388\n",
      "[195]\ttrain-error:0.139135\n",
      "[196]\ttrain-error:0.139102\n",
      "[197]\ttrain-error:0.139287\n",
      "[198]\ttrain-error:0.139152\n",
      "[199]\ttrain-error:0.139085\n",
      "[200]\ttrain-error:0.139102\n",
      "[201]\ttrain-error:0.138799\n",
      "[202]\ttrain-error:0.138900\n",
      "[203]\ttrain-error:0.138900\n",
      "[204]\ttrain-error:0.138916\n",
      "[205]\ttrain-error:0.138799\n",
      "[206]\ttrain-error:0.138832\n",
      "[207]\ttrain-error:0.138815\n",
      "[208]\ttrain-error:0.138782\n",
      "[209]\ttrain-error:0.138647\n",
      "[210]\ttrain-error:0.138883\n",
      "[211]\ttrain-error:0.139018\n",
      "[212]\ttrain-error:0.138933\n",
      "[213]\ttrain-error:0.138849\n",
      "[214]\ttrain-error:0.138597\n",
      "[215]\ttrain-error:0.138664\n",
      "[216]\ttrain-error:0.138731\n",
      "[217]\ttrain-error:0.138714\n",
      "[218]\ttrain-error:0.138597\n",
      "[219]\ttrain-error:0.138344\n",
      "[220]\ttrain-error:0.138142\n",
      "[221]\ttrain-error:0.137940\n",
      "[222]\ttrain-error:0.138108\n",
      "[223]\ttrain-error:0.138007\n",
      "[224]\ttrain-error:0.137923\n",
      "[225]\ttrain-error:0.137687\n",
      "[226]\ttrain-error:0.137384\n",
      "[227]\ttrain-error:0.137586\n",
      "[228]\ttrain-error:0.137401\n",
      "[229]\ttrain-error:0.137502\n",
      "[230]\ttrain-error:0.137418\n",
      "[231]\ttrain-error:0.137148\n",
      "[232]\ttrain-error:0.136997\n",
      "[233]\ttrain-error:0.137199\n",
      "[234]\ttrain-error:0.136710\n",
      "[235]\ttrain-error:0.136828\n",
      "[236]\ttrain-error:0.136576\n",
      "[237]\ttrain-error:0.136862\n",
      "[238]\ttrain-error:0.136542\n",
      "[239]\ttrain-error:0.136357\n",
      "[240]\ttrain-error:0.136677\n",
      "[241]\ttrain-error:0.136508\n",
      "[242]\ttrain-error:0.136390\n",
      "[243]\ttrain-error:0.136155\n",
      "[244]\ttrain-error:0.136155\n",
      "[245]\ttrain-error:0.135936\n",
      "[246]\ttrain-error:0.135936\n",
      "[247]\ttrain-error:0.136155\n",
      "[248]\ttrain-error:0.135902\n",
      "[249]\ttrain-error:0.136003\n",
      "[250]\ttrain-error:0.135801\n",
      "[251]\ttrain-error:0.135801\n",
      "[252]\ttrain-error:0.135464\n",
      "[253]\ttrain-error:0.135666\n",
      "[254]\ttrain-error:0.135599\n",
      "[255]\ttrain-error:0.135447\n",
      "[256]\ttrain-error:0.135296\n",
      "[257]\ttrain-error:0.134757\n",
      "[258]\ttrain-error:0.134690\n",
      "[259]\ttrain-error:0.134589\n",
      "[260]\ttrain-error:0.134572\n",
      "[261]\ttrain-error:0.134521\n",
      "[262]\ttrain-error:0.134370\n",
      "[263]\ttrain-error:0.134066\n",
      "[264]\ttrain-error:0.134269\n",
      "[265]\ttrain-error:0.134269\n",
      "[266]\ttrain-error:0.134083\n",
      "[267]\ttrain-error:0.134134\n",
      "[268]\ttrain-error:0.134269\n",
      "[269]\ttrain-error:0.134151\n",
      "[270]\ttrain-error:0.134336\n",
      "[271]\ttrain-error:0.134302\n",
      "[272]\ttrain-error:0.133999\n",
      "[273]\ttrain-error:0.134134\n",
      "[274]\ttrain-error:0.133982\n",
      "[275]\ttrain-error:0.133679\n",
      "[276]\ttrain-error:0.133258\n",
      "[277]\ttrain-error:0.133157\n",
      "[278]\ttrain-error:0.133359\n",
      "[279]\ttrain-error:0.133123\n",
      "[280]\ttrain-error:0.133359\n",
      "[281]\ttrain-error:0.133241\n",
      "[282]\ttrain-error:0.133224\n",
      "[283]\ttrain-error:0.132938\n",
      "[284]\ttrain-error:0.133022\n",
      "[285]\ttrain-error:0.133073\n",
      "[286]\ttrain-error:0.132787\n",
      "[287]\ttrain-error:0.132551\n",
      "[288]\ttrain-error:0.132467\n",
      "[289]\ttrain-error:0.132298\n",
      "[290]\ttrain-error:0.132197\n",
      "[291]\ttrain-error:0.132012\n",
      "[292]\ttrain-error:0.131844\n",
      "[293]\ttrain-error:0.131961\n",
      "[294]\ttrain-error:0.132012\n",
      "[295]\ttrain-error:0.131945\n",
      "[296]\ttrain-error:0.131608\n",
      "[297]\ttrain-error:0.131574\n",
      "[298]\ttrain-error:0.131692\n",
      "[299]\ttrain-error:0.131709\n",
      "[300]\ttrain-error:0.131204\n",
      "[301]\ttrain-error:0.131321\n",
      "[302]\ttrain-error:0.131355\n",
      "[303]\ttrain-error:0.131423\n",
      "[304]\ttrain-error:0.131170\n",
      "[305]\ttrain-error:0.131271\n",
      "[306]\ttrain-error:0.131439\n",
      "[307]\ttrain-error:0.131136\n",
      "[308]\ttrain-error:0.131153\n",
      "[309]\ttrain-error:0.131321\n",
      "[310]\ttrain-error:0.130850\n",
      "[311]\ttrain-error:0.130698\n",
      "[312]\ttrain-error:0.130833\n",
      "[313]\ttrain-error:0.130783\n",
      "[314]\ttrain-error:0.130816\n",
      "[315]\ttrain-error:0.130884\n",
      "[316]\ttrain-error:0.130530\n",
      "[317]\ttrain-error:0.130698\n",
      "[318]\ttrain-error:0.130631\n",
      "[319]\ttrain-error:0.130648\n",
      "[320]\ttrain-error:0.130328\n",
      "[321]\ttrain-error:0.130075\n",
      "[322]\ttrain-error:0.129991\n",
      "[323]\ttrain-error:0.129991\n",
      "[324]\ttrain-error:0.129840\n",
      "[325]\ttrain-error:0.129755\n",
      "[326]\ttrain-error:0.129671\n",
      "[327]\ttrain-error:0.129789\n",
      "[328]\ttrain-error:0.129755\n",
      "[329]\ttrain-error:0.129553\n",
      "[330]\ttrain-error:0.129553\n",
      "[331]\ttrain-error:0.129216\n",
      "[332]\ttrain-error:0.129183\n",
      "[333]\ttrain-error:0.129233\n",
      "[334]\ttrain-error:0.129031\n",
      "[335]\ttrain-error:0.128846\n",
      "[336]\ttrain-error:0.128997\n",
      "[337]\ttrain-error:0.128863\n",
      "[338]\ttrain-error:0.128593\n",
      "[339]\ttrain-error:0.128172\n",
      "[340]\ttrain-error:0.128240\n",
      "[341]\ttrain-error:0.127953\n",
      "[342]\ttrain-error:0.127835\n",
      "[343]\ttrain-error:0.127785\n",
      "[344]\ttrain-error:0.127718\n",
      "[345]\ttrain-error:0.127852\n",
      "[346]\ttrain-error:0.127785\n",
      "[347]\ttrain-error:0.127482\n",
      "[348]\ttrain-error:0.127650\n",
      "[349]\ttrain-error:0.127650\n",
      "[350]\ttrain-error:0.127482\n",
      "[351]\ttrain-error:0.127229\n",
      "[352]\ttrain-error:0.127297\n",
      "[353]\ttrain-error:0.127330\n",
      "[354]\ttrain-error:0.127196\n",
      "[355]\ttrain-error:0.127044\n",
      "[356]\ttrain-error:0.126960\n",
      "[357]\ttrain-error:0.126791\n",
      "[358]\ttrain-error:0.126876\n",
      "[359]\ttrain-error:0.126909\n",
      "[360]\ttrain-error:0.126539\n",
      "[361]\ttrain-error:0.126690\n",
      "[362]\ttrain-error:0.126707\n",
      "[363]\ttrain-error:0.126775\n",
      "[364]\ttrain-error:0.126589\n",
      "[365]\ttrain-error:0.126556\n",
      "[366]\ttrain-error:0.126354\n",
      "[367]\ttrain-error:0.126236\n",
      "[368]\ttrain-error:0.125916\n",
      "[369]\ttrain-error:0.125865\n",
      "[370]\ttrain-error:0.125916\n",
      "[371]\ttrain-error:0.125949\n",
      "[372]\ttrain-error:0.125899\n",
      "[373]\ttrain-error:0.125882\n",
      "[374]\ttrain-error:0.125326\n",
      "[375]\ttrain-error:0.125663\n",
      "[376]\ttrain-error:0.125208\n",
      "[377]\ttrain-error:0.125377\n",
      "[378]\ttrain-error:0.125293\n",
      "[379]\ttrain-error:0.125040\n",
      "[380]\ttrain-error:0.124973\n",
      "[381]\ttrain-error:0.124568\n",
      "[382]\ttrain-error:0.124602\n",
      "[383]\ttrain-error:0.124787\n",
      "[384]\ttrain-error:0.124872\n",
      "[385]\ttrain-error:0.124636\n",
      "[386]\ttrain-error:0.124737\n",
      "[387]\ttrain-error:0.124350\n",
      "[388]\ttrain-error:0.124316\n",
      "[389]\ttrain-error:0.124164\n",
      "[390]\ttrain-error:0.124316\n",
      "[391]\ttrain-error:0.124366\n",
      "[392]\ttrain-error:0.124484\n",
      "[393]\ttrain-error:0.124434\n",
      "[394]\ttrain-error:0.124046\n",
      "[395]\ttrain-error:0.123996\n",
      "[396]\ttrain-error:0.123811\n",
      "[397]\ttrain-error:0.124063\n",
      "[398]\ttrain-error:0.123861\n",
      "[399]\ttrain-error:0.123491\n",
      "[400]\ttrain-error:0.123390\n",
      "[401]\ttrain-error:0.123322\n",
      "[402]\ttrain-error:0.123390\n",
      "[403]\ttrain-error:0.123440\n",
      "[404]\ttrain-error:0.123575\n",
      "[405]\ttrain-error:0.123255\n",
      "[406]\ttrain-error:0.123238\n",
      "[407]\ttrain-error:0.123171\n",
      "[408]\ttrain-error:0.123103\n",
      "[409]\ttrain-error:0.122952\n",
      "[410]\ttrain-error:0.122817\n",
      "[411]\ttrain-error:0.122615\n",
      "[412]\ttrain-error:0.122396\n",
      "[413]\ttrain-error:0.122346\n",
      "[414]\ttrain-error:0.122430\n",
      "[415]\ttrain-error:0.122497\n",
      "[416]\ttrain-error:0.122211\n",
      "[417]\ttrain-error:0.121925\n",
      "[418]\ttrain-error:0.121621\n",
      "[419]\ttrain-error:0.121925\n",
      "[420]\ttrain-error:0.121790\n",
      "[421]\ttrain-error:0.121739\n",
      "[422]\ttrain-error:0.122026\n",
      "[423]\ttrain-error:0.121790\n",
      "[424]\ttrain-error:0.122127\n",
      "[425]\ttrain-error:0.122076\n",
      "[426]\ttrain-error:0.121958\n",
      "[427]\ttrain-error:0.121857\n",
      "[428]\ttrain-error:0.121958\n",
      "[429]\ttrain-error:0.121823\n",
      "[430]\ttrain-error:0.121773\n",
      "[431]\ttrain-error:0.121386\n",
      "[432]\ttrain-error:0.121706\n",
      "[433]\ttrain-error:0.121537\n",
      "[434]\ttrain-error:0.121672\n",
      "[435]\ttrain-error:0.121638\n",
      "[436]\ttrain-error:0.121318\n",
      "[437]\ttrain-error:0.121436\n",
      "[438]\ttrain-error:0.121268\n",
      "[439]\ttrain-error:0.121200\n",
      "[440]\ttrain-error:0.120965\n",
      "[441]\ttrain-error:0.120779\n",
      "[442]\ttrain-error:0.120914\n",
      "[443]\ttrain-error:0.120897\n",
      "[444]\ttrain-error:0.120678\n",
      "[445]\ttrain-error:0.120729\n",
      "[446]\ttrain-error:0.120796\n",
      "[447]\ttrain-error:0.120443\n",
      "[448]\ttrain-error:0.120645\n",
      "[449]\ttrain-error:0.120375\n",
      "[450]\ttrain-error:0.120156\n",
      "[451]\ttrain-error:0.120308\n",
      "[452]\ttrain-error:0.120022\n",
      "[453]\ttrain-error:0.119904\n",
      "[454]\ttrain-error:0.120005\n",
      "[455]\ttrain-error:0.120072\n",
      "[456]\ttrain-error:0.119516\n",
      "[457]\ttrain-error:0.119836\n",
      "[458]\ttrain-error:0.119735\n",
      "[459]\ttrain-error:0.119702\n",
      "[460]\ttrain-error:0.119617\n",
      "[461]\ttrain-error:0.119500\n",
      "[462]\ttrain-error:0.119483\n",
      "[463]\ttrain-error:0.119365\n",
      "[464]\ttrain-error:0.119466\n",
      "[465]\ttrain-error:0.119230\n",
      "[466]\ttrain-error:0.119129\n",
      "[467]\ttrain-error:0.119180\n",
      "[468]\ttrain-error:0.118624\n",
      "[469]\ttrain-error:0.118826\n",
      "[470]\ttrain-error:0.118910\n",
      "[471]\ttrain-error:0.118607\n",
      "[472]\ttrain-error:0.118809\n",
      "[473]\ttrain-error:0.119045\n",
      "[474]\ttrain-error:0.118792\n",
      "[475]\ttrain-error:0.118624\n",
      "[476]\ttrain-error:0.118455\n",
      "[477]\ttrain-error:0.118573\n",
      "[478]\ttrain-error:0.118674\n",
      "[479]\ttrain-error:0.118725\n",
      "[480]\ttrain-error:0.118506\n",
      "[481]\ttrain-error:0.118742\n",
      "[482]\ttrain-error:0.118657\n",
      "[483]\ttrain-error:0.118876\n",
      "[484]\ttrain-error:0.118843\n",
      "[485]\ttrain-error:0.118826\n",
      "[486]\ttrain-error:0.118775\n",
      "[487]\ttrain-error:0.118725\n",
      "[488]\ttrain-error:0.118860\n",
      "[489]\ttrain-error:0.118826\n",
      "[490]\ttrain-error:0.118455\n",
      "[491]\ttrain-error:0.118388\n",
      "[492]\ttrain-error:0.118506\n",
      "[493]\ttrain-error:0.118253\n",
      "[494]\ttrain-error:0.118236\n",
      "[495]\ttrain-error:0.118186\n",
      "[496]\ttrain-error:0.117900\n",
      "[497]\ttrain-error:0.117815\n",
      "[498]\ttrain-error:0.117479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model\n",
      "[array([ 0.18189684,  0.04002952,  0.04911706, ...,  0.48660439,\n",
      "        0.06455781,  0.11096793], dtype=float32), array([ 0.17126404,  0.0457582 ,  0.05060061, ...,  0.04708283,\n",
      "        0.50229567,  0.10515675], dtype=float32), array([ 0.0008391 ,  0.0017683 ,  0.00484451, ...,  0.00301457,\n",
      "        0.00108978,  0.23855457], dtype=float32), array([ 0.00146568,  0.00941631,  0.00471496, ...,  0.00029976,\n",
      "        0.0004955 ,  0.14125332], dtype=float32), array([ 0.01392779,  0.03226906,  0.0184226 , ...,  0.15576909,\n",
      "        0.15073209,  0.00133464], dtype=float32), array([ 0.35438439,  0.20778076,  0.4477694 , ...,  0.45147094,\n",
      "        0.04487349,  0.01616646], dtype=float32), array([ 0.43252948,  0.10000362,  0.22745346, ...,  0.0650955 ,\n",
      "        0.00200009,  0.01805603], dtype=float32), array([  1.94472168e-02,   6.49320722e-01,   1.42768472e-01, ...,\n",
      "         1.84513605e-03,   9.91599634e-03,   3.55532655e-04], dtype=float32)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[499]\ttrain-error:0.117613\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "def get_params():\n",
    "    \n",
    "    params = {}\n",
    "    params[\"objective\"] = \"binary:logistic\"     \n",
    "    params[\"eta\"] = 0.1\n",
    "    params[\"min_child_weight\"] = 50\n",
    "    params[\"subsample\"] = 0.5\n",
    "    params[\"colsample_bytree\"] = 0.30\n",
    "    params[\"silent\"] = 1\n",
    "    params[\"max_depth\"] = 9\n",
    "    plst = list(params.items())\n",
    "\n",
    "    return plst\n",
    "\n",
    "\n",
    "\n",
    "DATA_DIR = '/Users/patrickkennedy/Desktop'\n",
    "\n",
    "train = pd.read_csv(DATA_DIR + '/Project DATA/train.csv')\n",
    "test = pd.read_csv(DATA_DIR + '/Project DATA/test.csv')\n",
    "\n",
    "features = train.columns\n",
    "features = features.drop(\"Id\")\n",
    "features = features.drop(\"Response\")\n",
    "\n",
    "\n",
    "\n",
    "y_train = train[\"Response\"]\n",
    "\n",
    "y_train_1 = y_train.map({1:1, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0})\n",
    "y_train_2 = y_train.map({1:0, 2:1, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0})\n",
    "y_train_3 = y_train.map({1:0, 2:0, 3:1, 4:0, 5:0, 6:0, 7:0, 8:0})\n",
    "y_train_4 = y_train.map({1:0, 2:0, 3:0, 4:1, 5:0, 6:0, 7:0, 8:0})\n",
    "y_train_5 = y_train.map({1:0, 2:0, 3:0, 4:0, 5:1, 6:0, 7:0, 8:0})\n",
    "y_train_6 = y_train.map({1:0, 2:0, 3:0, 4:0, 5:0, 6:1, 7:0, 8:0})\n",
    "y_train_7 = y_train.map({1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:1, 8:0})\n",
    "y_train_8 = y_train.map({1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:1})\n",
    "\n",
    "y_train_list = [y_train_1, y_train_2, y_train_3, y_train_4, y_train_5, y_train_6, y_train_7, y_train_8]\n",
    "results = []\n",
    "\n",
    "train['Product_Info_2'] = pd.factorize(train['Product_Info_2'])[0]\n",
    "test['Product_Info_2'] = pd.factorize(test['Product_Info_2'])[0]\n",
    "\n",
    "train_features = train[features].fillna(-1)\n",
    "test_features = test[features].fillna(-1)\n",
    "print(train_features)\n",
    "\n",
    "               \n",
    "num_round=500\n",
    "plst = get_params()\n",
    "print plst\n",
    "\n",
    "\n",
    "for y_train_item in y_train_list:\n",
    "    \n",
    "    dtrain=xgb.DMatrix(train_features,label=y_train_item)\n",
    "    dtest=xgb.DMatrix(test_features)\n",
    "\n",
    "    watchlist  = [(dtrain,'train')]\n",
    "\n",
    "    bst = xgb.train(plst, dtrain, num_round, watchlist)\n",
    "\n",
    "    print(\"Training the model\")\n",
    "    y_test_bst=bst.predict(dtest)\n",
    "    results.append(y_test_bst)\n",
    "\n",
    "print results\n",
    "\n",
    "#def output_function(x):\n",
    "#    if x<1:\n",
    "#        return 1\n",
    "#    elif x>8:\n",
    "#        return 8\n",
    "    #elif int(round(x))==3:\n",
    "    #    return 2\n",
    "#    else:\n",
    "#        return int(round(x))\n",
    "        \n",
    "#y_test_bst_result=[output_function(y) for y in y_test_bst]\n",
    "\n",
    "#write results\n",
    "#ids=test.Id.values.tolist()\n",
    "#n_ids=len(ids)\n",
    "\n",
    "\n",
    "#import csv\n",
    "#prediction_file = open(\"pythonxgb.csv\", \"w\")\n",
    "#prediction_file_object = csv.writer(prediction_file)\n",
    "#prediction_file_object.writerow([\"Id\",\"Response\"])\n",
    "#for i in range(0,n_ids):\n",
    "#    prediction_file_object.writerow([ids[i],y_test_bst_result[i]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.18189684,  0.04002952,  0.04911706, ...,  0.48660439,\n",
      "        0.06455781,  0.11096793], dtype=float32), array([ 0.17126404,  0.0457582 ,  0.05060061, ...,  0.04708283,\n",
      "        0.50229567,  0.10515675], dtype=float32), array([ 0.0008391 ,  0.0017683 ,  0.00484451, ...,  0.00301457,\n",
      "        0.00108978,  0.23855457], dtype=float32), array([ 0.00146568,  0.00941631,  0.00471496, ...,  0.00029976,\n",
      "        0.0004955 ,  0.14125332], dtype=float32), array([ 0.01392779,  0.03226906,  0.0184226 , ...,  0.15576909,\n",
      "        0.15073209,  0.00133464], dtype=float32), array([ 0.35438439,  0.20778076,  0.4477694 , ...,  0.45147094,\n",
      "        0.04487349,  0.01616646], dtype=float32), array([ 0.43252948,  0.10000362,  0.22745346, ...,  0.0650955 ,\n",
      "        0.00200009,  0.01805603], dtype=float32), array([  1.94472168e-02,   6.49320722e-01,   1.42768472e-01, ...,\n",
      "         1.84513605e-03,   9.91599634e-03,   3.55532655e-04], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8    8405\n",
      "6    3971\n",
      "7    2509\n",
      "5    1624\n",
      "1    1306\n",
      "2    1199\n",
      "4     477\n",
      "3     274\n",
      "dtype: int64\n",
      "<bound method Series.tail of 0        7\n",
      "1        8\n",
      "2        6\n",
      "3        8\n",
      "4        8\n",
      "5        8\n",
      "6        8\n",
      "7        8\n",
      "8        7\n",
      "9        8\n",
      "10       8\n",
      "11       8\n",
      "12       4\n",
      "13       8\n",
      "14       6\n",
      "15       8\n",
      "16       7\n",
      "17       8\n",
      "18       8\n",
      "19       2\n",
      "20       8\n",
      "21       8\n",
      "22       8\n",
      "23       8\n",
      "24       6\n",
      "25       7\n",
      "26       8\n",
      "27       8\n",
      "28       2\n",
      "29       8\n",
      "        ..\n",
      "19735    7\n",
      "19736    8\n",
      "19737    2\n",
      "19738    2\n",
      "19739    6\n",
      "19740    8\n",
      "19741    8\n",
      "19742    1\n",
      "19743    8\n",
      "19744    6\n",
      "19745    6\n",
      "19746    6\n",
      "19747    5\n",
      "19748    6\n",
      "19749    1\n",
      "19750    2\n",
      "19751    8\n",
      "19752    2\n",
      "19753    6\n",
      "19754    8\n",
      "19755    6\n",
      "19756    2\n",
      "19757    7\n",
      "19758    6\n",
      "19759    8\n",
      "19760    8\n",
      "19761    8\n",
      "19762    1\n",
      "19763    2\n",
      "19764    3\n",
      "Name: Response, dtype: int64>\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df['1'] = results[0]\n",
    "df['2'] = results[1]\n",
    "df['3'] = results[2]\n",
    "df['4'] = results[3]\n",
    "df['5'] = results[4]\n",
    "df['6'] = results[5]\n",
    "df['7'] = results[6]\n",
    "df['8'] = results[7]\n",
    "\n",
    "\n",
    "#print df.max(axis=1)\n",
    "df['Response'] = df.idxmax(axis=1)\n",
    "\n",
    "print df['Response'].value_counts()\n",
    "df['Response'] = [int(x) for x in df['Response']]\n",
    "\n",
    "print df['Response'].tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x110ed2bd0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4UAAAFICAYAAADnOVvjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110ef1050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# response\n",
    "\n",
    "fig, (axis1) = plt.subplots(1,1,figsize=(15,5))\n",
    "\n",
    "sns.countplot(x=df_train[\"Response\"], order=[1,2,3,4,5,6,7,8], ax=axis1)\n",
    "\n",
    "sns.countplot(x=df['Response'], order=[1,2,3,4,5,6,7,8], ax=axis1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19765\n"
     ]
    }
   ],
   "source": [
    "ids=test.Id.values.tolist()\n",
    "n_ids=len(ids)\n",
    "print n_ids\n",
    "\n",
    "import csv\n",
    "prediction_file = open(\"pythonxgb_log_models.csv\", \"w\")\n",
    "prediction_file_object = csv.writer(prediction_file)\n",
    "prediction_file_object.writerow([\"Id\",\"Response\"])\n",
    "for i in range(0,n_ids):\n",
    "    prediction_file_object.writerow([ids[i],df.Response[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.52752712  0.50186294  1.79943693 ...,  0.55049441 -0.43970673\n",
      "   1.99920694]\n",
      " [-0.69371596  0.2604454   0.00731395 ...,  0.6305631  -0.23592076\n",
      "   2.05640008]\n",
      " [-0.86663126  0.24700322  0.30434995 ..., -0.69134298  0.37171367\n",
      "  -0.48218256]\n",
      " ..., \n",
      " [ 1.54360409  0.58399225 -0.4725698  ...,  0.48453356  0.37030027\n",
      "   0.17605427]\n",
      " [ 1.53546352  0.50760388 -0.04449644 ...,  1.10299749 -0.05368743\n",
      "   1.13230296]\n",
      " [-0.91236242  0.0948748  -1.86525751 ..., -1.27072336 -1.40417435\n",
      "   0.36537064]]\n"
     ]
    }
   ],
   "source": [
    "#gives .56 value... something isn't quite working here... distribution looks ok, but something is going on...\n",
    "#why is it that the offset works better? can we do some sort of automatic stacking of models?\n",
    "#look at the blend_proba code\n",
    "\n",
    "\n",
    "#also, look at splitting the train data into train/test, build a model, use offsets, then train on the whole train set\n",
    "#then build another model on the test set ... ?\n",
    "\n",
    "#also look at PCA for some of the features? what matters?\n",
    "\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "pca = RandomizedPCA(n_components=len(continuous), whiten=True)\n",
    "pca.fit(all_data[continuous])\n",
    "all_data_PCA = pca.transform(all_data[continuous])\n",
    "\n",
    "print all_data_PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'BMI', u'Employment_Info_1', u'Employment_Info_2',\n",
      "       u'Employment_Info_3', u'Employment_Info_4', u'Employment_Info_5',\n",
      "       u'Employment_Info_6', u'Family_Hist_1', u'Family_Hist_2',\n",
      "       u'Family_Hist_3', \n",
      "       ...\n",
      "       u'PCA4', u'PCA5', u'PCA6', u'PCA7', u'PCA8', u'PCA9', u'PCA10',\n",
      "       u'PCA11', u'PCA12', u'PCA13'],\n",
      "      dtype='object', length=142)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import RandomizedPCA\n",
    "pca = RandomizedPCA(n_components=len(continuous), whiten=True)\n",
    "pca.fit(all_data[continuous])\n",
    "all_data_PCA = pca.transform(all_data[continuous])\n",
    "\n",
    "PCA_titles = ['PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5', 'PCA6', 'PCA7', 'PCA8', 'PCA9', 'PCA10', 'PCA11', 'PCA12', 'PCA13']\n",
    "all_data_PCA_df = pd.DataFrame(all_data_PCA, index=all_data.index, columns=PCA_titles)\n",
    "    \n",
    "\n",
    "\n",
    "#add back into all_data and drop the continuous variables\n",
    "all_data_new = pd.concat([all_data, all_data_PCA_df], axis=1)\n",
    "all_data_new.drop(continuous, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the data using pandas\n",
      "Eliminate missing values\n",
      "    |    Population Average   |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    25.56    8995657.34371        3 0.00928108977957 0.00942919683448      7.51m\n"
     ]
    }
   ],
   "source": [
    "from gplearn.genetic import SymbolicRegressor\n",
    "from gplearn.genetic import SymbolicTransformer\n",
    "\n",
    "\n",
    "DATA_DIR = '/Users/patrickkennedy/Desktop'\n",
    "\n",
    "print(\"Load the data using pandas\")\n",
    "train = pd.read_csv(DATA_DIR + '/Project DATA/train.csv')\n",
    "test = pd.read_csv(DATA_DIR + '/Project DATA/test.csv')\n",
    "\n",
    "# combine train and test\n",
    "all_data = train.append(test)\n",
    "\n",
    "print('Eliminate missing values')    \n",
    "# Use -1 for any others\n",
    "all_data.fillna(-1, inplace=True)\n",
    "\n",
    "categorical = ['Product_Info_1', 'Product_Info_2', 'Product_Info_3', 'Product_Info_5', 'Product_Info_6', 'Product_Info_7', 'Employment_Info_2', 'Employment_Info_3', 'Employment_Info_5', 'InsuredInfo_1', 'InsuredInfo_2', 'InsuredInfo_3', 'InsuredInfo_4', 'InsuredInfo_5', 'InsuredInfo_6', 'InsuredInfo_7', 'Insurance_History_1', 'Insurance_History_2', 'Insurance_History_3', 'Insurance_History_4', 'Insurance_History_7', 'Insurance_History_8', 'Insurance_History_9', 'Family_Hist_1', 'Medical_History_2', 'Medical_History_3', 'Medical_History_4', 'Medical_History_5', 'Medical_History_6', 'Medical_History_7', 'Medical_History_8', 'Medical_History_9', 'Medical_History_10', 'Medical_History_11', 'Medical_History_12', 'Medical_History_13', 'Medical_History_14', 'Medical_History_16', 'Medical_History_17', 'Medical_History_18', 'Medical_History_19', 'Medical_History_20', 'Medical_History_21', 'Medical_History_22', 'Medical_History_23', 'Medical_History_25', 'Medical_History_26', 'Medical_History_27', 'Medical_History_28', 'Medical_History_29', 'Medical_History_30', 'Medical_History_31', 'Medical_History_33', 'Medical_History_34', 'Medical_History_35', 'Medical_History_36', 'Medical_History_37', 'Medical_History_38', 'Medical_History_39', 'Medical_History_40', 'Medical_History_41','Medical_History_1', 'Medical_History_15', 'Medical_History_24', 'Medical_History_32']\n",
    "continuous = ['Product_Info_4', 'Ins_Age', 'Ht', 'Wt', 'BMI', 'Employment_Info_1', 'Employment_Info_4', 'Employment_Info_6',\n",
    "              'Insurance_History_5', 'Family_Hist_2', 'Family_Hist_3', 'Family_Hist_4', 'Family_Hist_5'] \n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(all_data[continuous])\n",
    "pca = PCA(n_components=len(continuous), whiten=True).fit(X_scaled)\n",
    "all_data_PCA = pca.transform(X_scaled)\n",
    "\n",
    "PCA_titles = ['PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5', 'PCA6', 'PCA7', 'PCA8', 'PCA9', 'PCA10', 'PCA11', 'PCA12', 'PCA13']\n",
    "all_data_PCA_df = pd.DataFrame(all_data_PCA, index=all_data.index, columns=PCA_titles)\n",
    "    \n",
    "#add back into all_data and drop the continuous variables\n",
    "all_data_new = pd.concat([all_data, all_data_PCA_df], axis=1)\n",
    "all_data_new.drop(continuous, axis=1)\n",
    "\n",
    "all_data_new['Product_Info_2'] = pd.factorize(all_data_new['Product_Info_2'])[0]\n",
    "\n",
    "# fix the dtype on the label column\n",
    "all_data_new['Response'] = all_data_new['Response'].astype(int)\n",
    "\n",
    "# Provide split column\n",
    "all_data_new['Split'] = np.random.randint(5, size=all_data_new.shape[0])\n",
    "\n",
    "# split train and test\n",
    "train = all_data_new[all_data_new['Response']>0].copy()\n",
    "test = all_data_new[all_data_new['Response']<1].copy()\n",
    "\n",
    "\n",
    "#using dict vectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "cat_train_df = train[categorical]\n",
    "cat_train_dict = cat_train_df.T.to_dict().values()\n",
    "cat_test_df = test[categorical]\n",
    "cat_test_dict = cat_test_df.T.to_dict().values()\n",
    "x_train_no_cat = train.drop(categorical, axis=1)\n",
    "x_test_no_cat = test.drop(categorical, axis=1)\n",
    "vectorizer = DV(sparse=False)\n",
    "vec_x_cat_train = pd.DataFrame(vectorizer.fit_transform(cat_train_dict))\n",
    "vec_x_cat_test = pd.DataFrame(vectorizer.transform(cat_test_dict)) \n",
    "\n",
    "x_train = pd.concat([x_train_no_cat, vec_x_cat_train], axis=1)\n",
    "x_test = pd.concat([x_test_no_cat, vec_x_cat_test], axis=1)\n",
    "\n",
    "\n",
    "est_gp = SymbolicRegressor(population_size=5000,\n",
    "                           generations=20, stopping_criteria=0.01,\n",
    "                           comparison=False, transformer=False,\n",
    "                           p_crossover=0.7, p_subtree_mutation=0.1,\n",
    "                           p_hoist_mutation=0.05, p_point_mutation=0.1,\n",
    "                           max_samples=0.9, verbose=1,\n",
    "                           parsimony_coefficient=0.01, random_state=0)\n",
    "est_gp.fit(x_train, y_train)\n",
    "y_gp = est_gp.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "#gp = SymbolicTransformer(generations=20, population_size=2000,\n",
    "#                         hall_of_fame=100, n_components=10,\n",
    "#                         parsimony_coefficient=0.0005,\n",
    "#                         max_samples=0.9, verbose=1,\n",
    "#                         random_state=0, n_jobs=3)\n",
    "#gp.fit(boston.data[:300, :], boston.target[:300])\n",
    "\n",
    "#gp_features = gp.transform(boston.data)\n",
    "#new_boston = np.hstack((boston.data, gp_features))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#how to do a multiclass classification with a genetic algorithm like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration [0]\n",
      "X_test.shape = (11877, 126)\n",
      "blend_train.shape = (47504, 3)\n",
      "blend_test.shape = (11877, 3)\n",
      "Training classifier [0]\n",
      "Fold [0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (10452,) could not be broadcast to indexing result of shape (9504,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-0108db4587ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'Iteration [%s]'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-0108db4587ea>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;31m# This output will be the basis for our blended classifier to train against,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# which is also the output of our classifiers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mblend_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcv_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0mblend_test_j\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# Take the mean of the predictions of the cross validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: value array of shape (10452,) could not be broadcast to indexing result of shape (9504,)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Purpose: This script tries to implement a technique called stacking/blending/stacked generalization.\n",
    "The reason I have to make this a runnable script because I found that there isn't really any\n",
    "readable code that demonstrates this technique. You may find the pseudocode in various papers but they\n",
    "are all each kind of different.\n",
    "Author: Eric Chio \"log0\" <im.ckieric@gmail.com>\n",
    "======================================================================================================\n",
    "Summary:\n",
    "Just to test an implementation of stacking. Using a cross-validated random forest and SVMs, I was\n",
    "only able to achieve an accuracy of about 88% (with 1000 trees and up). Using stacked generalization \n",
    "I have seen a maximum of 93.5% accuracy. It does take runs to find it out though. This uses only \n",
    "(10, 20, 10) trees for the three classifiers.\n",
    "This code is heavily inspired from the code shared by Emanuele (https://github.com/emanuele) , but I\n",
    "have cleaned it up to makeit available for easy download and execution.\n",
    "======================================================================================================\n",
    "Methodology:\n",
    "Three classifiers (RandomForestClassifier, ExtraTreesClassifier and a GradientBoostingClassifier\n",
    "are built to be stacked by a LogisticRegression in the end.\n",
    "Some terminologies first, since everyone has their own, I'll define mine to be clear:\n",
    "- DEV SET, this is to be split into the training and validation data. It will be cross-validated.\n",
    "- TEST SET, this is the unseen data to validate the generalization error of our final classifier. This\n",
    "set will never be used to train.\n",
    "======================================================================================================\n",
    "Log Output:\n",
    "X_test.shape = (62L, 6L)\n",
    "blend_train.shape = (247L, 3L)\n",
    "blend_test.shape = (62L, 3L)\n",
    "Training classifier [0]\n",
    "Fold [0]\n",
    "Fold [1]\n",
    "Fold [2]\n",
    "Fold [3]\n",
    "Fold [4]\n",
    "Training classifier [1]\n",
    "Fold [0]\n",
    "Fold [1]\n",
    "Fold [2]\n",
    "Fold [3]\n",
    "Fold [4]\n",
    "Training classifier [2]\n",
    "Fold [0]\n",
    "Fold [1]\n",
    "Fold [2]\n",
    "Fold [3]\n",
    "Fold [4]\n",
    "Y_dev.shape = 247\n",
    "Accuracy = 0.935483870968\n",
    "======================================================================================================\n",
    "Data Set Information:\n",
    "Biomedical data set built by Dr. Henrique da Mota during a medical residence period in the Group\n",
    "of Applied Research in Orthopaedics (GARO) of the Centre MÃƒÂ©dico-Chirurgical de RÃƒÂ©adaptation des\n",
    "Massues, Lyon, France. The data have been organized in two different but related classification\n",
    "tasks. The first task consists in classifying patients as belonging to one out of three\n",
    "categories: Normal (100 patients), Disk Hernia (60 patients) or Spondylolisthesis (150\n",
    "patients). For the second task, the categories Disk Hernia and Spondylolisthesis were merged \n",
    "into a single category labelled as 'abnormal'. Thus, the second task consists in classifying\n",
    "patients as belonging to one out of two categories: Normal (100 patients) or Abnormal (210 \n",
    "patients). We provide files also for use within the WEKA environment.\n",
    "Attribute Information:\n",
    "Each patient is represented in the data set by six biomechanical attributes derived from the \n",
    "shape and orientation of the pelvis and lumbar spine (in this order): pelvic incidence, pelvic\n",
    "tilt, lumbar lordosis angle, sacral slope, pelvic radius and grade of spondylolisthesis. The\n",
    "following convention is used for the class labels: DH (Disk Hernia), Spondylolisthesis (SL),\n",
    "Normal (NO) and Abnormal (AB).\n",
    "\"\"\"\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from scipy.optimize import fmin_powell\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "\n",
    "def run(data):\n",
    "    data['Product_Info_2'] = pd.factorize(data['Product_Info_2'])[0]\n",
    "    data.fillna(-1, inplace=True)\n",
    "\n",
    "    #X = np.array([ i[1:-1] for i in data ], dtype=float)\n",
    "    #Y = np.array([ i[-1] for i in data ])\n",
    "    features = data.columns\n",
    "    features = features.drop(\"Id\")\n",
    "    features = features.drop(\"Response\")\n",
    "    X = data[features]\n",
    "    Y = data[\"Response\"]\n",
    "    \n",
    "    \n",
    "    # We need to transform the string output to numeric\n",
    "    #label_encoder = LabelEncoder()\n",
    "    #label_encoder.fit(Y)\n",
    "    #Y = label_encoder.transform(Y)\n",
    "    \n",
    "    # The DEV SET will be used for all training and validation purposes\n",
    "    # The TEST SET will never be used for training, it is the unseen set.\n",
    "    dev_cutoff = len(Y) * 4/5\n",
    "    X_dev = X[:dev_cutoff]\n",
    "    Y_dev = Y[:dev_cutoff]\n",
    "    X_test = X[dev_cutoff:]\n",
    "    Y_test = Y[dev_cutoff:]\n",
    "    \n",
    "    #print X_dev.shape, Y_dev.shape, X_test.shape, Y_test.shape\n",
    "    \n",
    "    n_trees = 10\n",
    "    n_folds = 5\n",
    "    \n",
    "    # Our level 0 classifiers\n",
    "    clfs = [\n",
    "        RandomForestClassifier(n_estimators = n_trees, criterion = 'gini'),\n",
    "        ExtraTreesClassifier(n_estimators = n_trees * 2, criterion = 'gini'),\n",
    "        GradientBoostingClassifier(n_estimators = n_trees),\n",
    "    ]\n",
    "    \n",
    "    # Ready for cross validation\n",
    "    skf = list(StratifiedKFold(Y_dev, n_folds))\n",
    "\n",
    "    # Pre-allocate the data\n",
    "    blend_train = np.zeros((X_dev.shape[0], len(clfs))) # Number of training data x Number of classifiers\n",
    "    blend_test = np.zeros((X_test.shape[0], len(clfs))) # Number of testing data x Number of classifiers\n",
    "    \n",
    "    print 'X_test.shape = %s' % (str(X_test.shape))\n",
    "    print 'blend_train.shape = %s' % (str(blend_train.shape))\n",
    "    print 'blend_test.shape = %s' % (str(blend_test.shape))\n",
    "    \n",
    "    # For each classifier, we train the number of fold times (=len(skf))\n",
    "    for j, clf in enumerate(clfs):\n",
    "        print 'Training classifier [%s]' % (j)\n",
    "        blend_test_j = np.zeros((X_test.shape[0], len(skf))) # Number of testing data x Number of folds , we will take the mean of the predictions later\n",
    "        for i, (train_index, cv_index) in enumerate(skf):\n",
    "            print 'Fold [%s]' % (i)\n",
    "            # This is the training and validation set\n",
    "            X_train = X_dev[train_index[0]:train_index[-1]]\n",
    "            Y_train = Y_dev[train_index[0]:train_index[-1]]\n",
    "            X_cv = X_dev[cv_index[0]:cv_index[-1]]\n",
    "            Y_cv = Y_dev[cv_index[0]:cv_index[-1]]\n",
    "            \n",
    "            clf.fit(X_train, Y_train)\n",
    "            \n",
    "            # This output will be the basis for our blended classifier to train against,\n",
    "            # which is also the output of our classifiers\n",
    "            blend_train[cv_index, j] = clf.predict(X_cv)\n",
    "            blend_test_j[:, i] = clf.predict(X_test)\n",
    "        # Take the mean of the predictions of the cross validation set\n",
    "        blend_test[:, j] = blend_test_j.mean(1)\n",
    "    \n",
    "    print 'Y_dev.shape = %s' % (Y_dev.shape)\n",
    "    \n",
    "    # Start blending!\n",
    "    bclf = LogisticRegression()\n",
    "    bclf.fit(blend_train, Y_dev)\n",
    "    \n",
    "    # Predict now\n",
    "    Y_test_predict = bclf.predict(blend_test)\n",
    "    score = metrics.accuracy_score(Y_test, Y_test_predict)\n",
    "    print 'Accuracy = %s' % (score)\n",
    "    \n",
    "    return score\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    DATA_DIR = '/Users/patrickkennedy/Desktop'\n",
    "\n",
    "    train = pd.read_csv(DATA_DIR + '/Project DATA/train.csv')\n",
    "\n",
    "    #data = [ i for i in csv.reader(file(train_file, 'rb'), delimiter=' ') ]\n",
    "    #data = data[1:] # remove header\n",
    "    \n",
    "    best_score = 0.0\n",
    "    \n",
    "    # run many times to get a better result, it's not quite stable.\n",
    "    for i in xrange(1):\n",
    "        print 'Iteration [%s]' % (i)\n",
    "        train.sample(frac=1)\n",
    "        score = run(train)\n",
    "        best_score = max(best_score, score)\n",
    "        print\n",
    "        \n",
    "    print 'Best score = %s' % (best_score)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_dev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-98ca9fa0589b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#step 4: pick models... see if training 100 different models makes a difference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_dev' is not defined"
     ]
    }
   ],
   "source": [
    "#the above code shows a stacked and blended model... stacking means using multiple models\n",
    "#taking their outputs and putting into another DF composed of just those results,\n",
    "#then running another model (like a logistic regression) on that DF\n",
    "#using that model to predict the test set(?notquite... seems to predict the test set across each clf and then mean()\n",
    "#them into one list, then use *this* set to run the 2nd layer model)\n",
    "\n",
    "#try to organize this info with the dataset I have... train a handful of xgboosts, randomforests, whatever else,\n",
    "#then run it through a second layer or xgboosts and/or NN or whatever, then maybe a third level?\n",
    "\n",
    "#what i also want to know is how well the model is doing so that i don't submit multiple times... how?\n",
    "#thought: i can do a quadratic weighted kappa on my current model and see if it performs better or worse,\n",
    "#as well as compare it to it's general training accuracy -- if high acc but lower than my model, overfit\n",
    "\n",
    "#step 1: organize the current flow to train multiple classifiers\n",
    "#step 2: use a second level classifier and test the result\n",
    "#step 3: organize code into distinct functions so that i can plug and play\n",
    "#step 4: pick models... see if training 100 different models makes a difference\n",
    "\n",
    "print X_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the data using pandas\n",
      "Eliminate missing values\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '/Users/patrickkennedy/Desktop'\n",
    "\n",
    "print(\"Load the data using pandas\")\n",
    "train = pd.read_csv(DATA_DIR + '/Project DATA/train.csv')\n",
    "test = pd.read_csv(DATA_DIR + '/Project DATA/test.csv')\n",
    "\n",
    "# combine train and test\n",
    "all_data = train.append(test)\n",
    "\n",
    "print('Eliminate missing values')    \n",
    "# Use -1 for any others\n",
    "all_data.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BMI</th>\n",
       "      <th>Employment_Info_1</th>\n",
       "      <th>Employment_Info_2</th>\n",
       "      <th>Employment_Info_3</th>\n",
       "      <th>Employment_Info_4</th>\n",
       "      <th>Employment_Info_5</th>\n",
       "      <th>Employment_Info_6</th>\n",
       "      <th>Family_Hist_1</th>\n",
       "      <th>Family_Hist_2</th>\n",
       "      <th>Family_Hist_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Medical_Keyword_9</th>\n",
       "      <th>Product_Info_1</th>\n",
       "      <th>Product_Info_2</th>\n",
       "      <th>Product_Info_3</th>\n",
       "      <th>Product_Info_4</th>\n",
       "      <th>Product_Info_5</th>\n",
       "      <th>Product_Info_6</th>\n",
       "      <th>Product_Info_7</th>\n",
       "      <th>Response</th>\n",
       "      <th>Wt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.272288</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>2</td>\n",
       "      <td>0.188406</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.131799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.428780</td>\n",
       "      <td>0.030</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>3</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>E1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.288703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.352438</td>\n",
       "      <td>0.042</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.420290</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>D4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.205021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        BMI  Employment_Info_1  Employment_Info_2  Employment_Info_3  \\\n",
       "1  0.272288              0.000                  1                  3   \n",
       "2  0.428780              0.030                  9                  1   \n",
       "3  0.352438              0.042                  9                  1   \n",
       "\n",
       "   Employment_Info_4  Employment_Info_5  Employment_Info_6  Family_Hist_1  \\\n",
       "1                  0                  2             0.0018              2   \n",
       "2                  0                  2             0.0300              3   \n",
       "3                  0                  3             0.2000              3   \n",
       "\n",
       "   Family_Hist_2  Family_Hist_3    ...     Medical_Keyword_9  Product_Info_1  \\\n",
       "1       0.188406             -1    ...                     0               1   \n",
       "2       0.304348             -1    ...                     0               1   \n",
       "3       0.420290             -1    ...                     0               1   \n",
       "\n",
       "   Product_Info_2  Product_Info_3  Product_Info_4  Product_Info_5  \\\n",
       "1              A1              26        0.076923               2   \n",
       "2              E1              26        0.076923               2   \n",
       "3              D4              10        0.487179               2   \n",
       "\n",
       "   Product_Info_6  Product_Info_7  Response        Wt  \n",
       "1               3               1         4  0.131799  \n",
       "2               3               1         8  0.288703  \n",
       "3               3               1         8  0.205021  \n",
       "\n",
       "[3 rows x 128 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestNeighbors as kNN\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from scipy.optimize import fmin_powell\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "\n",
    "#from numba import autojit\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_params(n):\n",
    "    #think about varying colsample_bytree to 1, 0.5, .25, .1\n",
    "    #then running a few different xgboost models (?) \n",
    "    params = {}\n",
    "    \n",
    "    if \"XGB-LinReg\" in n:\n",
    "        params[\"objective\"] = \"reg:linear\"     \n",
    "    elif \"XGB-poisson\" in n:\n",
    "        params[\"objective\"] = \"count:poisson\"     \n",
    "    elif \"XGB-multisoftmax\" in n:\n",
    "        params[\"objective\"] = \"multi:softmax\" \n",
    "        params[\"num_class\"] = 8\n",
    "\n",
    "    params[\"eta\"] = 0.01\n",
    "    params[\"min_child_weight\"] = 50\n",
    "    params[\"subsample\"] = 0.5\n",
    "    params[\"colsample_bytree\"] = 0.3\n",
    "    params[\"silent\"] = 1\n",
    "    params[\"max_depth\"] = 9\n",
    "    #params[\"nthread\"] = 16  #apparently doesn't work on mac b/c mac doesn't support open-cl or something\n",
    "        \n",
    "        \n",
    "    plst = list(params.items())    \n",
    "    return plst\n",
    "\n",
    "def eval_wrapper(yhat, y):  \n",
    "    y = np.array(y)\n",
    "    y = y.astype(int)\n",
    "    yhat = np.array(yhat)\n",
    "    yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)   \n",
    "    return quadratic_weighted_kappa(yhat, y)\n",
    "\n",
    "def apply_offset(data, bin_offset, sv, scorer=eval_wrapper):\n",
    "    # data has the format of pred=0, offset_pred=1, labels=2 in the first dim\n",
    "    data[1, data[0].astype(int)==sv] = data[0, data[0].astype(int)==sv] + bin_offset\n",
    "    score = scorer(data[1], data[2])\n",
    "    return score\n",
    "\n",
    "def generate_clfs(n):\n",
    "    # Our level 0 classifiers\n",
    "    #think about adding any NN like lasagne? h20? adaboost? sofia? vowelwabbit?\n",
    "    #also put classifiers into a list of lists so that I can organize and do a for loop iterating\n",
    "    #     over the train_and_cv function? that way we can have auto stacking\n",
    "    #but what of the ranking or order? how do we preserve this rather than 1 in a 8,8,8,8,8 throwing it off to a 7?\n",
    "    \n",
    "    #next steps: 1) use PCA on variables before putting into model -- check! (doesn't work)\n",
    "    #            2) take t-SNE and with the returned dataset, use XGBoost in first stage (freezes comp)\n",
    "    #            3) use some models with raw data (like knn)\n",
    "    #            4) make use of a neural network algo like Lasagne or Keras (but how?)\n",
    "    #            5) try to mimic what the other first place winners did...\n",
    "    #            6) think about transforming dataset and running same models i.e. XGBoost\n",
    "    #            7) try minibatch k-means as well (with log, raw, t-SNE, other?)\n",
    "    #            8) what about gridsearch for SGD, SVC, XGB? (would minimize the run time which stands at about 6 hrs)\n",
    "    #            9) think about how to take the result of the model, find out how well it correlates to other models,\n",
    "    #               prefer uncorrelated models, optimize preds against actual training data, pick models that give\n",
    "    #               best and worst(?) scores and include those in the ensemble - somehow make use of spark/distributed\n",
    "    #               platform to get speeds up\n",
    "    #           10) could be worth trying logistic regression with multi_class=multinomial\n",
    "    \n",
    "    clfs = [[\n",
    "        GaussianNB(),\n",
    "        MiniBatchKMeans(n_clusters=8),\n",
    "        \"XGB-LinReg\", #(objective=\"reg:linear\"),\n",
    "        \"XGB-poisson\", #(objective=\"count:poisson\"),\n",
    "        \"XGB-multisoftmax\", #(objective=\"multi:softmax\"),\n",
    "        \"XGB-LinReg--squared(x)\", #(get_params()),\n",
    "        \"XGB-poisson--squared(x)\", #XGBClassifier(objective=\"count:poisson\"),\n",
    "        \"XGB-multisoftmax--squared(x)\"#, #XGBClassifier(objective=\"multi:softmax\"),\n",
    "        \"XGB-LinReg--log(x)\"#, #(get_params()),\n",
    "        \"XGB-poisson--log(x)\", #XGBClassifier(objective=\"count:poisson\"),\n",
    "        \"XGB-multisoftmax--log(x)\"#, #XGBClassifier(objective=\"multi:softmax\"),\n",
    "        RandomForestClassifier(n_estimators = n, criterion = 'gini')#,\n",
    "        ExtraTreesClassifier(n_estimators = n * 2, criterion = 'gini')#,\n",
    "        GradientBoostingClassifier(n_estimators = n),\n",
    "        KNeighborsClassifier(n_neighbors = 3),\n",
    "        KNeighborsClassifier(n_neighbors = 5),\n",
    "        KNeighborsClassifier(n_neighbors = 7),\n",
    "        KNeighborsClassifier(n_neighbors = 9),\n",
    "        KNeighborsClassifier(n_neighbors = 11),\n",
    "        KNeighborsClassifier(n_neighbors = 13),\n",
    "        KNeighborsClassifier(n_neighbors = 15),\n",
    "        KNeighborsClassifier(n_neighbors = 17),\n",
    "        KNeighborsClassifier(n_neighbors = 19),\n",
    "        KNeighborsClassifier(n_neighbors = 21),\n",
    "        LinearSVC(), #maybe use LinearSVC (better for larger samples)\n",
    "        SGDClassifier(loss='perceptron'),\n",
    "        SGDClassifier(loss='squared_loss'), #regression loss\n",
    "        ],\n",
    "        \n",
    "        [\n",
    "        \"XGB-LinReg\",\n",
    "        AdaBoostClassifier()\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    columns = [[#\"GaussianNB\", \n",
    "                #\"MiniBatchKMeans(8)\",\n",
    "                \"XGB-LinReg\"#, \n",
    "                #\"XGB-poisson\", \n",
    "                #\"XGB-multisoftmax\", \n",
    "                #\"XGB-LinReg--squared(x)\", \n",
    "                #\"XGB-poisson--squared(x)\", \n",
    "                #\"XGB-multisoftmax--squared(x)\"#,\n",
    "                #\"XGB-LinReg--log(x)\"#, \n",
    "                #\"XGB-poisson--log(x)\", \n",
    "                #\"XGB-multisoftmax--log(x)\"#,\n",
    "                #\"RF\"#, \n",
    "                #\"ExtraTrees\", \n",
    "                #\"GBoost\", \n",
    "                #\"kNN=3\", \n",
    "                #\"kNN=5\", \n",
    "                #\"kNN=7\", \n",
    "                #\"kNN=9\", \"kNN=11\", \"kNN=13\",\n",
    "                #\"kNN=15\", \"kNN=17\", \"kNN=19\", \"kNN=21\", \n",
    "                #\"LinearSVC\", \n",
    "                #\"SGD-squaredhinge\", \n",
    "                #\"SGD-hinge\",\n",
    "                #\"SGD-modified_huber\", \n",
    "                #\"SGD-perceptron\", \n",
    "                #\"SGD-squared_loss\"#, \n",
    "                #\"SGD-huber\", \n",
    "                #\"SGD-epsilon_insensitive\"#, \n",
    "                #\"SGD-squared_epsilon_insensitive\"\n",
    "                ],\n",
    "               [#\"XGB-LinReg\",\n",
    "                #\"XGB-multisoftmax\",\n",
    "                #\"XGB-LinReg--squared(x)\",\n",
    "                #\"AdaBoost\"\n",
    "               ]]\n",
    "    \n",
    "    return clfs, columns\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run(data, for_score):    \n",
    "    #data['Product_Info_2'] = pd.factorize(data['Product_Info_2'])[0]\n",
    "    data.fillna(-1, inplace=True)\n",
    "    \n",
    "    \n",
    "    #add a real_deal boolean so that i don't have to recode when i use test/train vs kaggle submission\n",
    "    features = data.columns\n",
    "    features = features.drop(\"Id\")\n",
    "    features = features.drop(\"Response\")\n",
    "    \n",
    "    \n",
    "    #DO NOT RUN TSNE!!!! FREEZES THE COMPUTER\n",
    "    #model = TSNE(n_components=3, random_state=0)\n",
    "    #data_TSNE = model.fit_transform(data[continuous])\n",
    "    #pd.concat([data[features]])\n",
    "    \n",
    "    if for_score:\n",
    "\n",
    "        train = data[data['Response']>0].copy()\n",
    "        test = data[data['Response']<1].copy()\n",
    "                \n",
    "        X = train[features].as_matrix()\n",
    "        init_y = train[\"Response\"]-1\n",
    "        Y = init_y.as_matrix()\n",
    "        \n",
    "        X_dev = X[:]\n",
    "        Y_dev = Y[:]\n",
    "        X_test = test[features]\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        data = data[data['Response']>0].copy()    \n",
    "        X = data[features].as_matrix()\n",
    "        \n",
    "        \n",
    "        transform_type = \"None\"\n",
    "        \n",
    "        if transform_type == \"log\":\n",
    "            X = np.log(X)\n",
    "        elif transform_type == \"log+X\":\n",
    "            x = np.log(X)\n",
    "            X = np.column_stack((X,x))\n",
    "            print \"new shape: X: \"\n",
    "            print X.shape\n",
    "        elif transform_type == \"squared\":\n",
    "            X = X**2\n",
    "        elif transform_type == \"squared+X\":\n",
    "            x = X**2\n",
    "            X = np.column_stack((X, x))\n",
    "            print \"new shape: X: \"\n",
    "            print X.shape\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        print \"Features transformed: \" + transform_type\n",
    "        \n",
    "        init_y = data[\"Response\"]-1\n",
    "        Y = init_y.as_matrix()\n",
    "        \n",
    "        # The DEV SET will be used for all training and validation purposes\n",
    "        # The TEST SET will never be used for training, it is the unseen set.\n",
    "        dev_cutoff = len(Y) * 4/5\n",
    "        X_dev = X[:dev_cutoff]\n",
    "        Y_dev = Y[:dev_cutoff]\n",
    "        X_test = X[dev_cutoff:]  #test[features]\n",
    "        Y_test = Y[dev_cutoff:]  #test[dev_cutoff:]\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    n_trees = 100\n",
    "    n_folds = 3\n",
    "    \n",
    "    clfs, columns = generate_clfs(n_trees)\n",
    "    print \"Number clfs level 1: \" + str(len(clfs[0])) + \"; level 2: \" + str(len(clfs[1]))\n",
    "    \n",
    "    # Ready for cross validation\n",
    "    skf = list(StratifiedKFold(Y_dev, n_folds))\n",
    "    # For each classifier, we train the number of fold times (=len(skf))\n",
    "    \n",
    "    \n",
    "    levels = 1\n",
    "    \n",
    "    for i in range(levels):        \n",
    "        blend_train, blend_test = train_and_cv(clfs[0], skf, X_dev, Y_dev, X_test, columns[0])\n",
    "        X_dev = blend_train\n",
    "        X_test = blend_test\n",
    "        \n",
    "    \n",
    "    blend_train_preds = blend_train.mean(1)\n",
    "    blend_test_preds = blend_test.mean(1)\n",
    "    \n",
    "    blend_train_preds = blend_train_preds+1\n",
    "    blend_test_preds = blend_test_preds+1\n",
    "    \n",
    "    int_train_preds = np.clip(blend_train_preds, -0.99, 8.99).astype(int)\n",
    "    int_test_preds = np.clip(blend_test_preds, -0.99, 8.99).astype(int)\n",
    "    #splits = [1, 2.5, 3.5, 4, 5.2, 6.8, 7.5, 8]\n",
    "    #int_train_preds = np.digitize(blend_train_preds, splits).astype(int)\n",
    "    #int_test_preds = np.digitize(blend_test_preds, splits).astype(int)\n",
    "    \n",
    "    num_classes=8\n",
    "    #train offsets \n",
    "    offsets = np.ones(num_classes) * -0.5\n",
    "    offset_train_preds = np.vstack((int_train_preds, int_train_preds, Y_dev))\n",
    "\n",
    "    for j in range(num_classes):\n",
    "        train_offset = lambda x: -apply_offset(offset_train_preds, x, j)\n",
    "        offsets[j] = fmin_powell(train_offset, offsets[j])  \n",
    "\n",
    "\n",
    "    # apply offsets to test\n",
    "    int_test_vals = (int_test_preds*0)-1\n",
    "    data_off = np.vstack((int_test_preds, int_test_preds, int_test_vals))  #here we exchange test response with -1\n",
    "    for j in range(num_classes):\n",
    "        data_off[1, data_off[0].astype(int)==j] = data_off[0, data_off[0].astype(int)==j] + offsets[j]    \n",
    "    final_test_preds = np.round(np.clip(data_off[1], 1, 8)).astype(int)\n",
    "    #final_test_preds = np.digitize(data_off[1], splits).astype(int)\n",
    "    \n",
    "    if for_score:\n",
    "        preds_out = pd.DataFrame({\"Id\": test['Id'].values, \"Response\": final_test_preds})\n",
    "        preds_out = preds_out.set_index('Id')\n",
    "        preds_out.to_csv('ensemble.csv')\n",
    "    \n",
    "    else:\n",
    "        # Predict now\n",
    "        score = eval_wrapper(final_test_preds, Y_test)\n",
    "        print 'Accuracy = %s' % (score)\n",
    "    \n",
    "    return final_test_preds, blend_train, blend_test #, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#@autojit\n",
    "def train_and_cv(clfs, skf, X_dev, Y_dev, X_test, columns):\n",
    "    blend_train = np.zeros((X_dev.shape[0], len(clfs))) # Number of training data x Number of classifiers\n",
    "    blend_test = np.zeros((X_test.shape[0], len(clfs))) # Number of testing data x Number of classifiers\n",
    "    \n",
    "    for j, clf in enumerate(clfs):\n",
    "        print 'Training classifier [%s]' % (columns[j])\n",
    "        blend_test_j = np.zeros((X_test.shape[0], len(skf))) # Number of testing data x Number of folds \n",
    "                                                             #, we will take the mean of the predictions later\n",
    "        \n",
    "        for i, (train_index, cv_index) in enumerate(skf):\n",
    "            print 'Fold [%s]' % (i)\n",
    "            # This is the training and validation set\n",
    "            \n",
    "            X_train = X_dev[train_index]\n",
    "            X_cv = X_dev[cv_index]\n",
    "                \n",
    "            Y_train = Y_dev[train_index]\n",
    "            Y_cv = Y_dev[cv_index]\n",
    "            \n",
    "            \n",
    "            \n",
    "            if \"XGB\" in columns[j]:\n",
    "                dtrain=xgb.DMatrix(X_train,Y_train)\n",
    "                dcv = xgb.DMatrix(X_cv, Y_cv)\n",
    "                \n",
    "                model = xgb.train(get_params(columns[j]), dtrain, 500)\n",
    "                blend_train[cv_index, j] = model.predict(dcv, ntree_limit=model.best_iteration)\n",
    "            \n",
    "                dtest=xgb.DMatrix(X_test, label=Y_dev)\n",
    "                blend_test_j[:,i] = model.predict(dtest)\n",
    "\n",
    "            else:\n",
    "                clf.fit(X_train, Y_train)\n",
    "                # This output will be the basis for our blended classifier to train against,\n",
    "                # which is also the output of our classifiers\n",
    "                blend_train[cv_index, j] = clf.predict(X_cv)\n",
    "                blend_test_j[:,i] = clf.predict(X_test)            \n",
    "\n",
    "                \n",
    "  \n",
    "\n",
    "    for j, clf in enumerate(clfs):\n",
    "        print 'Training classifier [%s]' % (columns[j])\n",
    "        \n",
    "        for i, (train_index, cv_index) in enumerate(skf):\n",
    "            print 'Fold [%s]' % (i)\n",
    "            # This is the training and validation set\n",
    "            \n",
    "            X_train = X[train_index]\n",
    "            X_cv = X[cv_index]\n",
    "                \n",
    "            y_train = y[train_index]\n",
    "            y_cv = y[cv_index]\n",
    "\n",
    "\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            # This output will be the basis for our \n",
    "            #   stacked classifier to train against,\n",
    "            #   which is also the output of our classifiers\n",
    "            stack_train[cv_index, j] = clf.predict(X_cv)\n",
    "            stack_test_j[:,i] = clf.predict(X_test) \n",
    "                \n",
    "        stack_test[:,j] = stack_test_j.mean(1)\n",
    "        \n",
    "    return stack_train, stack_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79146, 128)\n",
      "Features transformed: None\n",
      "Number clfs level 1: 1; level 2: 0\n",
      "Training classifier [XGB-LinReg]\n",
      "Fold [0]\n",
      "Fold [1]\n",
      "Fold [2]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.547817\n",
      "         Iterations: 1\n",
      "         Function evaluations: 14\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.552325\n",
      "         Iterations: 1\n",
      "         Function evaluations: 14\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.578331\n",
      "         Iterations: 2\n",
      "         Function evaluations: 40\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.610931\n",
      "         Iterations: 2\n",
      "         Function evaluations: 79\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.629357\n",
      "         Iterations: 1\n",
      "         Function evaluations: 56\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.630593\n",
      "         Iterations: 1\n",
      "         Function evaluations: 56\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.630593\n",
      "         Iterations: 2\n",
      "         Function evaluations: 41\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.630593\n",
      "         Iterations: 2\n",
      "         Function evaluations: 38\n",
      "Accuracy = 0.629012965124\n",
      "137.14799118\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    \n",
    "    DATA_DIR = '/Users/patrickkennedy/Desktop'\n",
    "    train = pd.read_csv(DATA_DIR + '/Project DATA/train.csv')\n",
    "    train.sample(frac=1)\n",
    "\n",
    "    test = pd.read_csv(DATA_DIR + '/Project DATA/test.csv')\n",
    "    # combine train and test\n",
    "    all_data = train.append(test)\n",
    "\n",
    "    # Use -1 for any others\n",
    "    all_data.fillna(-1, inplace=True)\n",
    "\n",
    "    # factorize categorical variables\n",
    "    # add all categorical vars -pk\n",
    "    # does this matter? and what of any feature selection beyond factorizing prod info 2?\n",
    "    categorical = ['Product_Info_1', 'Product_Info_2', 'Product_Info_3', 'Product_Info_5', 'Product_Info_6', 'Product_Info_7', 'Employment_Info_2', 'Employment_Info_3', 'Employment_Info_5', 'InsuredInfo_1', 'InsuredInfo_2', 'InsuredInfo_3', 'InsuredInfo_4', 'InsuredInfo_5', 'InsuredInfo_6', 'InsuredInfo_7', 'Insurance_History_1', 'Insurance_History_2', 'Insurance_History_3', 'Insurance_History_4', 'Insurance_History_7', 'Insurance_History_8', 'Insurance_History_9', 'Family_Hist_1', 'Medical_History_2', 'Medical_History_3', 'Medical_History_4', 'Medical_History_5', 'Medical_History_6', 'Medical_History_7', 'Medical_History_8', 'Medical_History_9', 'Medical_History_10', 'Medical_History_11', 'Medical_History_12', 'Medical_History_13', 'Medical_History_14', 'Medical_History_16', 'Medical_History_17', 'Medical_History_18', 'Medical_History_19', 'Medical_History_20', 'Medical_History_21', 'Medical_History_22', 'Medical_History_23', 'Medical_History_25', 'Medical_History_26', 'Medical_History_27', 'Medical_History_28', 'Medical_History_29', 'Medical_History_30', 'Medical_History_31', 'Medical_History_33', 'Medical_History_34', 'Medical_History_35', 'Medical_History_36', 'Medical_History_37', 'Medical_History_38', 'Medical_History_39', 'Medical_History_40', 'Medical_History_41','Medical_History_1', 'Medical_History_15', 'Medical_History_24', 'Medical_History_32']\n",
    "\n",
    "    #does it matter that i am running on normalized variables? seems like it would obviate the need to do PCA...\n",
    "    #doing PCA on continuous variables\n",
    "    continuous = ['Product_Info_4', 'Ins_Age', 'Ht', 'Wt', 'BMI', 'Employment_Info_1', 'Employment_Info_4', 'Employment_Info_6',\n",
    "              'Insurance_History_5', 'Family_Hist_2', 'Family_Hist_3', 'Family_Hist_4', 'Family_Hist_5'] \n",
    "\n",
    "    discrete = ['Medical_History_1', 'Medical_History_15', 'Medical_History_24', 'Medical_History_32']\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    #from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    #X_scaled = StandardScaler().fit_transform(all_data[continuous])\n",
    "    \n",
    "    \n",
    "    all_data['Product_Info_2'] = pd.factorize(all_data['Product_Info_2'])[0]\n",
    "    \n",
    "    cols = all_data.columns\n",
    "    cols = cols.drop(categorical)\n",
    "    cols = cols.drop(continuous)\n",
    "    #cols = cols.drop(discrete)\n",
    "    cols = cols.drop(\"Id\")\n",
    "    cols = cols.drop(\"Response\")\n",
    "    cols = list(cols)\n",
    "\n",
    "    \n",
    "    #var_list = [continuous]\n",
    "    #for elem in var_list:\n",
    "            \n",
    "    #    pca = PCA(n_components=len(elem), whiten=True).fit(all_data[elem])\n",
    "    #    all_data_PCA = pca.transform(all_data[elem])\n",
    "\n",
    "    #    PCA_titles = []\n",
    "    #    for i in range(len(elem)):\n",
    "    #        if elem == categorical:\n",
    "    #            PCA_titles.append(\"PCA_cat_\"+str(i))\n",
    "    #        elif elem == continuous:\n",
    "    #            PCA_titles.append(\"PCA_con_\"+str(i))\n",
    "    #        else:\n",
    "    #            PCA_titles.append(\"PCA_other_\"+str(i))\n",
    "            \n",
    "    #    all_data_PCA_df = pd.DataFrame(all_data_PCA, index=all_data.index, columns=PCA_titles)\n",
    "    \n",
    "    #    #add back into all_data and drop the continuous variables\n",
    "    #    all_data = pd.concat([all_data, all_data_PCA_df], axis=1)\n",
    "    #    #all_data = all_data.drop(elem, axis=1)\n",
    "\n",
    "\n",
    "    # fix the dtype on the label column\n",
    "    all_data['Response'] = all_data['Response'].astype(int)\n",
    "    print all_data.shape\n",
    "    #is this crossvalidated on train set or to build for submission?\n",
    "    for_reals = False\n",
    "    \n",
    "    #let's run this sucker\n",
    "    final_preds, train, test = run(all_data, for_reals)\n",
    "    print time.time() - start_time\n",
    "    \n",
    "    #best_score = 0.0\n",
    "    \n",
    "    #for i in xrange(1):\n",
    "    #print 'Iteration [%s]' % (i)\n",
    "    #train.sample(frac=1)\n",
    "    #score, int_train_preds, blend_train = run(train)\n",
    "    #best_score = max(best_score, score)\n",
    "     \n",
    "        \n",
    "    #print 'Best score = %s' % (best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-407412a702c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_clip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
     ]
    }
   ],
   "source": [
    "#still getting a '0' ... need to fix that either through clipping or whatever the hell digitize does\n",
    "%matplotlib inline\n",
    "#plt.hist(train[:,1])\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "train_clip = np.round(np.clip(train[:,2], 1, 8))\n",
    "\n",
    "print min(train_clip), max(train_clip)\n",
    "plt.hist(final_preds)\n",
    "plt.show()\n",
    "\n",
    "print min(final_preds), max(final_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19765, 140)\n"
     ]
    }
   ],
   "source": [
    "data = all_data_new\n",
    "features = data.columns\n",
    "features = features.drop(\"Id\")\n",
    "features = features.drop(\"Response\")\n",
    "    \n",
    "train = data[data['Response']>0].copy()\n",
    "test = data[data['Response']<1].copy()\n",
    "    \n",
    "X = train[features].as_matrix()\n",
    "init_y = train[\"Response\"]-1\n",
    "Y = init_y.as_matrix()\n",
    "\n",
    "X_dev = X[:]\n",
    "Y_dev = Y[:]\n",
    "X_test = test[features].as_matrix()\n",
    "\n",
    "\n",
    "#print final_preds\n",
    "#holder = np.array([final_preds])\n",
    "#print holder, X_test\n",
    "dummy = np.column_stack((X_test, final_preds))\n",
    "print dummy.shape\n",
    "#what the hell is going on here? i want to concat these for the second level....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([0.2, 6.4, 3.0, 1.6])\n",
    "bins = np.array([0.0, 1.0, 2.5, 4.0, 10.0])\n",
    "inds = np.digitize(x, bins)\n",
    "print inds\n",
    "\n",
    "for n in range(x.size):\n",
    "    print bins[inds[n]-1], \"<=\", x[n], \"<\", bins[inds[n]]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BMI</th>\n",
       "      <th>Employment_Info_1</th>\n",
       "      <th>Employment_Info_2</th>\n",
       "      <th>Employment_Info_3</th>\n",
       "      <th>Employment_Info_4</th>\n",
       "      <th>Employment_Info_5</th>\n",
       "      <th>Employment_Info_6</th>\n",
       "      <th>Family_Hist_1</th>\n",
       "      <th>Family_Hist_2</th>\n",
       "      <th>Family_Hist_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Medical_Keyword_9</th>\n",
       "      <th>Product_Info_1</th>\n",
       "      <th>Product_Info_2</th>\n",
       "      <th>Product_Info_3</th>\n",
       "      <th>Product_Info_4</th>\n",
       "      <th>Product_Info_5</th>\n",
       "      <th>Product_Info_6</th>\n",
       "      <th>Product_Info_7</th>\n",
       "      <th>Response</th>\n",
       "      <th>Wt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.323008</td>\n",
       "      <td>0.028</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.598039</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>D3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.148536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.272288</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>2</td>\n",
       "      <td>0.188406</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.131799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.428780</td>\n",
       "      <td>0.030</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>3</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>E1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.288703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.352438</td>\n",
       "      <td>0.042</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.420290</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>D4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.205021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.424046</td>\n",
       "      <td>0.027</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>2</td>\n",
       "      <td>0.463768</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>D2</td>\n",
       "      <td>26</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.234310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        BMI  Employment_Info_1  Employment_Info_2  Employment_Info_3  \\\n",
       "0  0.323008              0.028                 12                  1   \n",
       "1  0.272288              0.000                  1                  3   \n",
       "2  0.428780              0.030                  9                  1   \n",
       "3  0.352438              0.042                  9                  1   \n",
       "4  0.424046              0.027                  9                  1   \n",
       "\n",
       "   Employment_Info_4  Employment_Info_5  Employment_Info_6  Family_Hist_1  \\\n",
       "0                  0                  3            -1.0000              2   \n",
       "1                  0                  2             0.0018              2   \n",
       "2                  0                  2             0.0300              3   \n",
       "3                  0                  3             0.2000              3   \n",
       "4                  0                  2             0.0500              2   \n",
       "\n",
       "   Family_Hist_2  Family_Hist_3    ...     Medical_Keyword_9  Product_Info_1  \\\n",
       "0      -1.000000       0.598039    ...                     0               1   \n",
       "1       0.188406      -1.000000    ...                     0               1   \n",
       "2       0.304348      -1.000000    ...                     0               1   \n",
       "3       0.420290      -1.000000    ...                     0               1   \n",
       "4       0.463768      -1.000000    ...                     0               1   \n",
       "\n",
       "   Product_Info_2  Product_Info_3  Product_Info_4  Product_Info_5  \\\n",
       "0              D3              10        0.076923               2   \n",
       "1              A1              26        0.076923               2   \n",
       "2              E1              26        0.076923               2   \n",
       "3              D4              10        0.487179               2   \n",
       "4              D2              26        0.230769               2   \n",
       "\n",
       "   Product_Info_6  Product_Info_7  Response        Wt  \n",
       "0               1               1         8  0.148536  \n",
       "1               3               1         4  0.131799  \n",
       "2               3               1         8  0.288703  \n",
       "3               3               1         8  0.205021  \n",
       "4               3               1         8  0.234310  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data\n",
    "dummy_list = [0,1,2,3,4]\n",
    "all_data.iloc[dummy_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-cc6a5388db10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mblend_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Number of training data x Number of classifiers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mblend_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 1 with size 3"
     ]
    }
   ],
   "source": [
    "blend_train = np.zeros((all_data.shape[0], 3)) # Number of training data x Number of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=2000, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = '/Users/patrickkennedy/Desktop'\n",
    "data = pd.read_csv(DATA_DIR + '/Project DATA/train.csv')\n",
    "data['Product_Info_2'] = pd.factorize(data['Product_Info_2'])[0]\n",
    "data.fillna(-1, inplace=True)\n",
    "\n",
    "features = data.columns\n",
    "features = features.drop(\"Id\")\n",
    "features = features.drop(\"Response\")\n",
    "X = data[features]\n",
    "Y = data[\"Response\"]\n",
    "    \n",
    "# The DEV SET will be used for all training and validation purposes\n",
    "# The TEST SET will never be used for training, it is the unseen set.\n",
    "dev_cutoff = len(Y) * 4/5\n",
    "X_dev = X[:dev_cutoff]\n",
    "Y_dev = Y[:dev_cutoff]\n",
    "X_test = X[dev_cutoff:]\n",
    "Y_test = Y[dev_cutoff:]\n",
    "    \n",
    "     \n",
    "\n",
    "# train model\n",
    "model = xgb.XGBRegressor(n_estimators=2000)\n",
    "model.fit(X_dev, Y_dev) \n",
    "\n",
    "# get preds\n",
    "#test_preds = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "#test_preds = np.clip(test_preds, -0.99, 8.99)\n",
    "\n",
    "# Predict now\n",
    "#score = eval_wrapper(test_preds, Y_test)\n",
    "#print 'Accuracy = %s' % (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44306567048000456"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing the same params in xgboost wrapper for regressor?\n",
    "#using gridsearch to pick best params for each model? would that work?\n",
    "#running through iterations until an optimimum met and then switch to next layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the data using pandas\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminate missing values\n",
      "[('colsample_bytree', 0.8645938750638906), ('silent', 1), ('max_delta_step', 6.650258122365785), ('min_child_weight', 50.31287747915195), ('subsample', 0.8902686232425377), ('eta', 0.038968208713327845), ('objective', 'reg:linear'), ('max_depth', 10.208631729376426), ('gamma', 0.8374592697309676)]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1ad221b0e8f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# get preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0mtrain_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train score is:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, pred_leaf)\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0moption_mask\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0;36m0x02\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36m_validate_features\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;31m# Booster can't accept data with different feature names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'feature_names mismatch: {0} {1}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2148\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m             raise AttributeError(\"'%s' object has no attribute '%s'\" %\n\u001b[0;32m-> 2150\u001b[0;31m                                  (type(self).__name__, name))\n\u001b[0m\u001b[1;32m   2151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'feature_names'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.clock()\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from scipy.optimize import fmin_powell\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "from sklearn import grid_search\n",
    "\n",
    "def eval_wrapper(yhat, y):  \n",
    "    y = np.array(y)\n",
    "    y = y.astype(int)\n",
    "    yhat = np.array(yhat)\n",
    "    yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)   \n",
    "    return quadratic_weighted_kappa(yhat, y)\n",
    "    \n",
    "def get_params():\n",
    "    \n",
    "    params = {}\n",
    "    params[\"objective\"] = \"reg:linear\"     \n",
    "    params[\"eta\"] = 0.038968208713327845\n",
    "    params[\"gamma\"] = 0.83745926973096763\n",
    "    params[\"max_delta_step\"] = 6.6502581223657851\n",
    "    params[\"min_child_weight\"] = 50.31287747915195\n",
    "    params[\"subsample\"] = 0.89026862324253775\n",
    "    params[\"colsample_bytree\"] = 0.86459387506389063\n",
    "    params[\"silent\"] = 1\n",
    "    params[\"max_depth\"] = 10.208631729376426\n",
    "    plst = list(params.items())\n",
    "    return plst\n",
    "    \n",
    "def apply_offset(data, bin_offset, sv, scorer=eval_wrapper):\n",
    "    # data has the format of pred=0, offset_pred=1, labels=2 in the first dim\n",
    "    data[1, data[0].astype(int)==sv] = data[0, data[0].astype(int)==sv] + bin_offset\n",
    "    score = scorer(data[1], data[2])\n",
    "    return score\n",
    "\n",
    "\n",
    "# global variables\n",
    "columns_to_drop = ['Id', 'Response']\n",
    "xgb_num_rounds = 500 #5000 gives me good score (~15 min), 10000 (~32 min) (no improvement)\n",
    "num_classes = 8\n",
    "\n",
    "print(\"Load the data using pandas\")\n",
    "\n",
    "DATA_DIR = '/Users/patrickkennedy/Desktop'\n",
    "train = pd.read_csv(DATA_DIR + '/Project DATA/train.csv')\n",
    "test = pd.read_csv(DATA_DIR + '/Project DATA/test.csv')\n",
    "\n",
    "# combine train and test\n",
    "all_data = train.append(test)\n",
    "print('Eliminate missing values')    \n",
    "# Use -1 for any others\n",
    "all_data.fillna(-1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# factorize categorical variables\n",
    "# add all categorical vars -pk\n",
    "# does this matter? and what of any feature selection beyond factorizing prod info 2?\n",
    "categorical = ['Product_Info_1', 'Product_Info_2', 'Product_Info_3', 'Product_Info_5', 'Product_Info_6', 'Product_Info_7', 'Employment_Info_2', 'Employment_Info_3', 'Employment_Info_5', 'InsuredInfo_1', 'InsuredInfo_2', 'InsuredInfo_3', 'InsuredInfo_4', 'InsuredInfo_5', 'InsuredInfo_6', 'InsuredInfo_7', 'Insurance_History_1', 'Insurance_History_2', 'Insurance_History_3', 'Insurance_History_4', 'Insurance_History_7', 'Insurance_History_8', 'Insurance_History_9', 'Family_Hist_1', 'Medical_History_2', 'Medical_History_3', 'Medical_History_4', 'Medical_History_5', 'Medical_History_6', 'Medical_History_7', 'Medical_History_8', 'Medical_History_9', 'Medical_History_10', 'Medical_History_11', 'Medical_History_12', 'Medical_History_13', 'Medical_History_14', 'Medical_History_16', 'Medical_History_17', 'Medical_History_18', 'Medical_History_19', 'Medical_History_20', 'Medical_History_21', 'Medical_History_22', 'Medical_History_23', 'Medical_History_25', 'Medical_History_26', 'Medical_History_27', 'Medical_History_28', 'Medical_History_29', 'Medical_History_30', 'Medical_History_31', 'Medical_History_33', 'Medical_History_34', 'Medical_History_35', 'Medical_History_36', 'Medical_History_37', 'Medical_History_38', 'Medical_History_39', 'Medical_History_40', 'Medical_History_41','Medical_History_1', 'Medical_History_15', 'Medical_History_24', 'Medical_History_32']\n",
    "\n",
    "\n",
    "\n",
    "#does it matter that i am running on normalized variables? seems like it would obviate the need to do PCA...\n",
    "#doing PCA on continuous variables\n",
    "continuous = ['Product_Info_4', 'Ins_Age', 'Ht', 'Wt', 'BMI', 'Employment_Info_1', 'Employment_Info_4', 'Employment_Info_6',\n",
    "              'Insurance_History_5', 'Family_Hist_2', 'Family_Hist_3', 'Family_Hist_4', 'Family_Hist_5'] \n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(all_data[continuous])\n",
    "pca = PCA(n_components=len(continuous), whiten=True).fit(X_scaled)\n",
    "all_data_PCA = pca.transform(X_scaled)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PCA_titles = ['PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5', 'PCA6', 'PCA7', 'PCA8', 'PCA9', 'PCA10', 'PCA11', 'PCA12', 'PCA13']\n",
    "all_data_PCA_df = pd.DataFrame(all_data_PCA, index=all_data.index, columns=PCA_titles)\n",
    "    \n",
    "#add back into all_data and drop the continuous variables\n",
    "all_data_new = pd.concat([all_data, all_data_PCA_df], axis=1)\n",
    "all_data_new.drop(continuous, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "all_data_new['Product_Info_2'] = pd.factorize(all_data_new['Product_Info_2'])[0]\n",
    "\n",
    "# fix the dtype on the label column\n",
    "all_data_new['Response'] = all_data_new['Response'].astype(int)\n",
    "\n",
    "# Provide split column\n",
    "all_data_new['Split'] = np.random.randint(5, size=all_data_new.shape[0])\n",
    "\n",
    "# split train and test\n",
    "train = all_data_new[all_data_new['Response']>0].copy()\n",
    "test = all_data_new[all_data_new['Response']<1].copy()\n",
    "\n",
    "y = train['Response']\n",
    "y_test = test['Response']\n",
    "train = train.drop(columns_to_drop, axis=1)\n",
    "test = test.drop(columns_to_drop, axis=1)\n",
    "    \n",
    "# convert data to xgb data structure\n",
    "xgtrain = xgb.DMatrix(train, y)\n",
    "xgtest = xgb.DMatrix(test, label=y_test)    \n",
    "\n",
    "# get the parameters for xgboost\n",
    "plst = get_params()\n",
    "print plst      \n",
    "\n",
    "# train model\n",
    "\n",
    "#clf = grid_search.GridSearchCV(XGBClassifier(), plst)\n",
    "#model = clf.fit(train.drop(columns_to_drop, axis=1), train['Response'].values)\n",
    "\n",
    "model = xgb.train(plst, xgtrain, xgb_num_rounds) \n",
    "\n",
    "# get preds\n",
    "train_preds = model.predict(train, ntree_limit=model.best_iteration)\n",
    "print('Train score is:', eval_wrapper(train_preds, y)) \n",
    "test_preds = model.predict(test, ntree_limit=model.best_iteration)\n",
    "train_preds = np.clip(train_preds, -0.99, 8.99)\n",
    "test_preds = np.clip(test_preds, -0.99, 8.99)\n",
    "\n",
    "# train offsets \n",
    "offsets = np.ones(num_classes) * -0.5\n",
    "offset_train_preds = np.vstack((train_preds, train_preds, train['Response'].values))\n",
    "\n",
    "for j in range(num_classes):\n",
    "    train_offset = lambda x: -apply_offset(offset_train_preds, x, j)\n",
    "    offsets[j] = fmin_powell(train_offset, offsets[j])  \n",
    "\n",
    "\n",
    "# apply offsets to test\n",
    "data = np.vstack((test_preds, test_preds, test['Response'].values))\n",
    "for j in range(num_classes):\n",
    "    data[1, data[0].astype(int)==j] = data[0, data[0].astype(int)==j] + offsets[j]    \n",
    "final_test_preds = np.round(np.clip(data[1], 1, 8)).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "preds_out = pd.DataFrame({\"Id\": test['Id'].values, \"Response\": final_test_preds})\n",
    "preds_out = preds_out.set_index('Id')\n",
    "preds_out.to_csv('xgb_offset_submission.csv')\n",
    "\n",
    "print time.clock()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BMI</th>\n",
       "      <th>Employment_Info_1</th>\n",
       "      <th>Employment_Info_2</th>\n",
       "      <th>Employment_Info_3</th>\n",
       "      <th>Employment_Info_4</th>\n",
       "      <th>Employment_Info_5</th>\n",
       "      <th>Employment_Info_6</th>\n",
       "      <th>Family_Hist_1</th>\n",
       "      <th>Family_Hist_2</th>\n",
       "      <th>Family_Hist_3</th>\n",
       "      <th>...</th>\n",
       "      <th>PCA5</th>\n",
       "      <th>PCA6</th>\n",
       "      <th>PCA7</th>\n",
       "      <th>PCA8</th>\n",
       "      <th>PCA9</th>\n",
       "      <th>PCA10</th>\n",
       "      <th>PCA11</th>\n",
       "      <th>PCA12</th>\n",
       "      <th>PCA13</th>\n",
       "      <th>Split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.323008</td>\n",
       "      <td>0.0280</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.598039</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.435318</td>\n",
       "      <td>-0.624594</td>\n",
       "      <td>0.333129</td>\n",
       "      <td>1.596420</td>\n",
       "      <td>-0.241634</td>\n",
       "      <td>-0.705733</td>\n",
       "      <td>0.520622</td>\n",
       "      <td>-0.154380</td>\n",
       "      <td>-2.000318</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.272288</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>2</td>\n",
       "      <td>0.188406</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.631113</td>\n",
       "      <td>-1.123352</td>\n",
       "      <td>0.236646</td>\n",
       "      <td>-0.284226</td>\n",
       "      <td>-0.525978</td>\n",
       "      <td>2.313920</td>\n",
       "      <td>-0.062629</td>\n",
       "      <td>-0.381528</td>\n",
       "      <td>-2.059881</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.428780</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>3</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268272</td>\n",
       "      <td>0.623088</td>\n",
       "      <td>0.798516</td>\n",
       "      <td>-0.787735</td>\n",
       "      <td>0.546653</td>\n",
       "      <td>1.850409</td>\n",
       "      <td>0.455868</td>\n",
       "      <td>0.026867</td>\n",
       "      <td>0.462971</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.352438</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.420290</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244291</td>\n",
       "      <td>0.815034</td>\n",
       "      <td>-0.045112</td>\n",
       "      <td>-0.623537</td>\n",
       "      <td>-0.181130</td>\n",
       "      <td>0.221848</td>\n",
       "      <td>0.334142</td>\n",
       "      <td>0.068560</td>\n",
       "      <td>-0.118673</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.424046</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>2</td>\n",
       "      <td>0.463768</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045093</td>\n",
       "      <td>0.227593</td>\n",
       "      <td>-0.498479</td>\n",
       "      <td>-0.956560</td>\n",
       "      <td>0.773761</td>\n",
       "      <td>-0.962521</td>\n",
       "      <td>-0.146143</td>\n",
       "      <td>-0.269169</td>\n",
       "      <td>-0.052173</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.364887</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.470740</td>\n",
       "      <td>0.255784</td>\n",
       "      <td>1.272773</td>\n",
       "      <td>0.225816</td>\n",
       "      <td>2.362864</td>\n",
       "      <td>1.406493</td>\n",
       "      <td>-0.568458</td>\n",
       "      <td>0.074933</td>\n",
       "      <td>1.242026</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.376587</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.531430</td>\n",
       "      <td>-2.989041</td>\n",
       "      <td>-1.622892</td>\n",
       "      <td>-0.290317</td>\n",
       "      <td>0.695929</td>\n",
       "      <td>0.728572</td>\n",
       "      <td>0.336285</td>\n",
       "      <td>0.121552</td>\n",
       "      <td>-1.317165</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.571612</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.469147</td>\n",
       "      <td>-0.049105</td>\n",
       "      <td>0.242617</td>\n",
       "      <td>-0.866567</td>\n",
       "      <td>0.403036</td>\n",
       "      <td>1.204479</td>\n",
       "      <td>0.386636</td>\n",
       "      <td>0.672928</td>\n",
       "      <td>-0.629909</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.362643</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.374057</td>\n",
       "      <td>-0.294029</td>\n",
       "      <td>-0.884407</td>\n",
       "      <td>-0.479359</td>\n",
       "      <td>0.747074</td>\n",
       "      <td>0.951590</td>\n",
       "      <td>0.208346</td>\n",
       "      <td>0.150190</td>\n",
       "      <td>-0.924575</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.587796</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.286829</td>\n",
       "      <td>-1.455369</td>\n",
       "      <td>-1.549614</td>\n",
       "      <td>-0.310188</td>\n",
       "      <td>0.044915</td>\n",
       "      <td>-0.391896</td>\n",
       "      <td>0.893115</td>\n",
       "      <td>0.092392</td>\n",
       "      <td>1.073714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.521668</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>...</td>\n",
       "      <td>2.142871</td>\n",
       "      <td>-2.126064</td>\n",
       "      <td>-0.379518</td>\n",
       "      <td>0.357727</td>\n",
       "      <td>-0.188803</td>\n",
       "      <td>-0.009312</td>\n",
       "      <td>0.144243</td>\n",
       "      <td>0.153207</td>\n",
       "      <td>0.298551</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.455050</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.405797</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.649588</td>\n",
       "      <td>-1.143642</td>\n",
       "      <td>-1.038115</td>\n",
       "      <td>-1.135672</td>\n",
       "      <td>0.969197</td>\n",
       "      <td>0.692037</td>\n",
       "      <td>-0.067224</td>\n",
       "      <td>-0.144824</td>\n",
       "      <td>0.260200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.320784</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.445758</td>\n",
       "      <td>-0.413456</td>\n",
       "      <td>0.679147</td>\n",
       "      <td>1.947960</td>\n",
       "      <td>0.023879</td>\n",
       "      <td>-0.188323</td>\n",
       "      <td>0.536857</td>\n",
       "      <td>-0.101907</td>\n",
       "      <td>-1.317684</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.507515</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>3</td>\n",
       "      <td>0.420290</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.623648</td>\n",
       "      <td>-0.591880</td>\n",
       "      <td>0.105017</td>\n",
       "      <td>0.406744</td>\n",
       "      <td>0.228920</td>\n",
       "      <td>-0.158225</td>\n",
       "      <td>0.182410</td>\n",
       "      <td>-0.068153</td>\n",
       "      <td>-0.081613</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.264648</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194646</td>\n",
       "      <td>0.204213</td>\n",
       "      <td>-0.009744</td>\n",
       "      <td>0.429113</td>\n",
       "      <td>-0.255045</td>\n",
       "      <td>1.286197</td>\n",
       "      <td>0.664466</td>\n",
       "      <td>-0.154224</td>\n",
       "      <td>-1.295430</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.581279</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.657456</td>\n",
       "      <td>-0.397296</td>\n",
       "      <td>0.064632</td>\n",
       "      <td>1.822636</td>\n",
       "      <td>0.149296</td>\n",
       "      <td>-0.306889</td>\n",
       "      <td>0.557576</td>\n",
       "      <td>0.024681</td>\n",
       "      <td>0.516303</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.360969</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.275362</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236703</td>\n",
       "      <td>1.036884</td>\n",
       "      <td>0.604693</td>\n",
       "      <td>-0.422766</td>\n",
       "      <td>0.414180</td>\n",
       "      <td>0.224801</td>\n",
       "      <td>-0.238612</td>\n",
       "      <td>-0.102260</td>\n",
       "      <td>0.703904</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.430949</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.343137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.726176</td>\n",
       "      <td>1.408020</td>\n",
       "      <td>-0.487949</td>\n",
       "      <td>-0.040542</td>\n",
       "      <td>-0.487281</td>\n",
       "      <td>0.559884</td>\n",
       "      <td>0.351415</td>\n",
       "      <td>0.045563</td>\n",
       "      <td>-0.127719</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.427394</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.339912</td>\n",
       "      <td>-0.204245</td>\n",
       "      <td>1.997544</td>\n",
       "      <td>1.459832</td>\n",
       "      <td>0.251155</td>\n",
       "      <td>0.237729</td>\n",
       "      <td>0.550775</td>\n",
       "      <td>-0.218273</td>\n",
       "      <td>0.439489</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.285254</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.289855</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223463</td>\n",
       "      <td>0.699152</td>\n",
       "      <td>-0.495044</td>\n",
       "      <td>-0.144331</td>\n",
       "      <td>-0.001034</td>\n",
       "      <td>-0.117370</td>\n",
       "      <td>-0.182523</td>\n",
       "      <td>-0.084246</td>\n",
       "      <td>-1.914280</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.360969</td>\n",
       "      <td>0.0830</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146745</td>\n",
       "      <td>0.156581</td>\n",
       "      <td>0.988081</td>\n",
       "      <td>-0.275236</td>\n",
       "      <td>-0.697846</td>\n",
       "      <td>0.654608</td>\n",
       "      <td>0.305932</td>\n",
       "      <td>-0.308269</td>\n",
       "      <td>0.667310</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.605334</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.421569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435183</td>\n",
       "      <td>2.598913</td>\n",
       "      <td>-1.616393</td>\n",
       "      <td>0.462298</td>\n",
       "      <td>-0.762719</td>\n",
       "      <td>-0.550628</td>\n",
       "      <td>0.165893</td>\n",
       "      <td>0.557584</td>\n",
       "      <td>0.276189</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.753765</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055944</td>\n",
       "      <td>0.859293</td>\n",
       "      <td>-0.308982</td>\n",
       "      <td>-0.746276</td>\n",
       "      <td>0.086842</td>\n",
       "      <td>0.475919</td>\n",
       "      <td>0.589493</td>\n",
       "      <td>0.182417</td>\n",
       "      <td>-2.518351</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.428780</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178219</td>\n",
       "      <td>0.025490</td>\n",
       "      <td>0.981665</td>\n",
       "      <td>-0.159339</td>\n",
       "      <td>-0.624500</td>\n",
       "      <td>2.511302</td>\n",
       "      <td>0.383012</td>\n",
       "      <td>-0.283663</td>\n",
       "      <td>0.426257</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.576961</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.376812</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.807399</td>\n",
       "      <td>-0.294876</td>\n",
       "      <td>0.870401</td>\n",
       "      <td>-0.394546</td>\n",
       "      <td>-1.344827</td>\n",
       "      <td>0.697171</td>\n",
       "      <td>0.188094</td>\n",
       "      <td>-0.195991</td>\n",
       "      <td>-1.158878</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.517129</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.700731</td>\n",
       "      <td>1.616949</td>\n",
       "      <td>0.558935</td>\n",
       "      <td>-0.434541</td>\n",
       "      <td>0.498131</td>\n",
       "      <td>1.260431</td>\n",
       "      <td>0.532112</td>\n",
       "      <td>-0.308232</td>\n",
       "      <td>-0.156612</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.545946</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.704008</td>\n",
       "      <td>0.533805</td>\n",
       "      <td>-1.707634</td>\n",
       "      <td>0.285621</td>\n",
       "      <td>-1.830221</td>\n",
       "      <td>-0.084382</td>\n",
       "      <td>-5.234805</td>\n",
       "      <td>-3.150769</td>\n",
       "      <td>0.540941</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.296359</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.388634</td>\n",
       "      <td>-0.177121</td>\n",
       "      <td>2.504709</td>\n",
       "      <td>1.637004</td>\n",
       "      <td>-0.858790</td>\n",
       "      <td>1.833551</td>\n",
       "      <td>-3.963258</td>\n",
       "      <td>-2.478623</td>\n",
       "      <td>1.164471</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.546823</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.812505</td>\n",
       "      <td>-0.330135</td>\n",
       "      <td>-1.473044</td>\n",
       "      <td>0.684154</td>\n",
       "      <td>-0.422482</td>\n",
       "      <td>-0.256670</td>\n",
       "      <td>-3.425539</td>\n",
       "      <td>2.658969</td>\n",
       "      <td>0.610659</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.506623</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.343499</td>\n",
       "      <td>0.619287</td>\n",
       "      <td>-1.436284</td>\n",
       "      <td>-0.170812</td>\n",
       "      <td>-1.896677</td>\n",
       "      <td>-1.237500</td>\n",
       "      <td>-0.082141</td>\n",
       "      <td>0.026729</td>\n",
       "      <td>0.367043</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59351</th>\n",
       "      <td>0.491491</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698025</td>\n",
       "      <td>1.550856</td>\n",
       "      <td>1.151048</td>\n",
       "      <td>-0.019854</td>\n",
       "      <td>0.346423</td>\n",
       "      <td>2.842645</td>\n",
       "      <td>0.891075</td>\n",
       "      <td>0.713442</td>\n",
       "      <td>0.038296</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59352</th>\n",
       "      <td>0.488220</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368341</td>\n",
       "      <td>0.754189</td>\n",
       "      <td>-1.055998</td>\n",
       "      <td>-0.579632</td>\n",
       "      <td>0.080757</td>\n",
       "      <td>0.241224</td>\n",
       "      <td>-3.022340</td>\n",
       "      <td>2.712680</td>\n",
       "      <td>0.165292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59353</th>\n",
       "      <td>0.539563</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.275362</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133426</td>\n",
       "      <td>1.301313</td>\n",
       "      <td>0.285852</td>\n",
       "      <td>-0.063558</td>\n",
       "      <td>-0.300559</td>\n",
       "      <td>-0.045567</td>\n",
       "      <td>-0.092993</td>\n",
       "      <td>-0.102230</td>\n",
       "      <td>-0.303225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59354</th>\n",
       "      <td>0.494104</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.996523</td>\n",
       "      <td>-0.693892</td>\n",
       "      <td>0.411760</td>\n",
       "      <td>-0.641480</td>\n",
       "      <td>-0.546958</td>\n",
       "      <td>-0.832228</td>\n",
       "      <td>0.221863</td>\n",
       "      <td>-0.076134</td>\n",
       "      <td>0.091239</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59355</th>\n",
       "      <td>0.547451</td>\n",
       "      <td>0.0580</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>2</td>\n",
       "      <td>0.405797</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.916770</td>\n",
       "      <td>-0.760500</td>\n",
       "      <td>0.160159</td>\n",
       "      <td>-0.454350</td>\n",
       "      <td>-0.244532</td>\n",
       "      <td>1.053545</td>\n",
       "      <td>0.200204</td>\n",
       "      <td>-0.111275</td>\n",
       "      <td>-0.025142</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59356</th>\n",
       "      <td>0.618050</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.598039</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.598299</td>\n",
       "      <td>-0.575046</td>\n",
       "      <td>-0.991329</td>\n",
       "      <td>1.532107</td>\n",
       "      <td>-0.265074</td>\n",
       "      <td>-0.529997</td>\n",
       "      <td>0.599624</td>\n",
       "      <td>-0.381345</td>\n",
       "      <td>1.352786</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59357</th>\n",
       "      <td>0.491491</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02500</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645055</td>\n",
       "      <td>2.580393</td>\n",
       "      <td>-0.038604</td>\n",
       "      <td>-0.406257</td>\n",
       "      <td>-1.269436</td>\n",
       "      <td>-1.174894</td>\n",
       "      <td>0.544916</td>\n",
       "      <td>0.439225</td>\n",
       "      <td>0.046392</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59358</th>\n",
       "      <td>0.586182</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>3</td>\n",
       "      <td>0.275362</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.538206</td>\n",
       "      <td>-0.861555</td>\n",
       "      <td>-0.742680</td>\n",
       "      <td>0.429259</td>\n",
       "      <td>0.544649</td>\n",
       "      <td>0.282629</td>\n",
       "      <td>-0.127242</td>\n",
       "      <td>-0.066939</td>\n",
       "      <td>0.063125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59359</th>\n",
       "      <td>0.603660</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.377630</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>0.474938</td>\n",
       "      <td>-0.273354</td>\n",
       "      <td>-0.477143</td>\n",
       "      <td>0.429826</td>\n",
       "      <td>0.593701</td>\n",
       "      <td>0.004946</td>\n",
       "      <td>-0.845174</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59360</th>\n",
       "      <td>0.258890</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326342</td>\n",
       "      <td>1.447788</td>\n",
       "      <td>-0.087519</td>\n",
       "      <td>0.205574</td>\n",
       "      <td>-1.185225</td>\n",
       "      <td>-0.911974</td>\n",
       "      <td>-0.232211</td>\n",
       "      <td>-0.073280</td>\n",
       "      <td>-0.840117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59361</th>\n",
       "      <td>0.411703</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055401</td>\n",
       "      <td>0.838372</td>\n",
       "      <td>-0.426446</td>\n",
       "      <td>-0.548343</td>\n",
       "      <td>2.551494</td>\n",
       "      <td>1.312795</td>\n",
       "      <td>0.042536</td>\n",
       "      <td>-0.576812</td>\n",
       "      <td>0.391339</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59362</th>\n",
       "      <td>0.491491</td>\n",
       "      <td>0.0540</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>3</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.880153</td>\n",
       "      <td>-0.790611</td>\n",
       "      <td>0.936156</td>\n",
       "      <td>-0.361392</td>\n",
       "      <td>-0.094295</td>\n",
       "      <td>0.592757</td>\n",
       "      <td>0.277303</td>\n",
       "      <td>0.102155</td>\n",
       "      <td>0.012255</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59363</th>\n",
       "      <td>0.464570</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519290</td>\n",
       "      <td>0.633810</td>\n",
       "      <td>-1.359557</td>\n",
       "      <td>-0.589705</td>\n",
       "      <td>0.462867</td>\n",
       "      <td>-1.148064</td>\n",
       "      <td>-2.075898</td>\n",
       "      <td>-7.004953</td>\n",
       "      <td>-0.446225</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59364</th>\n",
       "      <td>0.261651</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.598039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793318</td>\n",
       "      <td>1.124969</td>\n",
       "      <td>1.535999</td>\n",
       "      <td>-0.364550</td>\n",
       "      <td>0.860142</td>\n",
       "      <td>-0.012903</td>\n",
       "      <td>0.627822</td>\n",
       "      <td>0.008349</td>\n",
       "      <td>0.660601</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59365</th>\n",
       "      <td>0.458023</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.599913</td>\n",
       "      <td>1.095278</td>\n",
       "      <td>0.240791</td>\n",
       "      <td>-0.631241</td>\n",
       "      <td>0.711360</td>\n",
       "      <td>-0.356003</td>\n",
       "      <td>0.426407</td>\n",
       "      <td>-0.046189</td>\n",
       "      <td>0.254257</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59366</th>\n",
       "      <td>0.443418</td>\n",
       "      <td>0.0580</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.480392</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.274926</td>\n",
       "      <td>-0.187943</td>\n",
       "      <td>1.126487</td>\n",
       "      <td>-0.588205</td>\n",
       "      <td>-0.166072</td>\n",
       "      <td>-0.553881</td>\n",
       "      <td>-0.003515</td>\n",
       "      <td>0.311399</td>\n",
       "      <td>0.338293</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59367</th>\n",
       "      <td>0.661270</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.677380</td>\n",
       "      <td>0.110354</td>\n",
       "      <td>-1.805431</td>\n",
       "      <td>1.639918</td>\n",
       "      <td>-0.217886</td>\n",
       "      <td>2.010309</td>\n",
       "      <td>1.206010</td>\n",
       "      <td>0.221969</td>\n",
       "      <td>1.813217</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59368</th>\n",
       "      <td>0.464047</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.301212</td>\n",
       "      <td>-0.066819</td>\n",
       "      <td>0.499170</td>\n",
       "      <td>0.089847</td>\n",
       "      <td>-0.168635</td>\n",
       "      <td>0.777089</td>\n",
       "      <td>0.429372</td>\n",
       "      <td>-0.686118</td>\n",
       "      <td>0.265494</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59369</th>\n",
       "      <td>0.693246</td>\n",
       "      <td>0.0920</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>...</td>\n",
       "      <td>2.941306</td>\n",
       "      <td>-0.186215</td>\n",
       "      <td>-0.864977</td>\n",
       "      <td>0.573886</td>\n",
       "      <td>0.411984</td>\n",
       "      <td>-0.284647</td>\n",
       "      <td>0.437649</td>\n",
       "      <td>-0.932446</td>\n",
       "      <td>-0.609521</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59370</th>\n",
       "      <td>0.477625</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.331831</td>\n",
       "      <td>0.194998</td>\n",
       "      <td>1.132736</td>\n",
       "      <td>0.532543</td>\n",
       "      <td>-0.979805</td>\n",
       "      <td>2.216440</td>\n",
       "      <td>0.123112</td>\n",
       "      <td>0.975328</td>\n",
       "      <td>0.176583</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59371</th>\n",
       "      <td>0.558626</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489361</td>\n",
       "      <td>1.509071</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>1.358325</td>\n",
       "      <td>-0.127850</td>\n",
       "      <td>-2.150035</td>\n",
       "      <td>0.321440</td>\n",
       "      <td>0.088141</td>\n",
       "      <td>0.241240</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59372</th>\n",
       "      <td>0.438076</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.681159</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.144469</td>\n",
       "      <td>0.831525</td>\n",
       "      <td>0.017172</td>\n",
       "      <td>-0.189109</td>\n",
       "      <td>1.882216</td>\n",
       "      <td>-1.440999</td>\n",
       "      <td>0.231761</td>\n",
       "      <td>0.023926</td>\n",
       "      <td>0.391942</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59373</th>\n",
       "      <td>0.332885</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.275362</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278945</td>\n",
       "      <td>0.983909</td>\n",
       "      <td>2.134334</td>\n",
       "      <td>0.731478</td>\n",
       "      <td>-0.086258</td>\n",
       "      <td>-1.035804</td>\n",
       "      <td>-0.114977</td>\n",
       "      <td>0.078802</td>\n",
       "      <td>1.412788</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59374</th>\n",
       "      <td>0.484658</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.405797</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094414</td>\n",
       "      <td>0.721757</td>\n",
       "      <td>-0.338121</td>\n",
       "      <td>-0.365979</td>\n",
       "      <td>0.315521</td>\n",
       "      <td>0.582089</td>\n",
       "      <td>0.345803</td>\n",
       "      <td>-0.143213</td>\n",
       "      <td>0.321499</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59375</th>\n",
       "      <td>0.494827</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290540</td>\n",
       "      <td>0.702677</td>\n",
       "      <td>-0.594574</td>\n",
       "      <td>-0.417062</td>\n",
       "      <td>0.695912</td>\n",
       "      <td>0.461759</td>\n",
       "      <td>-4.873032</td>\n",
       "      <td>-3.154543</td>\n",
       "      <td>0.263072</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59376</th>\n",
       "      <td>0.519103</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>3</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200877</td>\n",
       "      <td>0.770816</td>\n",
       "      <td>-0.115162</td>\n",
       "      <td>-0.600066</td>\n",
       "      <td>-0.072607</td>\n",
       "      <td>1.319440</td>\n",
       "      <td>0.133092</td>\n",
       "      <td>-0.004566</td>\n",
       "      <td>0.321082</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59377</th>\n",
       "      <td>0.551119</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.048328</td>\n",
       "      <td>-0.870780</td>\n",
       "      <td>0.498376</td>\n",
       "      <td>-0.719409</td>\n",
       "      <td>0.615512</td>\n",
       "      <td>-0.207289</td>\n",
       "      <td>0.069272</td>\n",
       "      <td>-0.163199</td>\n",
       "      <td>-0.679988</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59378</th>\n",
       "      <td>0.360969</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236012</td>\n",
       "      <td>0.578030</td>\n",
       "      <td>1.617199</td>\n",
       "      <td>0.655020</td>\n",
       "      <td>0.759524</td>\n",
       "      <td>0.697413</td>\n",
       "      <td>-0.093563</td>\n",
       "      <td>-0.250235</td>\n",
       "      <td>0.679923</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59379</th>\n",
       "      <td>0.462452</td>\n",
       "      <td>0.0380</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>...</td>\n",
       "      <td>3.092865</td>\n",
       "      <td>-0.613517</td>\n",
       "      <td>0.190290</td>\n",
       "      <td>1.553298</td>\n",
       "      <td>0.319295</td>\n",
       "      <td>-1.590514</td>\n",
       "      <td>0.123663</td>\n",
       "      <td>0.633035</td>\n",
       "      <td>0.209416</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59380</th>\n",
       "      <td>0.539563</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.401961</td>\n",
       "      <td>...</td>\n",
       "      <td>3.067362</td>\n",
       "      <td>-0.173626</td>\n",
       "      <td>0.271316</td>\n",
       "      <td>0.496686</td>\n",
       "      <td>0.933920</td>\n",
       "      <td>1.056058</td>\n",
       "      <td>0.647204</td>\n",
       "      <td>0.950906</td>\n",
       "      <td>-0.368992</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59381 rows Ã— 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            BMI  Employment_Info_1  Employment_Info_2  Employment_Info_3  \\\n",
       "0      0.323008             0.0280                 12                  1   \n",
       "1      0.272288             0.0000                  1                  3   \n",
       "2      0.428780             0.0300                  9                  1   \n",
       "3      0.352438             0.0420                  9                  1   \n",
       "4      0.424046             0.0270                  9                  1   \n",
       "5      0.364887             0.3250                 15                  1   \n",
       "6      0.376587             0.1100                  1                  3   \n",
       "7      0.571612             0.1200                 12                  1   \n",
       "8      0.362643             0.1650                  9                  1   \n",
       "9      0.587796             0.0250                  1                  3   \n",
       "10     0.521668             0.0500                  9                  1   \n",
       "11     0.455050             0.0900                  3                  1   \n",
       "12     0.320784             0.0750                  9                  1   \n",
       "13     0.507515             0.1000                  9                  1   \n",
       "14     0.264648             0.1600                  3                  1   \n",
       "15     0.581279             0.0750                  9                  1   \n",
       "16     0.360969             0.1000                 14                  1   \n",
       "17     0.430949             0.0378                  9                  1   \n",
       "18     0.427394             0.0800                  9                  1   \n",
       "19     0.285254             0.0550                  9                  1   \n",
       "20     0.360969             0.0830                  9                  1   \n",
       "21     0.605334             0.2100                  3                  1   \n",
       "22     0.753765             0.0310                  9                  1   \n",
       "23     0.428780             0.0650                  9                  1   \n",
       "24     0.576961             0.0270                  9                  1   \n",
       "25     0.517129             0.1000                 12                  1   \n",
       "26     0.545946             0.1500                 12                  1   \n",
       "27     0.296359             0.0420                  9                  1   \n",
       "28     0.546823             0.1200                  9                  1   \n",
       "29     0.506623             0.1150                  9                  1   \n",
       "...         ...                ...                ...                ...   \n",
       "59351  0.491491             0.0700                 12                  1   \n",
       "59352  0.488220             0.0200                 12                  1   \n",
       "59353  0.539563             0.0800                  9                  1   \n",
       "59354  0.494104             0.0500                 12                  1   \n",
       "59355  0.547451             0.0580                  9                  1   \n",
       "59356  0.618050             0.0400                  9                  1   \n",
       "59357  0.491491             0.1250                  9                  1   \n",
       "59358  0.586182             0.0900                  9                  1   \n",
       "59359  0.603660             0.0600                 14                  1   \n",
       "59360  0.258890             0.0800                  9                  1   \n",
       "59361  0.411703             0.2500                  3                  1   \n",
       "59362  0.491491             0.0540                  9                  1   \n",
       "59363  0.464570             0.0000                  1                  3   \n",
       "59364  0.261651             0.0500                 12                  1   \n",
       "59365  0.458023             0.0450                  9                  1   \n",
       "59366  0.443418             0.0580                 14                  1   \n",
       "59367  0.661270             0.1600                  9                  1   \n",
       "59368  0.464047             0.0900                  9                  1   \n",
       "59369  0.693246             0.0920                  1                  3   \n",
       "59370  0.477625             0.0650                  9                  1   \n",
       "59371  0.558626             0.0650                 12                  1   \n",
       "59372  0.438076             0.2000                 14                  1   \n",
       "59373  0.332885             0.0320                  9                  1   \n",
       "59374  0.484658             0.0590                  9                  1   \n",
       "59375  0.494827             0.0450                  9                  1   \n",
       "59376  0.519103             0.0200                  1                  3   \n",
       "59377  0.551119             0.1000                  9                  1   \n",
       "59378  0.360969             0.0350                  9                  1   \n",
       "59379  0.462452             0.0380                  9                  1   \n",
       "59380  0.539563             0.1230                  9                  1   \n",
       "\n",
       "       Employment_Info_4  Employment_Info_5  Employment_Info_6  Family_Hist_1  \\\n",
       "0                0.00000                  3            -1.0000              2   \n",
       "1                0.00000                  2             0.0018              2   \n",
       "2                0.00000                  2             0.0300              3   \n",
       "3                0.00000                  3             0.2000              3   \n",
       "4                0.00000                  2             0.0500              2   \n",
       "5                0.00000                  2             1.0000              2   \n",
       "6               -1.00000                  3             0.8000              3   \n",
       "7                0.00000                  2             1.0000              2   \n",
       "8                0.00000                  2             1.0000              3   \n",
       "9                0.00000                  3             0.0500              3   \n",
       "10              -1.00000                  2             0.1500              3   \n",
       "11              -1.00000                  2             1.0000              2   \n",
       "12               0.00000                  2            -1.0000              3   \n",
       "13              -1.00000                  2             0.0750              3   \n",
       "14               0.00000                  2             0.6000              3   \n",
       "15               0.00000                  2            -1.0000              3   \n",
       "16               0.00000                  2             0.2500              3   \n",
       "17               0.00000                  2             0.0360              2   \n",
       "18               0.00000                  2            -1.0000              3   \n",
       "19               0.00000                  2             0.0000              3   \n",
       "20               0.00000                  2             0.5000              3   \n",
       "21               0.00000                  2             1.0000              2   \n",
       "22               0.00000                  2             0.0000              3   \n",
       "23               0.00000                  2             0.3500              3   \n",
       "24               0.00000                  2             0.1500              3   \n",
       "25               0.00000                  2             0.3500              2   \n",
       "26               0.00000                  2             1.0000              3   \n",
       "27               0.00000                  2            -1.0000              3   \n",
       "28               0.00000                  2             0.1200              3   \n",
       "29               0.00000                  2             1.0000              3   \n",
       "...                  ...                ...                ...            ...   \n",
       "59351            0.00000                  2             0.0700              2   \n",
       "59352            0.00000                  3             0.1000              3   \n",
       "59353            0.00000                  2             0.0000              3   \n",
       "59354            0.02000                  2             0.2500              3   \n",
       "59355            0.00000                  2             0.2250              2   \n",
       "59356            0.00000                  3            -1.0000              3   \n",
       "59357            0.02500                  2             1.0000              2   \n",
       "59358           -1.00000                  2             0.0050              3   \n",
       "59359            0.00750                  2             0.2250              3   \n",
       "59360            0.00000                  2             0.1500              3   \n",
       "59361            0.00000                  2             0.9000              2   \n",
       "59362            0.00000                  2             0.0250              3   \n",
       "59363            0.00000                  3             0.0000              3   \n",
       "59364            0.00000                  2             0.0000              3   \n",
       "59365            0.00000                  3             0.2000              2   \n",
       "59366            0.00000                  2             0.3000              2   \n",
       "59367            0.00000                  2             0.0000              2   \n",
       "59368            0.00000                  2             0.2000              3   \n",
       "59369           -1.00000                  2             0.1600              3   \n",
       "59370            0.00000                  2             0.0500              2   \n",
       "59371            0.00000                  2            -1.0000              2   \n",
       "59372            0.00000                  2             0.3000              3   \n",
       "59373            0.00000                  3            -1.0000              3   \n",
       "59374            0.00000                  2             0.0200              3   \n",
       "59375            0.00000                  2             0.0450              2   \n",
       "59376            0.00000                  3             0.0250              3   \n",
       "59377            0.00001                  2             0.3500              3   \n",
       "59378            0.00000                  2            -1.0000              3   \n",
       "59379           -1.00000                  3            -1.0000              2   \n",
       "59380           -1.00000                  2             0.3000              2   \n",
       "\n",
       "       Family_Hist_2  Family_Hist_3  ...        PCA5      PCA6      PCA7  \\\n",
       "0          -1.000000       0.598039  ...   -0.435318 -0.624594  0.333129   \n",
       "1           0.188406      -1.000000  ...   -0.631113 -1.123352  0.236646   \n",
       "2           0.304348      -1.000000  ...    0.268272  0.623088  0.798516   \n",
       "3           0.420290      -1.000000  ...    0.244291  0.815034 -0.045112   \n",
       "4           0.463768      -1.000000  ...    0.045093  0.227593 -0.498479   \n",
       "5          -1.000000       0.294118  ...   -0.470740  0.255784  1.272773   \n",
       "6           0.594203      -1.000000  ...    1.531430 -2.989041 -1.622892   \n",
       "7          -1.000000       0.490196  ...   -0.469147 -0.049105  0.242617   \n",
       "8          -1.000000       0.529412  ...   -0.374057 -0.294029 -0.884407   \n",
       "9           0.797101      -1.000000  ...   -1.286829 -1.455369 -1.549614   \n",
       "10         -1.000000       0.470588  ...    2.142871 -2.126064 -0.379518   \n",
       "11          0.405797      -1.000000  ...    2.649588 -1.143642 -1.038115   \n",
       "12         -1.000000       0.549020  ...   -0.445758 -0.413456  0.679147   \n",
       "13          0.420290      -1.000000  ...    2.623648 -0.591880  0.105017   \n",
       "14         -1.000000       0.578431  ...   -0.194646  0.204213 -0.009744   \n",
       "15         -1.000000       0.549020  ...   -0.657456 -0.397296  0.064632   \n",
       "16          0.275362      -1.000000  ...    0.236703  1.036884  0.604693   \n",
       "17         -1.000000       0.343137  ...    0.726176  1.408020 -0.487949   \n",
       "18         -1.000000       0.509804  ...   -0.339912 -0.204245  1.997544   \n",
       "19          0.289855      -1.000000  ...    0.223463  0.699152 -0.495044   \n",
       "20         -1.000000       0.509804  ...   -0.146745  0.156581  0.988081   \n",
       "21         -1.000000       0.421569  ...    0.435183  2.598913 -1.616393   \n",
       "22          0.434783      -1.000000  ...   -0.055944  0.859293 -0.308982   \n",
       "23         -1.000000       0.313725  ...   -0.178219  0.025490  0.981665   \n",
       "24          0.376812      -1.000000  ...   -0.807399 -0.294876  0.870401   \n",
       "25         -1.000000       0.441176  ...    0.700731  1.616949  0.558935   \n",
       "26         -1.000000      -1.000000  ...   -0.704008  0.533805 -1.707634   \n",
       "27         -1.000000      -1.000000  ...   -0.388634 -0.177121  2.504709   \n",
       "28         -1.000000      -1.000000  ...   -0.812505 -0.330135 -1.473044   \n",
       "29         -1.000000       0.529412  ...   -0.343499  0.619287 -1.436284   \n",
       "...              ...            ...  ...         ...       ...       ...   \n",
       "59351      -1.000000       0.254902  ...    0.698025  1.550856  1.151048   \n",
       "59352      -1.000000      -1.000000  ...    0.368341  0.754189 -1.055998   \n",
       "59353       0.275362      -1.000000  ...    0.133426  1.301313  0.285852   \n",
       "59354       0.608696      -1.000000  ...   -0.996523 -0.693892  0.411760   \n",
       "59355       0.405797      -1.000000  ...   -0.916770 -0.760500  0.160159   \n",
       "59356      -1.000000       0.598039  ...   -0.598299 -0.575046 -0.991329   \n",
       "59357      -1.000000       0.617647  ...    0.645055  2.580393 -0.038604   \n",
       "59358       0.275362      -1.000000  ...    2.538206 -0.861555 -0.742680   \n",
       "59359      -1.000000       0.568627  ...   -0.377630  0.004469  0.474938   \n",
       "59360       0.304348      -1.000000  ...    0.326342  1.447788 -0.087519   \n",
       "59361       0.478261      -1.000000  ...   -0.055401  0.838372 -0.426446   \n",
       "59362       0.449275      -1.000000  ...   -0.880153 -0.790611  0.936156   \n",
       "59363      -1.000000       0.519608  ...    0.519290  0.633810 -1.359557   \n",
       "59364      -1.000000       0.598039  ...    0.793318  1.124969  1.535999   \n",
       "59365      -1.000000       0.578431  ...    0.599913  1.095278  0.240791   \n",
       "59366      -1.000000       0.480392  ...   -0.274926 -0.187943  1.126487   \n",
       "59367      -1.000000       0.568627  ...   -0.677380  0.110354 -1.805431   \n",
       "59368      -1.000000       0.568627  ...   -0.301212 -0.066819  0.499170   \n",
       "59369      -1.000000       0.627451  ...    2.941306 -0.186215 -0.864977   \n",
       "59370      -1.000000       0.078431  ...   -0.331831  0.194998  1.132736   \n",
       "59371      -1.000000       0.519608  ...    0.489361  1.509071  0.022068   \n",
       "59372       0.681159      -1.000000  ...   -0.144469  0.831525  0.017172   \n",
       "59373       0.275362      -1.000000  ...    0.278945  0.983909  2.134334   \n",
       "59374       0.405797      -1.000000  ...    0.094414  0.721757 -0.338121   \n",
       "59375      -1.000000      -1.000000  ...    0.290540  0.702677 -0.594574   \n",
       "59376       0.217391      -1.000000  ...    0.200877  0.770816 -0.115162   \n",
       "59377       0.565217      -1.000000  ...   -1.048328 -0.870780  0.498376   \n",
       "59378       0.173913      -1.000000  ...    0.236012  0.578030  1.617199   \n",
       "59379      -1.000000       0.372549  ...    3.092865 -0.613517  0.190290   \n",
       "59380      -1.000000       0.401961  ...    3.067362 -0.173626  0.271316   \n",
       "\n",
       "           PCA8      PCA9     PCA10     PCA11     PCA12     PCA13  Split  \n",
       "0      1.596420 -0.241634 -0.705733  0.520622 -0.154380 -2.000318      4  \n",
       "1     -0.284226 -0.525978  2.313920 -0.062629 -0.381528 -2.059881      4  \n",
       "2     -0.787735  0.546653  1.850409  0.455868  0.026867  0.462971      0  \n",
       "3     -0.623537 -0.181130  0.221848  0.334142  0.068560 -0.118673      4  \n",
       "4     -0.956560  0.773761 -0.962521 -0.146143 -0.269169 -0.052173      2  \n",
       "5      0.225816  2.362864  1.406493 -0.568458  0.074933  1.242026      2  \n",
       "6     -0.290317  0.695929  0.728572  0.336285  0.121552 -1.317165      4  \n",
       "7     -0.866567  0.403036  1.204479  0.386636  0.672928 -0.629909      2  \n",
       "8     -0.479359  0.747074  0.951590  0.208346  0.150190 -0.924575      2  \n",
       "9     -0.310188  0.044915 -0.391896  0.893115  0.092392  1.073714      0  \n",
       "10     0.357727 -0.188803 -0.009312  0.144243  0.153207  0.298551      1  \n",
       "11    -1.135672  0.969197  0.692037 -0.067224 -0.144824  0.260200      1  \n",
       "12     1.947960  0.023879 -0.188323  0.536857 -0.101907 -1.317684      4  \n",
       "13     0.406744  0.228920 -0.158225  0.182410 -0.068153 -0.081613      3  \n",
       "14     0.429113 -0.255045  1.286197  0.664466 -0.154224 -1.295430      2  \n",
       "15     1.822636  0.149296 -0.306889  0.557576  0.024681  0.516303      4  \n",
       "16    -0.422766  0.414180  0.224801 -0.238612 -0.102260  0.703904      0  \n",
       "17    -0.040542 -0.487281  0.559884  0.351415  0.045563 -0.127719      2  \n",
       "18     1.459832  0.251155  0.237729  0.550775 -0.218273  0.439489      1  \n",
       "19    -0.144331 -0.001034 -0.117370 -0.182523 -0.084246 -1.914280      4  \n",
       "20    -0.275236 -0.697846  0.654608  0.305932 -0.308269  0.667310      0  \n",
       "21     0.462298 -0.762719 -0.550628  0.165893  0.557584  0.276189      2  \n",
       "22    -0.746276  0.086842  0.475919  0.589493  0.182417 -2.518351      1  \n",
       "23    -0.159339 -0.624500  2.511302  0.383012 -0.283663  0.426257      0  \n",
       "24    -0.394546 -1.344827  0.697171  0.188094 -0.195991 -1.158878      3  \n",
       "25    -0.434541  0.498131  1.260431  0.532112 -0.308232 -0.156612      3  \n",
       "26     0.285621 -1.830221 -0.084382 -5.234805 -3.150769  0.540941      4  \n",
       "27     1.637004 -0.858790  1.833551 -3.963258 -2.478623  1.164471      1  \n",
       "28     0.684154 -0.422482 -0.256670 -3.425539  2.658969  0.610659      2  \n",
       "29    -0.170812 -1.896677 -1.237500 -0.082141  0.026729  0.367043      3  \n",
       "...         ...       ...       ...       ...       ...       ...    ...  \n",
       "59351 -0.019854  0.346423  2.842645  0.891075  0.713442  0.038296      4  \n",
       "59352 -0.579632  0.080757  0.241224 -3.022340  2.712680  0.165292      0  \n",
       "59353 -0.063558 -0.300559 -0.045567 -0.092993 -0.102230 -0.303225      1  \n",
       "59354 -0.641480 -0.546958 -0.832228  0.221863 -0.076134  0.091239      2  \n",
       "59355 -0.454350 -0.244532  1.053545  0.200204 -0.111275 -0.025142      0  \n",
       "59356  1.532107 -0.265074 -0.529997  0.599624 -0.381345  1.352786      0  \n",
       "59357 -0.406257 -1.269436 -1.174894  0.544916  0.439225  0.046392      0  \n",
       "59358  0.429259  0.544649  0.282629 -0.127242 -0.066939  0.063125      0  \n",
       "59359 -0.273354 -0.477143  0.429826  0.593701  0.004946 -0.845174      4  \n",
       "59360  0.205574 -1.185225 -0.911974 -0.232211 -0.073280 -0.840117      1  \n",
       "59361 -0.548343  2.551494  1.312795  0.042536 -0.576812  0.391339      3  \n",
       "59362 -0.361392 -0.094295  0.592757  0.277303  0.102155  0.012255      1  \n",
       "59363 -0.589705  0.462867 -1.148064 -2.075898 -7.004953 -0.446225      3  \n",
       "59364 -0.364550  0.860142 -0.012903  0.627822  0.008349  0.660601      2  \n",
       "59365 -0.631241  0.711360 -0.356003  0.426407 -0.046189  0.254257      3  \n",
       "59366 -0.588205 -0.166072 -0.553881 -0.003515  0.311399  0.338293      1  \n",
       "59367  1.639918 -0.217886  2.010309  1.206010  0.221969  1.813217      4  \n",
       "59368  0.089847 -0.168635  0.777089  0.429372 -0.686118  0.265494      1  \n",
       "59369  0.573886  0.411984 -0.284647  0.437649 -0.932446 -0.609521      4  \n",
       "59370  0.532543 -0.979805  2.216440  0.123112  0.975328  0.176583      1  \n",
       "59371  1.358325 -0.127850 -2.150035  0.321440  0.088141  0.241240      4  \n",
       "59372 -0.189109  1.882216 -1.440999  0.231761  0.023926  0.391942      1  \n",
       "59373  0.731478 -0.086258 -1.035804 -0.114977  0.078802  1.412788      2  \n",
       "59374 -0.365979  0.315521  0.582089  0.345803 -0.143213  0.321499      2  \n",
       "59375 -0.417062  0.695912  0.461759 -4.873032 -3.154543  0.263072      4  \n",
       "59376 -0.600066 -0.072607  1.319440  0.133092 -0.004566  0.321082      1  \n",
       "59377 -0.719409  0.615512 -0.207289  0.069272 -0.163199 -0.679988      3  \n",
       "59378  0.655020  0.759524  0.697413 -0.093563 -0.250235  0.679923      3  \n",
       "59379  1.553298  0.319295 -1.590514  0.123663  0.633035  0.209416      0  \n",
       "59380  0.496686  0.933920  1.056058  0.647204  0.950906 -0.368992      2  \n",
       "\n",
       "[59381 rows x 140 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing function at point:  {'colsample_bytree': 0.64750716543260323, 'learning_rate': 0.42089852031457198, 'max_delta_step': 9.4180950996663206, 'min_child_weight': 14.645281956238097, 'n_estimators': 806.16332079867971, 'subsample': 0.30380951092283554, 'max_depth': 11.321827493657697, 'gamma': 0.78845289488452586} | result: -9.000973\n",
      "Initializing function at point:  {'colsample_bytree': 0.66317955346978064, 'learning_rate': 0.11197528452056715, 'max_delta_step': 3.1767722357844952, 'min_child_weight': 84.111273488447566, 'n_estimators': 261.82982617922573, 'subsample': 0.60726237034861941, 'max_depth': 10.896108087177911, 'gamma': 0.75125421766862455} | result: -3.457382\n",
      "Initializing function at point:  {'colsample_bytree': 0.5814613000906651, 'learning_rate': 0.080787925938475155, 'max_delta_step': 4.5425965305857599, 'min_child_weight': 41.936282466117632, 'n_estimators': 460.25641157074313, 'subsample': 0.78418587763916525, 'max_depth': 7.0251615057085122, 'gamma': 0.38463233861261192} | result: -3.406935\n",
      "Initializing function at point:  {'colsample_bytree': 0.98188296883902293, 'learning_rate': 0.0088068812583746749, 'max_delta_step': 7.7428427012658219, 'min_child_weight': 79.95766517334549, 'n_estimators': 464.48840042972216, 'subsample': 0.68338983380875129, 'max_depth': 6.7222137962824364, 'gamma': 0.95709199118235033} | result: -3.528187\n",
      "Initializing function at point:  {'colsample_bytree': 0.79037321836215746, 'learning_rate': 0.36665461713291214, 'max_delta_step': 0.2802731082565757, 'min_child_weight': 43.924906194016216, 'n_estimators': 453.14849835044038, 'subsample': 0.58023004295971636, 'max_depth': 6.9407834343816983, 'gamma': 0.39458125885081397} | result: -3.726193\n",
      "Iteration:   1 | Last sampled value:  -22.739373 | with parameters:  {'colsample_bytree': 0.85227019204212517, 'learning_rate': 0.41973365307097366, 'max_delta_step': 9.4682447904167937, 'min_child_weight': 52.858988347425118, 'n_estimators': 527.12108157040711, 'subsample': 0.096391412964757478, 'max_depth': 10.395942482794656, 'gamma': 0.057916782563892988}\n",
      "               | Current maximum:      -3.406935 | with parameters:  {'colsample_bytree': 0.5814613000906651, 'learning_rate': 0.080787925938475155, 'max_delta_step': 4.5425965305857599, 'min_child_weight': 41.936282466117632, 'n_estimators': 460.25641157074313, 'subsample': 0.78418587763916525, 'max_depth': 7.0251615057085122, 'gamma': 0.38463233861261192}\n",
      "               | Time taken: 1 minutes and 28.290589 seconds\n",
      "\n",
      "Iteration:   2 | Last sampled value:   -3.636985 | with parameters:  {'colsample_bytree': 0.70144119960585294, 'learning_rate': 0.27247056256069546, 'max_delta_step': 2.0021076299889273, 'min_child_weight': 72.760323920498465, 'n_estimators': 186.99275413046593, 'subsample': 0.73934460063890972, 'max_depth': 8.7054302782003958, 'gamma': 0.24341985911357311}\n",
      "               | Current maximum:      -3.406935 | with parameters:  {'colsample_bytree': 0.5814613000906651, 'learning_rate': 0.080787925938475155, 'max_delta_step': 4.5425965305857599, 'min_child_weight': 41.936282466117632, 'n_estimators': 460.25641157074313, 'subsample': 0.78418587763916525, 'max_depth': 7.0251615057085122, 'gamma': 0.38463233861261192}\n",
      "               | Time taken: 0 minutes and 44.430701 seconds\n",
      "\n",
      "Iteration:   3 | Last sampled value:   -4.007545 | with parameters:  {'colsample_bytree': 0.78723676651238672, 'learning_rate': 0.47482975025414015, 'max_delta_step': 0.19591083110175656, 'min_child_weight': 76.939825447926154, 'n_estimators': 393.27397410053072, 'subsample': 0.48072269031834708, 'max_depth': 8.441019911894859, 'gamma': 0.18532425301932975}\n",
      "               | Current maximum:      -3.406935 | with parameters:  {'colsample_bytree': 0.5814613000906651, 'learning_rate': 0.080787925938475155, 'max_delta_step': 4.5425965305857599, 'min_child_weight': 41.936282466117632, 'n_estimators': 460.25641157074313, 'subsample': 0.78418587763916525, 'max_depth': 7.0251615057085122, 'gamma': 0.38463233861261192}\n",
      "               | Time taken: 1 minutes and 47.703542 seconds\n",
      "\n",
      "Iteration:   4 | Last sampled value:   -3.441096 | with parameters:  {'colsample_bytree': 0.82665205711821854, 'learning_rate': 0.026381134120940654, 'max_delta_step': 2.9242265437256343, 'min_child_weight': 87.66112476823038, 'n_estimators': 602.01628347773828, 'subsample': 0.23405495678727301, 'max_depth': 6.8325100349347867, 'gamma': 0.42764928775463662}\n",
      "               | Current maximum:      -3.406935 | with parameters:  {'colsample_bytree': 0.5814613000906651, 'learning_rate': 0.080787925938475155, 'max_delta_step': 4.5425965305857599, 'min_child_weight': 41.936282466117632, 'n_estimators': 460.25641157074313, 'subsample': 0.78418587763916525, 'max_depth': 7.0251615057085122, 'gamma': 0.38463233861261192}\n",
      "               | Time taken: 1 minutes and 34.367695 seconds\n",
      "\n",
      "Iteration:   5 | Last sampled value:   -5.267313 | with parameters:  {'colsample_bytree': 0.78736749473037404, 'learning_rate': 0.23352099457734721, 'max_delta_step': 6.4459675079059728, 'min_child_weight': 9.324239779467085, 'n_estimators': 243.83839633833787, 'subsample': 0.16096909968115672, 'max_depth': 9.9997950747130631, 'gamma': 0.53872925360397683}\n",
      "               | Current maximum:      -3.406935 | with parameters:  {'colsample_bytree': 0.5814613000906651, 'learning_rate': 0.080787925938475155, 'max_delta_step': 4.5425965305857599, 'min_child_weight': 41.936282466117632, 'n_estimators': 460.25641157074313, 'subsample': 0.78418587763916525, 'max_depth': 7.0251615057085122, 'gamma': 0.38463233861261192}\n",
      "               | Time taken: 0 minutes and 44.407993 seconds\n",
      "\n",
      "Iteration:   6 | Last sampled value:   -4.207189 | with parameters:  {'colsample_bytree': 0.96101066769724208, 'learning_rate': 0.35952903415772408, 'max_delta_step': 6.0672484088229908, 'min_child_weight': 39.909647255167243, 'n_estimators': 974.24248727342126, 'subsample': 0.6399276853632293, 'max_depth': 5.5938175835984874, 'gamma': 0.92476699083599723}\n",
      "               | Current maximum:      -3.406935 | with parameters:  {'colsample_bytree': 0.5814613000906651, 'learning_rate': 0.080787925938475155, 'max_delta_step': 4.5425965305857599, 'min_child_weight': 41.936282466117632, 'n_estimators': 460.25641157074313, 'subsample': 0.78418587763916525, 'max_depth': 7.0251615057085122, 'gamma': 0.38463233861261192}\n",
      "               | Time taken: 3 minutes and 33.996389 seconds\n",
      "\n",
      "Iteration:   7 | Last sampled value:   -4.270186 | with parameters:  {'colsample_bytree': 0.88040641020800425, 'learning_rate': 0.27091290071934382, 'max_delta_step': 2.1687269370636466, 'min_child_weight': 36.378462437940939, 'n_estimators': 658.50000589948149, 'subsample': 0.57972003159076835, 'max_depth': 8.0168824109189494, 'gamma': 0.36102198276638353}\n",
      "               | Current maximum:      -3.406935 | with parameters:  {'colsample_bytree': 0.5814613000906651, 'learning_rate': 0.080787925938475155, 'max_delta_step': 4.5425965305857599, 'min_child_weight': 41.936282466117632, 'n_estimators': 460.25641157074313, 'subsample': 0.78418587763916525, 'max_depth': 7.0251615057085122, 'gamma': 0.38463233861261192}\n",
      "               | Time taken: 3 minutes and 41.909829 seconds\n",
      "\n",
      "Iteration:   8 | Last sampled value:   -6.117554 | with parameters:  {'colsample_bytree': 0.84394001908983685, 'learning_rate': 0.26545413347089553, 'max_delta_step': 8.4225483530042613, 'min_child_weight': 69.486295464907201, 'n_estimators': 533.71508695122259, 'subsample': 0.087714720678968475, 'max_depth': 8.0764724438375843, 'gamma': 0.95953545870116341}\n",
      "               | Current maximum:      -3.406935 | with parameters:  {'colsample_bytree': 0.5814613000906651, 'learning_rate': 0.080787925938475155, 'max_delta_step': 4.5425965305857599, 'min_child_weight': 41.936282466117632, 'n_estimators': 460.25641157074313, 'subsample': 0.78418587763916525, 'max_depth': 7.0251615057085122, 'gamma': 0.38463233861261192}\n",
      "               | Time taken: 1 minutes and 17.87159 seconds\n",
      "\n",
      "Iteration:   9 | Last sampled value:   -3.440873 | with parameters:  {'colsample_bytree': 0.56700728999697547, 'learning_rate': 0.029060923902447893, 'max_delta_step': 9.7928586359447554, 'min_child_weight': 15.930582811007806, 'n_estimators': 160.53162694491721, 'subsample': 0.25172205596011782, 'max_depth': 8.438665296243208, 'gamma': 0.74106327266610883}\n",
      "               | Current maximum:      -3.406935 | with parameters:  {'colsample_bytree': 0.5814613000906651, 'learning_rate': 0.080787925938475155, 'max_delta_step': 4.5425965305857599, 'min_child_weight': 41.936282466117632, 'n_estimators': 460.25641157074313, 'subsample': 0.78418587763916525, 'max_depth': 7.0251615057085122, 'gamma': 0.38463233861261192}\n",
      "               | Time taken: 0 minutes and 25.511214 seconds\n",
      "\n",
      "Iteration:  10 | Last sampled value:   -3.398589 | with parameters:  {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}\n",
      "               | Current maximum:      -3.398589 | with parameters:  {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}\n",
      "               | Time taken: 0 minutes and 49.936335 seconds\n",
      "\n",
      "Iteration:  11 | Last sampled value:   -3.813465 | with parameters:  {'colsample_bytree': 0.8102038463499408, 'learning_rate': 0.31130993761676545, 'max_delta_step': 8.07472243512081, 'min_child_weight': 90.970927731382019, 'n_estimators': 132.67182882199114, 'subsample': 0.34743727998752111, 'max_depth': 8.7263541207561062, 'gamma': 0.2590844587846618}\n",
      "               | Current maximum:      -3.398589 | with parameters:  {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}\n",
      "               | Time taken: 0 minutes and 34.135547 seconds\n",
      "\n",
      "Iteration:  12 | Last sampled value:   -5.312988 | with parameters:  {'colsample_bytree': 0.97881707932787299, 'learning_rate': 0.38438695875175843, 'max_delta_step': 5.9257354390267336, 'min_child_weight': 77.888573628039381, 'n_estimators': 962.01468476523564, 'subsample': 0.48699060936820315, 'max_depth': 11.427189958198458, 'gamma': 0.40380163860163665}\n",
      "               | Current maximum:      -3.398589 | with parameters:  {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}\n",
      "               | Time taken: 7 minutes and 38.000451 seconds\n",
      "\n",
      "Iteration:  13 | Last sampled value:   -3.446114 | with parameters:  {'colsample_bytree': 0.76678315716602374, 'learning_rate': 0.14046062514856614, 'max_delta_step': 2.9270566499794115, 'min_child_weight': 56.549423830531275, 'n_estimators': 88.790343810238639, 'subsample': 0.63014814914610728, 'max_depth': 11.603158716294594, 'gamma': 0.74891222280277203}\n",
      "               | Current maximum:      -3.398589 | with parameters:  {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}\n",
      "               | Time taken: 0 minutes and 32.846871 seconds\n",
      "\n",
      "Iteration:  14 | Last sampled value:   -5.065621 | with parameters:  {'colsample_bytree': 0.73208950472488421, 'learning_rate': 0.35519817500787765, 'max_delta_step': 8.8263118720422042, 'min_child_weight': 1.3519708696082311, 'n_estimators': 841.01921763712414, 'subsample': 0.47107815231121919, 'max_depth': 9.4243548009911517, 'gamma': 0.99240933300920975}\n",
      "               | Current maximum:      -3.398589 | with parameters:  {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}\n",
      "               | Time taken: 4 minutes and 29.464353 seconds\n",
      "\n",
      "Iteration:  15 | Last sampled value:   -4.154824 | with parameters:  {'colsample_bytree': 0.97990396903526245, 'learning_rate': 0.40028359416675002, 'max_delta_step': 2.34850350685862, 'min_child_weight': 3.6609182076094453, 'n_estimators': 972.77178133320956, 'subsample': 0.88892790948144296, 'max_depth': 5.2584050045563204, 'gamma': 0.49284034036783142}\n",
      "               | Current maximum:      -3.398589 | with parameters:  {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}\n",
      "               | Time taken: 3 minutes and 18.021927 seconds\n",
      "\n",
      "Iteration:  16 | Last sampled value:   -4.000201 | with parameters:  {'colsample_bytree': 0.72395598647150772, 'learning_rate': 0.35703025392133281, 'max_delta_step': 6.6462522727843201, 'min_child_weight': 45.615506246807747, 'n_estimators': 102.59790040830802, 'subsample': 0.32459392573377382, 'max_depth': 8.1924040834869238, 'gamma': 0.26705959754951658}\n",
      "               | Current maximum:      -3.398589 | with parameters:  {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}\n",
      "               | Time taken: 0 minutes and 22.808998 seconds\n",
      "\n",
      "Iteration:  17 | Last sampled value:   -4.689574 | with parameters:  {'colsample_bytree': 0.78041942301746015, 'learning_rate': 0.46642017193072793, 'max_delta_step': 5.4730811026211192, 'min_child_weight': 66.892348215931136, 'n_estimators': 797.10017042274421, 'subsample': 0.81324640706230844, 'max_depth': 10.360907699716808, 'gamma': 0.31308296797144142}\n",
      "               | Current maximum:      -3.398589 | with parameters:  {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}\n",
      "               | Time taken: 4 minutes and 28.961142 seconds\n",
      "\n",
      "Iteration:  18 | Last sampled value:   -3.460024 | with parameters:  {'colsample_bytree': 0.6563232528127203, 'learning_rate': 0.018335075263123134, 'max_delta_step': 8.6317250077315482, 'min_child_weight': 88.014190887343929, 'n_estimators': 974.22198408615907, 'subsample': 0.17741170389934041, 'max_depth': 8.8780153600501102, 'gamma': 0.17058495563602283}\n",
      "               | Current maximum:      -3.398589 | with parameters:  {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}\n",
      "               | Time taken: 2 minutes and 15.861325 seconds\n",
      "\n",
      "Iteration:  19 | Last sampled value:   -9.184983 | with parameters:  {'colsample_bytree': 0.59328805284342612, 'learning_rate': 0.41704059519666453, 'max_delta_step': 4.0710255565203539, 'min_child_weight': 33.853011250681234, 'n_estimators': 753.34087762791364, 'subsample': 0.26690141582315013, 'max_depth': 10.453701149072129, 'gamma': 0.42090282869338347}\n",
      "               | Current maximum:      -3.398589 | with parameters:  {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}\n",
      "               | Time taken: 2 minutes and 25.461663 seconds\n",
      "\n",
      "Iteration:  20 | Last sampled value:   -4.076288 | with parameters:  {'colsample_bytree': 0.56699795610514381, 'learning_rate': 0.25215989045012738, 'max_delta_step': 2.5250975765158588, 'min_child_weight': 66.24789412002454, 'n_estimators': 289.50163071467415, 'subsample': 0.39944212232352744, 'max_depth': 10.904522552865084, 'gamma': 0.22994776123480676}\n",
      "               | Current maximum:      -3.398589 | with parameters:  {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}\n",
      "               | Time taken: 1 minutes and 7.175069 seconds\n",
      "\n",
      "Iteration:  21 | Last sampled value:   -3.604411 | with parameters:  {'colsample_bytree': 0.92890270836665678, 'learning_rate': 0.1811337171275916, 'max_delta_step': 1.6732491827390195, 'min_child_weight': 51.414857922145977, 'n_estimators': 356.37556492756886, 'subsample': 0.85013496885281992, 'max_depth': 8.6653440287799128, 'gamma': 0.4353837517495367}\n",
      "               | Current maximum:      -3.398589 | with parameters:  {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}\n",
      "               | Time taken: 1 minutes and 42.121768 seconds\n",
      "\n",
      "Iteration:  22 | Last sampled value:  -10.114358 | with parameters:  {'colsample_bytree': 0.5238914317442922, 'learning_rate': 0.45089471469875281, 'max_delta_step': 5.1581057839072555, 'min_child_weight': 98.963538378219965, 'n_estimators': 843.36480526036928, 'subsample': 0.021260528204093222, 'max_depth': 6.5052687225422243, 'gamma': 0.34069334130596363}\n",
      "               | Current maximum:      -3.398589 | with parameters:  {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}\n",
      "               | Time taken: 0 minutes and 39.657971 seconds\n",
      "\n",
      "Iteration:  23 | Last sampled value:   -3.864791 | with parameters:  {'colsample_bytree': 0.67697904386214669, 'learning_rate': 0.21857425240710215, 'max_delta_step': 9.3846552111155717, 'min_child_weight': 90.387206586526233, 'n_estimators': 769.68523087347501, 'subsample': 0.73348543651616616, 'max_depth': 8.5312318635912661, 'gamma': 0.08963801501272739}\n",
      "               | Current maximum:      -3.398589 | with parameters:  {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}\n",
      "               | Time taken: 2 minutes and 56.455318 seconds\n",
      "\n",
      "Iteration:  24 | Last sampled value:   -5.342613 | with parameters:  {'colsample_bytree': 0.9803510104618689, 'learning_rate': 0.37714872199031058, 'max_delta_step': 6.12977710922606, 'min_child_weight': 90.493514975808864, 'n_estimators': 384.13808634893587, 'subsample': 0.34281525460405898, 'max_depth': 11.082928201190441, 'gamma': 0.4828310319514163}\n",
      "               | Current maximum:      -3.398589 | with parameters:  {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}\n",
      "               | Time taken: 2 minutes and 24.093149 seconds\n",
      "\n",
      "Iteration:  25 | Last sampled value:   -4.056449 | with parameters:  {'colsample_bytree': 0.89542808527648643, 'learning_rate': 0.31947688853047557, 'max_delta_step': 9.8633585614871588, 'min_child_weight': 41.949975572286334, 'n_estimators': 246.28781271611717, 'subsample': 0.8609615128624587, 'max_depth': 11.377589688513456, 'gamma': 0.3476683396424427}\n",
      "               | Current maximum:      -3.398589 | with parameters:  {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}\n",
      "               | Time taken: 1 minutes and 34.859796 seconds\n",
      "\n",
      "Optimization finished with maximum: -3.398589, at position: {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}.\n",
      "Time taken: 62 minutes and 31.452084 seconds.\n",
      "-----------------------------------------------------\n",
      "Final Results\n",
      "XGBOOST: -3.398589\n"
     ]
    }
   ],
   "source": [
    "#seems like the message is to use a parameter optimization method and then use stacking/blending\n",
    "\n",
    "\n",
    "%timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn import grid_search\n",
    "from scipy.optimize import fmin_powell\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer\n",
    "\n",
    "\n",
    "DATA_TRAIN_PATH = '/Users/patrickkennedy/Desktop/Project DATA/train.csv'\n",
    "DATA_TEST_PATH = '/Users/patrickkennedy/Desktop/Project DATA/test.csv'\n",
    "\n",
    "def eval_wrapper(yhat, y):  \n",
    "    y = np.array(y)\n",
    "    y = y.astype(int)\n",
    "    yhat = np.array(yhat)\n",
    "    yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)   \n",
    "    return quadratic_weighted_kappa(yhat, y)\n",
    "\n",
    "\n",
    "def load_data(path_train = DATA_TRAIN_PATH, path_test = DATA_TEST_PATH):\n",
    "    columns_to_drop = ['Id', 'Response']\n",
    "    num_classes = 8\n",
    "\n",
    "    train = pd.read_csv(path_train)\n",
    "    test = pd.read_csv(path_test)\n",
    "\n",
    "    # combine train and test\n",
    "    all_data = train.append(test)\n",
    "    all_data.fillna(-1, inplace=True)\n",
    "    all_data['Product_Info_2'] = pd.factorize(all_data['Product_Info_2'])[0]\n",
    "    all_data['Response'] = all_data['Response'].astype(int)\n",
    "\n",
    "    train = all_data[all_data['Response']>0].copy()\n",
    "    test = all_data[all_data['Response']<1].copy()\n",
    "    train_labels = train[\"Response\"]\n",
    "    \n",
    "    #train_labels = train_labels.reshape(-1,1) #need to reshape b/c one dim array\n",
    "    \n",
    "    train_ids = train[\"Id\"].values\n",
    "    test_ids = test[\"Id\"].values\n",
    "\n",
    "    return  train.drop(columns_to_drop, axis=1), \\\n",
    "            train_labels, \\\n",
    "            test.drop(columns_to_drop, axis=1),\\\n",
    "            train_ids, \\\n",
    "            test_ids\n",
    "\n",
    "\n",
    "def xgboostcv(max_depth,\n",
    "              learning_rate,\n",
    "              n_estimators,\n",
    "              gamma,\n",
    "              min_child_weight,\n",
    "              max_delta_step,\n",
    "              subsample,\n",
    "              colsample_bytree,\n",
    "              silent =True,\n",
    "              nthread = -1,\n",
    "              seed = 1234):\n",
    "    \n",
    "    #XGBClassifier only gets up to .47\n",
    "    return cross_val_score(XGBRegressor(max_depth = int(max_depth),\n",
    "                                         learning_rate = learning_rate,\n",
    "                                         n_estimators = int(n_estimators),\n",
    "                                         silent = silent,\n",
    "                                         nthread = nthread,\n",
    "                                         gamma = gamma,\n",
    "                                         min_child_weight = min_child_weight,\n",
    "                                         max_delta_step = max_delta_step,\n",
    "                                         subsample = subsample,\n",
    "                                         colsample_bytree = colsample_bytree,\n",
    "                                         seed = seed,\n",
    "                                         objective = 'reg:linear'),\n",
    "                           train,\n",
    "                           labels,\n",
    "                           'mean_squared_error',\n",
    "                           n_jobs=-1,\n",
    "                           cv=3).mean()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data set and target values\n",
    "    train, labels, test, _, _ = load_data()\n",
    "    scorer = make_scorer(cohen_kappa_score)\n",
    "    \n",
    "    xgboostBO = BayesianOptimization(xgboostcv,\n",
    "                                     {'max_depth': (5, 12),\n",
    "                                      'learning_rate': (0.001, 0.5),\n",
    "                                      'n_estimators': (50, 1000),\n",
    "                                      'gamma': (1., 0.01),\n",
    "                                      'min_child_weight': (1, 100),\n",
    "                                      'max_delta_step': (0, 10),\n",
    "                                      'subsample': (0.01, 0.9),\n",
    "                                      'colsample_bytree' :(0.5, 0.99)\n",
    "                                     })\n",
    "\n",
    "\n",
    "    \n",
    "    xgboostBO.maximize()\n",
    "    print('-'*53)\n",
    "\n",
    "    print('Final Results')\n",
    "    print('XGBOOST: %f' % xgboostBO.res['max']['max_val'])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#try again with data excepting response and id variables...\n",
    "#seems like XGBRegressor may be a better fit, try that after XGBClassifier is done\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Optimization finished with maximum: -3.398589, at position: {'colsample_bytree': 0.86459387506389063, 'learning_rate': 0.038968208713327845, 'max_delta_step': 6.6502581223657851, 'min_child_weight': 50.31287747915195, 'n_estimators': 132.29526469869677, 'subsample': 0.89026862324253775, 'max_depth': 10.208631729376426, 'gamma': 0.83745926973096763}.\n",
    "Time taken: 62 minutes and 31.452084 seconds.\n",
    "-----------------------------------------------------\n",
    "Final Results\n",
    "XGBOOST: -3.398589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: OMP_NUM_THREADS=None =>\n",
      "... If you are using openblas if you are using openblas set OMP_NUM_THREADS=1 or risk subprocess calls hanging indefinitely\n"
     ]
    }
   ],
   "source": [
    "import hpsklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 0.028499577187215047}\n"
     ]
    }
   ],
   "source": [
    "#wtf is this?\n",
    "import pickle\n",
    "import time\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK\n",
    "\n",
    "def objective(x):\n",
    "    return {'loss': x ** 2, 'status': STATUS_OK }\n",
    "\n",
    "best = fmin(objective,\n",
    "    space=hp.uniform('x', -10, 10),\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100)\n",
    "\n",
    "print best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 0.028499577187215047}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "def objective(x):\n",
    "    return {\n",
    "        'loss': x ** 2,\n",
    "        'status': STATUS_OK,\n",
    "        # -- store other results like this\n",
    "        'eval_time': time.time(),\n",
    "        'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
    "        # -- attachments are handled differently\n",
    "        'attachments':\n",
    "            {'time_module': pickle.dumps(time.time)}\n",
    "        }\n",
    "trials = Trials()\n",
    "best = fmin(objective,\n",
    "    space=hp.uniform('x', -10, 10),\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials)\n",
    "\n",
    "print best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [0]},\n",
       "   'tid': 0,\n",
       "   'vals': {'x': [3.9293837119572324]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.908867,\n",
       "   'loss': 15.440056355794798,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 0,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [1]},\n",
       "   'tid': 1,\n",
       "   'vals': {'x': [-7.8787018809053455]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.9104,\n",
       "   'loss': 62.07394332818143,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 1,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [2]},\n",
       "   'tid': 2,\n",
       "   'vals': {'x': [0.13452022999139857]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.911871,\n",
       "   'loss': 0.018095692276938767,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 2,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [3]},\n",
       "   'tid': 3,\n",
       "   'vals': {'x': [-7.896557455646496]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.913176,\n",
       "   'loss': 62.35561965032627,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 3,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [4]},\n",
       "   'tid': 4,\n",
       "   'vals': {'x': [0.47666639909351716]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.914788,\n",
       "   'loss': 0.22721085602478017,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 4,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [5]},\n",
       "   'tid': 5,\n",
       "   'vals': {'x': [7.332740857007959]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.916228,\n",
       "   'loss': 53.769088476033815,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 5,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [6]},\n",
       "   'tid': 6,\n",
       "   'vals': {'x': [1.4106325029081361]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.917722,\n",
       "   'loss': 1.9898840582608728,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 6,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [7]},\n",
       "   'tid': 7,\n",
       "   'vals': {'x': [-7.167394618465186]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.918907,\n",
       "   'loss': 51.371545616803715,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 7,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [8]},\n",
       "   'tid': 8,\n",
       "   'vals': {'x': [3.0030721094349815]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.920187,\n",
       "   'loss': 9.01844209446627,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 8,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [9]},\n",
       "   'tid': 9,\n",
       "   'vals': {'x': [5.618077975849323]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.921561,\n",
       "   'loss': 31.562800142723223,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 9,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [10]},\n",
       "   'tid': 10,\n",
       "   'vals': {'x': [-1.6098276730258867]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.92309,\n",
       "   'loss': 2.591545136839941,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 10,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [11]},\n",
       "   'tid': 11,\n",
       "   'vals': {'x': [6.228072526171697]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.924514,\n",
       "   'loss': 38.78888739125471,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 11,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [12]},\n",
       "   'tid': 12,\n",
       "   'vals': {'x': [3.226578848999292]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.925708,\n",
       "   'loss': 10.410811068809597,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 12,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [13]},\n",
       "   'tid': 13,\n",
       "   'vals': {'x': [-6.935681262004067]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.927045,\n",
       "   'loss': 48.10367456811433,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 13,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [14]},\n",
       "   'tid': 14,\n",
       "   'vals': {'x': [8.877004162392915]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.92834,\n",
       "   'loss': 78.80120289914113,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 14,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [15]},\n",
       "   'tid': 15,\n",
       "   'vals': {'x': [4.9868452263737755]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.929621,\n",
       "   'loss': 24.868625311806912,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 15,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [16]},\n",
       "   'tid': 16,\n",
       "   'vals': {'x': [-0.18075224952469426]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.931133,\n",
       "   'loss': 0.03267137570823734,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 16,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [17]},\n",
       "   'tid': 17,\n",
       "   'vals': {'x': [4.958110329927074]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.932388,\n",
       "   'loss': 24.582858043729555,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 17,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [18]},\n",
       "   'tid': 18,\n",
       "   'vals': {'x': [8.347709679032977]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.933605,\n",
       "   'loss': 69.68425688542085,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 18,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [19]},\n",
       "   'tid': 19,\n",
       "   'vals': {'x': [8.041230496250165]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.934772,\n",
       "   'loss': 64.66138789382367,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 19,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [20]},\n",
       "   'tid': 20,\n",
       "   'vals': {'x': [-2.671109426965816]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.937031,\n",
       "   'loss': 7.13482557082565,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 20,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [21]},\n",
       "   'tid': 21,\n",
       "   'vals': {'x': [-2.6844211358062497]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.939646,\n",
       "   'loss': 7.206116834363316,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 21,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [22]},\n",
       "   'tid': 22,\n",
       "   'vals': {'x': [-0.9228401971441693]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.942024,\n",
       "   'loss': 0.8516340294650893,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 22,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [23]},\n",
       "   'tid': 23,\n",
       "   'vals': {'x': [-3.4582936832293085]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.944625,\n",
       "   'loss': 11.959795199463738,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 23,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [24]},\n",
       "   'tid': 24,\n",
       "   'vals': {'x': [-4.853228549173655]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.947183,\n",
       "   'loss': 23.553827350514215,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 24,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [25]},\n",
       "   'tid': 25,\n",
       "   'vals': {'x': [1.1710777235615186]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.949598,\n",
       "   'loss': 1.3714230346220286,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 25,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [26]},\n",
       "   'tid': 26,\n",
       "   'vals': {'x': [-9.71360635023815]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.952241,\n",
       "   'loss': 94.35414832738691,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 26,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [27]},\n",
       "   'tid': 27,\n",
       "   'vals': {'x': [-5.0790107695850235]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.954821,\n",
       "   'loss': 25.79635039756065,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 27,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [28]},\n",
       "   'tid': 28,\n",
       "   'vals': {'x': [2.007456950422798]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.957271,\n",
       "   'loss': 4.029883407800801,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 28,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [29]},\n",
       "   'tid': 29,\n",
       "   'vals': {'x': [-1.062441249044227]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.959508,\n",
       "   'loss': 1.128781407670657,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 29,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [30]},\n",
       "   'tid': 30,\n",
       "   'vals': {'x': [9.937384811417449]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.961723,\n",
       "   'loss': 98.7516168901902,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 30,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [31]},\n",
       "   'tid': 31,\n",
       "   'vals': {'x': [-0.07808813094026912]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.963914,\n",
       "   'loss': 0.006097756193744615,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 31,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [32]},\n",
       "   'tid': 32,\n",
       "   'vals': {'x': [-5.275282942913748]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.967301,\n",
       "   'loss': 27.828610127796733,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 32,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [33]},\n",
       "   'tid': 33,\n",
       "   'vals': {'x': [2.4391122802641974]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.969587,\n",
       "   'loss': 5.949268715735612,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 33,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [34]},\n",
       "   'tid': 34,\n",
       "   'vals': {'x': [0.2160574374514561]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.971825,\n",
       "   'loss': 0.04668081627808987,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 34,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [35]},\n",
       "   'tid': 35,\n",
       "   'vals': {'x': [-3.7757077542077155]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.974207,\n",
       "   'loss': 14.25596904518427,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 35,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [36]},\n",
       "   'tid': 36,\n",
       "   'vals': {'x': [3.9027714861321074]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.977022,\n",
       "   'loss': 15.231625272965818,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 36,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [37]},\n",
       "   'tid': 37,\n",
       "   'vals': {'x': [-9.010254631771833]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.97933,\n",
       "   'loss': 81.18468852936577,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 37,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [38]},\n",
       "   'tid': 38,\n",
       "   'vals': {'x': [-2.0866728844018]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.981663,\n",
       "   'loss': 4.354203726497727,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 38,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [39]},\n",
       "   'tid': 39,\n",
       "   'vals': {'x': [1.0936993723196156]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.983926,\n",
       "   'loss': 1.196178317012321,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 39,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [40]},\n",
       "   'tid': 40,\n",
       "   'vals': {'x': [6.53738228476267]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.986213,\n",
       "   'loss': 42.73736713712879,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 40,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [41]},\n",
       "   'tid': 41,\n",
       "   'vals': {'x': [-5.9343792440728516]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.988788,\n",
       "   'loss': 35.21685701248267,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 41,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [42]},\n",
       "   'tid': 42,\n",
       "   'vals': {'x': [3.870362557017226]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.99102,\n",
       "   'loss': 14.979706322760922,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 42,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [43]},\n",
       "   'tid': 43,\n",
       "   'vals': {'x': [-3.9026706195913072]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.99356,\n",
       "   'loss': 15.230837965021198,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 43,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [44]},\n",
       "   'tid': 44,\n",
       "   'vals': {'x': [-0.9537096347668226]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.995873,\n",
       "   'loss': 0.9095620674470661,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 44,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [45]},\n",
       "   'tid': 45,\n",
       "   'vals': {'x': [2.8625084923066475]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950324.998199,\n",
       "   'loss': 8.193954868527676,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 45,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [46]},\n",
       "   'tid': 46,\n",
       "   'vals': {'x': [1.8505479267383722]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.000547,\n",
       "   'loss': 3.424527629155688,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 46,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [47]},\n",
       "   'tid': 47,\n",
       "   'vals': {'x': [0.028840363274607553]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.003212,\n",
       "   'loss': 0.000831766553811332,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 47,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [48]},\n",
       "   'tid': 48,\n",
       "   'vals': {'x': [-7.804196405313958]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.005894,\n",
       "   'loss': 60.9054815327153,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 48,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [49]},\n",
       "   'tid': 49,\n",
       "   'vals': {'x': [4.913789729430655]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.008375,\n",
       "   'loss': 24.14532950505819,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 49,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [50]},\n",
       "   'tid': 50,\n",
       "   'vals': {'x': [0.4916141954724802]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.010846,\n",
       "   'loss': 0.24168451719005396,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 50,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [51]},\n",
       "   'tid': 51,\n",
       "   'vals': {'x': [-0.23771725634313245]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.013199,\n",
       "   'loss': 0.05650949396330655,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 51,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [52]},\n",
       "   'tid': 52,\n",
       "   'vals': {'x': [-2.820692724048963]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.015725,\n",
       "   'loss': 7.9563074435027605,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 52,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [53]},\n",
       "   'tid': 53,\n",
       "   'vals': {'x': [6.425283663948258]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.018119,\n",
       "   'loss': 41.28427016220034,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 53,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [54]},\n",
       "   'tid': 54,\n",
       "   'vals': {'x': [-1.94655184324699]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.020498,\n",
       "   'loss': 3.789064078448254,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 54,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [55]},\n",
       "   'tid': 55,\n",
       "   'vals': {'x': [-6.121004253928929]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.022937,\n",
       "   'loss': 37.46669307661604,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 55,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [56]},\n",
       "   'tid': 56,\n",
       "   'vals': {'x': [7.402566301681664]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.025512,\n",
       "   'loss': 54.79798785079294,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 56,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [57]},\n",
       "   'tid': 57,\n",
       "   'vals': {'x': [9.613555991952484]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.027898,\n",
       "   'loss': 92.42045881040552,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 57,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [58]},\n",
       "   'tid': 58,\n",
       "   'vals': {'x': [4.450138893729752]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.030338,\n",
       "   'loss': 19.803736173486264,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 58,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [59]},\n",
       "   'tid': 59,\n",
       "   'vals': {'x': [-4.3956470366276506]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.046677,\n",
       "   'loss': 19.321712870613446,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 59,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [60]},\n",
       "   'tid': 60,\n",
       "   'vals': {'x': [-0.39562318427368937]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.04917,\n",
       "   'loss': 0.15651770393485356,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 60,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [61]},\n",
       "   'tid': 61,\n",
       "   'vals': {'x': [5.731750470783767]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.051559,\n",
       "   'loss': 32.85296345932994,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 61,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [62]},\n",
       "   'tid': 62,\n",
       "   'vals': {'x': [0.8095867279118262]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.053958,\n",
       "   'loss': 0.6554306700109773,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 62,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [63]},\n",
       "   'tid': 63,\n",
       "   'vals': {'x': [-3.1023708331266087]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.056347,\n",
       "   'loss': 9.624704786234688,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 63,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [64]},\n",
       "   'tid': 64,\n",
       "   'vals': {'x': [-1.5230837335766925]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.058746,\n",
       "   'loss': 2.319784059485917,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 64,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [65]},\n",
       "   'tid': 65,\n",
       "   'vals': {'x': [3.1283179425620595]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.061133,\n",
       "   'loss': 9.786373149755716,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 65,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [66]},\n",
       "   'tid': 66,\n",
       "   'vals': {'x': [1.685973613673501]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.063546,\n",
       "   'loss': 2.842507026003284,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 66,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [67]},\n",
       "   'tid': 67,\n",
       "   'vals': {'x': [2.561968555905587]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.066067,\n",
       "   'loss': 6.563682881448959,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 67,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [68]},\n",
       "   'tid': 68,\n",
       "   'vals': {'x': [-2.3744597576324336]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.068799,\n",
       "   'loss': 5.638059140615875,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 68,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [69]},\n",
       "   'tid': 69,\n",
       "   'vals': {'x': [-0.6060336586690696]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.071319,\n",
       "   'loss': 0.3672767954398183,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 69,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [70]},\n",
       "   'tid': 70,\n",
       "   'vals': {'x': [0.37643377624889646]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.073717,\n",
       "   'loss': 0.14170238790100423,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 70,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [71]},\n",
       "   'tid': 71,\n",
       "   'vals': {'x': [3.750818853115845]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.076143,\n",
       "   'loss': 14.068642068889261,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 71,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [72]},\n",
       "   'tid': 72,\n",
       "   'vals': {'x': [-1.2184073822747925]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.078531,\n",
       "   'loss': 1.4845165491817123,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 72,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [73]},\n",
       "   'tid': 73,\n",
       "   'vals': {'x': [2.220318445768187]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.080953,\n",
       "   'loss': 4.929814000618458,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 73,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [74]},\n",
       "   'tid': 74,\n",
       "   'vals': {'x': [1.2811279699610472]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.083364,\n",
       "   'loss': 1.6412888754165138,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 74,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [75]},\n",
       "   'tid': 75,\n",
       "   'vals': {'x': [-4.165303519248342]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.085889,\n",
       "   'loss': 17.34975340746262,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 75,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [76]},\n",
       "   'tid': 76,\n",
       "   'vals': {'x': [-1.752762387593871]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.0883,\n",
       "   'loss': 3.072175987363767,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 76,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [77]},\n",
       "   'tid': 77,\n",
       "   'vals': {'x': [3.4961140484546527]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.090716,\n",
       "   'loss': 12.222813439801982,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 77,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [78]},\n",
       "   'tid': 78,\n",
       "   'vals': {'x': [-3.4444746780803586]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.093133,\n",
       "   'loss': 11.86440580793679,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 78,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [79]},\n",
       "   'tid': 79,\n",
       "   'vals': {'x': [-5.606912154329081]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.095532,\n",
       "   'loss': 31.43746390636318,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 79,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [80]},\n",
       "   'tid': 80,\n",
       "   'vals': {'x': [-4.732676952317843]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.09794,\n",
       "   'loss': 22.398231135000508,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 80,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [81]},\n",
       "   'tid': 81,\n",
       "   'vals': {'x': [4.528836808390443]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.100356,\n",
       "   'loss': 20.51036283703213,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 81,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [82]},\n",
       "   'tid': 82,\n",
       "   'vals': {'x': [5.649505370843463]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.103026,\n",
       "   'loss': 31.916910935189136,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 82,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [83]},\n",
       "   'tid': 83,\n",
       "   'vals': {'x': [0.8401385862483072]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.105555,\n",
       "   'loss': 0.7058328441033043,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 83,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [84]},\n",
       "   'tid': 84,\n",
       "   'vals': {'x': [-6.524430435327301]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.108129,\n",
       "   'loss': 42.568192505425195,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 84,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [85]},\n",
       "   'tid': 85,\n",
       "   'vals': {'x': [-7.356624368767511]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.110587,\n",
       "   'loss': 54.11992210314398,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 85,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [86]},\n",
       "   'tid': 86,\n",
       "   'vals': {'x': [0.028499577187215047]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.113221,\n",
       "   'loss': 0.0008122258998500283,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 86,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [87]},\n",
       "   'tid': 87,\n",
       "   'vals': {'x': [0.059682116262555984]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.116442,\n",
       "   'loss': 0.0035619550015772495,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 87,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [88]},\n",
       "   'tid': 88,\n",
       "   'vals': {'x': [-2.459343495561979]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.11899,\n",
       "   'loss': 6.048370429163014,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 88,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [89]},\n",
       "   'tid': 89,\n",
       "   'vals': {'x': [-8.834378313486406]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.121514,\n",
       "   'loss': 78.04624018579891,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 89,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [90]},\n",
       "   'tid': 90,\n",
       "   'vals': {'x': [1.6152559884804691]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.124052,\n",
       "   'loss': 2.6090519083220176,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 90,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [91]},\n",
       "   'tid': 91,\n",
       "   'vals': {'x': [-0.7990351684493548]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.126646,\n",
       "   'loss': 0.6384572004188889,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 91,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [92]},\n",
       "   'tid': 92,\n",
       "   'vals': {'x': [2.6062929072954266]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.129192,\n",
       "   'loss': 6.792762718618447,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 92,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [93]},\n",
       "   'tid': 93,\n",
       "   'vals': {'x': [-3.199994615363141]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.131751,\n",
       "   'loss': 10.239965538353097,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 93,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [94]},\n",
       "   'tid': 94,\n",
       "   'vals': {'x': [7.107976483217664]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.134302,\n",
       "   'loss': 50.52332968597535,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 94,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [95]},\n",
       "   'tid': 95,\n",
       "   'vals': {'x': [0.08717668234097489]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.13684,\n",
       "   'loss': 0.0075997739439792435,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 95,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [96]},\n",
       "   'tid': 96,\n",
       "   'vals': {'x': [4.317537555712176]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.139395,\n",
       "   'loss': 18.641130544985067,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 96,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [97]},\n",
       "   'tid': 97,\n",
       "   'vals': {'x': [0.7371735032500174]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.142096,\n",
       "   'loss': 0.5434247738939034,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 97,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [98]},\n",
       "   'tid': 98,\n",
       "   'vals': {'x': [-1.2402091757228908]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.144656,\n",
       "   'loss': 1.5381187995472523,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 98,\n",
       "  'version': 0},\n",
       " {'book_time': None,\n",
       "  'exp_key': None,\n",
       "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'idxs': {'x': [99]},\n",
       "   'tid': 99,\n",
       "   'vals': {'x': [5.2686657119125035]},\n",
       "   'workdir': None},\n",
       "  'owner': None,\n",
       "  'refresh_time': None,\n",
       "  'result': {'eval_time': 1453950325.147226,\n",
       "   'loss': 27.758838383882487,\n",
       "   'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
       "   'status': 'ok'},\n",
       "  'spec': None,\n",
       "  'state': 2,\n",
       "  'tid': 99,\n",
       "  'version': 0}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 Âµs, sys: 3 Âµs, total: 5 Âµs\n",
      "Wall time: 4.05 Âµs\n",
      "[[3084    0    0    0    0    0    0    0]\n",
      " [   0 3272    0    0    0    0    0    0]\n",
      " [   0    0  493    0    0    0    0    0]\n",
      " [   0    0    0  695    0    0    0    0]\n",
      " [   0    0    0    0 2777    0    0    0]\n",
      " [   0    0    0    0    0 5625    0    0]\n",
      " [   0    0    0    0    0    0 3990    0]\n",
      " [   0    0    0    0    0    0    0 9755]]\n",
      "[[3123    0    0    0    0    0    0    0]\n",
      " [   0 3280    0    0    0    0    0    0]\n",
      " [   0    0  520    0    0    0    0    0]\n",
      " [   0    0    0  733    0    0    0    0]\n",
      " [   0    0    0    0 2655    0    0    0]\n",
      " [   0    0    0    0    0 5608    0    0]\n",
      " [   0    0    0    0    0    0 4037    0]\n",
      " [   0    0    0    0    0    0    0 9734]]\n",
      "[8 8 8 ..., 7 8 7] [8 8 8 ..., 7 8 7]\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.datasets import load_iris, load_digits, load_boston\n",
    "\n",
    "rng = np.random.RandomState(31337)\n",
    "\n",
    "y = labels\n",
    "X = train\n",
    "kf = KFold(y.shape[0], n_folds=2, shuffle=True, random_state=rng)\n",
    "for train_index, test_index in kf:\n",
    "    xgb_model = XGBClassifier().fit(X[train_index],y[train_index])\n",
    "    predictions = xgb_model.predict(X[test_index])\n",
    "    actuals = y[test_index]\n",
    "    print(confusion_matrix(actuals, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 8\n"
     ]
    }
   ],
   "source": [
    "print min(predictions), max(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 8 artists>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112365410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "holder = np.bincount(predictions)\n",
    "ii = np.nonzero(y)[0]\n",
    "y = list(holder[1:])\n",
    "X = range(1,9)\n",
    "plt.bar(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train and valid ...\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 23.0, 'n_estimators': 358.0, 'subsample': 0.8500000000000001, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 7.3500000000000005}\n",
      "\tScore -0.616892331202\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 58.0, 'n_estimators': 771.0, 'subsample': 0.55, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 6.0, 'gamma': 4.15}\n",
      "\tScore -0.611208987697\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 63.0, 'n_estimators': 152.0, 'subsample': 0.75, 'eta': 0.225, 'objective': 'reg:linear', 'max_depth': 3.0, 'gamma': 1.75}\n",
      "\tScore -0.597117493065\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 10.0, 'n_estimators': 215.0, 'subsample': 0.55, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 2.0, 'gamma': 7.6000000000000005}\n",
      "\tScore -0.574564724572\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 19.0, 'n_estimators': 136.0, 'subsample': 0.75, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 5.75}\n",
      "\tScore -0.567027250263\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 14.0, 'n_estimators': 337.0, 'subsample': 0.9500000000000001, 'eta': 0.2, 'objective': 'reg:linear', 'max_depth': 1.0, 'gamma': 2.75}\n",
      "\tScore -0.552153598137\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 61.0, 'n_estimators': 341.0, 'subsample': 0.8, 'eta': 0.15000000000000002, 'objective': 'reg:linear', 'max_depth': 2.0, 'gamma': 9.75}\n",
      "\tScore -0.588828633009\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 56.0, 'n_estimators': 380.0, 'subsample': 0.55, 'eta': 0.275, 'objective': 'reg:linear', 'max_depth': 3.0, 'gamma': 5.2}\n",
      "\tScore -0.604027388033\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'min_child_weight': 39.0, 'n_estimators': 953.0, 'subsample': 0.8500000000000001, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 7.050000000000001}\n",
      "\tScore -0.618591108914\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 83.0, 'n_estimators': 443.0, 'subsample': 0.9, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 8.1}\n",
      "\tScore -0.610906333781\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'min_child_weight': 12.0, 'n_estimators': 217.0, 'subsample': 0.7000000000000001, 'eta': 0.225, 'objective': 'reg:linear', 'max_depth': 3.0, 'gamma': 4.65}\n",
      "\tScore -0.601229347674\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 63.0, 'n_estimators': 503.0, 'subsample': 0.9, 'eta': 0.275, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 6.800000000000001}\n",
      "\tScore -0.593141753142\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 20.0, 'n_estimators': 395.0, 'subsample': 0.8500000000000001, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 7.95}\n",
      "\tScore -0.616229506653\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 22.0, 'n_estimators': 233.0, 'subsample': 0.6000000000000001, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 3.0, 'gamma': 2.8000000000000003}\n",
      "\tScore -0.591363713082\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 70.0, 'n_estimators': 180.0, 'subsample': 0.9500000000000001, 'eta': 0.15000000000000002, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 5.25}\n",
      "\tScore -0.613049346888\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 33.0, 'n_estimators': 735.0, 'subsample': 0.8500000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 8.5}\n",
      "\tScore -0.618466998191\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 93.0, 'n_estimators': 778.0, 'subsample': 0.75, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 2.0}\n",
      "\tScore -0.612043749732\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.55, 'silent': 1, 'min_child_weight': 1.0, 'n_estimators': 198.0, 'subsample': 0.8500000000000001, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 5.0, 'gamma': 0.5}\n",
      "\tScore -0.605570037563\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.55, 'silent': 1, 'min_child_weight': 5.0, 'n_estimators': 570.0, 'subsample': 0.9500000000000001, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 6.0, 'gamma': 7.300000000000001}\n",
      "\tScore -0.613695744725\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 66.0, 'n_estimators': 602.0, 'subsample': 0.9500000000000001, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 2.4000000000000004}\n",
      "\tScore -0.614430694979\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.5, 'silent': 1, 'min_child_weight': 39.0, 'n_estimators': 985.0, 'subsample': 0.8, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 9.55}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'min_child_weight': 40.0, 'n_estimators': 996.0, 'subsample': 0.7000000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 8.700000000000001}\n",
      "\tScore -0.614901816367\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 38.0, 'n_estimators': 877.0, 'subsample': 1.0, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 6.300000000000001}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.5, 'silent': 1, 'min_child_weight': 48.0, 'n_estimators': 661.0, 'subsample': 0.65, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 8.85}\n",
      "\tScore -0.612222273693\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 31.0, 'n_estimators': 907.0, 'subsample': 0.8, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 7.0, 'gamma': 10.0}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'min_child_weight': 49.0, 'n_estimators': 739.0, 'subsample': 0.9, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 8.85}\n",
      "\tScore -0.614876859283\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.55, 'silent': 1, 'min_child_weight': 30.0, 'n_estimators': 864.0, 'subsample': 0.8500000000000001, 'eta': 0.17500000000000002, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 6.45}\n",
      "\tScore -0.601352178569\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 31.0, 'n_estimators': 695.0, 'subsample': 1.0, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 3.9000000000000004}\n",
      "\tScore -0.613275803044\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 75.0, 'n_estimators': 930.0, 'subsample': 0.8500000000000001, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 7.1000000000000005}\n",
      "\tScore -0.605996697961\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 45.0, 'n_estimators': 814.0, 'subsample': 0.7000000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 8.25}\n",
      "\tScore -0.61582617246\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 53.0, 'n_estimators': 682.0, 'subsample': 0.8, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 7.0, 'gamma': 9.35}\n",
      "\tScore -0.616155188775\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'min_child_weight': 28.0, 'n_estimators': 958.0, 'subsample': 0.9, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 5.0, 'gamma': 5.95}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 38.0, 'n_estimators': 821.0, 'subsample': 0.65, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 6.0, 'gamma': 3.85}\n",
      "\tScore -0.611855843292\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.55, 'silent': 1, 'min_child_weight': 25.0, 'n_estimators': 606.0, 'subsample': 0.8, 'eta': 0.17500000000000002, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 6.8500000000000005}\n",
      "\tScore -0.595536671826\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 43.0, 'n_estimators': 742.0, 'subsample': 0.75, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 7.65}\n",
      "\tScore -0.604776426274\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 14.0, 'n_estimators': 844.0, 'subsample': 1.0, 'eta': 0.25, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 5.4}\n",
      "\tScore -0.595647400351\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 80.0, 'n_estimators': 494.0, 'subsample': 0.8500000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 4.5}\n",
      "\tScore -0.613005877982\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 54.0, 'n_estimators': 636.0, 'subsample': 0.9, 'eta': 0.17500000000000002, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 8.25}\n",
      "\tScore -0.602937352528\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.5, 'silent': 1, 'min_child_weight': 9.0, 'n_estimators': 766.0, 'subsample': 0.9500000000000001, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 0.8}\n",
      "\tScore -0.618699903894\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.5, 'silent': 1, 'min_child_weight': 2.0, 'n_estimators': 786.0, 'subsample': 0.9500000000000001, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 4.0, 'gamma': 1.05}\n",
      "\tScore -0.614337857371\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.5, 'silent': 1, 'min_child_weight': 96.0, 'n_estimators': 900.0, 'subsample': 0.9500000000000001, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 6.0, 'gamma': 3.25}\n",
      "\tScore -0.613960742622\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.55, 'silent': 1, 'min_child_weight': 17.0, 'n_estimators': 959.0, 'subsample': 1.0, 'eta': 0.15000000000000002, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 0.9500000000000001}\n",
      "\tScore -0.605281408219\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'min_child_weight': 8.0, 'n_estimators': 714.0, 'subsample': 0.9, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 4.65}\n",
      "\tScore -0.61450546586\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.5, 'silent': 1, 'min_child_weight': 60.0, 'n_estimators': 281.0, 'subsample': 0.75, 'eta': 0.2, 'objective': 'reg:linear', 'max_depth': 7.0, 'gamma': 3.4000000000000004}\n",
      "\tScore -0.603985620787\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.55, 'silent': 1, 'min_child_weight': 86.0, 'n_estimators': 526.0, 'subsample': 1.0, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 5.0, 'gamma': 5.75}\n",
      "\tScore -0.606292892736\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'min_child_weight': 16.0, 'n_estimators': 824.0, 'subsample': 0.5, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 4.0, 'gamma': 1.3}\n",
      "\tScore -0.612671606778\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 68.0, 'n_estimators': 461.0, 'subsample': 0.9, 'eta': 0.30000000000000004, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 1.85}\n",
      "\tScore -0.581920821221\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.5, 'silent': 1, 'min_child_weight': 9.0, 'n_estimators': 933.0, 'subsample': 0.9500000000000001, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 6.4}\n",
      "\tScore -0.610945723852\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 24.0, 'n_estimators': 570.0, 'subsample': 0.8, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 7.7}\n",
      "\tScore -0.618883129244\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 23.0, 'n_estimators': 307.0, 'subsample': 0.65, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 2.0, 'gamma': 9.25}\n",
      "\tScore -0.544908131057\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 4.0, 'n_estimators': 397.0, 'subsample': 0.6000000000000001, 'eta': 0.2, 'objective': 'reg:linear', 'max_depth': 4.0, 'gamma': 7.7}\n",
      "\tScore -0.606717832587\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 35.0, 'n_estimators': 555.0, 'subsample': 0.7000000000000001, 'eta': 0.15000000000000002, 'objective': 'reg:linear', 'max_depth': 6.0, 'gamma': 2.5}\n",
      "\tScore -0.609056080883\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 26.0, 'n_estimators': 113.0, 'subsample': 0.6000000000000001, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 7.0, 'gamma': 5.15}\n",
      "\tScore -0.60693528012\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 19.0, 'n_estimators': 418.0, 'subsample': 0.8, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 9.75}\n",
      "\tScore -0.610857453428\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 12.0, 'n_estimators': 594.0, 'subsample': 0.55, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 1.0, 'gamma': 3.2}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 20.0, 'n_estimators': 479.0, 'subsample': 0.7000000000000001, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 5.0, 'gamma': 7.3500000000000005}\n",
      "\tScore -0.613785995093\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 6.0, 'n_estimators': 642.0, 'subsample': 0.8500000000000001, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 6.050000000000001}\n",
      "\tScore -0.622026787862\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 5.0, 'n_estimators': 647.0, 'subsample': 0.75, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 4.95}\n",
      "\tScore -0.617434871855\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 2.0, 'n_estimators': 440.0, 'subsample': 0.8, 'eta': 0.25, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 6.15}\n",
      "\tScore -0.598052943844\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 36.0, 'n_estimators': 358.0, 'subsample': 0.8500000000000001, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 5.65}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 45.0, 'n_estimators': 519.0, 'subsample': 0.75, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 6.800000000000001}\n",
      "\tScore -0.616501657865\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 12.0, 'n_estimators': 294.0, 'subsample': 0.8, 'eta': 0.225, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 8.05}\n",
      "\tScore -0.60052487276\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 28.0, 'n_estimators': 578.0, 'subsample': 0.65, 'eta': 0.17500000000000002, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 4.2}\n",
      "\tScore -0.592376719605\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 58.0, 'n_estimators': 628.0, 'subsample': 0.7000000000000001, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 8.450000000000001}\n",
      "\tScore -0.604548438329\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 42.0, 'n_estimators': 153.0, 'subsample': 0.8500000000000001, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 7.800000000000001}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 6.0, 'n_estimators': 773.0, 'subsample': 0.9500000000000001, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 9.15}\n",
      "\tScore -0.619584464243\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 5.0, 'n_estimators': 673.0, 'subsample': 0.9, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 6.0, 'gamma': 9.1}\n",
      "\tScore -0.614582899736\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 16.0, 'n_estimators': 527.0, 'subsample': 0.9, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 7.0, 'gamma': 6.550000000000001}\n",
      "\tScore -0.615375339135\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 21.0, 'n_estimators': 761.0, 'subsample': 0.8500000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 7.45}\n",
      "\tScore -0.618563611233\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 7.0, 'n_estimators': 701.0, 'subsample': 0.8500000000000001, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 7.15}\n",
      "\tScore -0.617857753452\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 24.0, 'n_estimators': 794.0, 'subsample': 0.8, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 7.0, 'gamma': 8.65}\n",
      "\tScore -0.615426946155\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 14.0, 'n_estimators': 610.0, 'subsample': 0.9500000000000001, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 10.0}\n",
      "\tScore -0.615318737015\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 33.0, 'n_estimators': 855.0, 'subsample': 0.75, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 9.5}\n",
      "\tScore -0.620274973848\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 33.0, 'n_estimators': 866.0, 'subsample': 1.0, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 9.55}\n",
      "\tScore -0.617254906745\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 64.0, 'n_estimators': 726.0, 'subsample': 0.75, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 9.05}\n",
      "\tScore -0.618035095563\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 72.0, 'n_estimators': 884.0, 'subsample': 0.9, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 9.850000000000001}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 29.0, 'n_estimators': 805.0, 'subsample': 0.75, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 8.3}\n",
      "\tScore -0.609222615621\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 52.0, 'n_estimators': 975.0, 'subsample': 0.8500000000000001, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 8.9}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 1.0, 'n_estimators': 841.0, 'subsample': 0.5, 'eta': 0.15000000000000002, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 9.4}\n",
      "\tScore -0.600070042372\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 11.0, 'n_estimators': 929.0, 'subsample': 0.9500000000000001, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 9.65}\n",
      "\tScore -0.617832729311\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 50.0, 'n_estimators': 755.0, 'subsample': 0.7000000000000001, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 5.550000000000001}\n",
      "\tScore -0.616840976132\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 46.0, 'n_estimators': 670.0, 'subsample': 1.0, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 6.050000000000001}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 32.0, 'n_estimators': 906.0, 'subsample': 0.9500000000000001, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 4.9}\n",
      "\tScore -0.610772140681\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 17.0, 'n_estimators': 847.0, 'subsample': 0.8500000000000001, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 7.95}\n",
      "\tScore -0.614857345989\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 88.0, 'n_estimators': 772.0, 'subsample': 0.9, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 2.0, 'gamma': 6.65}\n",
      "\tScore -0.596453068338\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 26.0, 'n_estimators': 699.0, 'subsample': 0.65, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 8.6}\n",
      "\tScore -0.618527324612\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 55.0, 'n_estimators': 995.0, 'subsample': 0.8, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 7.0}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 40.0, 'n_estimators': 721.0, 'subsample': 0.75, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 4.25}\n",
      "\tScore -0.615073923674\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 35.0, 'n_estimators': 547.0, 'subsample': 0.7000000000000001, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 3.85}\n",
      "\tScore -0.607766158355\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 77.0, 'n_estimators': 651.0, 'subsample': 0.8, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 3.0, 'gamma': 9.950000000000001}\n",
      "\tScore -0.576585627688\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 14.0, 'n_estimators': 829.0, 'subsample': 0.9, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 2.85}\n",
      "\tScore -0.614212210659\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 19.0, 'n_estimators': 887.0, 'subsample': 1.0, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 3.6}\n",
      "\tScore -0.616640874757\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 42.0, 'n_estimators': 944.0, 'subsample': 0.55, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 5.0, 'gamma': 2.3000000000000003}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 7.0, 'n_estimators': 624.0, 'subsample': 0.6000000000000001, 'eta': 0.15000000000000002, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 6.25}\n",
      "\tScore -0.605124125787\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 47.0, 'n_estimators': 687.0, 'subsample': 0.8500000000000001, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 6.0, 'gamma': 1.5}\n",
      "\tScore -0.612883480372\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 22.0, 'n_estimators': 921.0, 'subsample': 0.9, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 5.8500000000000005}\n",
      "\tScore -0.616811973611\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 4.0, 'n_estimators': 798.0, 'subsample': 0.9500000000000001, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 4.0, 'gamma': 8.450000000000001}\n",
      "\tScore -0.612662419887\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 11.0, 'n_estimators': 856.0, 'subsample': 0.65, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 7.5}\n",
      "\tScore -0.618224678514\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 37.0, 'n_estimators': 745.0, 'subsample': 1.0, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 9.25}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 99.0, 'n_estimators': 714.0, 'subsample': 0.8, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 8.85}\n",
      "\tScore -0.609290076753\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 59.0, 'n_estimators': 968.0, 'subsample': 0.7000000000000001, 'eta': 0.2, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 4.6000000000000005}\n",
      "\tScore -0.5848583935\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'min_child_weight': 1.0, 'n_estimators': 496.0, 'subsample': 0.75, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 7.0, 'gamma': 5.300000000000001}\n",
      "\tScore -0.607019756928\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 7.0, 'n_estimators': 654.0, 'subsample': 0.9500000000000001, 'eta': 0.17500000000000002, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 8.25}\n",
      "\tScore -0.607193636259\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 63.0, 'n_estimators': 584.0, 'subsample': 0.8500000000000001, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 9.5}\n",
      "\tScore -0.61173670659\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 3.0, 'n_estimators': 475.0, 'subsample': 0.9, 'eta': 0.25, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 7.8500000000000005}\n",
      "\tScore -0.598430776926\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 28.0, 'n_estimators': 816.0, 'subsample': 0.8, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 0.6000000000000001}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 33.0, 'n_estimators': 547.0, 'subsample': 0.8500000000000001, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 5.0, 'gamma': 7.25}\n",
      "\tScore -0.611260510613\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 30.0, 'n_estimators': 328.0, 'subsample': 0.75, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 9.05}\n",
      "\tScore -0.615404948606\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 14.0, 'n_estimators': 778.0, 'subsample': 0.8, 'eta': 0.225, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 2.95}\n",
      "\tScore -0.59463115269\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 26.0, 'n_estimators': 234.0, 'subsample': 0.9, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 6.0, 'gamma': 6.95}\n",
      "\tScore -0.574467034543\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 18.0, 'n_estimators': 606.0, 'subsample': 0.7000000000000001, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 1.0, 'gamma': 4.95}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 57.0, 'n_estimators': 748.0, 'subsample': 1.0, 'eta': 0.275, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 9.75}\n",
      "\tScore -0.598242008079\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 71.0, 'n_estimators': 415.0, 'subsample': 0.9500000000000001, 'eta': 0.17500000000000002, 'objective': 'reg:linear', 'max_depth': 3.0, 'gamma': 6.7}\n",
      "\tScore -0.605760864357\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 9.0, 'n_estimators': 871.0, 'subsample': 0.65, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 8.1}\n",
      "\tScore -0.615024456946\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 44.0, 'n_estimators': 446.0, 'subsample': 0.9, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 7.0, 'gamma': 4.3500000000000005}\n",
      "\tScore -0.616823095231\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 68.0, 'n_estimators': 623.0, 'subsample': 0.8500000000000001, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 2.15}\n",
      "\tScore -0.597456435515\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 22.0, 'n_estimators': 565.0, 'subsample': 0.75, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 2.6}\n",
      "\tScore -0.610493669206\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'min_child_weight': 41.0, 'n_estimators': 893.0, 'subsample': 0.55, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 8.700000000000001}\n",
      "\tScore -0.61182635511\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 51.0, 'n_estimators': 510.0, 'subsample': 0.9500000000000001, 'eta': 0.15000000000000002, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 6.4}\n",
      "\tScore -0.607834169239\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 91.0, 'n_estimators': 994.0, 'subsample': 0.8, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 3.95}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 15.0, 'n_estimators': 787.0, 'subsample': 1.0, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 10.0}\n",
      "\tScore -0.618435932651\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 48.0, 'n_estimators': 836.0, 'subsample': 0.6000000000000001, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 7.0, 'gamma': 3.6500000000000004}\n",
      "\tScore -0.611925539726\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 6.0, 'n_estimators': 374.0, 'subsample': 0.7000000000000001, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 4.0, 'gamma': 5.45}\n",
      "\tScore -0.608217670948\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 38.0, 'n_estimators': 948.0, 'subsample': 0.8500000000000001, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 9.200000000000001}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 35.0, 'n_estimators': 704.0, 'subsample': 0.75, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 7.45}\n",
      "\tScore -0.605918272697\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 10.0, 'n_estimators': 914.0, 'subsample': 0.9, 'eta': 0.30000000000000004, 'objective': 'reg:linear', 'max_depth': 6.0, 'gamma': 5.95}\n",
      "\tScore -0.595436193654\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 61.0, 'n_estimators': 730.0, 'subsample': 0.8, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 7.65}\n",
      "\tScore -0.617075937702\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.55, 'silent': 1, 'min_child_weight': 82.0, 'n_estimators': 639.0, 'subsample': 0.9500000000000001, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 9.4}\n",
      "\tScore -0.613010857616\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 12.0, 'n_estimators': 539.0, 'subsample': 0.7000000000000001, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 8.15}\n",
      "\tScore -0.616760991315\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 74.0, 'n_estimators': 687.0, 'subsample': 0.65, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 8.4}\n",
      "\tScore -0.613900468701\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 20.0, 'n_estimators': 669.0, 'subsample': 0.8500000000000001, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 4.800000000000001}\n",
      "\tScore -0.614022925812\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 2.0, 'n_estimators': 590.0, 'subsample': 0.75, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 8.75}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 24.0, 'n_estimators': 763.0, 'subsample': 1.0, 'eta': 0.2, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 5.15}\n",
      "\tScore -0.60433214688\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 54.0, 'n_estimators': 858.0, 'subsample': 0.8, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 9.0}\n",
      "\tScore -0.615306452833\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 17.0, 'n_estimators': 810.0, 'subsample': 0.6000000000000001, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 5.0, 'gamma': 1.7000000000000002}\n",
      "\tScore -0.616433169003\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 27.0, 'n_estimators': 980.0, 'subsample': 0.9, 'eta': 0.15000000000000002, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 7.9}\n",
      "\tScore -0.608408533748\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 32.0, 'n_estimators': 256.0, 'subsample': 0.8500000000000001, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 5.65}\n",
      "\tScore -0.610609453853\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 4.0, 'n_estimators': 879.0, 'subsample': 0.9500000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 2.0, 'gamma': 7.2}\n",
      "\tScore -0.558605262253\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 22.0, 'n_estimators': 739.0, 'subsample': 0.8, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 7.0, 'gamma': 9.600000000000001}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 12.0, 'n_estimators': 481.0, 'subsample': 0.75, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 3.1500000000000004}\n",
      "\tScore -0.607942286691\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 78.0, 'n_estimators': 785.0, 'subsample': 0.9500000000000001, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 6.0, 'gamma': 6.2}\n",
      "\tScore -0.614141496947\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 8.0, 'n_estimators': 665.0, 'subsample': 0.8500000000000001, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 6.8500000000000005}\n",
      "\tScore -0.61946181921\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 39.0, 'n_estimators': 710.0, 'subsample': 0.5, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 6.5}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 19.0, 'n_estimators': 822.0, 'subsample': 0.9, 'eta': 0.17500000000000002, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 9.850000000000001}\n",
      "\tScore -0.605100751096\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 29.0, 'n_estimators': 609.0, 'subsample': 0.7000000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 1.1500000000000001}\n",
      "\tScore -0.613191030012\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 1.0, 'n_estimators': 665.0, 'subsample': 0.8500000000000001, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 6.75}\n",
      "\tScore -0.6200608465\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 1.0, 'n_estimators': 570.0, 'subsample': 0.8500000000000001, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 7.050000000000001}\n",
      "\tScore -0.620969703259\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'min_child_weight': 2.0, 'n_estimators': 396.0, 'subsample': 0.8500000000000001, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 6.0}\n",
      "\tScore -0.618568233954\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 5.0, 'n_estimators': 564.0, 'subsample': 0.8, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 4.5}\n",
      "\tScore -0.619022838015\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 1.0, 'n_estimators': 517.0, 'subsample': 0.75, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 5.8500000000000005}\n",
      "\tScore -0.617143500742\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.55, 'silent': 1, 'min_child_weight': 16.0, 'n_estimators': 445.0, 'subsample': 0.8, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 5.2}\n",
      "\tScore -0.616994516078\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 9.0, 'n_estimators': 423.0, 'subsample': 0.8500000000000001, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 7.050000000000001}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 14.0, 'n_estimators': 533.0, 'subsample': 0.9, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 6.75}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'min_child_weight': 12.0, 'n_estimators': 342.0, 'subsample': 0.8, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 6.300000000000001}\n",
      "\tScore -0.605518809314\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 7.0, 'n_estimators': 636.0, 'subsample': 0.8500000000000001, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 5.4}\n",
      "\tScore -0.615792125761\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 4.0, 'n_estimators': 467.0, 'subsample': 0.9, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 6.6000000000000005}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 10.0, 'n_estimators': 583.0, 'subsample': 0.8500000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 7.65}\n",
      "\tScore -0.617420787377\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 65.0, 'n_estimators': 493.0, 'subsample': 0.75, 'eta': 0.275, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 7.300000000000001}\n",
      "\tScore -0.587878179196\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'min_child_weight': 21.0, 'n_estimators': 686.0, 'subsample': 0.8, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 4.75}\n",
      "\tScore -0.615391743965\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 34.0, 'n_estimators': 614.0, 'subsample': 0.9, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 4.1000000000000005}\n",
      "\tScore -0.611353957666\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 24.0, 'n_estimators': 654.0, 'subsample': 0.8, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 5.1000000000000005}\n",
      "\tScore -0.614596980725\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 3.0, 'n_estimators': 563.0, 'subsample': 0.75, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 5.65}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 31.0, 'n_estimators': 597.0, 'subsample': 0.8500000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 5.800000000000001}\n",
      "\tScore -0.618038255286\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 49.0, 'n_estimators': 502.0, 'subsample': 0.7000000000000001, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 7.0, 'gamma': 7.0}\n",
      "\tScore -0.615516146969\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'min_child_weight': 1.0, 'n_estimators': 190.0, 'subsample': 0.8500000000000001, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 7.45}\n",
      "\tScore -0.612613991181\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 18.0, 'n_estimators': 730.0, 'subsample': 0.8, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 6.15}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 14.0, 'n_estimators': 638.0, 'subsample': 0.9, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 8.05}\n",
      "\tScore -0.616747301829\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.55, 'silent': 1, 'min_child_weight': 43.0, 'n_estimators': 383.0, 'subsample': 0.7000000000000001, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 4.45}\n",
      "\tScore -0.606791443847\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 68.0, 'n_estimators': 422.0, 'subsample': 0.65, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 6.45}\n",
      "\tScore -0.613461600841\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 6.0, 'n_estimators': 550.0, 'subsample': 0.75, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 7.0, 'gamma': 8.55}\n",
      "\tScore -0.614954010815\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 26.0, 'n_estimators': 756.0, 'subsample': 0.8500000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 5.45}\n",
      "\tScore -0.619822258804\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 61.0, 'n_estimators': 577.0, 'subsample': 0.8, 'eta': 0.225, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 7.75}\n",
      "\tScore -0.593504818198\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 46.0, 'n_estimators': 675.0, 'subsample': 0.9, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 3.7}\n",
      "\tScore -0.617751361801\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 36.0, 'n_estimators': 519.0, 'subsample': 0.75, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 3.4000000000000004}\n",
      "\tScore -0.615682014939\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 10.0, 'n_estimators': 454.0, 'subsample': 0.8500000000000001, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 7.2}\n",
      "\tScore -0.612023503535\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 87.0, 'n_estimators': 798.0, 'subsample': 0.8, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 4.95}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 8.0, 'n_estimators': 714.0, 'subsample': 0.9, 'eta': 0.15000000000000002, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 6.75}\n",
      "\tScore -0.604794318601\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 56.0, 'n_estimators': 316.0, 'subsample': 0.8500000000000001, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 8.25}\n",
      "\tScore -0.615285140072\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 16.0, 'n_estimators': 276.0, 'subsample': 0.9500000000000001, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 6.050000000000001}\n",
      "\tScore -0.613654004077\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 13.0, 'n_estimators': 369.0, 'subsample': 0.75, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 6.9}\n",
      "\tScore -0.615473989949\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 20.0, 'n_estimators': 484.0, 'subsample': 0.8, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 6.3500000000000005}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 23.0, 'n_estimators': 630.0, 'subsample': 0.65, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 5.6000000000000005}\n",
      "\tScore -0.616641596667\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 4.0, 'n_estimators': 691.0, 'subsample': 0.8500000000000001, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 7.6000000000000005}\n",
      "\tScore -0.617924734586\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 30.0, 'n_estimators': 530.0, 'subsample': 0.9, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 4.0}\n",
      "\tScore -0.611275950895\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 52.0, 'n_estimators': 902.0, 'subsample': 1.0, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 7.9}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 18.0, 'n_estimators': 838.0, 'subsample': 0.9500000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 7.0, 'gamma': 5.300000000000001}\n",
      "\tScore -0.615425292766\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'min_child_weight': 1.0, 'n_estimators': 936.0, 'subsample': 0.8, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 8.8}\n",
      "\tScore -0.617371405882\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.55, 'silent': 1, 'min_child_weight': 6.0, 'n_estimators': 143.0, 'subsample': 0.75, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 8.4}\n",
      "\tScore -0.600716348189\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 94.0, 'n_estimators': 600.0, 'subsample': 0.8500000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 6.65}\n",
      "\tScore -0.610880985177\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.5, 'silent': 1, 'min_child_weight': 9.0, 'n_estimators': 410.0, 'subsample': 0.7000000000000001, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 7.4}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 41.0, 'n_estimators': 652.0, 'subsample': 0.6000000000000001, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 7.0, 'gamma': 7.050000000000001}\n",
      "\tScore -0.612482810248\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 11.0, 'n_estimators': 108.0, 'subsample': 0.9, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 5.8500000000000005}\n",
      "\tScore -0.612062268363\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 27.0, 'n_estimators': 354.0, 'subsample': 0.8, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 6.0, 'gamma': 4.3}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 83.0, 'n_estimators': 774.0, 'subsample': 0.8500000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 9.3}\n",
      "\tScore -0.616930670889\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 15.0, 'n_estimators': 957.0, 'subsample': 0.7000000000000001, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 6.2}\n",
      "\tScore -0.616547674697\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 1.0, 'n_estimators': 620.0, 'subsample': 0.9500000000000001, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 6.5}\n",
      "\tScore -0.618386001269\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 3.0, 'n_estimators': 732.0, 'subsample': 0.75, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 5.1000000000000005}\n",
      "\tScore -0.608075301073\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 73.0, 'n_estimators': 556.0, 'subsample': 0.8, 'eta': 0.2, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 4.7}\n",
      "\tScore -0.596161617825\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 37.0, 'n_estimators': 858.0, 'subsample': 0.9, 'eta': 0.25, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 8.1}\n",
      "\tScore -0.591383528366\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 24.0, 'n_estimators': 467.0, 'subsample': 0.8500000000000001, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 7.2}\n",
      "\tScore -0.616365637694\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 33.0, 'n_estimators': 436.0, 'subsample': 0.9, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 8.950000000000001}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'min_child_weight': 21.0, 'n_estimators': 704.0, 'subsample': 0.8, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 5.7}\n",
      "\tScore -0.615582950215\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 100.0, 'n_estimators': 809.0, 'subsample': 0.65, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 6.0}\n",
      "\tScore -0.607537890827\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 59.0, 'n_estimators': 750.0, 'subsample': 0.7000000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 7.550000000000001}\n",
      "\tScore -0.615646041129\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 7.0, 'n_estimators': 580.0, 'subsample': 0.9500000000000001, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 7.0, 'gamma': 6.8500000000000005}\n",
      "\tScore -0.616938534554\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 12.0, 'n_estimators': 670.0, 'subsample': 0.75, 'eta': 0.17500000000000002, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 8.55}\n",
      "\tScore -0.602232401427\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 76.0, 'n_estimators': 540.0, 'subsample': 0.8500000000000001, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 5.45}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 17.0, 'n_estimators': 649.0, 'subsample': 0.8, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 6.0, 'gamma': 4.55}\n",
      "\tScore -0.613593000912\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 44.0, 'n_estimators': 507.0, 'subsample': 1.0, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 7.3500000000000005}\n",
      "\tScore -0.616416394568\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 5.0, 'n_estimators': 875.0, 'subsample': 0.9, 'eta': 0.15000000000000002, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 7.8500000000000005}\n",
      "\tScore -0.614073884328\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'min_child_weight': 69.0, 'n_estimators': 723.0, 'subsample': 0.8500000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 4.95}\n",
      "\tScore -0.614711080047\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 39.0, 'n_estimators': 596.0, 'subsample': 0.75, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 6.4}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 19.0, 'n_estimators': 914.0, 'subsample': 0.7000000000000001, 'eta': 0.275, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 6.6000000000000005}\n",
      "\tScore -0.576722323643\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 25.0, 'n_estimators': 822.0, 'subsample': 0.55, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 9.600000000000001}\n",
      "\tScore -0.606637490551\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 29.0, 'n_estimators': 765.0, 'subsample': 0.8, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 8.3}\n",
      "\tScore -0.617865708535\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 14.0, 'n_estimators': 567.0, 'subsample': 0.8500000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 6.2}\n",
      "\tScore -0.616345287741\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 8.0, 'n_estimators': 688.0, 'subsample': 0.9500000000000001, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 3.0500000000000003}\n",
      "\tScore -0.616857018322\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 10.0, 'n_estimators': 616.0, 'subsample': 0.9, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 7.0, 'gamma': 5.25}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 3.0, 'n_estimators': 214.0, 'subsample': 0.8, 'eta': 0.30000000000000004, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 9.15}\n",
      "\tScore -0.595707771141\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 22.0, 'n_estimators': 665.0, 'subsample': 0.75, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 7.050000000000001}\n",
      "\tScore -0.618107086625\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.55, 'silent': 1, 'min_child_weight': 27.0, 'n_estimators': 993.0, 'subsample': 0.9, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 3.5}\n",
      "\tScore -0.611384710972\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.75, 'silent': 1, 'min_child_weight': 31.0, 'n_estimators': 847.0, 'subsample': 0.8, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 8.700000000000001}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 48.0, 'n_estimators': 392.0, 'subsample': 0.5, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 5.0, 'gamma': 3.75}\n",
      "\tScore -0.592124372686\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.7000000000000001, 'silent': 1, 'min_child_weight': 16.0, 'n_estimators': 786.0, 'subsample': 0.65, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 4.15}\n",
      "\tScore -0.60505063483\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 53.0, 'n_estimators': 522.0, 'subsample': 1.0, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 5.8500000000000005}\n",
      "\tScore -0.615114826045\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 35.0, 'n_estimators': 487.0, 'subsample': 0.7000000000000001, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 7.7}\n",
      "\tScore -0.611274462149\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 90.0, 'n_estimators': 461.0, 'subsample': 0.8500000000000001, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 8.0}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8, 'silent': 1, 'min_child_weight': 85.0, 'n_estimators': 545.0, 'subsample': 0.6000000000000001, 'eta': 0.2, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 9.850000000000001}\n",
      "\tScore -0.589286342103\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.65, 'silent': 1, 'min_child_weight': 50.0, 'n_estimators': 743.0, 'subsample': 0.8500000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 9.4}\n",
      "\tScore -0.617311063798\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.6000000000000001, 'silent': 1, 'min_child_weight': 2.0, 'n_estimators': 633.0, 'subsample': 0.75, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 6.0, 'gamma': 5.550000000000001}\n",
      "\tScore -0.616147921601\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 13.0, 'n_estimators': 712.0, 'subsample': 0.9, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 2.75}\n",
      "\tScore -0.621482556809\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 20.0, 'n_estimators': 698.0, 'subsample': 1.0, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 1.0, 'gamma': 0.8500000000000001}\n",
      "\tScore -0.541169282506\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 13.0, 'n_estimators': 970.0, 'subsample': 0.9500000000000001, 'eta': 0.15000000000000002, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 1.75}\n",
      "\tScore -0.604925899013\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 41.0, 'n_estimators': 892.0, 'subsample': 0.9, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 7.0, 'gamma': 1.25}\n",
      "\tScore -0.617255261786\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 33.0, 'n_estimators': 806.0, 'subsample': 0.9500000000000001, 'eta': 0.225, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 2.5}\n",
      "\tScore -0.597208775674\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 28.0, 'n_estimators': 833.0, 'subsample': 0.9500000000000001, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 0.55}\n",
      "\tScore -0.615185482772\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 23.0, 'n_estimators': 788.0, 'subsample': 1.0, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 2.0}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 38.0, 'n_estimators': 932.0, 'subsample': 0.9, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 3.3000000000000003}\n",
      "\tScore -0.612065657711\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 25.0, 'n_estimators': 757.0, 'subsample': 0.9, 'eta': 0.125, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 1.05}\n",
      "\tScore -0.603827543193\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 6.0, 'n_estimators': 680.0, 'subsample': 0.9500000000000001, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 2.0}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 45.0, 'n_estimators': 716.0, 'subsample': 0.9, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 3.85}\n",
      "\tScore -0.618187416706\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 18.0, 'n_estimators': 863.0, 'subsample': 1.0, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 13.0, 'gamma': 2.7}\n",
      "\tScore -0.612400052904\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 10.0, 'n_estimators': 584.0, 'subsample': 0.55, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 9.0, 'gamma': 2.4000000000000004}\n",
      "\tScore -0.604444515756\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 12.0, 'n_estimators': 161.0, 'subsample': 0.8500000000000001, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 5.0, 'gamma': 2.85}\n",
      "\tScore -0.599612227391\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 16.0, 'n_estimators': 772.0, 'subsample': 0.9500000000000001, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 4.0, 'gamma': 4.75}\n",
      "\tScore -0.59904436318\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 1.0, 'silent': 1, 'min_child_weight': 63.0, 'n_estimators': 616.0, 'subsample': 0.8, 'eta': 0.0, 'objective': 'reg:linear', 'max_depth': 11.0, 'gamma': 3.5500000000000003}\n",
      "\tScore -0.0\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 31.0, 'n_estimators': 641.0, 'subsample': 1.0, 'eta': 0.07500000000000001, 'objective': 'reg:linear', 'max_depth': 10.0, 'gamma': 1.4000000000000001}\n",
      "\tScore -0.616088424987\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9, 'silent': 1, 'min_child_weight': 5.0, 'n_estimators': 722.0, 'subsample': 0.8500000000000001, 'eta': 0.05, 'objective': 'reg:linear', 'max_depth': 8.0, 'gamma': 1.55}\n",
      "\tScore -0.615921284496\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.8500000000000001, 'silent': 1, 'min_child_weight': 36.0, 'n_estimators': 947.0, 'subsample': 0.75, 'eta': 0.025, 'objective': 'reg:linear', 'max_depth': 12.0, 'gamma': 2.3000000000000003}\n",
      "\tScore -0.620143078602\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'colsample_bytree': 0.9500000000000001, 'silent': 1, 'min_child_weight': 8.0, 'n_estimators': 737.0, 'subsample': 0.8, 'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 7.0, 'gamma': 4.25}\n",
      "\tScore -0.613491639355\n",
      "\n",
      "\n",
      "{'colsample_bytree': 0.8500000000000001, 'min_child_weight': 6.0, 'n_estimators': 642.0, 'subsample': 0.8500000000000001, 'eta': 0.05, 'max_depth': 11.0, 'gamma': 6.050000000000001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DATA_TRAIN_PATH = '/Users/patrickkennedy/Desktop/Project DATA/train.csv'\n",
    "DATA_TEST_PATH = '/Users/patrickkennedy/Desktop/Project DATA/test.csv'\n",
    "\n",
    "\n",
    "def eval_wrapper(yhat, y):  \n",
    "    y = np.array(y)\n",
    "    y = y.astype(int)\n",
    "    yhat = np.array(yhat)\n",
    "    yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)   \n",
    "    return quadratic_weighted_kappa(yhat, y)\n",
    "\n",
    "def load_data(path_train = DATA_TRAIN_PATH, path_test = DATA_TEST_PATH):\n",
    "    columns_to_drop = ['Id', 'Response']\n",
    "    num_classes = 8\n",
    "\n",
    "    train = pd.read_csv(path_train)\n",
    "    test = pd.read_csv(path_test)\n",
    "\n",
    "    # combine train and test\n",
    "    all_data = train.append(test)\n",
    "    all_data.fillna(-1, inplace=True)\n",
    "    all_data['Product_Info_2'] = pd.factorize(all_data['Product_Info_2'])[0]\n",
    "    all_data['Response'] = all_data['Response'].astype(int)\n",
    "\n",
    "    train = all_data[all_data['Response']>0].copy()\n",
    "    test = all_data[all_data['Response']<1].copy()\n",
    "    train_labels = train[\"Response\"]\n",
    "    \n",
    "    #train_labels = train_labels.reshape(-1,1) #need to reshape b/c one dim array\n",
    "    \n",
    "    train_ids = train[\"Id\"].values\n",
    "    test_ids = test[\"Id\"].values\n",
    "\n",
    "    return  train.drop(columns_to_drop, axis=1), \\\n",
    "            train_labels, \\\n",
    "            test.drop(columns_to_drop, axis=1),\\\n",
    "            train_ids, \\\n",
    "            test_ids\n",
    "            \n",
    "\n",
    "def write_submission(preds, output):\n",
    "    sample = pd.read_csv('../data/sampleSubmission.csv')\n",
    "    preds = pd.DataFrame(\n",
    "        preds, index=sample.id.values, columns=sample.columns[1:])\n",
    "    preds.to_csv(output, index_label='id')\n",
    "\n",
    "\n",
    "def score(params):\n",
    "    #change this around and try it with cross_val_score? so that i can have some cross validation?\n",
    "    print \"Training with params : \"\n",
    "    print params\n",
    "    num_round = int(params['n_estimators'])\n",
    "    del params['n_estimators']\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_test, label=y_test)\n",
    "    # watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "    model = xgb.train(params, dtrain, num_round)\n",
    "    predictions = model.predict(dvalid)\n",
    "    score = -eval_wrapper(predictions, y_test)\n",
    "    print \"\\tScore {0}\\n\\n\".format(score)\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def optimize(trials):\n",
    "    space = {\n",
    "             'n_estimators' : hp.quniform('n_estimators', 100, 1000, 1),\n",
    "             'eta' : hp.quniform('eta', 0.001, 0.3, 0.025),\n",
    "             'max_depth' : hp.quniform('max_depth', 1, 13, 1),\n",
    "             'min_child_weight' : hp.quniform('min_child_weight', 1, 100, 1),\n",
    "             'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "             'gamma' : hp.quniform('gamma', 0.5, 10, 0.05),\n",
    "             'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "             'objective': 'reg:linear',\n",
    "             'silent' : 1\n",
    "             }\n",
    "\n",
    "    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=250)\n",
    "\n",
    "    print best\n",
    "\n",
    "\n",
    "X, y, _, _, _ = load_data()\n",
    "print \"Splitting data into train and valid ...\\n\\n\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "#Trials object where the history of search will be stored\n",
    "trials = Trials()\n",
    "\n",
    "optimize(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the data using pandas\n",
      "Eliminate missing values\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-236a9f1f64b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Eliminate missing values'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Use -1 for any others\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# fix the dtype on the label column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_data' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import xgboost as xgb\n",
    "from scipy.optimize import fmin_powell\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "\n",
    "def eval_wrapper(yhat, y):  \n",
    "    y = np.array(y)\n",
    "    y = y.astype(int)\n",
    "    yhat = np.array(yhat)\n",
    "    yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)   \n",
    "    return quadratic_weighted_kappa(yhat, y)\n",
    "    \n",
    "def get_params():\n",
    "    \n",
    "    params = {}\n",
    "    params[\"objective\"] = \"reg:linear\"     \n",
    "    params[\"eta\"] = 0.038968208713327845\n",
    "    params[\"gamma\"] = 0.83745926973096763\n",
    "    params[\"max_delta_step\"] = 6.6502581223657851\n",
    "    params[\"min_child_weight\"] = 50.31287747915195\n",
    "    params[\"subsample\"] = 0.89026862324253775\n",
    "    params[\"colsample_bytree\"] = 0.86459387506389063\n",
    "    params[\"silent\"] = 1\n",
    "    params[\"max_depth\"] = 10\n",
    "    params[\"n_estimators\"] = 200\n",
    "    plst = list(params.items())\n",
    "\n",
    "    return plst\n",
    "    \n",
    "def apply_offset(data, bin_offset, sv, scorer=eval_wrapper):\n",
    "    # data has the format of pred=0, offset_pred=1, labels=2 in the first dim\n",
    "    data[1, data[0].astype(int)==sv] = data[0, data[0].astype(int)==sv] + bin_offset\n",
    "    score = scorer(data[1], data[2])\n",
    "    return score\n",
    "\n",
    "# global variables\n",
    "#columns_to_drop = ['Id', 'Response', 'Medical_History_10','Medical_History_24']\n",
    "columns_to_drop = ['Id', 'Response']\n",
    "xgb_num_rounds = 200\n",
    "num_classes = 8\n",
    "eta_list = [0.038968208713327845] * 150 \n",
    "eta_list = eta_list + [0.01] * 35\n",
    "eta_list = eta_list + [0.005] * 15\n",
    "\n",
    "print(\"Load the data using pandas\")\n",
    "\n",
    "DATA_TRAIN_PATH = '/Users/patrickkennedy/Desktop/Project DATA/train.csv'\n",
    "DATA_TEST_PATH = '/Users/patrickkennedy/Desktop/Project DATA/test.csv'\n",
    "\n",
    "train = pd.read_csv(DATA_TRAIN_PATH)\n",
    "test = pd.read_csv(DATA_TEST_PATH)\n",
    "\n",
    "# combine train and test\n",
    "#all_data = train.append(test)\n",
    "\n",
    "# Found at https://www.kaggle.com/marcellonegro/prudential-life-insurance-assessment/xgb-offset0501/run/137585/code\n",
    "# create any new variables    \n",
    "#all_data['Product_Info_2_char'] = all_data.Product_Info_2.str[0]\n",
    "#all_data['Product_Info_2_num'] = all_data.Product_Info_2.str[1]\n",
    "\n",
    "# factorize categorical variables\n",
    "#all_data['Product_Info_2'] = pd.factorize(all_data['Product_Info_2'])[0]\n",
    "#all_data['Product_Info_2_char'] = pd.factorize(all_data['Product_Info_2_char'])[0]\n",
    "#all_data['Product_Info_2_num'] = pd.factorize(all_data['Product_Info_2_num'])[0]\n",
    "\n",
    "#all_data['BMI_Age'] = all_data['BMI'] * all_data['Ins_Age']\n",
    "\n",
    "#med_keyword_columns = all_data.columns[all_data.columns.str.startswith('Medical_Keyword_')]\n",
    "#all_data['Med_Keywords_Count'] = all_data[med_keyword_columns].sum(axis=1)\n",
    "\n",
    "print('Eliminate missing values')    \n",
    "# Use -1 for any others\n",
    "all_data.fillna(-1, inplace=True)\n",
    "\n",
    "# fix the dtype on the label column\n",
    "all_data['Response'] = all_data['Response'].astype(int)\n",
    "\n",
    "# split train and test\n",
    "train = all_data[all_data['Response']>0].copy()\n",
    "test = all_data[all_data['Response']<1].copy()\n",
    "\n",
    "# convert data to xgb data structure\n",
    "xgtrain = xgb.DMatrix(train.drop(columns_to_drop, axis=1), train['Response'].values)\n",
    "xgtest = xgb.DMatrix(test.drop(columns_to_drop, axis=1), label=test['Response'].values)    \n",
    "\n",
    "# get the parameters for xgboost\n",
    "plst = get_params()\n",
    "print(plst)      \n",
    "\n",
    "# train model\n",
    "model = xgb.train(plst, xgtrain, xgb_num_rounds, learning_rates=eta_list) \n",
    "\n",
    "# get preds\n",
    "train_preds = model.predict(xgtrain, ntree_limit=model.best_iteration)\n",
    "print('Train score is:', eval_wrapper(train_preds, train['Response'])) \n",
    "test_preds = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "train_preds = np.clip(train_preds, -0.99, 8.99)\n",
    "test_preds = np.clip(test_preds, -0.99, 8.99)\n",
    "\n",
    "\n",
    "# train offsets \n",
    "offsets = np.array([-5.1, -0.9, 0.1, -0.9, 1.0, 0.0, 0.0, 1.0])\n",
    "\n",
    "data = np.vstack((train_preds, train_preds, train['Response']))\n",
    "for j in range(num_classes):\n",
    "    data[1, data[0].astype(int)==j] = data[0, data[0].astype(int)==j] + offsets[j] \n",
    "    \n",
    "for j in [6,4,5,3]:\n",
    "    train_offset = lambda x: -apply_offset(data, x, j)\n",
    "    offsets[j] = fmin_powell(train_offset, offsets[j])  \n",
    "    \n",
    "# apply offsets to test\n",
    "data = np.vstack((test_preds, test_preds, test['Response']))\n",
    "for j in range(num_classes):\n",
    "    data[1, data[0].astype(int)==j] = data[0, data[0].astype(int)==j] + offsets[j] \n",
    "final_test_preds = np.round(np.clip(data[1], 1, 8)).astype(int)\n",
    "\n",
    "preds_out = pd.DataFrame({\"Id\": test['Id'].values, \"Response\": final_test_preds})\n",
    "preds_out = preds_out.set_index('Id')\n",
    "preds_out.to_csv('xgb_offset_submission_2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the data using pandas\n",
      "Eliminate missing values\n",
      "[('colsample_bytree', 0.531945842645514), ('silent', 1), ('max_delta_step', 2.080110230507324), ('min_child_weight', 87.44974973285693), ('subsample', 0.16906773086480992), ('eta', 0.027414206118598777), ('objective', 'reg:linear'), ('max_depth', 9), ('gamma', 0.8855332076550115)]\n",
      "('Train score is:', 0.6228827530695871)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.636977\n",
      "         Iterations: 2\n",
      "         Function evaluations: 30\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.668411\n",
      "         Iterations: 2\n",
      "         Function evaluations: 48\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.668755\n",
      "         Iterations: 2\n",
      "         Function evaluations: 34\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.669661\n",
      "         Iterations: 2\n",
      "         Function evaluations: 55\n",
      "0.637890174596\n",
      "CPU times: user 1min 7s, sys: 320 ms, total: 1min 7s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import xgboost as xgb\n",
    "from scipy.optimize import fmin_powell\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "\n",
    "def eval_wrapper(yhat, y):  \n",
    "    y = np.array(y)\n",
    "    y = y.astype(int)\n",
    "    yhat = np.array(yhat)\n",
    "    yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)   \n",
    "    return quadratic_weighted_kappa(yhat, y)\n",
    "    \n",
    "def get_params():\n",
    "    \n",
    "    params = {}\n",
    "    params[\"objective\"] = \"reg:linear\"     \n",
    "    params[\"eta\"] = 0.027414206118598777\n",
    "    params[\"gamma\"] = 0.88553320765501153\n",
    "    params[\"max_delta_step\"] = 2.0801102305073238\n",
    "    params[\"min_child_weight\"] = 87.4497497328569295\n",
    "    params[\"subsample\"] = 0.16906773086480992\n",
    "    params[\"colsample_bytree\"] = 0.53194584264551403\n",
    "    params[\"silent\"] = 1\n",
    "    params[\"max_depth\"] = 9\n",
    "    #params[\"n_estimators\"] = 829.49940166736644\n",
    "    plst = list(params.items())\n",
    "\n",
    "    return plst\n",
    "    \n",
    "def apply_offset(data, bin_offset, sv, scorer=eval_wrapper):\n",
    "    # data has the format of pred=0, offset_pred=1, labels=2 in the first dim\n",
    "    data[1, data[0].astype(int)==sv] = data[0, data[0].astype(int)==sv] + bin_offset\n",
    "    score = scorer(data[1], data[2])\n",
    "    return score\n",
    "\n",
    "# global variables\n",
    "#columns_to_drop = ['Id', 'Response', 'Medical_History_10','Medical_History_24']\n",
    "columns_to_drop = ['Id', 'Response']\n",
    "xgb_num_rounds = 500\n",
    "num_classes = 8\n",
    "eta_list = [0.027414206118598777] * 250 \n",
    "eta_list = eta_list + [0.01] * 150\n",
    "eta_list = eta_list + [0.005] * 50\n",
    "\n",
    "print(\"Load the data using pandas\")\n",
    "\n",
    "DATA_TRAIN_PATH = '/Users/patrickkennedy/Desktop/Project DATA/train.csv'\n",
    "DATA_TEST_PATH = '/Users/patrickkennedy/Desktop/Project DATA/test.csv'\n",
    "\n",
    "train = pd.read_csv(DATA_TRAIN_PATH)\n",
    "#test = pd.read_csv(DATA_TEST_PATH)\n",
    "\n",
    "# combine train and test\n",
    "#all_data = train.append(test)\n",
    "\n",
    "# Found at https://www.kaggle.com/marcellonegro/prudential-life-insurance-assessment/xgb-offset0501/run/137585/code\n",
    "# create any new variables    \n",
    "train['Product_Info_2_char'] = train.Product_Info_2.str[0]\n",
    "train['Product_Info_2_num'] = train.Product_Info_2.str[1]\n",
    "\n",
    "# factorize categorical variables\n",
    "train['Product_Info_2'] = pd.factorize(train['Product_Info_2'])[0]\n",
    "train['Product_Info_2_char'] = pd.factorize(train['Product_Info_2_char'])[0]\n",
    "train['Product_Info_2_num'] = pd.factorize(train['Product_Info_2_num'])[0]\n",
    "\n",
    "train['BMI_Age'] = train['BMI'] * train['Ins_Age']\n",
    "\n",
    "med_keyword_columns = train.columns[train.columns.str.startswith('Medical_Keyword_')]\n",
    "train['Med_Keywords_Count'] = train[med_keyword_columns].sum(axis=1)\n",
    "\n",
    "print('Eliminate missing values')    \n",
    "# Use -1 for any others\n",
    "train.fillna(-1000, inplace=True)\n",
    "\n",
    "# fix the dtype on the label column\n",
    "train['Response'] = train['Response'].astype(int)\n",
    "\n",
    "# split train and test\n",
    "#train = all_data[all_data['Response']>0].copy()\n",
    "#test = all_data[all_data['Response']<1].copy()\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.drop(columns_to_drop, axis=1), train['Response'].values)\n",
    "xgtrain = xgb.DMatrix(X_train, y_train)\n",
    "xgtest = xgb.DMatrix(X_test, label=y_test)\n",
    "  \n",
    "\n",
    "# get the parameters for xgboost\n",
    "plst = get_params()\n",
    "print(plst)      \n",
    "\n",
    "# train model\n",
    "model = xgb.train(plst, xgtrain, xgb_num_rounds, learning_rates=eta_list) \n",
    "\n",
    "# get preds\n",
    "train_preds = model.predict(xgtrain, ntree_limit=model.best_iteration)\n",
    "print('Train score is:', eval_wrapper(train_preds, y_train)) \n",
    "test_preds = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "train_preds = np.clip(train_preds, -0.99, 8.99)\n",
    "test_preds = np.clip(test_preds, -0.99, 8.99)\n",
    "\n",
    "\n",
    "# train offsets \n",
    "offsets = np.array([-5.1, -0.9, 0.1, -0.9, 1.0, 0.0, 0.0, 1.0])\n",
    "\n",
    "data = np.vstack((train_preds, train_preds, y_train))\n",
    "for j in range(num_classes):\n",
    "    data[1, data[0].astype(int)==j] = data[0, data[0].astype(int)==j] + offsets[j] \n",
    "    \n",
    "for j in [6,4,5,3]:\n",
    "    train_offset = lambda x: -apply_offset(data, x, j)\n",
    "    offsets[j] = fmin_powell(train_offset, offsets[j])  \n",
    "    \n",
    "# apply offsets to test\n",
    "data = np.vstack((test_preds, test_preds, y_test))\n",
    "for j in range(num_classes):\n",
    "    data[1, data[0].astype(int)==j] = data[0, data[0].astype(int)==j] + offsets[j] \n",
    "final_test_preds = np.round(np.clip(data[1], 1, 8)).astype(int)\n",
    "\n",
    "\n",
    "print quadratic_weighted_kappa(final_test_preds, y_test)\n",
    "#preds_out = pd.DataFrame({\"Id\": test['Id'].values, \"Response\": final_test_preds})\n",
    "#preds_out = preds_out.set_index('Id')\n",
    "#preds_out.to_csv('xgb_offset_submission_2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
