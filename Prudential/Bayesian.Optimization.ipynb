{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, run the code to load data and call the scorer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#seems like the message is to use a parameter optimization method and then use stacking/blending\n",
    "\n",
    "\n",
    "%timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn import grid_search\n",
    "from scipy.optimize import fmin_powell\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "from sknn.mlp import Classifier, Layer, Regressor\n",
    "\n",
    "\n",
    "\n",
    "DATA_TRAIN_PATH = '/Users/patrickkennedy/Desktop/Project DATA/train.csv'\n",
    "DATA_TEST_PATH = '/Users/patrickkennedy/Desktop/Project DATA/test.csv'\n",
    "\n",
    "def eval_wrapper(yhat, y):  \n",
    "    y = np.array(y)\n",
    "    y = y.astype(int)\n",
    "    yhat = np.array(yhat)\n",
    "    yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)   \n",
    "    return quadratic_weighted_kappa(yhat, y)\n",
    "\n",
    "\n",
    "def load_data(path_train = DATA_TRAIN_PATH, path_test = DATA_TEST_PATH):\n",
    "    columns_to_drop = ['Id', 'Response']\n",
    "    num_classes = 8\n",
    "\n",
    "    train = pd.read_csv(path_train)\n",
    "    test = pd.read_csv(path_test)\n",
    "\n",
    "    # combine train and test\n",
    "    all_data = train.append(test)\n",
    "    all_data.fillna(-1, inplace=True)\n",
    "    all_data['Product_Info_2'] = pd.factorize(all_data['Product_Info_2'])[0]\n",
    "    all_data['Response'] = all_data['Response'].astype(int)\n",
    "\n",
    "    train = all_data[all_data['Response']>0].copy()\n",
    "    test = all_data[all_data['Response']<1].copy()\n",
    "    train_labels = train[\"Response\"]\n",
    "    \n",
    "    #train_labels = train_labels.reshape(-1,1) #need to reshape b/c one dim array\n",
    "    \n",
    "    train_ids = train[\"Id\"].values\n",
    "    test_ids = test[\"Id\"].values\n",
    "\n",
    "    return  train.drop(columns_to_drop, axis=1), \\\n",
    "            train_labels, \\\n",
    "            test.drop(columns_to_drop, axis=1),\\\n",
    "            train_ids, \\\n",
    "            test_ids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, define the functions that test different algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xgblinearcv(max_depth, learning_rate, n_estimators, gamma, min_child_weight, max_delta_step, subsample,\n",
    "              colsample_bytree, silent =True, nthread = -1, seed = 1234):\n",
    "    \n",
    "    #XGBClassifier only gets up to .47\n",
    "    return cross_val_score(XGBRegressor(max_depth = int(max_depth), learning_rate = learning_rate,\n",
    "                                         n_estimators = int(n_estimators), silent = silent,\n",
    "                                         nthread = nthread, gamma = gamma, min_child_weight = min_child_weight,\n",
    "                                         max_delta_step = max_delta_step, subsample = subsample,\n",
    "                                         colsample_bytree = colsample_bytree, seed = seed,\n",
    "                                         objective = 'reg:linear'),\n",
    "                           train,\n",
    "                           labels,\n",
    "                           'mean_squared_error',\n",
    "                           n_jobs=-1,\n",
    "                           cv=3).mean()\n",
    "\n",
    "\n",
    "\n",
    "def RFcv(n_estimators, max_depth, min_samples_split, min_samples_leaf, \n",
    "         min_weight_fraction_leaf, max_leaf_nodes=None, bootstrap=True, \n",
    "         oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None):\n",
    "    \n",
    "    return cross_val_score(RandomForestClassifier(n_estimators = int(n_estimators), criterion='gini',\n",
    "                                                 max_depth=int(max_depth), min_samples_split=int(min_samples_split),\n",
    "                                                 min_samples_leaf=int(min_samples_leaf),\n",
    "                                                 min_weight_fraction_leaf=min_weight_fraction_leaf, max_features='auto',\n",
    "                                                 max_leaf_nodes=max_leaf_nodes, bootstrap=bootstrap,\n",
    "                                                 oob_score=oob_score, n_jobs=n_jobs, random_state=None, verbose=0,\n",
    "                                                 warm_start=False, class_weight=None),\n",
    "                           train,\n",
    "                           labels,\n",
    "                           'mean_squared_error',\n",
    "                           n_jobs=-1,\n",
    "                           cv=3).mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def kNNcv(n_neighbors):\n",
    "    return cross_val_score(KNeighborsClassifier(n_neighbors=int(n_neighbors)),\n",
    "                           train,\n",
    "                           labels,\n",
    "                           'mean_squared_error',\n",
    "                           n_jobs=-1,\n",
    "                           cv=3).mean()\n",
    "\n",
    "\n",
    "\n",
    "def kMeanscv(n_clusters):\n",
    "    return cross_val_score(MiniBatchKMeans(n_clusters = int(n_clusters)),\n",
    "                          train,\n",
    "                          labels,\n",
    "                          'mean_squared_error',\n",
    "                          n_jobs=-1,\n",
    "                          cv=3).mean()\n",
    "\n",
    "\n",
    "\n",
    "def xgbpoissoncv(max_depth, learning_rate, n_estimators, gamma, min_child_weight, max_delta_step, subsample,\n",
    "              colsample_bytree, silent =True, nthread = -1, seed = 1234):\n",
    "    \n",
    "    #XGBClassifier only gets up to .47\n",
    "    return cross_val_score(XGBRegressor(max_depth = int(max_depth), learning_rate = learning_rate,\n",
    "                                         n_estimators = int(n_estimators), silent = silent,\n",
    "                                         nthread = nthread, gamma = gamma, min_child_weight = min_child_weight,\n",
    "                                         max_delta_step = max_delta_step, subsample = subsample,\n",
    "                                         colsample_bytree = colsample_bytree, seed = seed,\n",
    "                                         objective = 'count:poisson'),\n",
    "                           train,\n",
    "                           labels,\n",
    "                           'mean_squared_error',\n",
    "                           n_jobs=-1,\n",
    "                           cv=3).mean()\n",
    "\n",
    "def xgbmultisoftmaxcv(max_depth, learning_rate, n_estimators, gamma, min_child_weight, max_delta_step, subsample,\n",
    "              colsample_bytree, silent =True, nthread = -1, seed = 1234):\n",
    "    \n",
    "    #XGBClassifier only gets up to .47\n",
    "    return cross_val_score(XGBClassifier(max_depth = int(max_depth), learning_rate = learning_rate,\n",
    "                                         n_estimators = int(n_estimators), silent = silent,\n",
    "                                         nthread = nthread, gamma = gamma, min_child_weight = min_child_weight,\n",
    "                                         max_delta_step = max_delta_step, subsample = subsample,\n",
    "                                         colsample_bytree = colsample_bytree, seed = seed,\n",
    "                                         objective = 'reg:linear'),  #will be reset to softmax\n",
    "                           train,\n",
    "                           labels,\n",
    "                           'mean_squared_error',\n",
    "                           n_jobs=-1,\n",
    "                           cv=3).mean()\n",
    "\n",
    "\n",
    "\n",
    "def NNcv(layers, learning_rate, n_iter):\n",
    "    return cross_val_score(Classifier(layers=[Layer(\"Linear\", units=100),\n",
    "                                              Layer(\"Softmax\")],\n",
    "                                     learning_rate=learning_rate,\n",
    "                                     n_iter=n_iter),\n",
    "                           train,\n",
    "                           labels,\n",
    "                           'mean_squared_error',\n",
    "                           n_jobs=-1,\n",
    "                           cv=3).mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the variables for the BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgblinearBO = BayesianOptimization(xgblinearcv,\n",
    "                                 {'max_depth': (5, 12),\n",
    "                                  'learning_rate': (0.001, 0.5),\n",
    "                                  'n_estimators': (50, 1000),\n",
    "                                  'gamma': (1., 0.01),\n",
    "                                  'min_child_weight': (1, 100),\n",
    "                                  'max_delta_step': (0, 10),\n",
    "                                  'subsample': (0.01, 0.9),\n",
    "                                  'colsample_bytree' :(0.5, 0.99)\n",
    "                                 })\n",
    "\n",
    "RFBO = BayesianOptimization(RFcv,\n",
    "                           {'n_estimators' : (50,1000), \n",
    "                            'max_depth' : (5, 12), \n",
    "                            'min_samples_split' : (2, 10),             \n",
    "                            'min_samples_leaf' : (1, 10),      \n",
    "                            'min_weight_fraction_leaf' : (0.0, 0.5),        \n",
    "                           })\n",
    "\n",
    "\n",
    "\n",
    "kNNBO = BayesianOptimization(kNNcv,\n",
    "                            {'n_neighbors' : (3, 1001)\n",
    "                            })\n",
    "\n",
    "\n",
    "kMeansBO = BayesianOptimization(kMeanscv,\n",
    "                               {'n_clusters' : (3,1001)\n",
    "                               })\n",
    "\n",
    "\n",
    "\n",
    "xgbpoissonBO = BayesianOptimization(xgbpoissoncv,\n",
    "                                 {'max_depth': (5, 12),\n",
    "                                  'learning_rate': (0.001, 0.5),\n",
    "                                  'n_estimators': (50, 1000),\n",
    "                                  'gamma': (1., 0.01),\n",
    "                                  'min_child_weight': (1, 100),\n",
    "                                  'max_delta_step': (0, 10),\n",
    "                                  'subsample': (0.01, 0.9),\n",
    "                                  'colsample_bytree' :(0.5, 0.99)\n",
    "                                 })\n",
    "\n",
    "xgbmultisoftmaxBO = BayesianOptimization(xgbmultisoftmaxcv,\n",
    "                                 {'max_depth': (5, 12),\n",
    "                                  'learning_rate': (0.001, 0.5),\n",
    "                                  'n_estimators': (50, 1000),\n",
    "                                  'gamma': (1., 0.01),\n",
    "                                  'min_child_weight': (1, 100),\n",
    "                                  'max_delta_step': (0, 10),\n",
    "                                  'subsample': (0.01, 0.9),\n",
    "                                  'colsample_bytree' :(0.5, 0.99)\n",
    "                                 })\n",
    "\n",
    "\n",
    "NNBO = BayesianOptimization(NNcv,\n",
    "                           {'learning_rate': (0.0001, 0.5),\n",
    "                            'hidden0__units': (2, 200),\n",
    "                            'hidden0__type': [\"Linear\", \"Rectifier\", \"Sigmoid\", \"Tanh\"],\n",
    "                            'n_iter' : (5, 1000)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last, run the optimization procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: Linear",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fb722ee4345f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'if __name__ == \"__main__\":\\n    # Load data set and target values\\n    train, labels, test, _, _ = load_data()\\n    scorer = make_scorer(cohen_kappa_score)\\n    \\n    \\n    \\n    NNBO.maximize()\\n    print(\\'-\\'*53)\\n    print(\\'Final Results\\')\\n    print(\\'Neural Net (4 layer: lin, rec, lin, softmax): %f\\' % NNBO.res[\\'max\\'][\\'max_val\\'])\\n    \\n    \\n    \\n    \\n    #xgbpoissonBO.maximize()\\n    #print(\\'-\\'*53)\\n    #print(\\'Final Results\\')\\n    #print(\\'XGB-Poisson: %f\\' % xgbpoissonBO.res[\\'max\\'][\\'max_val\\'])\\n    \\n    #xgbmultisoftmaxBO.maximize()\\n    #print(\\'-\\'*53)\\n    #print(\\'Final Results\\')\\n    #print(\\'XGB-multisoftmax: %f\\' % xgbmultisoftmaxBO.res[\\'max\\'][\\'max_val\\'])\\n    \\n    \\n    \\n    \\n    \\n    \\n    #kMeansBO.maximize()\\n    #print(\\'-\\'*53)\\n    #print(\\'Final Results\\')\\n    #print(\\'kMeans: %f\\' % kMeansBO.res[\\'max\\'][\\'max_val\\'])\\n    #print(\\'-\\'*53)\\n    #print(\\'-\\'*53) \\n    \\n    #kNNBO.maximize()\\n    #print(\\'-\\'*53)\\n    #print(\\'Final Results\\')\\n    #print(\\'kNN: %f\\' % kNNBO.res[\\'max\\'][\\'max_val\\'])\\n    #print(\\'-\\'*53)\\n    #print(\\'-\\'*53) \\n    \\n    #RFBO.maximize()\\n    #print(\\'-\\'*53)\\n    #print(\\'Final Results\\')\\n    #print(\\'Random Forest: %f\\' % RFBO.res[\\'max\\'][\\'max_val\\'])\\n    #print(\\'-\\'*53)\\n    #print(\\'-\\'*53)    \\n    \\n    #xgblinearBO.maximize()\\n    #print(\\'-\\'*53)\\n    #print(\\'Final Results\\')\\n    #print(\\'XGBlinear: %f\\' % xgblinearBO.res[\\'max\\'][\\'max_val\\'])\\n    \\n\\n\\n\\n#try again with data excepting response and id variables...\\n#seems like XGBRegressor may be a better fit, try that after XGBClassifier is done\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2291\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2292\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2293\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2294\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/bayes_opt/bayesian_optimization.pyc\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, restarts, n_iter, acq, **gp_params)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# Initialize x, y and find current ymax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mymax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/bayes_opt/bayesian_optimization.pyc\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, init_points)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Generate random points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Concatenate new random points to possible existing points from self.explore method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.uniform (numpy/random/mtrand/mtrand.c:11886)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: Linear"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data set and target values\n",
    "    train, labels, test, _, _ = load_data()\n",
    "    scorer = make_scorer(cohen_kappa_score)\n",
    "    \n",
    "    \n",
    "    \n",
    "    NNBO.maximize()\n",
    "    print('-'*53)\n",
    "    print('Final Results')\n",
    "    print('Neural Net (4 layer: lin, rec, lin, softmax): %f' % NNBO.res['max']['max_val'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #xgbpoissonBO.maximize()\n",
    "    #print('-'*53)\n",
    "    #print('Final Results')\n",
    "    #print('XGB-Poisson: %f' % xgbpoissonBO.res['max']['max_val'])\n",
    "    \n",
    "    #xgbmultisoftmaxBO.maximize()\n",
    "    #print('-'*53)\n",
    "    #print('Final Results')\n",
    "    #print('XGB-multisoftmax: %f' % xgbmultisoftmaxBO.res['max']['max_val'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #kMeansBO.maximize()\n",
    "    #print('-'*53)\n",
    "    #print('Final Results')\n",
    "    #print('kMeans: %f' % kMeansBO.res['max']['max_val'])\n",
    "    #print('-'*53)\n",
    "    #print('-'*53) \n",
    "    \n",
    "    #kNNBO.maximize()\n",
    "    #print('-'*53)\n",
    "    #print('Final Results')\n",
    "    #print('kNN: %f' % kNNBO.res['max']['max_val'])\n",
    "    #print('-'*53)\n",
    "    #print('-'*53) \n",
    "    \n",
    "    #RFBO.maximize()\n",
    "    #print('-'*53)\n",
    "    #print('Final Results')\n",
    "    #print('Random Forest: %f' % RFBO.res['max']['max_val'])\n",
    "    #print('-'*53)\n",
    "    #print('-'*53)    \n",
    "    \n",
    "    #xgblinearBO.maximize()\n",
    "    #print('-'*53)\n",
    "    #print('Final Results')\n",
    "    #print('XGBlinear: %f' % xgblinearBO.res['max']['max_val'])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#try again with data excepting response and id variables...\n",
    "#seems like XGBRegressor may be a better fit, try that after XGBClassifier is done\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RESULTS:\n",
    "#1) Random Forest\n",
    "\n",
    "#Optimization finished with maximum: -5.606285, at position: \n",
    "#{'n_estimators': 1000.0, 'min_samples_split': 2.0, 'min_weight_fraction_leaf': 0.0, \n",
    "#'max_depth': 12.0, 'min_samples_leaf': 1.0}.\n",
    "#Time taken: 7 minutes and 59.378842 seconds.\n",
    "#-----------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#2) kNN\n",
    "\n",
    "#Optimization finished with maximum: -9.324511, at position: {'n_neighbors': 10.490161708465756}.\n",
    "#Time taken: 18 minutes and 37.048673 seconds.\n",
    "#-----------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#3) minibatch k means\n",
    "#Optimization finished with maximum: -23.923574, at position: {'n_clusters': 13.378058163512708}.\n",
    "#Time taken: 1 minutes and 4.024164 seconds.\n",
    "#-----------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#4) XGBClassifier ('multi:softmax')\n",
    "#Optimization finished with maximum: -4.879299, at position: \n",
    "#{'colsample_bytree': 0.91539988110733117, 'learning_rate': 0.061404496828089954, \n",
    "#'max_delta_step': 7.4175422582480843, 'min_child_weight': 19.76322700303788, \n",
    "#'n_estimators': 358.89453967398725, 'subsample': 0.89312153200130462, \n",
    "#'max_depth': 5.1217667820189829, 'gamma': 0.041040228052564909}.\n",
    "#Time taken: 451 minutes and 16.516589 seconds.\n",
    "#-----------------------------------------------------\n",
    "\n",
    "\n",
    "#5) XGBRegressor 'count:poisson'\n",
    "#Optimization finished with maximum: -3.366464, at position: \n",
    "#{'colsample_bytree': 0.51169359165916362, 'learning_rate': 0.36748550147548548, \n",
    "#'max_delta_step': 3.7759341131094057, 'min_child_weight': 6.61613391710471, \n",
    "#'n_estimators': 623.62927525119301, 'subsample': 0.85582404890610575, \n",
    "#'max_depth': 11.777985571852648, 'gamma': 0.13481639738974682}.\n",
    "#Time taken: 72 minutes and 2.069944 seconds.\n",
    "#-----------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#6) XGBRegressor 'reg:linear'\n",
    "#Optimization finished with maximum: -3.390418, at position: \n",
    "#{'colsample_bytree': 0.53194584264551403, 'learning_rate': 0.027414206118598777, \n",
    "#'max_delta_step': 2.0801102305073238, 'min_child_weight': 7.4497497328569295, \n",
    "#'n_estimators': 829.49940166736644, 'subsample': 0.16906773086480992, \n",
    "#'max_depth': 6.1294423576531134, 'gamma': 0.88553320765501153}.\n",
    "#Time taken: 67 minutes and 16.470244 seconds.\n",
    "#-----------------------------------------------------\n",
    "\n",
    "\n",
    "#7)  log / squares of each\n",
    "\n",
    "#can i do feature importance? and then square those? or something...\n",
    "\n",
    "#weight each prediction before blending... or double up some of the columns for xgboost?\n",
    "#then given the blended predictions... find the best params for the new data set against training labels\n",
    "#then put everything into the big model with the specified params and see the result\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For tomorrow: run through all the BO scripts to get best models -- still need some time\n",
    "#Go through Udacity R course (1 section) -- check\n",
    "#Install Linux on USB drive -- check! :-)\n",
    "#Bonus: get auto-sklearn up and running\n",
    "#Bonus bonus: get all see dave work pages up, loaded and submitted for another book at createspace\n",
    "\n",
    "\n",
    "\n",
    "#build a script that runs through my algos with BO and gets the best params, then runs through those models\n",
    "#with my existing classifier amalgam with those best params and see what pops out the other end\n",
    "#note if it is any better than before?\n",
    "#note next steps (i.e. correlated models, whatever else i need...)\n",
    "#then use autosklearn (after installing linux) and see how well that places me in the competition\n",
    "#next next steps... genetic algos? neural networks? what else? building my own auto learner? how to optimize?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#can i use BO with a neural network? -- Yes but what about manipulating the units component? or layers?\n",
    "#also get Linux customized with installations (Anacondas, scikit, etc... also autolearn!)\n",
    "\n",
    "#using Theano or Lasagna (because it has layers)...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train and valid ...\n",
      "\n",
      "\n",
      "Training with params : \n",
      "{'hidden0__units': (2, 200), 'learning_rate': (0.0001, 0.5), 'hidden0__type': ('Linear', 'Rectifier', 'Sigmoid', 'Tanh'), 'n_iter': (5, 1000)}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Specify each layer as an instance of a `sknn.mlp.Layer` object.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f93f2e75c86f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-f93f2e75c86f>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(trials)\u001b[0m\n\u001b[1;32m     89\u001b[0m              }\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rseed)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFMinIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    112\u001b[0m             pyll_rval = pyll.rec_eval(self.expr, memo=memo,\n\u001b[1;32m    113\u001b[0m                     print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-f93f2e75c86f>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m#predictions = model.predict(dvalid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/patrickkennedy/anaconda/lib/python2.7/site-packages/sknn/nn.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, warning, weights, random_state, learning_rule, learning_rate, learning_momentum, regularize, weight_decay, dropout_rate, batch_size, n_iter, n_stable, f_stable, valid_set, valid_size, loss_type, callback, debug, verbose, **params)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0;34m\"Specify each layer as an instance of a `sknn.mlp.Layer` object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# Layer names are optional, if not specified then generate one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Specify each layer as an instance of a `sknn.mlp.Layer` object."
     ]
    }
   ],
   "source": [
    "#running the NN script in hyperopt to handle \n",
    "\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "from sknn.mlp import Classifier, Layer, Regressor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DATA_TRAIN_PATH = '/Users/patrickkennedy/Desktop/Project DATA/train.csv'\n",
    "DATA_TEST_PATH = '/Users/patrickkennedy/Desktop/Project DATA/test.csv'\n",
    "\n",
    "\n",
    "def eval_wrapper(yhat, y):  \n",
    "    y = np.array(y)\n",
    "    y = y.astype(int)\n",
    "    yhat = np.array(yhat)\n",
    "    yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)   \n",
    "    return quadratic_weighted_kappa(yhat, y)\n",
    "\n",
    "def load_data(path_train = DATA_TRAIN_PATH, path_test = DATA_TEST_PATH):\n",
    "    columns_to_drop = ['Id', 'Response']\n",
    "    num_classes = 8\n",
    "\n",
    "    train = pd.read_csv(path_train)\n",
    "    test = pd.read_csv(path_test)\n",
    "\n",
    "    # combine train and test\n",
    "    all_data = train.append(test)\n",
    "    all_data.fillna(-1, inplace=True)\n",
    "    all_data['Product_Info_2'] = pd.factorize(all_data['Product_Info_2'])[0]\n",
    "    all_data['Response'] = all_data['Response'].astype(int)\n",
    "\n",
    "    train = all_data[all_data['Response']>0].copy()\n",
    "    test = all_data[all_data['Response']<1].copy()\n",
    "    train_labels = train[\"Response\"]\n",
    "    \n",
    "    #train_labels = train_labels.reshape(-1,1) #need to reshape b/c one dim array\n",
    "    \n",
    "    train_ids = train[\"Id\"].values\n",
    "    test_ids = test[\"Id\"].values\n",
    "\n",
    "    return  train.drop(columns_to_drop, axis=1), \\\n",
    "            train_labels, \\\n",
    "            test.drop(columns_to_drop, axis=1),\\\n",
    "            train_ids, \\\n",
    "            test_ids\n",
    "            \n",
    "\n",
    "def write_submission(preds, output):\n",
    "    sample = pd.read_csv('../data/sampleSubmission.csv')\n",
    "    preds = pd.DataFrame(\n",
    "        preds, index=sample.id.values, columns=sample.columns[1:])\n",
    "    preds.to_csv(output, index_label='id')\n",
    "\n",
    "\n",
    "def score(params):\n",
    "    #change this around and try it with cross_val_score? so that i can have some cross validation?\n",
    "    print \"Training with params : \"\n",
    "    print params\n",
    "    #num_round = int(params['n_estimators'])\n",
    "    #del params['n_estimators']\n",
    "    #dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    #dvalid = xgb.DMatrix(X_test, label=y_test)\n",
    "    # watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "    #model = xgb.train(params, dtrain, num_round)\n",
    "    #predictions = model.predict(dvalid)\n",
    "    \n",
    "    model = Classifier(params)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    score = -eval_wrapper(predictions, y_test)\n",
    "    print \"\\tScore {0}\\n\\n\".format(score)\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def optimize(trials):\n",
    "    space = {\n",
    "            'learning_rate': (0.0001, 0.5),\n",
    "            'hidden0__units': (2, 200),\n",
    "            'hidden0__type': [\"Linear\", \"Rectifier\", \"Sigmoid\", \"Tanh\"],\n",
    "            'n_iter' : (5, 1000)\n",
    "             }\n",
    "\n",
    "    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=250)\n",
    "\n",
    "    print best\n",
    "\n",
    "\n",
    "X, y, _, _, _ = load_data()\n",
    "print \"Splitting data into train and valid ...\\n\\n\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "#Trials object where the history of search will be stored\n",
    "trials = Trials()\n",
    "\n",
    "optimize(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train and valid ...\n",
      "\n",
      "\n",
      "RandomizedSearchCV took 256.84 seconds for 3 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.475 (std: 0.017)\n",
      "Parameters: {'hidden1__type': 'Sigmoid', 'learning_rate': 0.05, 'hidden0__type': 'Rectifier', 'hidden1__units': 12, 'hidden0__units': 8}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.418 (std: 0.022)\n",
      "Parameters: {'hidden1__type': 'Tanh', 'learning_rate': 0.1, 'hidden0__type': 'Tanh', 'hidden1__units': 4, 'hidden0__units': 4}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.328 (std: 0.000)\n",
      "Parameters: {'hidden1__type': 'Sigmoid', 'learning_rate': 0.3, 'hidden0__type': 'Rectifier', 'hidden1__units': 4, 'hidden0__units': 4}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#trying with randomsearchcv\n",
    "\n",
    "#using randomizedsearchCV takes much less time\n",
    "#try using MOE? how?\n",
    "\n",
    "#begin developing structure of the presentation and beef up on what the hell XGBoost, QWK, fmin_powell, and the rest are doing\n",
    "\n",
    "\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint\n",
    "from sknn.mlp import Classifier, Layer, Regressor\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "def load_data(path_train = DATA_TRAIN_PATH, path_test = DATA_TEST_PATH):\n",
    "    columns_to_drop = ['Id', 'Response']\n",
    "    num_classes = 8\n",
    "\n",
    "    train = pd.read_csv(path_train)\n",
    "    test = pd.read_csv(path_test)\n",
    "\n",
    "    # combine train and test\n",
    "    all_data = train.append(test)\n",
    "    all_data.fillna(-1, inplace=True)\n",
    "    all_data['Product_Info_2'] = pd.factorize(all_data['Product_Info_2'])[0]\n",
    "    all_data['Response'] = all_data['Response'].astype(int)\n",
    "\n",
    "    train = all_data[all_data['Response']>0].copy()\n",
    "    test = all_data[all_data['Response']<1].copy()\n",
    "    train_labels = train[\"Response\"]\n",
    "    \n",
    "    #train_labels = train_labels.reshape(-1,1) #need to reshape b/c one dim array\n",
    "    \n",
    "    train_ids = train[\"Id\"].values\n",
    "    test_ids = test[\"Id\"].values\n",
    "\n",
    "    return  train.drop(columns_to_drop, axis=1), \\\n",
    "            train_labels, \\\n",
    "            test.drop(columns_to_drop, axis=1),\\\n",
    "            train_ids, \\\n",
    "            test_ids\n",
    "            \n",
    "            \n",
    "\n",
    "#using something other than randint... we don't want to round numbers here (for some cases)\n",
    "\n",
    "param_dist = {'learning_rate': [0.3, 0.1, 0.05, ], #0.01, 0.005, 0.001, 0.0005, 0.0001],\n",
    "              'hidden0__units': [4, 8, 12], #, 25, 50, 100, 200],\n",
    "              'hidden0__type': [\"Rectifier\", \"Sigmoid\", \"Tanh\"],\n",
    "              'hidden1__units' : [4, 8, 12],\n",
    "              'hidden1__type': ['Rectifier', 'Sigmoid', 'Tanh']\n",
    "             }\n",
    "\n",
    "    \n",
    "    \n",
    "clf = Classifier(layers=[Layer(\"Rectifier\", units=100),\n",
    "                         Layer(\"Rectifier\", units=100),\n",
    "                         Layer(\"Softmax\")],\n",
    "                 learning_rate=0.01,\n",
    "                 n_iter=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "X, y, _, _, _ = load_data()\n",
    "print \"Splitting data into train and valid ...\\n\\n\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "\n",
    "X_train_minmax = MinMaxScaler().fit_transform(X_train)\n",
    "\n",
    "        \n",
    "# run randomized search\n",
    "n_iter_search = 3\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, cv=3)\n",
    "\n",
    "start = time()\n",
    "\n",
    "\n",
    "random_search.fit(X_train_minmax, y_train)\n",
    "\n",
    "#clf.fit(X_train_minmax, y_train)\n",
    "\n",
    "\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.grid_scores_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
